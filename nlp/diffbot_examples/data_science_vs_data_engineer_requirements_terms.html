<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}

svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.grid line {
  stroke: lightgrey;
  stroke-opacity: 0.4;
  shape-rendering: crispEdges;
}

.grid path {
  stroke-width: 0;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 50px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}
/*
input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}*/

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}
/*
input:focus {
  background-color: lightyellow;
  outline: none;
}*/

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}

.topic_preview {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;
  font-weight: normal;
  text-decoration: none;
}


#d3-div-1-categoryinfo {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;    

}


#d3-div-1-title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}

.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}

.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  color: darkblue;
}

.not_match {
    background-color: #F0F8FF;
}
    
.contexts {
  width: 45%;
  float: left;
}

.neut_display {
  display: none;
  float: left
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.obscured {
  /*font-size: 14px;
  font-weight: normal;
  color: dimgrey;
  font-family: Helvetica;*/
  text-align: center;
}

.small_label {
  font-size: 10px;
}

#d3-div-1-corpus-stats {
  text-align: center;
}

#d3-div-1-cat {
}

#d3-div-1-notcat {
}

#d3-div-1-neut {
}

#d3-div-1-neutcol {
  display: none;
}
/* Adapted from https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_autocomplete */

.autocomplete {
  position: relative;
  display: inline-block;
}

input {
  border: 1px solid transparent;
  background-color: #f1f1f1;
  padding: 10px;
  font-size: 16px;
}

input[type=text] {
  background-color: #f1f1f1;
  width: 100%;
}

input[type=submit] {
  background-color: DodgerBlue;
  color: #fff;
  cursor: pointer;
}

.autocomplete-items {
  position: absolute;
  border: 2px solid #d4d4d4;
  border-bottom: none;
  border-top: none;
  z-index: 99;
  /*position the autocomplete items to be the same width as the container:*/
  top: 100%;
  left: 0;
  right: 0;
}

.autocomplete-items div {
  padding: 10px;
  cursor: pointer;
  background-color: #fff;
  border-bottom: 2px solid #d4d4d4;
}

/*when hovering an item:*/
.autocomplete-items div:hover {
  background-color: #e9e9e9;
}

/*when navigating through the items using the arrow keys:*/
.autocomplete-active {
  background-color: DodgerBlue !important;
  color: #ffffff;
}
</style>

<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>

<!-- INSERT SEMIOTIC SQUARE -->
<!--<a onclick="maxFreq = Math.log(data.map(d => d.cat + d.ncat).reduce((a,b) => Math.max(a,b))); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, false); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, true)">View Score Plot</a>-->
<span id="d3-div-1-title-div"></span>
<div class="scattertext" id="d3-div-1" style="float: left"></div>
<div style="floag: left;">
    <div autocomplete="off">
        <div class="autocomplete">
            <input id="searchInput" type="text" placeholder="Search the chart">
        </div>
    </div>
</div>
<br/>
<div id="d3-div-1-corpus-stats"></div>
<div id="d3-div-1-overlapped-terms"></div>
<a name="d3-div-1-snippets"></a>
<a name="d3-div-1-snippetsalt"></a>
<div id="d3-div-1-termstats"></div>
<div id="d3-div-1-overlapped-terms-clicked"></div>
<div id="d3-div-1-categoryinfo" style="display: hidden"></div>
<div id="d3-div-2">
  <div class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-cathead"></div>
    <div class="snippet" id="d3-div-1-cat"></div>
  </div>
  <div id="d3-div-1-notcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-notcathead"></div>
    <div class="snippet" id="d3-div-1-notcat"></div>
  </div>
  <div id="d3-div-1-neutcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-neuthead"></div>
    <div class="snippet" id="d3-div-1-neut"></div>
  </div>
</div>
<script charset="utf-8">
    // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 1000,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.1,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null,
                     fullData = null,
                     showTopTerms = true,
                     showNeutral = false,
                     getTooltipContent = null,
                     xAxisValues = null,
                     yAxisValues = null,
                     colorFunc = null,
                     showAxes = true,
                     showExtra = false,
                     doCensorPoints = true,
                     centerLabelsOverPoints = false,
                     xAxisLabels = null,
                     yAxisLabels = null,
                     topic_model_preview_size = 10,
                     verticalLines = null,
                     horizontal_line_y_position = null,
                     vertical_line_x_position = null,
                     unifiedContexts = false,
                     showCategoryHeadings = true,
                     showCrossAxes = true,
                     divName = 'd3-div-1',
                     alternativeTermFunc = null,
                     includeAllContexts = false,
                     showAxesAndCrossHairs = false,
                     x_axis_values_format = '.3f',
                     y_axis_values_format = '.3f',
                     matchFullLine = false,
                     maxOverlapping = -1,
                     showCorpusStats = true,
                     sortDocLabelsByName = false,
                     alwaysJump = true,
                     highlightSelectedCategory = false,
                     showDiagonal = false,
                     useGlobalScale = false,
                     enableTermCategoryDescription = true,
                     getCustomTermHtml = null,
                     headerNames = null,
                     headerSortingAlgos = null,
                     ignoreCategories = false,
                     backgroundLabels = null,
                     labelPriorityColumn = null,
                     textColorColumn = undefined,
                     suppressTextColumn = undefined,
                     backgroundColor = undefined,
                     censorPointColumn = undefined,
                     rightOrderColumn = undefined,
                     subwordEncoding = null,
                     topTermsLength = 14,
                     topTermsLeftBuffer = 0
    ) {
        function formatTermForDisplay(term) {
            if (subwordEncoding === 'RoBERTa' && (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289))
                term = '_' + term.substr(1, term.length - 1);
            return term;
        }

        //var divName = 'd3-div-1';
        // Set the dimensions of the canvas / graph
        var padding = {top: 30, right: 20, bottom: 30, left: 50};
        if (!showAxes) {
            padding = {top: 30, right: 20, bottom: 30, left: 50};
        }
        var margin = padding,
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;
        fullData.data.forEach(function (x, i) {
            x.i = i
        });

        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        if (unifiedContexts) {
            document.querySelectorAll('#' + divName + '-' + 'notcol')
                .forEach(function (x) {
                    x.style.display = 'none'
                });
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '90%'
                });
        } else if (showNeutral) {
            if (showExtra) {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '25%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol', 'extracol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '25%'
                        });
                })

            } else {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '33%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '33%'
                        });
                })


            }
        } else {
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '45%'
                    //x.style.display = 'inline'
                    x.style.float = 'left'
                });

            ['notcol'].forEach(function (columnName) {
                document.querySelectorAll('#' + divName + '-' + columnName)
                    .forEach(function (x) {
                        //x.style.display = 'inline'
                        x.style.float = 'left'
                        x.style.width = '45%'
                    });
            })
        }

        var yAxis = null;
        var xAxis = null;

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }


        function bs(ar, x) {
            function bsa(s, e) {
                var mid = Math.floor((s + e) / 2);
                var midval = ar[mid];
                if (s == e) {
                    return s;
                }
                if (midval == x) {
                    return mid;
                } else if (midval < x) {
                    return bsa(mid + 1, e);
                } else {
                    return bsa(s, mid);
                }
            }

            return bsa(0, ar.length);
        }


        console.log("fullData");
        console.log(fullData);


        var sortedX = fullData.data.map(x => x).sort(function (a, b) {
            return a.x < b.x ? -1 : (a.x == b.x ? 0 : 1);
        }).map(function (x) {
            return x.x
        });

        var sortedOx = fullData.data.map(x => x).sort(function (a, b) {
            return a.ox < b.ox ? -1 : (a.ox == b.ox ? 0 : 1);
        }).map(function (x) {
            return x.ox
        });

        var sortedY = fullData.data.map(x => x).sort(function (a, b) {
            return a.y < b.y ? -1 : (a.y == b.y ? 0 : 1);
        }).map(function (x) {
            return x.y
        });

        var sortedOy = fullData.data.map(x => x).sort(function (a, b) {
            return a.oy < b.oy ? -1 : (a.oy == b.oy ? 0 : 1);
        }).map(function (x) {
            return x.oy
        });
        console.log(fullData.data[0])

        function labelWithZScore(axis, axisName, tickPoints, axis_values_format) {
            var myVals = axisName === 'x' ? sortedOx : sortedOy;
            var myPlotedVals = axisName === 'x' ? sortedX : sortedY;
            var ticks = tickPoints.map(function (x) {
                return myPlotedVals[bs(myVals, x)]
            });
            return axis.tickValues(ticks).tickFormat(
                function (d, i) {
                    return d3.format(axis_values_format)(tickPoints[i]);
                })
        }

        if (xAxisValues) {
            xAxis = labelWithZScore(d3.axisBottom(x), 'x', xAxisValues, x_axis_values_format);
        } else if (xAxisLabels) {
            xAxis = d3.axisBottom(x)
                .ticks(xAxisLabels.length)
                .tickFormat(function (d, i) {
                    return xAxisLabels[i];
                });
        } else {
            xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        }
        if (yAxisValues) {
            yAxis = labelWithZScore(d3.axisLeft(y), 'y', yAxisValues, y_axis_values_format);
        } else if (yAxisLabels) {
            yAxis = d3.axisLeft(y)
                .ticks(yAxisLabels.length)
                .tickFormat(function (d, i) {
                    return yAxisLabels[i];
                });
        } else {
            yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));
        }

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");


        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230),
            d3.rgb(130, 130, 130));
        // setup fill color
        if (color == null) {
            color = d3.interpolateRdYlBu;
        }
        if ((headerNames !== undefined && headerNames !== null)
            && (headerSortingAlgos !== undefined && headerSortingAlgos !== null)) {
            showTopTerms = true;
        }

        var pixelsToAddToWidth = 200;
        if (!showTopTerms && !showCharacteristic) {
            pixelsToAddToWidth = 0;
        }

        if (backgroundColor !== undefined) {
            document.body.style.backgroundColor = backgroundColor;
        }

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + pixelsToAddToWidth)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");


        origSVGLeft = svg.node().getBoundingClientRect().left;
        origSVGTop = svg.node().getBoundingClientRect().top;
        var lastCircleSelected = null;

        function getCorpusWordCounts() {
            var binaryLabels = fullData.docs.labels.map(function (label) {
                return 1 * (fullData.docs.categories[label] != fullData.info.category_internal_name);
            });
            var wordCounts = {}; // word -> [cat counts, not-cat-counts]
            var wordCountSums = [0, 0];
            fullData.docs.texts.forEach(function (text, i) {
                text.toLowerCase().trim().split(/\W+/).forEach(function (word) {
                    if (word.trim() !== '') {
                        if (!(word in wordCounts))
                            wordCounts[word] = [0, 0];
                        wordCounts[word][binaryLabels[i]]++;
                        wordCountSums[binaryLabels[i]]++;
                    }
                })
            });
            return {
                avgDocLen: (wordCountSums[0] + wordCountSums[1]) / fullData.docs.texts.length,
                counts: wordCounts,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)]
                })
            };
        }

        function getContextWordCounts(query) {
            var wordCounts = {};
            var wordCountSums = [0, 0];
            var priorCountSums = [0, 0];
            gatherTermContexts(termDict[query])
                .contexts
                .forEach(function (contextSet, categoryIdx) {
                    contextSet.forEach(function (context) {
                        context.snippets.forEach(function (snippet) {
                            var tokens = snippet.toLowerCase().trim().replace('<b>', '').replace('</b>', '').split(/\W+/);
                            var matchIndices = [];
                            tokens.forEach(function (word, i) {
                                if (word === query) matchIndices.push(i)
                            });
                            tokens.forEach(function (word, i) {
                                if (word.trim() !== '') {
                                    var isValid = false;
                                    for (var matchI in matchIndices) {
                                        if (Math.abs(i - matchI) < 3) {
                                            isValid = true;
                                            break
                                        }
                                    }
                                    if (isValid) {
                                        //console.log([word, i, matchI, isValid]);
                                        if (!(word in wordCounts)) {
                                            var priorCounts = corpusWordCounts.counts[word]
                                            wordCounts[word] = [0, 0].concat(priorCounts);
                                            priorCountSums[0] += priorCounts[0];
                                            priorCountSums[1] += priorCounts[1];
                                        }
                                        wordCounts[word][categoryIdx]++;
                                        wordCountSums[categoryIdx]++;
                                    }
                                }
                            })
                        })
                    })
                });
            return {
                counts: wordCounts,
                priorSums: priorCountSums,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)];
                })
            }

        }

        function denseRank(ar) {
            var markedAr = ar.map((x, i) => [x, i]).sort((a, b) => a[0] - b[0]);
            var curRank = 1
            var rankedAr = markedAr.map(
                function (x, i) {
                    if (i > 0 && x[0] != markedAr[i - 1][0]) {
                        curRank++;
                    }
                    return [curRank, x[0], x[1]];
                }
            )
            return rankedAr.map(x => x).sort((a, b) => (a[2] - b[2])).map(x => x[0]);
        }


        function getDenseRanks(fullData, categoryNum) {
            console.log("GETTING DENSE RANKS")
            console.log("CAT NUM " + categoryNum)
            console.log(fullData)

            var fgFreqs = Array(fullData.data.length).fill(0);
            var bgFreqs = Array(fullData.data.length).fill(0);
            var categoryTermCounts = fullData.termCounts[categoryNum];

            Object.keys(categoryTermCounts).forEach(
                key => fgFreqs[key] = categoryTermCounts[key][0]
            )
            fullData.termCounts.forEach(
                function (categoryTermCounts, otherCategoryNum) {
                    if (otherCategoryNum != categoryNum) {
                        Object.keys(categoryTermCounts).forEach(
                            key => bgFreqs[key] += categoryTermCounts[key][0]
                        )
                    }
                }
            )
            var fgDenseRanks = denseRank(fgFreqs);
            var bgDenseRanks = denseRank(bgFreqs);

            var maxfgDenseRanks = Math.max(...fgDenseRanks);
            var minfgDenseRanks = Math.min(...fgDenseRanks);
            var scalefgDenseRanks = fgDenseRanks.map(
                x => (x - minfgDenseRanks) / (maxfgDenseRanks - minfgDenseRanks)
            )

            var maxbgDenseRanks = Math.max(...bgDenseRanks);
            var minbgDenseRanks = Math.min(...bgDenseRanks);
            var scalebgDenseRanks = bgDenseRanks.map(
                x => (x - minbgDenseRanks) / (maxbgDenseRanks - minbgDenseRanks)
            )

            return {'fg': scalefgDenseRanks,
                'bg': scalebgDenseRanks,
                'bgFreqs': bgFreqs,
                'fgFreqs': fgFreqs,
                'term': fullData.data.map((x)=>x.term)}
        }

        function getCategoryDenseRankScores(fullData, categoryNum) {
            var denseRanks = getDenseRanks(fullData, categoryNum)
            return denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
        }

        function getTermCounts(fullData) {
            var counts = Array(fullData.data.length).fill(0);
            fullData.termCounts.forEach(
                function (categoryTermCounts) {
                    Object.keys(categoryTermCounts).forEach(
                        key => counts[key] = categoryTermCounts[key][0]
                    )
                }
            )
            return counts;
        }

        function getContextWordLORIPs(query) {
            var contextWordCounts = getContextWordCounts(query);
            var ni_k = contextWordCounts.sums[0];
            var nj_k = contextWordCounts.sums[1];
            var n = ni_k + nj_k;
            //var ai_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            //var aj_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            var a0 = 0.00001 //corpusWordCounts.avgDocLen;
            var a_k0 = Object.keys(contextWordCounts.counts)
                .map(function (x) {
                    var counts = contextWordCounts.counts[x];
                    return a0 * (counts[2] + counts[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                })
                .reduce(function (a, b) {
                    return a + b
                });
            var ai_k0 = a_k0 / ni_k;
            var aj_k0 = a_k0 / nj_k;
            var scores = Object.keys(contextWordCounts.counts).map(
                function (word) {
                    var countData = contextWordCounts.counts[word];
                    var yi = countData[0];
                    var yj = countData[1];
                    //var ai = countData[2];
                    //var aj = countData[3];
                    //var ai = countData[2] + countData[3];
                    //var aj = ai;
                    //var ai = (countData[2] + countData[3]) * a0/ni_k;
                    //var aj = (countData[2] + countData[3]) * a0/nj_k;
                    var ai = a0 * (countData[2] + countData[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                    var aj = ai;
                    var deltahat_i_j =
                        +Math.log((yi + ai) * 1. / (ni_k + ai_k0 - yi - ai))
                        - Math.log((yj + aj) * 1. / (nj_k + aj_k0 - yj - aj));
                    var var_deltahat_i_j = 1. / (yi + ai) + 1. / (ni_k + ai_k0 - yi - ai)
                        + 1. / (yj + aj) + 1. / (nj_k + aj_k0 - yj - aj);
                    var zeta_ij = deltahat_i_j / Math.sqrt(var_deltahat_i_j);
                    return [word, yi, yj, ai, aj, ai_k0, zeta_ij];
                }
            ).sort(function (a, b) {
                return b[5] - a[5];
            });
            return scores;
        }

        function getContextWordSFS(query) {
            // from https://stackoverflow.com/questions/14846767/std-normal-cdf-normal-cdf-or-error-function
            function cdf(x, mean, variance) {
                return 0.5 * (1 + erf((x - mean) / (Math.sqrt(2 * variance))));
            }

            function erf(x) {
                // save the sign of x
                var sign = (x >= 0) ? 1 : -1;
                x = Math.abs(x);

                // constants
                var a1 = 0.254829592;
                var a2 = -0.284496736;
                var a3 = 1.421413741;
                var a4 = -1.453152027;
                var a5 = 1.061405429;
                var p = 0.3275911;

                // A&S formula 7.1.26
                var t = 1.0 / (1.0 + p * x);
                var y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);
                return sign * y; // erf(-x) = -erf(x);
            }

            function scale(a) {
                return Math.log(a + 0.0000001);
            }

            var contextWordCounts = getContextWordCounts(query);
            var wordList = Object.keys(contextWordCounts.counts).map(function (word) {
                return contextWordCounts.counts[word].concat([word]);
            });
            var cat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - cat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - cat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            var ncat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - ncat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - ncat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            function scaledFScore(cnt, other, freq_xbar, freq_var, prec_xbar, prec_var) {
                var beta = 1.5;
                var normFreq = cdf(scale(cnt), freq_xbar, freq_var);
                var normPrec = cdf(scale(cnt / (cnt + other)), prec_xbar, prec_var);
                return (1 + Math.pow(beta, 2)) * normFreq * normPrec / (Math.pow(beta, 2) * normFreq + normPrec);
            }

            var sfs = wordList.map(function (x) {
                cat_sfs = scaledFScore(x[0], x[1], cat_freq_xbar,
                    cat_freq_var, cat_prec_xbar, cat_prec_var);
                ncat_sfs = scaledFScore(x[1], x[0], ncat_freq_xbar,
                    ncat_freq_var, ncat_prec_xbar, ncat_prec_var);
                return [cat_sfs > ncat_sfs ? cat_sfs : -ncat_sfs].concat(x);

            }).sort(function (a, b) {
                return b[0] - a[0];
            });
            return sfs;
        }

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
                + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d, includeAll = true) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], [], [], []];
            console.log("searching")

            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d, includeAll);
            } else {
                return searchInExtraFeatures(d, includeAll);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], [], [], []];
            var term = d.term;
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });

            var pattern = null;
            if ('metalists' in fullData && term in fullData.metalists) {
                // from https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(str) {
                    return str.replace(/[\\?\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|\']/g, "\\$&");
                }

                console.log('term');
                console.log(term);
                pattern = new RegExp(
                    '\\W(' + fullData.metalists[term].map(escapeRegExp).join('|') + ')\\W',
                    'gim'
                );
            }

            for (var i in fullData.docs.extra) {
                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }
                    var text = fullData.docs.texts[i];

                    if (fullData.offsets !== undefined) {

                        if (fullData.offsets[term] !== undefined && fullData.offsets[term][i] !== undefined) {
                            var curMatch = {
                                'id': i,
                                'snippets': [],
                                'strength': strength,
                                'docLabel': docLabel,
                                'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                            }
                            for (const offset_i in fullData.offsets[term][i]) {
                                var offset = fullData.offsets[term][i][offset_i];
                                var spanStart = Math.max(offset[0] - 50, 0);
                                var spanEnd = Math.min(50, text.length-offset[1]);
                                var leftContext = text.substr(spanStart, offset[0] - spanStart);
                                var matchStr = text.substr(offset[0], offset[1] - offset[0]);
                                var rightContext = text.substr(offset[1], spanEnd);
                                var snippet = leftContext + '<b style="background-color: lightgoldenrodyellow">' + matchStr + '</b>' + rightContext;
                                if(spanStart > 0)
                                    snippet = '...' + snippet;
                                if(text.length - offset[1] > 50)
                                    snippet = snippet + '...'
                                curMatch.snippets.push(snippet)
                            }
                            matches[numericLabel].push(curMatch);
                        }
                    } else {

                        if (!useFullDoc)
                            text = text.slice(0, 300);
                        if (pattern !== null) {
                            text = text.replace(pattern, '<b>$&</b>');
                        }
                        var curMatch = {
                            'id': i,
                            'snippets': [text],
                            'strength': strength,
                            'docLabel': docLabel,
                            'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                        }

                        matches[numericLabel].push(curMatch);
                    }
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://mathiasbynens.be/notes/es-unicode-property-escapes#emoji
        var emojiRE = (/(?:[\u261D\u26F9\u270A-\u270D]|\uD83C[\uDF85\uDFC2-\uDFC4\uDFC7\uDFCA-\uDFCC]|\uD83D[\uDC42\uDC43\uDC46-\uDC50\uDC66-\uDC69\uDC6E\uDC70-\uDC78\uDC7C\uDC81-\uDC83\uDC85-\uDC87\uDCAA\uDD74\uDD75\uDD7A\uDD90\uDD95\uDD96\uDE45-\uDE47\uDE4B-\uDE4F\uDEA3\uDEB4-\uDEB6\uDEC0\uDECC]|\uD83E[\uDD18-\uDD1C\uDD1E\uDD1F\uDD26\uDD30-\uDD39\uDD3D\uDD3E\uDDD1-\uDDDD])(?:\uD83C[\uDFFB-\uDFFF])?|(?:[\u231A\u231B\u23E9-\u23EC\u23F0\u23F3\u25FD\u25FE\u2614\u2615\u2648-\u2653\u267F\u2693\u26A1\u26AA\u26AB\u26BD\u26BE\u26C4\u26C5\u26CE\u26D4\u26EA\u26F2\u26F3\u26F5\u26FA\u26FD\u2705\u270A\u270B\u2728\u274C\u274E\u2753-\u2755\u2757\u2795-\u2797\u27B0\u27BF\u2B1B\u2B1C\u2B50\u2B55]|\uD83C[\uDC04\uDCCF\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE1A\uDE2F\uDE32-\uDE36\uDE38-\uDE3A\uDE50\uDE51\uDF00-\uDF20\uDF2D-\uDF35\uDF37-\uDF7C\uDF7E-\uDF93\uDFA0-\uDFCA\uDFCF-\uDFD3\uDFE0-\uDFF0\uDFF4\uDFF8-\uDFFF]|\uD83D[\uDC00-\uDC3E\uDC40\uDC42-\uDCFC\uDCFF-\uDD3D\uDD4B-\uDD4E\uDD50-\uDD67\uDD7A\uDD95\uDD96\uDDA4\uDDFB-\uDE4F\uDE80-\uDEC5\uDECC\uDED0-\uDED2\uDEEB\uDEEC\uDEF4-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])|(?:[#\*0-9\xA9\xAE\u203C\u2049\u2122\u2139\u2194-\u2199\u21A9\u21AA\u231A\u231B\u2328\u23CF\u23E9-\u23F3\u23F8-\u23FA\u24C2\u25AA\u25AB\u25B6\u25C0\u25FB-\u25FE\u2600-\u2604\u260E\u2611\u2614\u2615\u2618\u261D\u2620\u2622\u2623\u2626\u262A\u262E\u262F\u2638-\u263A\u2640\u2642\u2648-\u2653\u2660\u2663\u2665\u2666\u2668\u267B\u267F\u2692-\u2697\u2699\u269B\u269C\u26A0\u26A1\u26AA\u26AB\u26B0\u26B1\u26BD\u26BE\u26C4\u26C5\u26C8\u26CE\u26CF\u26D1\u26D3\u26D4\u26E9\u26EA\u26F0-\u26F5\u26F7-\u26FA\u26FD\u2702\u2705\u2708-\u270D\u270F\u2712\u2714\u2716\u271D\u2721\u2728\u2733\u2734\u2744\u2747\u274C\u274E\u2753-\u2755\u2757\u2763\u2764\u2795-\u2797\u27A1\u27B0\u27BF\u2934\u2935\u2B05-\u2B07\u2B1B\u2B1C\u2B50\u2B55\u3030\u303D\u3297\u3299]|\uD83C[\uDC04\uDCCF\uDD70\uDD71\uDD7E\uDD7F\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE02\uDE1A\uDE2F\uDE32-\uDE3A\uDE50\uDE51\uDF00-\uDF21\uDF24-\uDF93\uDF96\uDF97\uDF99-\uDF9B\uDF9E-\uDFF0\uDFF3-\uDFF5\uDFF7-\uDFFF]|\uD83D[\uDC00-\uDCFD\uDCFF-\uDD3D\uDD49-\uDD4E\uDD50-\uDD67\uDD6F\uDD70\uDD73-\uDD7A\uDD87\uDD8A-\uDD8D\uDD90\uDD95\uDD96\uDDA4\uDDA5\uDDA8\uDDB1\uDDB2\uDDBC\uDDC2-\uDDC4\uDDD1-\uDDD3\uDDDC-\uDDDE\uDDE1\uDDE3\uDDE8\uDDEF\uDDF3\uDDFA-\uDE4F\uDE80-\uDEC5\uDECB-\uDED2\uDEE0-\uDEE5\uDEE9\uDEEB\uDEEC\uDEF0\uDEF3-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])\uFE0F/g);

        function isEmoji(str) {
            if (str.match(emojiRE)) return true;
            return false;
        }

        function displayObscuredTerms(obscuredTerms, data, term, termInfo, div = '#' + divName + '-' + 'overlapped-terms') {
            d3.select('#' + divName + '-' + 'overlapped-terms')
                .selectAll('div')
                .remove();
            d3.select(div)
                .selectAll('div')
                .remove();
            if (obscuredTerms.length > 1 && maxOverlapping !== 0) {
                var obscuredDiv = d3.select(div)
                    .append('div')
                    .attr("class", "obscured")
                    .style('align', 'center')
                    .style('text-align', 'center')
                    .html("<b>\"" + term + "\" obstructs</b>: ");
                obscuredTerms.map(
                    function (term, i) {
                        if (maxOverlapping === -1 || i < maxOverlapping) {
                            makeWordInteractive(
                                data,
                                svg,
                                obscuredDiv.append("text").text(term),
                                term,
                                data.filter(t => t.term === term)[0],//termInfo
                                false
                            );
                            if (i < obscuredTerms.length - 1
                                && (maxOverlapping === -1 || i < maxOverlapping - 1)) {
                                obscuredDiv.append("text").text(", ");
                            }
                        } else if (i === maxOverlapping && i !== obscuredTerms.length - 1) {
                            obscuredDiv.append("text").text("...");
                        }
                    }
                )
            }
        }

        function displayTermContexts(data, termInfo, jump = alwaysJump, includeAll = false) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            var notmatches = termInfo.notmatches;
            if (contexts[0].length + contexts[1].length + contexts[2].length + contexts[3].length == 0) {
                //return null;
            }
            //!!! Future feature: context words
            //var contextWords = getContextWordSFS(info.term);
            //var contextWords = getContextWordLORIPs(info.term);
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;


            function addSnippets(contexts, divId, isMatch = true) {
                var meta = contexts.meta ? contexts.meta : '&nbsp;';
                var headClass = 'snippet_meta docLabel' + contexts.docLabel;
                var snippetClass = 'snippet docLabel' + contexts.docLabel;
                if (!isMatch) {
                    headClass = 'snippet_meta not_match docLabel' + contexts.docLabel;
                    snippetClass = 'snippet not_match docLabel' + contexts.docLabel;
                }
                d3.select(divId)
                    .append("div")
                    .attr('class', headClass)
                    .html(meta);
                contexts.snippets.forEach(function (snippet) {
                    d3.select(divId)
                        .append("div")
                        .attr('class', snippetClass)
                        .html(snippet);
                })
            }


            if (ignoreCategories) {
                divId = '#' + divName + '-' + 'cat';

                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                var numDocs = fullData.docs.texts.length.toLocaleString('en');
                var numMatches = allContexts.length;
                d3.select(divId)
                    .append("div")
                    .attr('class', 'topic_preview')
                    .attr('text-align', "center")
                    .html(
                        "Matched " + numMatches + " out of " + numDocs + ' documents: '
                        + (100 * numMatches / numDocs).toFixed(2) + '%'
                    );

                if (allContexts.length > 0) {
                    var headerClassName = 'text_header';
                    allContexts.forEach(function (singleDoc) {
                        addSnippets(singleDoc, divId);
                    });
                    if (includeAll) {
                        allNotMatches.forEach(function (singleDoc) {
                            addSnippets(singleDoc, divId, false);
                        });
                    }
                }

            } else if (unifiedContexts) {
                divId = '#' + divName + '-' + 'cat';
                var docLabelCounts = fullData.docs.labels.reduce(
                    function (map, label) {
                        map[label] = (map[label] || 0) + 1;
                        return map;
                    },
                    Object.create(null)
                );
                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                allContexts.forEach(function (singleDoc) {
                    numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel] || 0) + 1;
                });
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);

                /*contexts.forEach(function(context) {
                     context.forEach(function (singleDoc) {
                         numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel]||0) + 1;
                         addSnippets(singleDoc, divId);
                     });
                 });*/
                console.log("ORDERING !!!!!");
                console.log(fullData.info.category_name);
                console.log(sortDocLabelsByName);
                var docLabelCountsSorted = Object.keys(docLabelCounts).map(key => (
                    {
                        "label": fullData.docs.categories[key],
                        "labelNum": key,
                        "matches": numMatches[key] || 0,
                        "overall": docLabelCounts[key],
                        'percent': (numMatches[key] || 0) * 100. / docLabelCounts[key]
                    }))
                    .sort(function (a, b) {
                        if (highlightSelectedCategory) {
                            if (a['label'] === fullData.info.category_name) {
                                return -1;
                            }
                            if (b['label'] === fullData.info.category_name) {
                                return 1;
                            }
                        }
                        if (sortDocLabelsByName) {
                            return a['label'] < b['label'] ? 1 : a['label'] > b['label'] ? -1 : 0;
                        } else {
                            return b.percent - a.percent;
                        }
                    });
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted);
                console.log(numMatches)
                console.log('#' + divName + '-' + 'categoryinfo')
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                if (showCategoryHeadings) {
                    d3.select('#' + divName + '-' + 'categoryinfo').attr('display', 'inline');
                }

                function getCategoryStatsHTML(counts) {
                    return counts.matches + " document"
                        + (counts.matches == 1 ? "" : "s") + " out of " + counts.overall + ': '
                        + counts['percent'].toFixed(2) + '%';
                }

                function getCategoryInlineHeadingHTML(counts) {
                    return '<a name="' + divName + '-category'
                        + counts.labelNum + '"></a>'
                        + (ignoreCategories ? "" : counts.label + ": ") + "<span class=topic_preview>"
                        + getCategoryStatsHTML(counts)
                        + "</span>";
                }


                docLabelCountsSorted.forEach(function (counts) {

                    var htmlToAdd = "";
                    if (!ignoreCategories) {
                        htmlToAdd += "<b>" + counts.label + "</b>: " + getCategoryStatsHTML(counts);
                        ;
                    }

                    if (counts.matches > 0) {
                        var headerClassName = 'text_header';
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId)
                                .append('div')
                                .attr('class', 'separator')
                                .html("<b>Selected category</b>");
                        }
                        d3.select(divId)
                            .append("div")
                            .attr('class', headerClassName)
                            .html(getCategoryInlineHeadingHTML(counts));

                        allContexts
                            .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                            .forEach(function (singleDoc) {
                                addSnippets(singleDoc, divId);
                            });
                        if (includeAll) {
                            allNotMatches
                                .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                                .forEach(function (singleDoc) {
                                    addSnippets(singleDoc, divId, false);
                                });
                        }
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId).append('div').attr('class', 'separator').html("<b>End selected category</b>");
                            d3.select(divId).append('div').html("<br />");
                        }
                    }


                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'categoryinfo')
                            .attr('display', 'inline')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }

                })


            } else {
                var contextColumns = [
                    fullData.info.category_internal_name,
                    fullData.info.not_category_name
                ];
                if (showNeutral) {
                    if ('neutral_category_name' in fullData.info) {
                        contextColumns.push(fullData.info.neutral_category_name)
                    } else {
                        contextColumns.push("Neutral")
                    }
                    if (showExtra) {
                        if ('extra_category_name' in fullData.info) {
                            contextColumns.push(fullData.info.extra_category_name)
                        } else {
                            contextColumns.push("Extra")
                        }
                    }

                }
                contextColumns.map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        //var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var divId = null
                        if (fullData.info.category_internal_name == catName) {
                            divId = '#' + divName + '-' + 'cat'
                        } else if (fullData.info.not_category_name == catName) {
                            divId = '#' + divName + '-' + 'notcat'
                        } else if (fullData.info.neutral_category_name == catName) {
                            divId = '#' + divName + '-' + 'neut';
                        } else if (fullData.info.extra_category_name == catName) {
                            divId = '#' + divName + '-' + 'extra'
                        } else {
                            return;
                        }

                        var temp = d3.select(divId).selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            addSnippets(context, divId);
                        });
                        if (includeAll) {
                            notmatches[catIndex].forEach(function (context) {
                                addSnippets(context, divId, false);
                            });
                        }
                    }
                );
            }

            var obscuredTerms = getObscuredTerms(data, termInfo.info);
            displayObscuredTerms(obscuredTerms, data, info.term, info, '#' + divName + '-' + 'overlapped-terms-clicked');

            d3.select('#' + divName + '-' + 'termstats')
                .selectAll("div")
                .remove();
            var termHtml = 'Term: <b>' + formatTermForDisplay(info.term) + '</b>';
            if ('metalists' in fullData && info.term in fullData.metalists) {
                termHtml = 'Topic: <b>' + formatTermForDisplay(info.term) + '</b>';
            }
            if (getCustomTermHtml !== null) {
                termHtml = getCustomTermHtml(info);
            }
            d3.select('#' + divName + '-' + 'termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html(termHtml);
            if ('metalists' in fullData && info.term in fullData.metalists && topic_model_preview_size > 0) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Topic preview</b>: "
                        + fullData.metalists[info.term]
                            .slice(0, topic_model_preview_size)
                            .reduce(function (x, y) {
                                return x + ', ' + y
                            }));
            }
            if ('metadescriptions' in fullData && info.term in fullData.metadescriptions) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Description</b>: " + fullData.metadescriptions[info.term]);
            }
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;


            var numCatDocs = fullData.docs.labels
                .map(function (x) {
                    return (x == fullData.docs.categories.indexOf(
                        fullData.info.category_internal_name)) + 0
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });


            var numNCatDocs = fullData.docs.labels
                .map(function (x) {
                    return notCategoryNumList.indexOf(x) > -1
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            function getFrequencyDescription(name, count25k, count, ndocs) {
                var desc = name;
                if (!enableTermCategoryDescription) {
                    return desc + ':';
                }
                desc += ' frequency: <div class=text_subhead>' + count25k + ' per 25,000 terms</div>';
                if (!isNaN(Math.round(ndocs))) {
                    desc += '<div class=text_subhead>' + Math.round(ndocs) + ' per 1,000 docs</div>';
                }
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    if (!isNaN(Math.round(ndocs))) {
                        desc += '<u>Some of the ' + count + ' mentions:</u>';
                    } else {
                        desc += count + ' mentions';
                    }
                }
                /*
                desc += '<br><b>Discriminative:</b> ';

                desc += contextWords
                    .slice(cat_name === name ? 0 : contextWords.length - 3,
                        cat_name === name ? 3 : contextWords.length)
                    .filter(function (x) {
                        //return Math.abs(x[5]) > 1.96;
                        return true;
                    })
                    .map(function (x) {return x.join(', ')}).join('<br>');
                */
                return desc;
            }

            if (!unifiedContexts && !ignoreCategories) {
                console.log("NOT UNIFIED CONTEXTS")
                d3.select('#' + divName + '-' + 'cathead')
                    .style('fill', color(1))
                    .html(
                        getFrequencyDescription(cat_name,
                            info.cat25k,
                            info.cat,
                            termInfo.contexts[0].length * 1000 / numCatDocs
                        )
                    );
                d3.select('#' + divName + '-' + 'notcathead')
                    .style('fill', color(0))
                    .html(
                        getFrequencyDescription(ncat_name,
                            info.ncat25k,
                            info.ncat,
                            termInfo.contexts[1].length * 1000 / numNCatDocs)
                    );
                if (showNeutral) {
                    var numList = fullData.docs.categories.map(function (x, i) {
                        if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                            return i;
                        } else {
                            return -1;
                        }
                    }).filter(function (x) {
                        return x > -1
                    });

                    var numDocs = fullData.docs.labels
                        .map(function (x) {
                            return numList.indexOf(x) > -1
                        })
                        .reduce(function (a, b) {
                            return a + b;
                        }, 0);

                    d3.select("#" + divName + "-neuthead")
                        .style('fill', color(0))
                        .html(
                            getFrequencyDescription(fullData.info.neutral_category_name,
                                info.neut25k,
                                info.neut,
                                termInfo.contexts[2].length * 1000 / numDocs)
                        );

                    if (showExtra) {
                        var numList = fullData.docs.categories.map(function (x, i) {
                            if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                                return i;
                            } else {
                                return -1;
                            }
                        }).filter(function (x) {
                            return x > -1
                        });

                        var numDocs = fullData.docs.labels
                            .map(function (x) {
                                return numList.indexOf(x) > -1
                            })
                            .reduce(function (a, b) {
                                return a + b;
                            }, 0);

                        d3.select("#" + divName + "-extrahead")
                            .style('fill', color(0))
                            .html(
                                getFrequencyDescription(fullData.info.extra_category_name,
                                    info.extra25k,
                                    info.extra,
                                    termInfo.contexts[3].length * 1000 / numDocs)
                            );

                    }
                }
            } else if (unifiedContexts && !ignoreCategories) {
                // extra unified context code goes here
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted)

                docLabelCountsSorted.forEach(function (counts) {
                    var htmlToAdd = (ignoreCategories ? "" : "<b>" + counts.label + "</b>: ") + getCategoryStatsHTML(counts);
                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'contexts')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }
                })
            }
            if (jump) {
                if (window.location.hash === '#' + divName + '-' + 'snippets') {
                    window.location.hash = '#' + divName + '-' + 'snippetsalt';
                } else {
                    window.location.hash = '#' + divName + '-' + 'snippets';
                }
            }
        }

        function searchInText(d, includeAll = true) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function removeUnderScoreJoin(term) {
                /*
                '_ _asjdklf_jaksdlf_jaksdfl skld_Jjskld asdfjkl_sjkdlf'
                  ->
                "_ _asjdklf jaksdlf jaksdfl skld Jjskld asdfjkl_sjkdlf"
                 */
                return term.replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3");
            }

            function buildMatcher(term) {


                var boundary = '(?:\\W|^|$)';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                if (matchFullLine) {
                    boundary = '($|^)';
                }
                var termToRegex = term;


                // https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(string) {
                    return string.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\,\\\^\$\|\'#?]/g, "\\$&");
                    //return string.replace(/[\?#.*+^${}()|[\]\\]'\%/g, '\\$&'); // $& means the whole matched string
                }

                /*
                ['[', ']', '(', ')', '{', '}', '^', '$', '|', '?', '"',
                    '*', '+', '-', '=', '~', '`', '{'].forEach(function (a) {
                    termToRegex = termToRegex.replace(a, '\\\\' + a)
                });
                ['.', '#'].forEach(function(a) {termToRegex = termToRegex.replace(a, '\\' + a)})
                */
                termToRegex = escapeRegExp(termToRegex);
                console.log("termToRegex")
                console.log(termToRegex)

                var regexp = new RegExp(boundary + '('
                    + removeUnderScoreJoin(
                        termToRegex.replace(' ', wordSep, 'gim')
                    ) + ')' + boundary, 'gim');
                console.log(regexp);

                if (subwordEncoding === 'RoBERTa') {
                    if (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289) {
                        // Starts with character  indicating it's a word start
                        console.log("START")
                        regexp = new RegExp(boundary + escapeRegExp(term.substr(1, term.length)), 'gim');
                    } else {
                        regexp = new RegExp("\w" + escapeRegExp(term), 'gim');
                    }
                    console.log("SP")
                    console.log(regexp)
                }


                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], [], [], []];
            var notmatches = [[], [], [], []];
            var pattern = buildMatcher(d.term);
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            console.log('extraCategoryNumList')
            console.log(extraCategoryNumList);
            console.log("categoryNum");
            console.log(categoryNum);
            console.log("categoryNum");
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    //var numericLabel = 1 * (fullData.docs.categories[fullData.docs.labels[i]] != fullData.info.category_internal_name);

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }

                    var text = removeUnderScoreJoin(fullData.docs.texts[i]);
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': [], 'notsnippets': [], 'docLabel': docLabel};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }

                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[numericLabel].push(curMatch);
                    } else {
                        if (includeAll) {
                            curMatch.snippets = [
                                text.replace(/\n$/g, '\n\n')
                            ];
                            notmatches[numericLabel].push(curMatch);
                        }

                    }
                }
            }
            var toRet = {
                'contexts': matches,
                'notmatches': notmatches,
                'info': d,
                'docLabel': docLabel
            };
            return toRet;
        }

        function getDefaultTooltipContent(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            message += '<br/>score: ' + d.os.toFixed(5);
            return message;
        }

        function getDefaultTooltipContentWithoutScore(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            return message;
        }

        function getObscuredTerms(data, d) {
            //data = fullData['data']
            var matches = (data.filter(function (term) {
                    return term.x === d.x && term.y === d.y && (term.display === undefined || term.display === true);
                }).map(function (term) {
                    return formatTermForDisplay(term.term)
                }).sort()
            );
            return matches;
        }

        function showTooltip(data, d, pageX, pageY, showObscured = true) {
            deselectLastCircle();

            var obscuredTerms = getObscuredTerms(data, d);
            var message = '';
            console.log("!!!!! " + obscuredTerms.length)
            console.log(showObscured)
            if (obscuredTerms.length > 1 && showObscured)
                displayObscuredTerms(obscuredTerms, data, d.term, d);
            if (getTooltipContent !== null) {
                message += getTooltipContent(d);
            } else {
                if (sortByDist) {
                    message += getDefaultTooltipContentWithoutScore(d);
                } else {
                    message += getDefaultTooltipContent(d);
                }
            }
            pageX -= (svg.node().getBoundingClientRect().left) - origSVGLeft;
            pageY -= (svg.node().getBoundingClientRect().top) - origSVGTop;
            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX - 40) + "px")
                .style("top", (pageY - 85 > 0 ? pageY - 85 : 0) + "px");
            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            }).on('mouseout', function () {
                tooltip.transition().style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            var searchTerm = document
                .getElementById(this.divName + "-searchTerm")
                .value;
            handleSearchTerm(searchTerm);
            return false;
        };

        function highlightTerm(searchTerm, showObscured) {
            deselectLastCircle();
            var cleanedTerm = searchTerm.toLowerCase()
                .replace("'", " '")
                .trim();
            if (this.termDict[cleanedTerm] === undefined) {
                cleanedTerm = searchTerm.replace("'", " '").trim();
            }
            if (this.termDict[cleanedTerm] !== undefined) {
                showToolTipForTerm(this.data, this.svg, cleanedTerm, this.termDict[cleanedTerm], showObscured);
            }
            return cleanedTerm;
        }

        function handleSearchTerm(searchTerm, jump = false) {
            console.log("Handle search term.");
            console.log(searchTerm);
            console.log("this");
            console.log(this)
            highlighted = highlightTerm.call(this, searchTerm, true);
            console.log("found searchTerm");
            console.log(searchTerm);
            if (this.termDict[searchTerm] != null) {
                var runDisplayTermContexts = true;
                if (alternativeTermFunc != null) {
                    runDisplayTermContexts = this.alternativeTermFunc(this.termDict[searchTerm]);
                }
                if (runDisplayTermContexts) {
                    displayTermContexts(
                        this.data,
                        this.gatherTermContexts(this.termDict[searchTerm], this.includeAllContexts),
                        alwaysJump,
                        this.includeAllContexts
                    );
                }
            }
        }

        function getCircleForSearchTerm(mysvg, searchTermInfo) {
            var circle = mysvg;
            if (circle.tagName !== "circle") { // need to clean this thing up
                circle = mysvg._groups[0][searchTermInfo.ci];
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0].children !== undefined) {
                        circle = mysvg._groups[0].children[searchTermInfo.ci];
                    }
                }
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0][0].children !== undefined) {
                        circle = Array.prototype.filter.call(
                            mysvg._groups[0][0].children,
                            x => (x.tagName == "circle" && x.__data__['term'] == searchTermInfo.term)
                        )[0];
                    }
                }
                if ((circle === undefined || circle.tagName != 'circle') && mysvg._groups[0][0].children !== undefined) {
                    circle = mysvg._groups[0][0].children[searchTermInfo.ci];
                }
            }
            return circle;
        }

        function showToolTipForTerm(data, mysvg, searchTerm, searchTermInfo, showObscured = true) {
            //var searchTermInfo = termDict[searchTerm];
            console.log("showing tool tip")
            console.log(searchTerm)
            console.log(searchTermInfo)
            if (searchTermInfo === undefined) {
                console.log("can't show")
                d3.select("#" + divName + "-alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#" + divName + "-alertMessage").text("");
                var circle = getCircleForSearchTerm(mysvg, searchTermInfo);
                if (circle) {
                    var mySVGMatrix = circle.getScreenCTM().translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;
                    circle.style["stroke"] = "black";
                    //var circlePos = circle.position();
                    //var el = circle.node()
                    //showTooltip(searchTermInfo, pageX, pageY, circle.cx.baseVal.value, circle.cx.baseVal.value);
                    showTooltip(
                        data,
                        searchTermInfo,
                        pageX,
                        pageY,
                        showObscured
                    );

                    lastCircleSelected = circle;
                }

            }
        };


        function makeWordInteractive(data, svg, domObj, term, termInfo, showObscured = true) {
            return domObj
                .on("mouseover", function (d) {
                    showToolTipForTerm(data, svg, term, termInfo, showObscured);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    if (showObscured) {
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    }
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(termInfo);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(termInfo, includeAllContexts), alwaysJump, includeAllContexts);
                    }
                });
        }



        function processData(fullData) {

            modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            var data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                //!!!
                //termDict[x.term].i = i;
            });

            var padding = 0.1;
            if (showAxes || showAxesAndCrossHairs) {
                padding = 0.1;
            }

            // Scale the range of the data.  Add some space on either end.
            if (useGlobalScale) {
                var axisMax = Math.max(
                    d3.max(data, function (d) {
                        return d.x;
                    }),
                    d3.max(data, function (d) {
                        return d.y;
                    }),
                )
                var axisMin = Math.min(
                    d3.min(data, function (d) {
                        return d.x;
                    }),
                    d3.min(data, function (d) {
                        return d.y;
                    }),
                )
                axisMin = axisMin - (axisMax - axisMin) * padding;
                axisMax = axisMax + (axisMax - axisMin) * padding;
                x.domain([axisMin, axisMax]);
                y.domain([axisMin, axisMax]);
            } else {
                var xMax = d3.max(data, function (d) {
                    return d.x;
                });
                var yMax = d3.max(data, function (d) {
                    return d.y;
                })
                x.domain([-1 * padding, xMax + padding]);
                y.domain([-1 * padding, yMax + padding]);
            }

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            var axisRectHolder = new RectangleHolder();
            // Add the scatterplot
            data.forEach(function (d, i) {
                d.ci = i
            });

            //console.log('XXXXX'); console.log(data)


            function getFilter(data) {
                return data.filter(d => d.display === undefined || d.display === true);
            }


            var mysvg = svg
                .selectAll("dot")
                .data(getFilter(data))
                //.filter(function (d) {return d.display === undefined || d.display === true})
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (colorFunc) {
                        return colorFunc(d);
                    } else if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else if (d.p <= minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        if (d.term === "the") {
                            console.log("COLS " + d.s + " " + color(d.s) + " " + d.term)
                            console.log(d)
                            console.log(color)
                        }
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    /*var mySVGMatrix = circle.getScreenCTM()n
                        .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;*/

                    /*showTooltip(
                        d,
                        d3.event.pageX,
                        d3.event.pageY
                    );*/
                    console.log("point MOUSOEVER")
                    console.log(d)
                    showToolTipForTerm(data, this, d.term, d, true);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(d);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                    }
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    d3.select('#' + divName + '-' + 'overlapped-terms')
                        .selectAll('div')
                        .remove();
                })


            coords = Object();

            var pointStore = [];
            var pointRects = [];

            function censorPoints(datum, getX, getY) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(getX(datum)))
                    .attr("y", y(getY(datum)) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            function censorCircle(xCoord, yCoord) {
                var curLabel = svg.append("text")
                    .attr("x", x(xCoord))
                    .attr("y", y(yCoord) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            var configs = [
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': 10, 'alignment-baseline': 'ideographic'},

                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
            ];
            if (centerLabelsOverPoints) {
                configs = [{'anchor': 'middle', 'xoff': 0, 'yoff': 0, 'alignment-baseline': 'middle'}];
            }

            function labelPointsIfPossible(datum, myX, myY) {
                if (suppressTextColumn !== undefined
                    && datum.etc !== undefined
                    && datum.etc[suppressTextColumn] === true) {
                    return false;
                }

                var term = datum.term;
                if (datum.x > datum.y) {
                    configs.sort((a, b) => a.anchor == 'end' && b.anchor == 'end'
                        ? a.group - b.group : (a.anchor == 'end') - (b.anchor == 'end'));
                } else {
                    configs.sort((a, b) => a.anchor == 'start' && b.anchor == 'start'
                        ? a.group - b.group : (a.anchor == 'start') - (b.anchor == 'start'));
                }
                var matchedElement = null;

                var termColor = 'rgb(0,0,0)';
                if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                    termColor = datum.etc[textColorColumn];
                }
                term = formatTermForDisplay(term);

                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = svg.append("text")
                        //.attr("x", x(data[i].x) + config['xoff'])
                        //.attr("y", y(data[i].y) + config['yoff'])
                        .attr("x", x(myX) + config['xoff'])
                        .attr("y", y(myY) + config['yoff'])
                        .attr('class', 'label')
                        .attr('class', 'pointlabel')
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("text-anchor", config['anchor'])
                        .attr("alignment-baseline", config['alignment'])
                        .attr("fill", termColor)
                        .text(term);
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = doCensorPoints ? 0.5 : .25;

                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        curLabel = makeWordInteractive(data, svg, curLabel, term, datum);
                        break;
                    }
                }

                if (!matchedElement) {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    var labelRect = new Rectangle(x1, y1, x2, y2)
                    rectHolder.add(labelRect);
                    pointStore.push([x1, y1]);
                    pointStore.push([x2, y1]);
                    pointStore.push([x1, y2]);
                    pointStore.push([x2, y2]);
                    return {label: curLabel, rect: labelRect};
                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                if (b.bg === a.bg)
                    return (b.cat + b.ncat) - (a.cat + a.ncat);
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return -1;
                    if (!aGood && bGood) return 1;
                }
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return 1;
                    if (!aGood && bGood) return -1;
                }
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            var sortedData = data.map(x => x).sort(sortByDist ? euclideanDistanceSort : scoreSort);
            if (doCensorPoints) {
                for (var i in data) {
                    var d = sortedData[i];

                    if (!(censorPointColumn !== undefined
                        && d.etc !== undefined
                        && d.etc[censorPointColumn] === false)) {

                        censorPoints(
                            d,
                            function (d) {
                                return d.x
                            },
                            function (d) {
                                return d.y
                            }
                        );
                    }

                }
            }


            function registerFigureBBox(curLabel, axis = false) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var rect = new Rectangle(x1, y1, x2, y2)
                if (axis) {
                    axisRectHolder.add(rect)
                } else {
                    rectHolder.add(rect);
                }
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }

            function drawXLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "x label")
                    .attr("text-anchor", "end")
                    .attr("x", width)
                    .attr("y", height - 6)
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            function drawYLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "y label")
                    .attr("text-anchor", "end")
                    .attr("y", 6)
                    .attr("dy", ".75em")
                    .attr("transform", "rotate(-90)")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            d3.selection.prototype.moveToBack = function () {
                return this.each(function () {
                    var firstChild = this.parentNode.firstChild;
                    if (firstChild) {
                        this.parentNode.insertBefore(this, firstChild);
                    }
                });
            };

            if (verticalLines) {
                if (typeof (verticalLines) === "number") {
                    verticalLines = [verticalLines]; // r likes to make single element vectors doubles; this is a hackish workaround
                }
                for (i in verticalLines) {
                    svg.append("g")
                        .attr("transform", "translate(" + x(verticalLines[i]) + ", 1)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#dddddd")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (fullData['line'] !== undefined) {
                var valueline = d3.line()
                    .x(function (d) {
                        return x(d.x);
                    })
                    .y(function (d) {
                        return y(d.y);
                    });
                fullData.line = fullData.line.sort((a, b) => b.x - a.x);
                svg.append("path")
                    .attr("class", "line")
                    .style("stroke-width", "1px")
                    .attr("d", valueline(fullData['line'])).moveToBack();
            }
            if (showAxes || showAxesAndCrossHairs) {

                var myXAxis = svg.append("g")
                    .attr("class", "x axis")
                    .attr("transform", "translate(0," + height + ")")
                    .call(xAxis);

                //rangeTree = registerFigureBBox(myXAxis);


                var xLabel = drawXLabel(svg, getLabelText('x'));

                //console.log('xLabel');
                //console.log(xLabel);

                //rangeTree = registerFigureBBox(xLabel);
                // Add the Y Axis

                if (!yAxisValues) {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr("dx", "30px")
                        .attr("dy", "-13px")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("transform", "rotate(-90)");
                } else {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px');
                }
                registerFigureBBox(myYAxis, true);
                registerFigureBBox(myXAxis, true);

                function getLabelText(axis) {
                    if (axis == 'y') {
                        if (yLabelText == null)
                            return modelInfo['category_name'] + " Frequency";
                        else
                            return yLabelText;
                    } else {
                        if (xLabelText == null)
                            return modelInfo['not_category_name'] + " Frequency";
                        else
                            return xLabelText;
                    }
                }

                var yLabel = drawYLabel(svg, getLabelText('y'))

            }

            if (!showAxes || showAxesAndCrossHairs) {
                horizontal_line_y_position_translated = 0.5;
                if (horizontal_line_y_position !== null) {
                    var loOy = null, hiOy = null, loY = null, hiY = null;
                    for (i in fullData.data) {
                        var curOy = fullData.data[i].oy;
                        if (curOy < horizontal_line_y_position && (curOy > loOy || loOy === null)) {
                            loOy = curOy;
                            loY = fullData.data[i].y
                        }
                        if (curOy > horizontal_line_y_position && (curOy < hiOy || hiOy === null)) {
                            hiOy = curOy;
                            hiY = fullData.data[i].y
                        }
                    }
                    horizontal_line_y_position_translated = loY + (hiY - loY) / 2.
                    if (loY === null) {
                        horizontal_line_y_position_translated = 0;
                    }
                }
                if (vertical_line_x_position === null) {
                    vertical_line_x_position_translated = 0.5;
                } else {
                    if (vertical_line_x_position !== null) {
                        var loOx = null, hiOx = null, loX = null, hiX = null;
                        for (i in fullData.data) {
                            var curOx = fullData.data[i].ox;
                            if (curOx < vertical_line_x_position && (curOx > loOx || loOx === null)) {
                                loOx = curOx;
                                loX = fullData.data[i].x;
                            }
                            if (curOx > vertical_line_x_position && (curOx < hiOx || hiOx === null)) {
                                hiOx = curOx;
                                hiX = fullData.data[i].x
                            }
                        }
                        vertical_line_x_position_translated = loX + (hiX - loX) / 2.
                        if (loX === null) {
                            vertical_line_x_position_translated = 0;
                        }
                    }
                }
                if (showCrossAxes) {
                    var x_line = svg.append("g")
                        .attr("transform", "translate(0, " + y(horizontal_line_y_position_translated) + ")")
                        .append("line")
                        .attr("x2", width)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                    var y_line = svg.append("g")
                        .attr("transform", "translate(" + x(vertical_line_x_position_translated) + ", 0)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (showDiagonal) {
                var diagonal = svg.append("g")
                    .append("line")
                    .attr("x1", 0)
                    .attr("y1", height)
                    .attr("x2", width)
                    .attr("y2", 0)
                    .style("stroke-dasharray", "5,5")
                    .style("stroke", "#cccccc")
                    .style("stroke-width", "1px")
                    .moveToBack();
            }

            function showWordList(word, termDataList, xOffset=null) {
                var maxWidth = word.node().getBBox().width;
                var wordObjList = [];
                for (var i in termDataList) {
                    var datum = termDataList[i];
                    var curTerm = datum.term;
                    word = (function (word, curTerm) {
                        var termColor = 'rgb(0,0,0)';
                        if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                            termColor = datum.etc[textColorColumn];
                        }
                        console.log("Show WORD "); console.log(word.node().getBBox().x)
                        var curWordPrinted = svg.append("text")
                            .attr("text-anchor", "start")
                            .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                            .attr('font-size', '12px')
                            .attr("fill", termColor)
                            .attr("x", xOffset == null ? word.node().getBBox().x : xOffset)
                            .attr("y", word.node().getBBox().y
                                + 2 * word.node().getBBox().height)
                            .text(formatTermForDisplay(curTerm));
                        wordObjList.push(curWordPrinted)
                        return makeWordInteractive(
                            termDataList, //data,
                            svg,
                            curWordPrinted,
                            curTerm,
                            termDataList[i]);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth,
                    'wordObjList': wordObjList
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(isTopPane) {
                console.log("PICK SCORE ALGO")
                console.log(isTopPane)
                if (isTopPane === true) {
                    if (headerSortingAlgos !== null && headerSortingAlgos['upper'] !== undefined)
                        return headerSortingAlgos['upper'];
                    return scoreSortForCategory;
                } else {
                    if (headerSortingAlgos !== null && headerSortingAlgos['lower'] !== undefined)
                        return headerSortingAlgos['lower'];
                    return scoreSortForNotCategory;
                }

            }

            function pickTermSortingAlgorithm(isUpperPane) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(isUpperPane);
                return pickScoreSortAlgo(isUpperPane);
            }

            function showAssociatedWordList(data, word, header, isUpperPane, xOffset, length = topTermsLength) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isUpperPane);
                console.log("showAssociatedWordList");
                console.log(header);
                console.log("WORD");
                console.log(word)
                sortedData = data.filter(term => (term.display === undefined || term.display === true)).sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isUpperPane)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(word, sortedData.slice(0, length), xOffset);

            }

            var characteristicXOffset = width;

            function showCatHeader(startingOffset, catName, registerFigureBBox) {
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset //width
                    )
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(catName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                return catHeader;
            }

            function showNotCatHeader(startingOffset, word, notCatName) {
                console.log("showNotCatHeader")
                return svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("y", word.node().getBBox().y + 3 * word.node().getBBox().height)
                    .text(notCatName);
            }

            function showTopTermsPane(data,
                                      registerFigureBBox,
                                      showAssociatedWordList,
                                      upperHeaderName,
                                      lowerHeaderName,
                                      startingOffset) {
                data = data.filter(term => (term.display === undefined || term.display === true));
                //var catHeader = showCatHeader(startingOffset, catName, registerFigureBBox);
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(upperHeaderName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                var word = catHeader;
                var wordListData = showAssociatedWordList(data, word, catHeader, true, startingOffset);
                word = wordListData.word;
                var maxWidth = wordListData.maxWidth;

                var notCatHeader = showNotCatHeader(startingOffset, word, lowerHeaderName);
                word = notCatHeader;
                characteristicXOffset = Math.max(
                    catHeader.node().getBBox().x + maxWidth + 10,
                    notCatHeader.node().getBBox().x + maxWidth + 10
                )
                console.log("characteristicXOffset", characteristicXOffset)
                console.log(catHeader.node().getBBox().x + maxWidth + 10)
                console.log(notCatHeader.node().getBBox().x + maxWidth + 10)

                var notWordListData = showAssociatedWordList(data, word, notCatHeader, false, startingOffset);
                word = wordListData.word;
                if (wordListData.maxWidth > maxWidth) {
                    maxWidth = wordListData.maxWidth;
                }
                return {
                    wordListData, notWordListData,
                    word, maxWidth, characteristicXOffset, startingOffset,
                    catHeader, notCatHeader, registerFigureBBox
                };
            }

            var payload = Object();
            if (showTopTerms) {
                var upperHeaderName = "Top " + fullData['info']['category_name'];
                var lowerHeaderName = "Top " + fullData['info']['not_category_name'];
                if (headerNames !== null) {
                    if (headerNames.upper !== undefined)
                        upperHeaderName = headerNames.upper;
                    if (headerNames.lower !== undefined)
                        lowerHeaderName = headerNames.lower;
                }
                payload.topTermsPane = showTopTermsPane(
                    data,
                    registerFigureBBox,
                    showAssociatedWordList,
                    upperHeaderName,
                    lowerHeaderName,
                    width + topTermsLeftBuffer
                );
                payload.showTopTermsPane = showTopTermsPane;
                payload.showAssociatedWordList = showAssociatedWordList;
                payload.showWordList = showWordList;

                /*var wordListData = topTermsPane.wordListData;
                var word = topTermsPane.word;
                var maxWidth = topTermsPane.maxWidth;
                var catHeader = topTermsPane.catHeader;
                var notCatHeader = topTermsPane.notCatHeader;
                var startingOffset = topTermsPane.startingOffset;*/
                characteristicXOffset = payload.topTermsPane.characteristicXOffset;
            }


            if ((!nonTextFeaturesMode && !asianMode && showCharacteristic)
                || (headerNames !== null && headerNames.right !== undefined)) {
                var sortMethod = backgroundScoreSort;
                var title = 'Characteristic';
                if (headerNames !== null && headerNames.right !== undefined) {
                    title = headerNames.right;
                }
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                    sortMethod = scoreSortReverse;
                } else if (data.reduce(function (a, b) {
                    return a + b.bg
                }, 0) === 0) {
                    title = 'Most frequent';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", characteristicXOffset)
                    .attr("dy", "6px")
                    .text(title);

                var rightSortMethod = sortMethod;
                if (rightOrderColumn !== undefined && rightOrderColumn !== null) {
                    rightSortMethod = ((a, b) => b.etc[rightOrderColumn] - a.etc[rightOrderColumn]);
                }

                var wordListData = showWordList(
                    word,
                    data.filter(term => (term.display === undefined || term.display === true))
                        .sort(rightSortMethod).slice(0, topTermsLength * 2 + 2),
                    characteristicXOffset
                );

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            function performPartialLabeling(
                data,
                existingLabels,
                getX,
                getY,
                labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            ) {
                for (i in existingLabels) {
                    rectHolder.remove(existingLabels[i].rect);
                    existingLabels[i].label.remove();
                }

                var labeledPoints = [];

                //var filteredData = data.filter(d=>d.display === undefined || d.display === true);
                //for (var i = 0; i < filteredData.length; i++) {
                data.sort(labelPriorityFunction).forEach(function (datum, i) {
                    //console.log(datum.i, datum.ci, i)
                    //var label = labelPointsIfPossible(i, getX(filteredData[i]), getY(filteredData[i]));
                    if (datum.display === undefined || datum.display === true) {
                        var label = labelPointsIfPossible(datum, getX(datum), getY(datum));
                        if (label !== false) {
                            //console.log("labeled")
                            labeledPoints.push(label)
                        }
                    }
                    //if (labelPointsIfPossible(i), true) numPointsLabeled++;
                })
                return labeledPoints;
            }

            //var labeledPoints = performPartialLabeling();
            var labeledPoints = [];
            var labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            if (labelPriorityColumn !== undefined && labelPriorityColumn !== null) {
                labelPriorityFunction = (a, b) => b.etc[labelPriorityColumn] - a.etc[labelPriorityColumn];
            }

            labeledPoints = performPartialLabeling(
                data,
                labeledPoints,
                function (d) {
                    return d.x
                },
                function (d) {
                    return d.y
                },
                labelPriorityFunction
            );

            if (backgroundLabels !== null) {
                backgroundLabels.map(
                    function (label) {
                        svg.append("text")
                            .attr("x", x(label.X))
                            .attr("y", y(label.Y))
                            .attr("text-anchor", "middle")
                            .style("font-size", "30")
                            .style("fill", "rgb(200,200,200)")
                            .text(label.Text)
                            .lower()
                            .on('mouseover', function (d) {
                                d3.select(this).style('stroke', 'black').style('stroke-width', '1px').raise()
                            })
                            .on('mouseout', function (d) {
                                d3.select(this).style('stroke-width', '0px').style('fill', 'rgb(200,200,200)').lower()
                            })
                    }
                )
            }


            /*
            // pointset has to be sorted by X
            function convex(pointset) {
                function _cross(o, a, b) {
                    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0]);
                }

                function _upperTangent(pointset) {
                    var lower = [];
                    for (var l = 0; l < pointset.length; l++) {
                        while (lower.length >= 2 && (_cross(lower[lower.length - 2], lower[lower.length - 1], pointset[l]) <= 0)) {
                            lower.pop();
                        }
                        lower.push(pointset[l]);
                    }
                    lower.pop();
                    return lower;
                }

                function _lowerTangent(pointset) {
                    var reversed = pointset.reverse(),
                        upper = [];
                    for (var u = 0; u < reversed.length; u++) {
                        while (upper.length >= 2 && (_cross(upper[upper.length - 2], upper[upper.length - 1], reversed[u]) <= 0)) {
                            upper.pop();
                        }
                        upper.push(reversed[u]);
                    }
                    upper.pop();
                    return upper;
                }

                var convex,
                    upper = _upperTangent(pointset),
                    lower = _lowerTangent(pointset);
                convex = lower.concat(upper);
                convex.push(pointset[0]);
                return convex;
            }

            console.log("POINTSTORE")
            console.log(pointStore);
            pointStore.sort();
            var convexHull = convex(pointStore);
            var minX = convexHull.sort(function (a,b) {
                return a[0] < b[0] ? -1 : 1;
            })[0][0];
            var minY = convexHull.sort(function (a,b) {
                return a[1] < b[1] ? -1 : 1;
            })[0][0];
            //svg.append("text").text("BLAH BLAH").attr("text-anchor", "middle").attr("cx", x(0)).attr("y", minY);
            console.log("POINTSTORE")
            console.log(pointStore);
            console.log(convexHull);
            for (i in convexHull) {
                var i = parseInt(i);
                if (i + 1 == convexHull.length) {
                    var nextI = 0;
                } else {
                    var nextI = i + 1;
                }
                console.log(i, ',', nextI);
                svg.append("line")
                    .attr("x2", width)
                    .style("stroke", "#cc0000")
                    .style("stroke-width", "1px")
                    .attr("x1", convexHull[i][0])     // x position of the first end of the line
                    .attr("y1", convexHull[i][1])      // y position of the first end of the line
                    .attr("x2", convexHull[nextI][0])     // x position of the second end of the line
                    .attr("y2", convexHull[nextI][1]);    // y position of the second end of the line
            }*/

            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?\-!]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    var name = null;
                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt;
                    } else {
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt
                        }
                    }
                    //!!!

                });
                fullData.docs.labels.forEach(function (x) {

                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                    } else {
                        var name = null;
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                        }
                    }
                });
                console.log("docCounts");
                console.log(docCounts)
                var messages = [];
                if (ignoreCategories) {
                    var wordCount = getCorpusWordCounts();
                    console.log("wordCount")
                    console.log(wordCount)
                    messages.push(
                        '<b>Document count: </b>' + fullData.docs.texts.length.toLocaleString('en') +
                        '; <b>word count: </b>'
                        + wordCount['sums'].reduce((a, b) => a + b, 0).toLocaleString('en')
                    )
                } else if (unifiedContexts) {
                    fullData.docs.categories.forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            var message = '<b>' + x + '</b>: ';
                            message += 'document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en')
                            messages.push(message);
                        }
                    });
                } else {
                    [fullData.info.category_name,
                        fullData.info.not_category_name,
                        fullData.info.neutral_category_name,
                        fullData.info.extra_category_name].forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            messages.push('<b>' + x + '</b> document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en'));
                        }
                    });
                }

                if (showCorpusStats) {
                    d3.select('#' + divName + '-' + 'corpus-stats')
                        .style('width', width + margin.left + margin.right + 200)
                        .append('div')
                        .html(messages.join('<br />'));
                }
            }


            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById(divName);

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }

            function rerender(xCoords, yCoords, color) {
                labeledPoints.forEach(function (p) {
                    p.label.remove();
                    rectHolder.remove(p.rect);
                });
                pointRects.forEach(function (rect) {
                    rectHolder.remove(rect);
                });
                pointRects = []
                /*
                var circles = d3.select('#' + divName).selectAll('circle')
                    .attr("cy", function (d) {return y(yCoords[d.i])})
                    .transition(0)
                    .attr("cx", function (d) {return x(xCoords[d.i])})
                    .transition(0);
                */
                d3.select('#' + divName).selectAll("dot").remove();
                d3.select('#' + divName).selectAll("circle").remove();
                console.log(this.fullData)
                console.log(this)
                console.log("X/Y coords")
                console.log(this.fullData.data.filter(d => d.display === undefined || d.display === true).map(d => [d.x, d.y]))
                var circles = this.svg//.select('#' + divName)
                    .selectAll("dot")
                    .data(this.fullData.data.filter(d => d.display === undefined || d.display === true))
                    //.filter(function (d) {return d.display === undefined || d.display === true})
                    .enter()
                    .append("circle")
                    .attr("cy", d => d.y)
                    .attr("cx", d => d.x)
                    .attr("r", d => 2)
                    .on("mouseover", function (d) {
                        /*var mySVGMatrix = circle.getScreenCTM()n
                            .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                        var pageX = mySVGMatrix.e;
                        var pageY = mySVGMatrix.f;*/

                        /*showTooltip(
                            d,
                            d3.event.pageX,
                            d3.event.pageY
                        );*/
                        console.log("point MOUSOEVER")
                        console.log(d)
                        showToolTipForTerm(data, this, d.term, d, true);
                        d3.select(this).style("stroke", "black");
                    })
                    .on("click", function (d) {
                        var runDisplayTermContexts = true;
                        if (alternativeTermFunc != null) {
                            runDisplayTermContexts = alternativeTermFunc(d);
                        }
                        if (runDisplayTermContexts) {
                            displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                        }
                    })
                    .on("mouseout", function (d) {
                        tooltip.transition()
                            .duration(0)
                            .style("opacity", 0);
                        d3.select(this).style("stroke", null);
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    });

                if (color !== null) {
                    console.log("COLOR")
                    console.log(color)
                    circles.style("fill", d => color(d));
                }
                xCoords.forEach((xCoord, i) => censorCircle(xCoord, yCoords[i]))
                labeledPoints = [];
                labeledPoints = performPartialLabeling(
                    this.fullData.data,
                    labeledPoints,
                    (d => d.ox), //function (d) {return xCoords[d.ci]},
                    (d => d.oy) //function (d) {return yCoords[d.ci]}

                );
            }

            //return [performPartialLabeling, labeledPoints];
            return {
                ...payload,
                ...{
                    'rerender': rerender,
                    'performPartialLabeling': performPartialLabeling,
                    'showToolTipForTerm': showToolTipForTerm,
                    'svg': svg,
                    'data': data,
                    'xLabel': xLabel,
                    'yLabel': yLabel,
                    'drawXLabel': drawXLabel,
                    'drawYLabel': drawYLabel,
                    'populateCorpusStats': populateCorpusStats
                }
            };
        }


        //fullData = getDataAndInfo();
        if (fullData.docs) {
            var corpusWordCounts = getCorpusWordCounts();
        }
        var payload = processData(fullData);

        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            //.attr("class", getTooltipContent == null && sortByDist ? "tooltip" : "tooltipscore")
            .attr("class", "tooltipscore")
            .style("opacity", 0);

        plotInterface = {}
        if (payload.topTermsPane) {
            plotInterface.topTermsPane = payload.topTermsPane;
            plotInterface.showTopTermsPane = payload.showTopTermsPane;
            plotInterface.showAssociatedWordList = payload.showAssociatedWordList;
        }
        plotInterface.includeAllContexts = includeAllContexts;
        plotInterface.divName = divName;
        plotInterface.displayTermContexts = displayTermContexts;
        plotInterface.gatherTermContexts = gatherTermContexts;
        plotInterface.xLabel = payload.xLabel;
        plotInterface.yLabel = payload.yLabel;
        plotInterface.drawXLabel = payload.drawXLabel;
        plotInterface.drawYLabel = payload.drawYLabel;
        plotInterface.svg = payload.svg;
        plotInterface.termDict = termDict;
        plotInterface.showToolTipForTerm = payload.showToolTipForTerm;
        plotInterface.fullData = fullData;
        plotInterface.data = payload.data;
        plotInterface.rerender = payload.rerender;
        plotInterface.populateCorpusStats = payload.populateCorpusStats;
        plotInterface.handleSearch = handleSearch;
        plotInterface.handleSearchTerm = handleSearchTerm;
        plotInterface.highlightTerm = highlightTerm;
        plotInterface.y = y;
        plotInterface.x = x;
        plotInterface.tooltip = tooltip;
        plotInterface.alternativeTermFunc = alternativeTermFunc;

        plotInterface.showTooltipSimple = function (term) {
            plotInterface.showToolTipForTerm(
                plotInterface.data,
                plotInterface.svg,
                term.replace("'", "\\'"),
                plotInterface.termDict[term.replace("'", "\\'")]
            )
        };

        plotInterface.drawCategoryAssociation = function (category, otherCategory = null) {
            console.log("+++++++ Entering drawCategoryAssociation")
            console.log("Category: " + category)
            console.log("Other Category: " + otherCategory)
            var categoryNum = this.fullData.info.categories.indexOf(category);

            var otherCategoryNum = null;
            if(otherCategory !== null)
                otherCategoryNum = this.fullData.info.categories.indexOf(otherCategory);

            console.log("cat/other: " + category + "/" + otherCategory + " ::: " + categoryNum + "/" + otherCategoryNum)

            console.log("Full Data")
            console.log(this.fullData)
            /*
            var rawLogTermCounts = getTermCounts(this.fullData).map(Math.log);
            var maxRawLogTermCounts = Math.max(...rawLogTermCounts);
            var minRawLogTermCounts = Math.min(...rawLogTermCounts);
            var logTermCounts = rawLogTermCounts.map(
                x => (x - minRawLogTermCounts) / maxRawLogTermCounts
            )
            */

            //var rawScores = getCategoryDenseRankScores(this.fullData, categoryNum);
            //console.log("RAW SCORES")
            //console.log(rawScores);
            /*
            function logOddsRatioUninformativeDirichletPrior(fgFreqs, bgFreqs, alpha) {
                var fgVocabSize = fgFreqs.reduce((x,y) => x+y);
                var fgL = fgFreqs.map(x => (x + alpha)/((1+alpha)*fgVocabSize - x - alpha))
                var bgVocabSize = bgFreqs.reduce((x,y) => x+y);
                var bgL = bgFreqs.map(x => (x + alpha)/((1+alpha)*bgVocabSize - x - alpha))
                var pooledVar = fgFreqs.map(function(x, i) {
                    return (
                        1/(x + alpha)
                        + 1/((1+alpha)*fgVocabSize - x - alpha)
                        + 1/(bgFreqs[i] + alpha)
                        + 1/((1+alpha)*bgVocabSize - bgFreqs[i] - alpha))
                })
                return pooledVar.map(function(x, i) {
                    return (Math.log(fgL[i]) - Math.log(bgL[i]))/x;
                })
            }
            var rawScores = logOddsRatioUninformativeDirichletPrior(
                denseRanks.fgFreqs, denseRanks.bgFreqs, 0.01);
            */


            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            if (otherCategoryNum !== null) {
                var otherDenseRanks = getDenseRanks(this.fullData, otherCategoryNum);
                denseRanks.bg = otherDenseRanks.fg;
                denseRanks.bgFreqs = otherDenseRanks.fgFreqs;
            }

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            //!!! OLD and good
            var ox = denseRanks.bg;
            var oy = denseRanks.fg;

            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            var ox = ox.map(x => (x - oxmin) / (oxmax - oxmin))
            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            var oy = oy.map(x => (x - oymin) / (oymax - oymin))
            //var ox = logTermCounts
            //var oy = scores;
            var xf = this.x;
            var yf = this.y;

            this.fullData.data = this.fullData.data.map(function (term, i) {
                //term.ci = i;
                term.s = scores[term.i];
                term.os = rawScores[term.i];
                term.cat = denseRanks.fgFreqs[term.i];
                term.ncat = denseRanks.bgFreqs[term.i];
                term.cat25k = parseInt(denseRanks.fgFreqs[term.i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[term.i] * 25000 / bgFreqSum);
                term.x = xf(ox[term.i]) // logTermCounts[term.i];
                term.y = yf(oy[term.i]) // scores[term.i];
                term.ox = ox[term.i];
                term.oy = oy[term.i];
                term.display = true;
                return term;
            })

            // Feature selection
            var targetTermsToShow = 1500;

            var sortedBg = denseRanks.bg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedFg = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedScores = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]);
            var myFullData = this.fullData

            sortedBg.concat(sortedFg)//.concat(sortedScores.slice(0, parseInt(targetTermsToShow/2))).concat(sortedScores.slice(-parseInt(targetTermsToShow/4)))
                .forEach(function (i) {
                    myFullData.data[i].display = true;
                })

            console.log('newly filtered')
            console.log(myFullData)

            // begin rescaling to ignore hidden terms
            /*
            function scaleDenseRanks(ranks) {
                var max = Math.max(...ranks);
                return ranks.map(x=>x/max)
            }
            var filteredData = myFullData.data.filter(d=>d.display);
            var catRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.cat)))
            var ncatRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.ncat)))
            var rawScores = catRanks.map((x,i) => x - ncatRanks[i]);
            function stretch_0_1(scores) {
                var max = 1.*Math.max(...rawScores);
                var min = -1.*Math.min(...rawScores);
                return scores.map(function(x, i) {
                    if(x == 0) return 0.5;
                    if(x > 0) return (x/max + 1)/2;
                    return (x/min + 1)/2;
                })
            }
            var scores = stretch_0_1(rawScores);
            console.log(scores)
            filteredData.forEach(function(d, i) {
                d.x = xf(catRanks[i]);
                d.y = yf(ncatRanks[i]);
                d.ox = catRanks[i];
                d.oy = ncatRanks[i];
                d.s = scores[i];
                d.os = rawScores[i];
            });
            console.log("rescaled");
            */
            // end rescaling


            this.rerender(//denseRanks.bg,
                fullData.data.map(x => x.ox), //ox
                //denseRanks.fg,
                fullData.data.map(x => x.oy), //oy,
                d => d3.interpolateRdYlBu(d.s));
            if (this.yLabel !== undefined) {
                this.yLabel.remove()
            }
            if (this.xLabel !== undefined) {
                this.xLabel.remove()
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];
            if (otherCategoryNum !== null) {
                bottomName = this.fullData.info.categories[otherCategoryNum];
            }


            this.yLabel = this.drawYLabel(this.svg, leftName + ' Frequncy Rank')
            this.xLabel = this.drawXLabel(this.svg, bottomName + ' Frequency Rank')
            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (
                data,
                word,
                header,
                isUpperPane,
                xOffset=this.topTermsPane.startingOffset,
                length = 14
            ) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            if (otherCategoryNum === null) {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x !== this.fullData.info.categories[categoryNum]);
            } else {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x === this.fullData.info.categories[otherCategoryNum]);

                fullData.info.neutral_category_internal_names = this.fullData.info.categories
                    .filter(x => (x !== this.fullData.info.categories[categoryNum]
                        && x !== this.fullData.info.categories[otherCategoryNum]));
                fullData.info.neutral_category_name = "All Others";

            }
            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();

            console.log(fullData)
        };

        plotInterface.yAxisLogCounts = function (categoryName) {
            var categoryNum = this.fullData.docs.categories.indexOf(categoryName);
            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            console.log("denseRanks")
            console.log(denseRanks);

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            var oy = denseRanks.fgFreqs.map(count => Math.log(count + 1) / Math.log(2))

            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            oy = oy.map(y => (y - oymin) / (oymax - oymin))
            var xf = this.x;
            var yf = this.y;
            var ox = this.fullData.data.map(term => term.ox);
            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            ox = ox.map(y => (y - oxmin) / (oxmax - oxmin))


            this.fullData.data = this.fullData.data.map(function (term, i) {
                term.s = 1;//scores[i];
                term.os = rawScores[i];
                term.cat = denseRanks.fgFreqs[i];
                term.ncat = denseRanks.bgFreqs[i];
                term.cat25k = parseInt(denseRanks.fgFreqs[i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[i] * 25000 / bgFreqSum);
                //term.x = xf(term.ox) // scores[term.i];
                //term.ox = term.ox;
                term.y = yf(oy[i]) // scores[term.i];
                term.oy = oy[i];
                term.x = xf(ox[i]) // scores[term.i];
                term.ox = ox[i];
                term.display = true;
                return term;
            })


            this.rerender(//denseRanks.bg,
                this.fullData.data.map(point => point.ox), //ox
                this.fullData.data.map(point => point.oy), //oy,
                d => d3.interpolateRdYlBu(d.s)
            );

            if (this.yLabel !== undefined) {
                this.yLabel.remove()
                this.yLabel = this.drawYLabel(this.svg, this.fullData.info.categories[categoryNum] + ' log freq.')
            }

            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (data, word, header, isUpperPane, xOffset=this.topTermsPane.startingOffset, length = 14) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];

            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            fullData.info.not_category_internal_names = this.fullData.info.categories
                .filter(x => x !== this.fullData.info.categories[categoryNum]);

            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();
        };

        return plotInterface
    };
}(d3);

; 
 
 // Adapted from https://www.w3schools.com/howto/howto_js_autocomplete.asp
function autocomplete(inputField, autocompleteValues, myPlotInterface) {
    var currentFocus; // current position in autocomplete list.

    inputField.addEventListener("input", function (e) {
        var matchedCandidateListDiv, matchedCandidateDiv, i, userInput = this.value;

        closeAllLists();
        if (!userInput) {
            return false;
        }
        currentFocus = -1;

        matchedCandidateListDiv = document.createElement("div");
        matchedCandidateListDiv.setAttribute("id", this.id + "autocomplete-list");
        matchedCandidateListDiv.setAttribute("class", "autocomplete-items");

        this.parentNode.appendChild(matchedCandidateListDiv);
        autocompleteValues.map(function (candidate) {
            var candidatePrefix = candidate.substr(0, userInput.length);
            if (candidatePrefix.toLowerCase() === userInput.toLowerCase()) {
                matchedCandidateDiv = document.createElement("div");
                matchedCandidateDiv.innerHTML = "<strong>" + candidatePrefix + "</strong>";
                matchedCandidateDiv.innerHTML += candidate.substr(userInput.length);
                matchedCandidateDiv.innerHTML += '<input type=hidden value="' + encodeURIComponent(candidate) + '">';
                matchedCandidateDiv.addEventListener("click", function (e) {
                    console.log("CLICK")
                    console.log(this.getElementsByTagName("input")[0].value)
                    inputField.value = decodeURIComponent(this.getElementsByTagName("input")[0].value);
                    console.log(inputField.value)
                    closeAllLists();
                    myPlotInterface.handleSearchTerm(inputField.value);
                });
                matchedCandidateListDiv.appendChild(matchedCandidateDiv);
            }
        });
    });

    inputField.addEventListener("keydown", function (keyboardEvent) {

        var candidateDivList = document.getElementById(this.id + "autocomplete-list");

        if (!candidateDivList)
            return true;

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList.children,
            x => x.className !== ""
        );

        if (keyboardEvent.keyCode === 40 || keyboardEvent.keyCode === 9) { // down or tab
            keyboardEvent.preventDefault();
            currentFocus++;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 38) { //up
            currentFocus--;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 13) { // enter
            keyboardEvent.preventDefault();
            var selectedTerm = inputField.value;
            console.log("selected term");console.log(selectedTerm);
            console.log(myPlotInterface);
            //if (selectedCandidate)
            //    selectedTerm = selectedCandidate.children[1].value;
            myPlotInterface.handleSearchTerm(selectedTerm);
            closeAllLists(null);
        } else if (keyboardEvent.keyCode === 27) { // esc
            closeAllLists(null);
        }
    });

    function addActive(candidateDivList) {
        if (!candidateDivList) return false;

        removeActive(candidateDivList);

        if (currentFocus >= candidateDivList.length)
            currentFocus = 0;
        if (currentFocus < 0)
            currentFocus = (candidateDivList.length - 1);

        candidateDivList[currentFocus].classList.add("autocomplete-active");

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList,
            x => x.className !== ""
        );

        if (selectedCandidate) {
            var candidateValue = decodeURIComponent(selectedCandidate.children[1].value);

            myPlotInterface.highlightTerm(candidateValue);
            inputField.value = candidateValue;
        }

    }

    function removeActive(candidateDivList) {
        Array.prototype.find.call(
            candidateDivList,
            x => x.classList.remove("autocomplete-active")
        );
    }

    function closeAllLists(elmnt) {
        /*close all autocomplete lists in the document,
        except the one passed as an argument:*/
        var x = document.getElementsByClassName("autocomplete-items");
        for (var i = 0; i < x.length; i++) {
            if (elmnt != x[i] && elmnt != inputField) {
                x[i].parentNode.removeChild(x[i]);
            }
        }
    }

    /*execute a function when someone clicks in the document:*/
    document.addEventListener("click", function (e) {
        closeAllLists(e.target);
    });
}

function getDataAndInfo() { return{"info": {"category_name": "Data scientist", "not_category_name": "Data Engineer", "category_terms": ["statistical", "quantitative", "statistics", "r", "math", "mining", "re", "master", "machine", "algorithms"], "not_category_terms": ["self", "dimensional", "implement", "distributed", "excited", "scala", "spark", "design", "good", "excellent"], "category_internal_name": "data scientist", "not_category_internal_names": ["data engineer"], "categories": ["data scientist", "data engineer"], "neutral_category_internal_names": [], "extra_category_internal_names": [], "neutral_category_name": "Neutral", "extra_category_name": "Extra"}, "data": [{"x": 0.29323308270676685, "y": 0.4814814814814815, "ox": 0.29323308270676685, "oy": 0.4814814814814815, "term": "2", "cat25k": 48, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 39, "s": 0.8494623655913979, "os": 0.18678665496049168, "bg": 0.0}, {"x": 0.9097744360902255, "y": 0.8814814814814816, "ox": 0.9097744360902255, "oy": 0.8814814814814816, "term": "years", "cat25k": 202, "ncat25k": 213, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 287, "ncat": 264, "s": 0.4548387096774193, "os": -0.028094820017559252, "bg": 3.2612327546818368e-06}, {"x": 0.9699248120300752, "y": 0.9703703703703705, "ox": 0.9699248120300752, "oy": 0.9703703703703705, "term": "of", "cat25k": 657, "ncat25k": 636, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 936, "ncat": 789, "s": 0.5806451612903225, "os": 0.0004389815627743321, "bg": 2.6231728544956235e-07}, {"x": 0.17293233082706763, "y": 0.3111111111111111, "ox": 0.17293233082706763, "oy": 0.3111111111111111, "term": "relevant", "cat25k": 30, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 23, "s": 0.7741935483870968, "os": 0.137071992976295, "bg": 2.869164657055921e-06}, {"x": 0.9849624060150376, "y": 0.9777777777777779, "ox": 0.9849624060150376, "oy": 0.9777777777777779, "term": "experience", "cat25k": 724, "ncat25k": 782, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1031, "ncat": 970, "s": 0.567741935483871, "os": -0.007133450395083396, "bg": 2.916828111377704e-05}, {"x": 0.35338345864661647, "y": 0.37777777777777777, "ox": 0.35338345864661647, "oy": 0.37777777777777777, "term": "preferred", "cat25k": 37, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 48, "s": 0.6021505376344086, "os": 0.024143985952589986, "bg": 1.0249894976633029e-05}, {"x": 0.022556390977443604, "y": 0.12592592592592594, "ox": 0.022556390977443604, "oy": 0.12592592592592594, "term": "healthcare", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.7376344086021506, "os": 0.10250219490781387, "bg": 1.398755995374174e-06}, {"x": 0.5789473684210525, "y": 0.26666666666666666, "ox": 0.5789473684210525, "oy": 0.26666666666666666, "term": "industry", "cat25k": 25, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 89, "s": 0.05483870967741935, "os": -0.31003072870939424, "bg": 1.5539540394721358e-06}, {"x": 0.8045112781954886, "y": 0.7925925925925926, "ox": 0.8045112781954886, "oy": 0.7925925925925926, "term": "knowledge", "cat25k": 129, "ncat25k": 128, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 184, "ncat": 159, "s": 0.5505376344086022, "os": -0.011852502194907855, "bg": 7.637764819036802e-06}, {"x": 0.0, "y": 0.2962962962962963, "ox": 0.0, "oy": 0.2962962962962963, "term": "pursuing", "cat25k": 28, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 0, "s": 0.9451612903225807, "os": 0.2940079016681299, "bg": 1.9196888568300852e-05}, {"x": 0.9248120300751879, "y": 0.9481481481481482, "ox": 0.9248120300751879, "oy": 0.9481481481481482, "term": "a", "cat25k": 622, "ncat25k": 331, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 885, "ncat": 411, "s": 0.6010752688172043, "os": 0.023156277436347628, "bg": 2.854234770399962e-07}, {"x": 0.05263157894736841, "y": 0.5259259259259259, "ox": 0.05263157894736841, "oy": 0.5259259259259259, "term": "phd", "cat25k": 54, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 7, "s": 0.986021505376344, "os": 0.4697102721685689, "bg": 1.4319135342421667e-05}, {"x": 0.9473684210526316, "y": 0.962962962962963, "ox": 0.9473684210526316, "oy": 0.962962962962963, "term": "or", "cat25k": 644, "ncat25k": 419, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 917, "ncat": 520, "s": 0.5956989247311828, "os": 0.015474100087796372, "bg": 1.1093062197998836e-06}, {"x": 0.21804511278195488, "y": 0.2962962962962963, "ox": 0.21804511278195488, "oy": 0.2962962962962963, "term": "ms", "cat25k": 28, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 29, "s": 0.689247311827957, "os": 0.07758999122036872, "bg": 2.0167567643008333e-06}, {"x": 0.9624060150375938, "y": 0.9925925925925927, "ox": 0.9624060150375938, "oy": 0.9925925925925927, "term": "in", "cat25k": 772, "ncat25k": 569, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1099, "ncat": 706, "s": 0.6053763440860216, "os": 0.02996049165935033, "bg": 4.2623662609868577e-07}, {"x": 0.05263157894736841, "y": 0.6518518518518518, "ox": 0.05263157894736841, "oy": 0.6518518518518518, "term": "math", "cat25k": 76, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 7, "s": 0.9956989247311827, "os": 0.594710272168569, "bg": 8.014163186690524e-06}, {"x": 0.15037593984962405, "y": 0.7925925925925926, "ox": 0.15037593984962405, "oy": 0.7925925925925926, "term": "statistics", "cat25k": 129, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 184, "ncat": 20, "s": 0.9978494623655914, "os": 0.6374012291483757, "bg": 5.890691207786755e-06}, {"x": 0.022556390977443604, "y": 0.28888888888888886, "ox": 0.022556390977443604, "oy": 0.28888888888888886, "term": "physics", "cat25k": 27, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 3, "s": 0.9204301075268817, "os": 0.26426690079016685, "bg": 2.819547357923009e-06}, {"x": 0.007518796992481202, "y": 0.25925925925925924, "ox": 0.007518796992481202, "oy": 0.25925925925925924, "term": "economics", "cat25k": 25, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.9032258064516129, "os": 0.24978050921861283, "bg": 2.3273255364048987e-06}, {"x": 0.5187969924812029, "y": 0.7925925925925926, "ox": 0.5187969924812029, "oy": 0.7925925925925926, "term": "other", "cat25k": 129, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 184, "ncat": 79, "s": 0.9290322580645161, "os": 0.271729587357331, "bg": 5.375307038994305e-07}, {"x": 0.08270676691729321, "y": 0.7703703703703704, "ox": 0.08270676691729321, "oy": 0.7703703703703704, "term": "quantitative", "cat25k": 119, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 170, "ncat": 11, "s": 0.9989247311827957, "os": 0.6825065847234416, "bg": 5.2284492924911523e-05}, {"x": 0.36842105263157887, "y": 0.837037037037037, "ox": 0.36842105263157887, "oy": 0.837037037037037, "term": "field", "cat25k": 152, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 216, "ncat": 51, "s": 0.9849462365591398, "os": 0.4651009657594381, "bg": 3.94513519690192e-06}, {"x": 0.9774436090225562, "y": 0.9407407407407409, "ox": 0.9774436090225562, "oy": 0.9407407407407409, "term": "with", "cat25k": 571, "ncat25k": 679, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 813, "ncat": 843, "s": 0.41612903225806447, "os": -0.03643546971027223, "bg": 1.04046912777588e-06}, {"x": 0.4135338345864661, "y": 0.7185185185185184, "ox": 0.4135338345864661, "oy": 0.7185185185185184, "term": "an", "cat25k": 91, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 130, "ncat": 60, "s": 0.9494623655913978, "os": 0.3026777875329236, "bg": 2.502743064027126e-07}, {"x": 0.6842105263157894, "y": 0.6074074074074074, "ox": 0.6842105263157894, "oy": 0.6074074074074074, "term": "we", "cat25k": 67, "ncat25k": 88, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 95, "ncat": 109, "s": 0.22043010752688172, "os": -0.07627304653204559, "bg": 2.933712576659069e-07}, {"x": 0.17293233082706763, "y": 0.3259259259259259, "ox": 0.17293233082706763, "oy": 0.3259259259259259, "term": "will", "cat25k": 31, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 23, "s": 0.7881720430107526, "os": 0.15177787532923617, "bg": 9.879376652236813e-08}, {"x": 0.060150375939849614, "y": 0.12592592592592594, "ox": 0.060150375939849614, "oy": 0.12592592592592594, "term": "also", "cat25k": 12, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.656989247311828, "os": 0.06518876207199298, "bg": 8.105081565504544e-08}, {"x": 0.06766917293233082, "y": 0.274074074074074, "ox": 0.06766917293233082, "oy": 0.274074074074074, "term": "candidates", "cat25k": 26, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 9, "s": 0.8634408602150537, "os": 0.20478489903424058, "bg": 4.57004677095149e-06}, {"x": 0.05263157894736841, "y": 0.5333333333333333, "ox": 0.05263157894736841, "oy": 0.5333333333333333, "term": "who", "cat25k": 55, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 7, "s": 0.9870967741935484, "os": 0.4770632133450395, "bg": 2.6941596295983127e-07}, {"x": 0.9172932330827067, "y": 0.6814814814814815, "ox": 0.9172932330827067, "oy": 0.6814814814814815, "term": "are", "cat25k": 85, "ncat25k": 298, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 121, "ncat": 370, "s": 0.07526881720430106, "os": -0.2340869183494293, "bg": 4.1024654977095344e-07}, {"x": 0.5864661654135337, "y": 0.7555555555555555, "ox": 0.5864661654135337, "oy": 0.7555555555555555, "term": "business", "cat25k": 106, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 151, "ncat": 90, "s": 0.8247311827956989, "os": 0.16780070237050038, "bg": 7.564325141693895e-07}, {"x": 0.6992481203007518, "y": 0.9111111111111111, "ox": 0.6992481203007518, "oy": 0.9111111111111111, "term": "ability", "cat25k": 248, "ncat25k": 93, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 353, "ncat": 115, "s": 0.867741935483871, "os": 0.21027216856892006, "bg": 1.797513623847383e-05}, {"x": 0.9548872180451127, "y": 0.9851851851851854, "ox": 0.9548872180451127, "oy": 0.9851851851851854, "term": "to", "cat25k": 751, "ncat25k": 442, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1069, "ncat": 548, "s": 0.6064516129032258, "os": 0.03007023705004397, "bg": 2.664568486020676e-07}, {"x": 1.0, "y": 1.0, "ox": 1.0, "oy": 1.0, "term": "and", "cat25k": 1476, "ncat25k": 1662, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2102, "ncat": 2063, "s": 0.5795698924731183, "os": 0.0, "bg": 6.408821590369186e-07}, {"x": 0.04511278195488721, "y": 0.02962962962962963, "ox": 0.04511278195488721, "oy": 0.02962962962962963, "term": "execute", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.5193548387096775, "os": -0.01547410008779631, "bg": 2.2816183244981467e-06}, {"x": 0.7969924812030074, "y": 0.6962962962962963, "ox": 0.7969924812030074, "oy": 0.6962962962962963, "term": "at", "cat25k": 87, "ncat25k": 117, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 145, "s": 0.17849462365591398, "os": -0.0999780509218613, "bg": 2.367603218738445e-07}, {"x": 0.25563909774436083, "y": 0.18518518518518517, "ox": 0.25563909774436083, "oy": 0.18518518518518517, "term": "multiple", "cat25k": 18, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 34, "s": 0.24516129032258063, "os": -0.07001755926251096, "bg": 1.7814389400933486e-06}, {"x": 0.45112781954887204, "y": 0.48888888888888893, "ox": 0.45112781954887204, "oy": 0.48888888888888893, "term": "from", "cat25k": 50, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 65, "s": 0.6096774193548387, "os": 0.03742317822651453, "bg": 1.1952562408411762e-07}, {"x": 0.04511278195488721, "y": 0.037037037037037035, "ox": 0.04511278195488721, "oy": 0.037037037037037035, "term": "strategy", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5623655913978495, "os": -0.00812115891132572, "bg": 3.843644927086667e-07}, {"x": 0.09774436090225562, "y": 0.05185185185185185, "ox": 0.09774436090225562, "oy": 0.05185185185185185, "term": "vision", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 13, "s": 0.35268817204301073, "os": -0.0456540825285338, "bg": 9.196932795321962e-07}, {"x": 0.037593984962406006, "y": 0.0962962962962963, "ox": 0.037593984962406006, "oy": 0.0962962962962963, "term": "execution", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.6419354838709678, "os": 0.05816505706760316, "bg": 1.645575493175547e-06}, {"x": 0.06766917293233082, "y": 0.11851851851851852, "ox": 0.06766917293233082, "oy": 0.11851851851851852, "term": "interpersonal", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 9, "s": 0.624731182795699, "os": 0.050373134328358216, "bg": 1.9854679628860442e-05}, {"x": 0.9022556390977443, "y": 0.7851851851851852, "ox": 0.9022556390977443, "oy": 0.7851851851851852, "term": "skills", "cat25k": 129, "ncat25k": 197, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 183, "ncat": 244, "s": 0.16021505376344086, "os": -0.1162203687445128, "bg": 1.1952214207862943e-05}, {"x": 0.21804511278195488, "y": 0.43703703703703706, "ox": 0.21804511278195488, "oy": 0.43703703703703706, "term": "demonstrated", "cat25k": 43, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 29, "s": 0.8720430107526881, "os": 0.2172958735733099, "bg": 1.5269388921600256e-05}, {"x": 0.08270676691729321, "y": 0.21481481481481485, "ox": 0.08270676691729321, "oy": 0.21481481481481485, "term": "communicate", "cat25k": 20, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 11, "s": 0.7698924731182796, "os": 0.1310359964881475, "bg": 6.125557856468225e-06}, {"x": 0.4285714285714285, "y": 0.8222222222222222, "ox": 0.4285714285714285, "oy": 0.8222222222222222, "term": "technical", "cat25k": 146, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 208, "ncat": 62, "s": 0.9677419354838709, "os": 0.3906935908691835, "bg": 5.249838757035731e-06}, {"x": 0.06766917293233082, "y": 0.06666666666666667, "ox": 0.06766917293233082, "oy": 0.06666666666666667, "term": "general", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.5763440860215053, "os": -0.0010974539069358996, "bg": 1.1544937730486264e-07}, {"x": 0.030075187969924807, "y": 0.11851851851851852, "ox": 0.030075187969924807, "oy": 0.11851851851851852, "term": "audiences", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.7107526881720431, "os": 0.08768656716417911, "bg": 8.093532094294505e-06}, {"x": 0.5037593984962404, "y": 0.548148148148148, "ox": 0.5037593984962404, "oy": 0.548148148148148, "term": "including", "cat25k": 59, "ncat25k": 61, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 76, "s": 0.6193548387096774, "os": 0.044007901668129956, "bg": 1.493493121166705e-06}, {"x": 0.0, "y": 0.19259259259259262, "ox": 0.0, "oy": 0.19259259259259262, "term": "tell", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 0, "s": 0.8591397849462366, "os": 0.1910667251975417, "bg": 4.743619801719064e-07}, {"x": 0.9924812030075187, "y": 0.9555555555555557, "ox": 0.9924812030075187, "oy": 0.9555555555555557, "term": "data", "cat25k": 635, "ncat25k": 934, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 904, "ncat": 1159, "s": 0.4150537634408602, "os": -0.03665496049165928, "bg": 1.0138151056101713e-05}, {"x": 0.09022556390977442, "y": 0.05185185185185185, "ox": 0.09022556390977442, "oy": 0.05185185185185185, "term": "write", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.39032258064516123, "os": -0.038191395961369626, "bg": 2.9989186294016935e-07}, {"x": 0.16541353383458646, "y": 0.28148148148148144, "ox": 0.16541353383458646, "oy": 0.28148148148148144, "term": "code", "cat25k": 27, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 22, "s": 0.7537634408602151, "os": 0.1151229148375768, "bg": 4.794010325147678e-07}, {"x": 0.6917293233082705, "y": 0.674074074074074, "ox": 0.6917293233082705, "oy": 0.674074074074074, "term": "development", "cat25k": 81, "ncat25k": 92, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 116, "ncat": 114, "s": 0.4989247311827956, "os": -0.017559262510974505, "bg": 1.6063762689447112e-06}, {"x": 0.8947368421052632, "y": 0.6666666666666666, "ox": 0.8947368421052632, "oy": 0.6666666666666666, "term": "sql", "cat25k": 78, "ncat25k": 183, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 111, "ncat": 227, "s": 0.07956989247311827, "os": -0.22640474100087804, "bg": 2.588580080730767e-05}, {"x": 0.8646616541353382, "y": 0.8296296296296296, "ox": 0.8646616541353382, "oy": 0.8296296296296296, "term": "python", "cat25k": 151, "ncat25k": 152, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 215, "ncat": 189, "s": 0.4172043010752688, "os": -0.034789288849868205, "bg": 4.5706132309184116e-05}, {"x": 0.18796992481203004, "y": 0.8074074074074075, "ox": 0.18796992481203004, "oy": 0.8074074074074075, "term": "r", "cat25k": 133, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 190, "ncat": 25, "s": 0.9967741935483871, "os": 0.6147936786654961, "bg": 1.328794266884379e-06}, {"x": 0.8421052631578946, "y": 0.34074074074074073, "ox": 0.8421052631578946, "oy": 0.34074074074074073, "term": "design", "cat25k": 32, "ncat25k": 136, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 169, "s": 0.006451612903225806, "os": -0.4976953467954346, "bg": 1.6256122391211337e-06}, {"x": 0.7669172932330827, "y": 0.6592592592592592, "ox": 0.7669172932330827, "oy": 0.6592592592592592, "term": "tools", "cat25k": 77, "ncat25k": 110, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 137, "s": 0.16881720430107527, "os": -0.10689201053555752, "bg": 2.6505294532247033e-06}, {"x": 0.8345864661654133, "y": 0.5629629629629629, "ox": 0.8345864661654133, "oy": 0.5629629629629629, "term": "e", "cat25k": 60, "ncat25k": 135, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 167, "s": 0.06559139784946236, "os": -0.26964442493415275, "bg": 8.530674097205252e-07}, {"x": 0.12781954887218044, "y": 0.22962962962962963, "ox": 0.12781954887218044, "oy": 0.22962962962962963, "term": "tableau", "cat25k": 22, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 17, "s": 0.7333333333333333, "os": 0.1009657594381036, "bg": 0.00016788910924334476}, {"x": 0.6015037593984962, "y": 0.7777777777777778, "ox": 0.6015037593984962, "oy": 0.7777777777777778, "term": "is", "cat25k": 121, "ncat25k": 74, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 172, "ncat": 92, "s": 0.832258064516129, "os": 0.17493415276558388, "bg": 1.1220169527364668e-07}, {"x": 0.4736842105263157, "y": 0.7185185185185184, "ox": 0.4736842105263157, "oy": 0.7185185185185184, "term": "plus", "cat25k": 91, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 130, "ncat": 70, "s": 0.8978494623655914, "os": 0.2429762949956102, "bg": 4.2195151266558144e-06}, {"x": 0.3984962406015037, "y": 0.28148148148148144, "ox": 0.3984962406015037, "oy": 0.28148148148148144, "term": "building", "cat25k": 27, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 58, "s": 0.16129032258064516, "os": -0.11622036874451275, "bg": 1.496275303184342e-06}, {"x": 0.45112781954887204, "y": 0.05185185185185185, "ox": 0.45112781954887204, "oy": 0.05185185185185185, "term": "pipelines", "cat25k": 5, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 65, "s": 0.040860215053763436, "os": -0.3964003511852502, "bg": 6.913033083664023e-05}, {"x": 0.060150375939849614, "y": 0.04444444444444444, "ox": 0.060150375939849614, "oy": 0.04444444444444444, "term": "stock", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5096774193548388, "os": -0.015693590869183496, "bg": 1.5876366290314758e-07}, {"x": 0.05263157894736841, "y": 0.22962962962962963, "ox": 0.05263157894736841, "oy": 0.22962962962962963, "term": "computational", "cat25k": 22, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 7, "s": 0.8344086021505377, "os": 0.17559262510974538, "bg": 1.161620723802816e-05}, {"x": 0.6015037593984962, "y": 0.8666666666666668, "ox": 0.6015037593984962, "oy": 0.8666666666666668, "term": "computer", "cat25k": 189, "ncat25k": 74, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 269, "ncat": 92, "s": 0.9161290322580645, "os": 0.26316944688323096, "bg": 3.2196987914700053e-06}, {"x": 0.6466165413533833, "y": 0.9259259259259259, "ox": 0.6466165413533833, "oy": 0.9259259259259259, "term": "science", "cat25k": 280, "ncat25k": 83, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 399, "ncat": 103, "s": 0.9333333333333333, "os": 0.2772168568920106, "bg": 5.760167662645936e-06}, {"x": 0.5112781954887217, "y": 0.8148148148148148, "ox": 0.5112781954887217, "oy": 0.8148148148148148, "term": "related", "cat25k": 140, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 199, "ncat": 77, "s": 0.9473118279569892, "os": 0.3012510974539069, "bg": 2.3875957239354383e-06}, {"x": 0.09022556390977442, "y": 0.07407407407407408, "ox": 0.09022556390977442, "oy": 0.07407407407407408, "term": "7", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 12, "s": 0.503225806451613, "os": -0.016132572431957856, "bg": 0.0}, {"x": 0.8421052631578946, "y": 0.9037037037037038, "ox": 0.8421052631578946, "oy": 0.9037037037037038, "term": "for", "cat25k": 240, "ncat25k": 136, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 342, "ncat": 169, "s": 0.6526881720430107, "os": 0.06112818261633013, "bg": 1.7224556364869803e-07}, {"x": 0.15789473684210525, "y": 0.2, "ox": 0.15789473684210525, "oy": 0.2, "term": "project", "cat25k": 19, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 21, "s": 0.6118279569892473, "os": 0.04170324846356452, "bg": 4.0793811364648474e-07}, {"x": 0.21804511278195488, "y": 0.22962962962962963, "ox": 0.21804511278195488, "oy": 0.22962962962962963, "term": "based", "cat25k": 22, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 29, "s": 0.5881720430107527, "os": 0.01141352063213344, "bg": 4.752994123893169e-07}, {"x": 0.7894736842105262, "y": 0.8962962962962963, "ox": 0.7894736842105262, "oy": 0.8962962962962963, "term": "work", "cat25k": 207, "ncat25k": 115, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 294, "ncat": 143, "s": 0.7462365591397849, "os": 0.10601404741000886, "bg": 2.0831766382484826e-06}, {"x": 0.3759398496240601, "y": 0.8740740740740741, "ox": 0.3759398496240601, "oy": 0.8740740740740741, "term": "analysis", "cat25k": 190, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 270, "ncat": 53, "s": 0.9881720430107527, "os": 0.4944029850746269, "bg": 5.167295842260983e-06}, {"x": 0.060150375939849614, "y": 0.2, "ox": 0.060150375939849614, "oy": 0.2, "term": "fields", "cat25k": 19, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7774193548387097, "os": 0.13871817383669885, "bg": 1.7171534727258012e-06}, {"x": 0.5714285714285713, "y": 0.5111111111111111, "ox": 0.5714285714285713, "oy": 0.5111111111111111, "term": "proficiency", "cat25k": 53, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 88, "s": 0.2913978494623655, "os": -0.05992098331870066, "bg": 9.206542858433872e-05}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "matlab", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 1.8144131798973385e-05}, {"x": 0.6240601503759398, "y": 0.7703703703703704, "ox": 0.6240601503759398, "oy": 0.7703703703703704, "term": "strong", "cat25k": 119, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 170, "ncat": 95, "s": 0.7795698924731183, "os": 0.14519315188762072, "bg": 8.683308484052767e-06}, {"x": 0.04511278195488721, "y": 0.21481481481481485, "ox": 0.04511278195488721, "oy": 0.21481481481481485, "term": "help", "cat25k": 20, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 6, "s": 0.8258064516129032, "os": 0.1683494293239684, "bg": 1.1454355603807847e-07}, {"x": 0.037593984962406006, "y": 0.16296296296296298, "ox": 0.037593984962406006, "oy": 0.16296296296296298, "term": "questions", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.764516129032258, "os": 0.12434152765583847, "bg": 3.4445165879307586e-07}, {"x": 0.5263157894736841, "y": 0.3555555555555555, "ox": 0.5263157894736841, "oy": 0.3555555555555555, "term": "analytical", "cat25k": 34, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 80, "s": 0.10537634408602149, "os": -0.16955662862159787, "bg": 3.09599455104959e-05}, {"x": 0.030075187969924807, "y": 0.08888888888888888, "ox": 0.030075187969924807, "oy": 0.08888888888888888, "term": "approach", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.6451612903225806, "os": 0.05827480245829675, "bg": 4.982442029987636e-07}, {"x": 0.007518796992481202, "y": 0.3333333333333333, "ox": 0.007518796992481202, "oy": 0.3333333333333333, "term": "linear", "cat25k": 32, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 1, "s": 0.9559139784946237, "os": 0.32330992098331873, "bg": 3.9979090935440765e-06}, {"x": 0.060150375939849614, "y": 0.4740740740740741, "ox": 0.060150375939849614, "oy": 0.4740740740740741, "term": "non", "cat25k": 48, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 8, "s": 0.9698924731182795, "os": 0.4107769973661106, "bg": 7.748113163680308e-07}, {"x": 0.022556390977443604, "y": 0.45925925925925926, "ox": 0.022556390977443604, "oy": 0.45925925925925926, "term": "regression", "cat25k": 45, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 3, "s": 0.9774193548387097, "os": 0.4333845478489904, "bg": 2.0998072878356285e-05}, {"x": 0.3834586466165413, "y": 0.4222222222222222, "ox": 0.3834586466165413, "oy": 0.4222222222222222, "term": "models", "cat25k": 41, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 54, "s": 0.610752688172043, "os": 0.03841088674275678, "bg": 2.6084807711684853e-06}, {"x": 0.0, "y": 0.07407407407407408, "ox": 0.0, "oy": 0.07407407407407408, "term": "reduction", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.6870967741935484, "os": 0.0734196663740123, "bg": 6.891457986140244e-07}, {"x": 0.0150375939849624, "y": 0.3555555555555555, "ox": 0.0150375939849624, "oy": 0.3555555555555555, "term": "clustering", "cat25k": 34, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 2, "s": 0.9591397849462365, "os": 0.3379060579455663, "bg": 3.718552258236684e-05}, {"x": 0.037593984962406006, "y": 0.14074074074074075, "ox": 0.037593984962406006, "oy": 0.14074074074074075, "term": "ai", "cat25k": 13, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.7365591397849462, "os": 0.1022827041264267, "bg": 3.897357567210946e-06}, {"x": 0.34586466165413526, "y": 0.8518518518518519, "ox": 0.34586466165413526, "oy": 0.8518518518518519, "term": "machine", "cat25k": 176, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 251, "ncat": 47, "s": 0.9913978494623655, "os": 0.5021949078138718, "bg": 8.546833644071444e-06}, {"x": 0.4661654135338345, "y": 0.888888888888889, "ox": 0.4661654135338345, "oy": 0.888888888888889, "term": "learning", "cat25k": 204, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 290, "ncat": 68, "s": 0.9720430107526881, "os": 0.4195566286215978, "bg": 6.08117653847439e-06}, {"x": 0.060150375939849614, "y": 0.2962962962962963, "ox": 0.060150375939849614, "oy": 0.2962962962962963, "term": "methods", "cat25k": 28, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 8, "s": 0.8827956989247312, "os": 0.2343064091308165, "bg": 1.091344301654295e-06}, {"x": 0.20300751879699244, "y": 0.17777777777777778, "ox": 0.20300751879699244, "oy": 0.17777777777777778, "term": "develop", "cat25k": 17, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 27, "s": 0.45698924731182794, "os": -0.02513169446883229, "bg": 1.8319538660853938e-06}, {"x": 0.037593984962406006, "y": 0.2518518518518518, "ox": 0.037593984962406006, "oy": 0.2518518518518518, "term": "apply", "cat25k": 24, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 5, "s": 0.8698924731182796, "os": 0.2125768217734855, "bg": 1.020826322786749e-06}, {"x": 0.007518796992481202, "y": 0.3111111111111111, "ox": 0.007518796992481202, "oy": 0.3111111111111111, "term": "predictive", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 1, "s": 0.9483870967741935, "os": 0.30125109745390694, "bg": 3.304983492376056e-05}, {"x": 0.06766917293233082, "y": 0.5703703703703703, "ox": 0.06766917293233082, "oy": 0.5703703703703703, "term": "algorithms", "cat25k": 61, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 9, "s": 0.9903225806451613, "os": 0.49890254609306406, "bg": 1.8873384752821573e-05}, {"x": 0.17293233082706763, "y": 0.22222222222222218, "ox": 0.17293233082706763, "oy": 0.22222222222222218, "term": "identify", "cat25k": 21, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 23, "s": 0.621505376344086, "os": 0.04883669885864794, "bg": 2.9508918973324576e-06}, {"x": 0.0, "y": 0.08148148148148147, "ox": 0.0, "oy": 0.08148148148148147, "term": "bayesian", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.7043010752688171, "os": 0.08077260755048289, "bg": 1.409969897142696e-05}, {"x": 0.8120300751879698, "y": 0.3333333333333333, "ox": 0.8120300751879698, "oy": 0.3333333333333333, "term": "excellent", "cat25k": 32, "ncat25k": 130, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 161, "s": 0.00967741935483871, "os": -0.47519754170324846, "bg": 6.823032916298079e-06}, {"x": 0.5714285714285713, "y": 0.25925925925925924, "ox": 0.5714285714285713, "oy": 0.25925925925925924, "term": "written", "cat25k": 25, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 88, "s": 0.05591397849462365, "os": -0.3099209833187006, "bg": 2.5070282396552676e-06}, {"x": 0.022556390977443604, "y": 0.11111111111111112, "ox": 0.022556390977443604, "oy": 0.11111111111111112, "term": "oral", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.7129032258064516, "os": 0.08779631255487269, "bg": 8.568514868348816e-07}, {"x": 0.6240601503759398, "y": 0.6148148148148148, "ox": 0.6240601503759398, "oy": 0.6148148148148148, "term": "communication", "cat25k": 68, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 95, "s": 0.5526881720430108, "os": -0.00921861281826164, "bg": 5.958484538283472e-06}, {"x": 0.5338345864661653, "y": 0.5407407407407407, "ox": 0.5338345864661653, "oy": 0.5407407407407407, "term": "team", "cat25k": 56, "ncat25k": 65, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 81, "s": 0.5860215053763441, "os": 0.0068042142230025915, "bg": 1.8747617575174178e-06}, {"x": 0.32330827067669166, "y": 0.674074074074074, "ox": 0.32330827067669166, "oy": 0.674074074074074, "term": "bachelor", "cat25k": 81, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 116, "ncat": 43, "s": 0.9612903225806452, "os": 0.34811237928007027, "bg": 2.5639609487784298e-05}, {"x": 0.6691729323308269, "y": 0.8592592592592592, "ox": 0.6691729323308269, "oy": 0.8592592592592592, "term": "s", "cat25k": 179, "ncat25k": 86, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 255, "ncat": 107, "s": 0.8505376344086021, "os": 0.1886523266022827, "bg": 2.2673335242803963e-07}, {"x": 0.6541353383458646, "y": 0.8444444444444444, "ox": 0.6541353383458646, "oy": 0.8444444444444444, "term": "degree", "cat25k": 158, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 225, "ncat": 104, "s": 0.8516129032258064, "os": 0.18887181738366987, "bg": 9.80667058079008e-06}, {"x": 0.13533834586466165, "y": 0.5851851851851851, "ox": 0.13533834586466165, "oy": 0.5851851851851851, "term": "mathematics", "cat25k": 63, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 89, "ncat": 18, "s": 0.9817204301075269, "os": 0.4464442493415277, "bg": 8.602444806774418e-06}, {"x": 0.7067669172932329, "y": 0.7037037037037036, "ox": 0.7067669172932329, "oy": 0.7037037037037036, "term": "engineering", "cat25k": 89, "ncat25k": 93, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 127, "ncat": 116, "s": 0.5688172043010752, "os": -0.0030728709394205467, "bg": 5.649058713387126e-06}, {"x": 0.030075187969924807, "y": 0.08888888888888888, "ox": 0.030075187969924807, "oy": 0.08888888888888888, "term": "bioinformatics", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.6451612903225806, "os": 0.05827480245829675, "bg": 1.2061584945031211e-05}, {"x": 0.05263157894736841, "y": 0.25925925925925924, "ox": 0.05263157894736841, "oy": 0.25925925925925924, "term": "operations", "cat25k": 25, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 7, "s": 0.864516129032258, "os": 0.20500438981562774, "bg": 1.3502436345858155e-06}, {"x": 0.09022556390977442, "y": 0.5925925925925927, "ox": 0.09022556390977442, "oy": 0.5925925925925927, "term": "research", "cat25k": 63, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 12, "s": 0.9892473118279569, "os": 0.4985733099209833, "bg": 6.546736130318303e-07}, {"x": 0.17293233082706763, "y": 0.4, "ox": 0.17293233082706763, "oy": 0.4, "term": "minimum", "cat25k": 39, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 23, "s": 0.8774193548387097, "os": 0.22530728709394207, "bg": 3.155651386163811e-06}, {"x": 0.04511278195488721, "y": 0.3851851851851852, "ox": 0.04511278195488721, "oy": 0.3851851851851852, "term": "if", "cat25k": 38, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 6, "s": 0.9580645161290322, "os": 0.33746707638279194, "bg": 1.0572173631269004e-07}, {"x": 0.12781954887218044, "y": 0.6444444444444445, "ox": 0.12781954887218044, "oy": 0.6444444444444445, "term": "master", "cat25k": 74, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 106, "ncat": 17, "s": 0.9924731182795699, "os": 0.5127304653204565, "bg": 3.982413209981915e-06}, {"x": 0.15037593984962405, "y": 0.28888888888888886, "ox": 0.15037593984962405, "oy": 0.28888888888888886, "term": "proven", "cat25k": 27, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 20, "s": 0.7752688172043011, "os": 0.13740122914837577, "bg": 9.085159822589926e-06}, {"x": 0.06766917293233082, "y": 0.28888888888888886, "ox": 0.06766917293233082, "oy": 0.28888888888888886, "term": "track", "cat25k": 27, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 9, "s": 0.875268817204301, "os": 0.21949078138718175, "bg": 8.536940702996792e-07}, {"x": 0.06766917293233082, "y": 0.11111111111111112, "ox": 0.06766917293233082, "oy": 0.11111111111111112, "term": "record", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.6172043010752689, "os": 0.043020193151887626, "bg": 4.6919932249963834e-07}, {"x": 0.19548872180451127, "y": 0.037037037037037035, "ox": 0.19548872180451127, "oy": 0.037037037037037035, "term": "value", "cat25k": 4, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 26, "s": 0.10967741935483871, "os": -0.1573748902546093, "bg": 3.404190852138647e-07}, {"x": 0.06766917293233082, "y": 0.21481481481481485, "ox": 0.06766917293233082, "oy": 0.21481481481481485, "term": "setting", "cat25k": 20, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 9, "s": 0.7817204301075269, "os": 0.14596136962247586, "bg": 1.637418847044267e-06}, {"x": 0.6015037593984962, "y": 0.45925925925925926, "ox": 0.6015037593984962, "oy": 0.45925925925925926, "term": "using", "cat25k": 45, "ncat25k": 74, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 92, "s": 0.12903225806451613, "os": -0.1412423178226514, "bg": 1.1576298266916468e-06}, {"x": 0.14285714285714285, "y": 0.12592592592592594, "ox": 0.14285714285714285, "oy": 0.12592592592592594, "term": "various", "cat25k": 12, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 19, "s": 0.5010752688172043, "os": -0.01690079016681298, "bg": 7.892716410555693e-07}, {"x": 0.2706766917293233, "y": 0.7259259259259259, "ox": 0.2706766917293233, "oy": 0.7259259259259259, "term": "modeling", "cat25k": 94, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 134, "ncat": 36, "s": 0.9827956989247312, "os": 0.45182177348551356, "bg": 2.187641050569575e-05}, {"x": 0.819548872180451, "y": 0.43703703703703706, "ox": 0.819548872180451, "oy": 0.43703703703703706, "term": "g", "cat25k": 43, "ncat25k": 131, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 162, "s": 0.04301075268817203, "os": -0.37971905179982446, "bg": 1.957522327297768e-06}, {"x": 0.0, "y": 0.34074074074074073, "ox": 0.0, "oy": 0.34074074074074073, "term": "trees", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 0, "s": 0.9602150537634409, "os": 0.33812554872695344, "bg": 3.3521181907320997e-06}, {"x": 0.0, "y": 0.274074074074074, "ox": 0.0, "oy": 0.274074074074074, "term": "neural", "cat25k": 26, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 0, "s": 0.9311827956989247, "os": 0.27194907813871816, "bg": 1.380627666920217e-05}, {"x": 0.007518796992481202, "y": 0.17777777777777778, "ox": 0.007518796992481202, "oy": 0.17777777777777778, "term": "networks", "cat25k": 17, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 1, "s": 0.8279569892473119, "os": 0.16889815627743637, "bg": 8.177884827380551e-07}, {"x": 0.4285714285714285, "y": 0.6370370370370371, "ox": 0.4285714285714285, "oy": 0.6370370370370371, "term": "time", "cat25k": 73, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 62, "s": 0.8666666666666666, "os": 0.20687006145741876, "bg": 3.6532779250207114e-07}, {"x": 0.022556390977443604, "y": 0.12592592592592594, "ox": 0.022556390977443604, "oy": 0.12592592592592594, "term": "series", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.7376344086021506, "os": 0.10250219490781387, "bg": 2.475465843683194e-07}, {"x": 0.037593984962406006, "y": 0.0962962962962963, "ox": 0.037593984962406006, "oy": 0.0962962962962963, "term": "cluster", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.6419354838709678, "os": 0.05816505706760316, "bg": 2.4422804332225583e-06}, {"x": 0.04511278195488721, "y": 0.22962962962962963, "ox": 0.04511278195488721, "oy": 0.22962962962962963, "term": "pandas", "cat25k": 22, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 6, "s": 0.8451612903225806, "os": 0.18305531167690958, "bg": 0.00015782323371274894}, {"x": 0.022556390977443604, "y": 0.12592592592592594, "ox": 0.022556390977443604, "oy": 0.12592592592592594, "term": "scikit", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.7376344086021506, "os": 0.10250219490781387, "bg": 0.0005953178253039842}, {"x": 0.09022556390977442, "y": 0.22222222222222218, "ox": 0.09022556390977442, "oy": 0.22222222222222218, "term": "learn", "cat25k": 21, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 12, "s": 0.7688172043010753, "os": 0.1309262510974539, "bg": 4.679783084700079e-07}, {"x": 0.8421052631578946, "y": 0.34074074074074073, "ox": 0.8421052631578946, "oy": 0.34074074074074073, "term": "spark", "cat25k": 32, "ncat25k": 136, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 169, "s": 0.006451612903225806, "os": -0.4976953467954346, "bg": 8.757978161675943e-05}, {"x": 0.12030075187969923, "y": 0.837037037037037, "ox": 0.12030075187969923, "oy": 0.837037037037037, "term": "statistical", "cat25k": 152, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 216, "ncat": 16, "s": 1.0, "os": 0.711369622475856, "bg": 2.525252800118948e-05}, {"x": 0.4210526315789473, "y": 0.3481481481481481, "ox": 0.4210526315789473, "oy": 0.3481481481481481, "term": "databases", "cat25k": 33, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 61, "s": 0.24301075268817204, "os": -0.07243195785776996, "bg": 9.960764732178555e-06}, {"x": 0.7744360902255637, "y": 0.48888888888888893, "ox": 0.7744360902255637, "oy": 0.48888888888888893, "term": "hadoop", "cat25k": 50, "ncat25k": 111, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 138, "s": 0.06236559139784946, "os": -0.2834723441615452, "bg": 0.00620362125259721}, {"x": 0.7218045112781953, "y": 0.17037037037037037, "ox": 0.7218045112781953, "oy": 0.17037037037037037, "term": "distributed", "cat25k": 16, "ncat25k": 98, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 122, "s": 0.003225806451612903, "os": -0.5474100087796312, "bg": 1.0297483284432714e-05}, {"x": 0.20300751879699244, "y": 0.25925925925925924, "ox": 0.20300751879699244, "oy": 0.25925925925925924, "term": "computing", "cat25k": 25, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 27, "s": 0.6344086021505376, "os": 0.05575065847234417, "bg": 3.4020778025562446e-06}, {"x": 0.18045112781954886, "y": 0.3333333333333333, "ox": 0.18045112781954886, "oy": 0.3333333333333333, "term": "frameworks", "cat25k": 32, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 24, "s": 0.7860215053763441, "os": 0.1516681299385426, "bg": 4.359663383439977e-05}, {"x": 0.44360902255639095, "y": 0.5703703703703703, "ox": 0.44360902255639095, "oy": 0.5703703703703703, "term": "software", "cat25k": 61, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 64, "s": 0.767741935483871, "os": 0.12576821773485508, "bg": 8.149291567512455e-07}, {"x": 0.45112781954887204, "y": 0.6296296296296297, "ox": 0.45112781954887204, "oy": 0.6296296296296297, "term": "environment", "cat25k": 72, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 65, "s": 0.8408602150537634, "os": 0.17712906057945565, "bg": 3.2932577516304e-06}, {"x": 0.29323308270676685, "y": 0.07407407407407408, "ox": 0.29323308270676685, "oy": 0.07407407407407408, "term": "agile", "cat25k": 7, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 39, "s": 0.08602150537634408, "os": -0.21762510974539068, "bg": 5.223393340173492e-05}, {"x": 0.7368421052631577, "y": 0.3555555555555555, "ox": 0.7368421052631577, "oy": 0.3555555555555555, "term": "management", "cat25k": 34, "ncat25k": 103, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 128, "s": 0.04408602150537634, "os": -0.37851185250219493, "bg": 1.1634457760402705e-06}, {"x": 0.13533834586466165, "y": 0.04444444444444444, "ox": 0.13533834586466165, "oy": 0.04444444444444444, "term": "git", "cat25k": 4, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 18, "s": 0.1935483870967742, "os": -0.09032045654082528, "bg": 3.063592521770654e-05}, {"x": 0.18796992481203004, "y": 0.11111111111111112, "ox": 0.18796992481203004, "oy": 0.11111111111111112, "term": "understand", "cat25k": 11, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 25, "s": 0.21935483870967742, "os": -0.07638279192273925, "bg": 1.2947669726956304e-06}, {"x": 0.6015037593984962, "y": 0.42962962962962964, "ox": 0.6015037593984962, "oy": 0.42962962962962964, "term": "complex", "cat25k": 42, "ncat25k": 74, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 92, "s": 0.1043010752688172, "os": -0.17065408252853376, "bg": 6.477277051591213e-06}, {"x": 0.0150375939849624, "y": 0.17777777777777778, "ox": 0.0150375939849624, "oy": 0.17777777777777778, "term": "needs", "cat25k": 17, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.8129032258064517, "os": 0.16143546971027217, "bg": 4.204977345967576e-07}, {"x": 0.0150375939849624, "y": 0.20740740740740743, "ox": 0.0150375939849624, "oy": 0.20740740740740743, "term": "applying", "cat25k": 20, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 2, "s": 0.8559139784946237, "os": 0.1908472344161545, "bg": 4.135734816683555e-06}, {"x": 0.9323308270676691, "y": 0.9333333333333335, "ox": 0.9323308270676691, "oy": 0.9333333333333335, "term": "the", "cat25k": 365, "ncat25k": 379, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 520, "ncat": 470, "s": 0.5817204301075269, "os": 0.0009877085162423027, "bg": 8.558121129129422e-08}, {"x": 0.022556390977443604, "y": 0.18518518518518517, "ox": 0.022556390977443604, "oy": 0.18518518518518517, "term": "right", "cat25k": 18, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 3, "s": 0.810752688172043, "os": 0.16132572431957856, "bg": 2.0461288271136127e-07}, {"x": 0.09774436090225562, "y": 0.15555555555555559, "ox": 0.09774436090225562, "oy": 0.15555555555555559, "term": "approaches", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.6376344086021505, "os": 0.057287093942054446, "bg": 3.968172687405854e-06}, {"x": 0.15789473684210525, "y": 0.48888888888888893, "ox": 0.15789473684210525, "oy": 0.48888888888888893, "term": "must", "cat25k": 50, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 21, "s": 0.9569892473118279, "os": 0.32846795434591747, "bg": 6.28325216993565e-07}, {"x": 0.8796992481203008, "y": 0.6592592592592592, "ox": 0.8796992481203008, "oy": 0.6592592592592592, "term": "be", "cat25k": 77, "ncat25k": 155, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 193, "s": 0.08387096774193548, "os": -0.2188323090430202, "bg": 2.5179344099221857e-07}, {"x": 0.45864661654135325, "y": 0.02962962962962963, "ox": 0.45864661654135325, "oy": 0.02962962962962963, "term": "curious", "cat25k": 3, "ncat25k": 53, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 66, "s": 0.023655913978494623, "os": -0.4259218612818262, "bg": 1.6417802104269694e-05}, {"x": 0.7669172932330827, "y": 0.12592592592592594, "ox": 0.7669172932330827, "oy": 0.12592592592592594, "term": "self", "cat25k": 12, "ncat25k": 110, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 137, "s": 0.0, "os": -0.6363037752414399, "bg": 2.726399579298841e-06}, {"x": 0.11278195488721804, "y": 0.28148148148148144, "ox": 0.11278195488721804, "oy": 0.28148148148148144, "term": "driven", "cat25k": 27, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 15, "s": 0.8236559139784946, "os": 0.16736172080772604, "bg": 5.958092128966839e-06}, {"x": 0.8496240601503758, "y": 0.5777777777777777, "ox": 0.8496240601503758, "oy": 0.5777777777777777, "term": "have", "cat25k": 62, "ncat25k": 140, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 174, "s": 0.06451612903225805, "os": -0.2698639157155399, "bg": 3.3498049486250415e-07}, {"x": 0.060150375939849614, "y": 0.15555555555555559, "ox": 0.060150375939849614, "oy": 0.15555555555555559, "term": "passion", "cat25k": 15, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 8, "s": 0.7236559139784946, "os": 0.09460052677787534, "bg": 4.316192665612871e-06}, {"x": 0.5413533834586466, "y": 0.3555555555555555, "ox": 0.5413533834586466, "oy": 0.3555555555555555, "term": "problem", "cat25k": 34, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 83, "s": 0.0989247311827957, "os": -0.18448200175592627, "bg": 1.8712876338005214e-06}, {"x": 0.5338345864661653, "y": 0.22222222222222218, "ox": 0.5338345864661653, "oy": 0.22222222222222218, "term": "solving", "cat25k": 21, "ncat25k": 65, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 81, "s": 0.05698924731182795, "os": -0.3093722563652327, "bg": 2.2193424088442595e-05}, {"x": 0.13533834586466165, "y": 0.13333333333333333, "ox": 0.13533834586466165, "oy": 0.13333333333333333, "term": "both", "cat25k": 13, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 18, "s": 0.5709677419354838, "os": -0.002085162423178216, "bg": 3.148012352870428e-07}, {"x": 0.5112781954887217, "y": 0.14074074074074075, "ox": 0.5112781954887217, "oy": 0.14074074074074075, "term": "verbal", "cat25k": 13, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 77, "s": 0.04946236559139785, "os": -0.3678665496049166, "bg": 3.6044235287756905e-05}, {"x": 0.24060150375939846, "y": 0.2518518518518518, "ox": 0.24060150375939846, "oy": 0.2518518518518518, "term": "developing", "cat25k": 24, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 32, "s": 0.5870967741935484, "os": 0.011084284460052662, "bg": 3.08148367275876e-06}, {"x": 0.15037593984962405, "y": 0.17777777777777778, "ox": 0.15037593984962405, "oy": 0.17777777777777778, "term": "testing", "cat25k": 17, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 20, "s": 0.603225806451613, "os": 0.02710711150131695, "bg": 1.5539131948773134e-06}, {"x": 0.18045112781954886, "y": 0.3333333333333333, "ox": 0.18045112781954886, "oy": 0.3333333333333333, "term": "projects", "cat25k": 32, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 24, "s": 0.7860215053763441, "os": 0.1516681299385426, "bg": 1.4891435607339e-06}, {"x": 0.49624060150375937, "y": 0.7407407407407407, "ox": 0.49624060150375937, "oy": 0.7407407407407407, "term": "analytics", "cat25k": 97, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 138, "ncat": 75, "s": 0.896774193548387, "os": 0.24264705882352944, "bg": 0.00010858289131260403}, {"x": 0.06766917293233082, "y": 0.3925925925925926, "ox": 0.06766917293233082, "oy": 0.3925925925925926, "term": "deep", "cat25k": 39, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 9, "s": 0.9548387096774194, "os": 0.32243195785776996, "bg": 2.3918686183940865e-06}, {"x": 0.20300751879699244, "y": 0.20740740740740743, "ox": 0.20300751879699244, "oy": 0.20740740740740743, "term": "applications", "cat25k": 20, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 27, "s": 0.5838709677419355, "os": 0.004280070237050043, "bg": 1.1067355037369024e-06}, {"x": 0.060150375939849614, "y": 0.007407407407407408, "ox": 0.060150375939849614, "oy": 0.007407407407407408, "term": "images", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.332258064516129, "os": -0.05245829675153643, "bg": 1.5809547093891686e-07}, {"x": 0.07518796992481203, "y": 0.037037037037037035, "ox": 0.07518796992481203, "oy": 0.037037037037037035, "term": "video", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.3924731182795699, "os": -0.03797190517998244, "bg": 8.208446314502413e-08}, {"x": 0.12781954887218044, "y": 0.02962962962962963, "ox": 0.12781954887218044, "oy": 0.02962962962962963, "term": "strategies", "cat25k": 3, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 17, "s": 0.18279569892473116, "os": -0.09756365232660227, "bg": 1.1373465980081215e-06}, {"x": 0.8721804511278194, "y": 0.762962962962963, "ox": 0.8721804511278194, "oy": 0.762962962962963, "term": "working", "cat25k": 113, "ncat25k": 153, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 161, "ncat": 190, "s": 0.16774193548387095, "os": -0.1084284460052678, "bg": 4.7514791618601936e-06}, {"x": 0.5864661654135337, "y": 0.6, "ox": 0.5864661654135337, "oy": 0.6, "term": "cloud", "cat25k": 66, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 94, "ncat": 90, "s": 0.5946236559139785, "os": 0.013388937664618017, "bg": 3.226775219593892e-05}, {"x": 0.12781954887218044, "y": 0.04444444444444444, "ox": 0.12781954887218044, "oy": 0.04444444444444444, "term": "amazon", "cat25k": 4, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 17, "s": 0.2118279569892473, "os": -0.08285776997366111, "bg": 7.833143152564256e-07}, {"x": 0.24060150375939846, "y": 0.15555555555555559, "ox": 0.24060150375939846, "oy": 0.15555555555555559, "term": "web", "cat25k": 15, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 32, "s": 0.20860215053763442, "os": -0.08450395083406495, "bg": 1.7106740276927043e-07}, {"x": 0.4812030075187969, "y": 0.14074074074074075, "ox": 0.4812030075187969, "oy": 0.14074074074074075, "term": "services", "cat25k": 13, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 72, "s": 0.051612903225806445, "os": -0.3380158033362599, "bg": 3.236855672443552e-07}, {"x": 0.4887218045112781, "y": 0.4, "ox": 0.4887218045112781, "oy": 0.4, "term": "big", "cat25k": 39, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 73, "s": 0.20537634408602148, "os": -0.08812554872695344, "bg": 1.1902683328934723e-06}, {"x": 0.06766917293233082, "y": 0.2518518518518518, "ox": 0.06766917293233082, "oy": 0.2518518518518518, "term": "variety", "cat25k": 24, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 9, "s": 0.843010752688172, "os": 0.18272607550482878, "bg": 1.7230392689665298e-06}, {"x": 0.31578947368421045, "y": 0.7481481481481481, "ox": 0.31578947368421045, "oy": 0.7481481481481481, "term": "techniques", "cat25k": 100, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 142, "ncat": 42, "s": 0.9763440860215054, "os": 0.4291044776119403, "bg": 9.121191536942088e-06}, {"x": 0.45112781954887204, "y": 0.5185185185185185, "ox": 0.45112781954887204, "oy": 0.5185185185185185, "term": "understanding", "cat25k": 53, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 65, "s": 0.6720430107526882, "os": 0.06683494293239689, "bg": 5.701729431587313e-06}, {"x": 0.08270676691729321, "y": 0.05185185185185185, "ox": 0.08270676691729321, "oy": 0.05185185185185185, "term": "how", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.4247311827956989, "os": -0.03072870939420544, "bg": 6.294638725583667e-08}, {"x": 0.12030075187969923, "y": 0.08888888888888888, "ox": 0.12030075187969923, "oy": 0.08888888888888888, "term": "where", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 16, "s": 0.42043010752688165, "os": -0.03127743634767341, "bg": 1.5532449397540387e-07}, {"x": 0.06766917293233082, "y": 0.05185185185185185, "ox": 0.06766917293233082, "oy": 0.05185185185185185, "term": "coding", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5043010752688172, "os": -0.01580333625987708, "bg": 2.8762545413362717e-06}, {"x": 0.781954887218045, "y": 0.3111111111111111, "ox": 0.781954887218045, "oy": 0.3111111111111111, "term": "java", "cat25k": 30, "ncat25k": 112, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 139, "s": 0.01075268817204301, "os": -0.46740561896400357, "bg": 6.531053961823031e-06}, {"x": 0.7218045112781953, "y": 0.19259259259259262, "ox": 0.7218045112781953, "oy": 0.19259259259259262, "term": "scala", "cat25k": 18, "ncat25k": 98, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 122, "s": 0.005376344086021504, "os": -0.5253511852502195, "bg": 0.0002550481877361457}, {"x": 0.04511278195488721, "y": 0.6370370370370371, "ox": 0.04511278195488721, "oy": 0.6370370370370371, "term": "mining", "cat25k": 73, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 6, "s": 0.9946236559139785, "os": 0.5874670763827918, "bg": 1.2893287010488043e-05}, {"x": 0.037593984962406006, "y": 0.11111111111111112, "ox": 0.037593984962406006, "oy": 0.11111111111111112, "term": "libraries", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.6763440860215053, "os": 0.07287093942054435, "bg": 1.4198995442370945e-06}, {"x": 0.6390977443609022, "y": 0.26666666666666666, "ox": 0.6390977443609022, "oy": 0.26666666666666666, "term": "high", "cat25k": 25, "ncat25k": 80, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 99, "s": 0.04838709677419354, "os": -0.3697322212467076, "bg": 7.815203142181733e-07}, {"x": 0.2706766917293233, "y": 0.2518518518518518, "ox": 0.2706766917293233, "oy": 0.2518518518518518, "term": "performance", "cat25k": 24, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 36, "s": 0.49784946236559136, "os": -0.018766461808604085, "bg": 1.001589372095109e-06}, {"x": 0.7593984962406014, "y": 0.5037037037037037, "ox": 0.7593984962406014, "oy": 0.5037037037037037, "term": "systems", "cat25k": 52, "ncat25k": 108, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 134, "s": 0.06774193548387096, "os": -0.2538410886742757, "bg": 1.8602713186042237e-06}, {"x": 0.007518796992481202, "y": 0.16296296296296298, "ox": 0.007518796992481202, "oy": 0.16296296296296298, "term": "mathematical", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.7913978494623656, "os": 0.1541922739244952, "bg": 4.167058460750204e-06}, {"x": 0.09022556390977442, "y": 0.3037037037037037, "ox": 0.09022556390977442, "oy": 0.3037037037037037, "term": "background", "cat25k": 29, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 12, "s": 0.8688172043010752, "os": 0.2118086040386304, "bg": 2.0274913287447507e-06}, {"x": 0.10526315789473684, "y": 0.08888888888888888, "ox": 0.10526315789473684, "oy": 0.08888888888888888, "term": "scalable", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 14, "s": 0.5021505376344085, "os": -0.01635206321334505, "bg": 1.650133597354709e-05}, {"x": 0.6090225563909772, "y": 0.45185185185185184, "ox": 0.6090225563909772, "oy": 0.45185185185185184, "term": "solutions", "cat25k": 44, "ncat25k": 75, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 93, "s": 0.11075268817204299, "os": -0.15605794556628622, "bg": 2.93865426418937e-06}, {"x": 0.5338345864661653, "y": 0.6148148148148148, "ox": 0.5338345864661653, "oy": 0.6148148148148148, "term": "that", "cat25k": 68, "ncat25k": 65, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 81, "s": 0.6956989247311828, "os": 0.08033362598770855, "bg": 1.0470285052792936e-07}, {"x": 0.13533834586466165, "y": 0.2962962962962963, "ox": 0.13533834586466165, "oy": 0.2962962962962963, "term": "production", "cat25k": 28, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 18, "s": 0.8075268817204301, "os": 0.1596795434591747, "bg": 1.2792240950062975e-06}, {"x": 0.12030075187969923, "y": 0.11851851851851852, "ox": 0.12030075187969923, "oy": 0.11851851851851852, "term": "environments", "cat25k": 11, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 16, "s": 0.5731182795698925, "os": -0.0018656716417910502, "bg": 4.776557848220396e-06}, {"x": 0.09022556390977442, "y": 0.26666666666666666, "ox": 0.09022556390977442, "oy": 0.26666666666666666, "term": "comfortable", "cat25k": 25, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 12, "s": 0.8333333333333333, "os": 0.1750438981562774, "bg": 4.9010333318255844e-06}, {"x": 0.007518796992481202, "y": 0.08888888888888888, "ox": 0.007518796992481202, "oy": 0.08888888888888888, "term": "nlp", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.7021505376344086, "os": 0.08066286215978928, "bg": 2.1612437459009107e-05}, {"x": 0.15789473684210525, "y": 0.22222222222222218, "ox": 0.15789473684210525, "oy": 0.22222222222222218, "term": "discipline", "cat25k": 21, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 21, "s": 0.6548387096774193, "os": 0.06376207199297629, "bg": 8.642464546195625e-06}, {"x": 0.22556390977443602, "y": 0.4074074074074074, "ox": 0.22556390977443602, "oy": 0.4074074074074074, "term": "similar", "cat25k": 40, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 30, "s": 0.8419354838709677, "os": 0.18042142230026337, "bg": 1.4595102294681408e-06}, {"x": 0.6541353383458646, "y": 0.5777777777777777, "ox": 0.6541353383458646, "oy": 0.5777777777777777, "term": "least", "cat25k": 62, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 104, "s": 0.221505376344086, "os": -0.07583406496927125, "bg": 3.450222777919735e-06}, {"x": 0.04511278195488721, "y": 0.12592592592592594, "ox": 0.04511278195488721, "oy": 0.12592592592592594, "term": "job", "cat25k": 12, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.6924731182795699, "os": 0.08011413520632134, "bg": 2.587553195242032e-07}, {"x": 0.18796992481203004, "y": 0.15555555555555559, "ox": 0.18796992481203004, "oy": 0.15555555555555559, "term": "4", "cat25k": 15, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 25, "s": 0.4182795698924731, "os": -0.03226514486391571, "bg": 0.0}, {"x": 0.744360902255639, "y": 0.5333333333333333, "ox": 0.744360902255639, "oy": 0.5333333333333333, "term": "programming", "cat25k": 55, "ncat25k": 106, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 131, "s": 0.08924731182795698, "os": -0.20950395083406503, "bg": 8.244938096196062e-06}, {"x": 0.36842105263157887, "y": 0.6222222222222222, "ox": 0.36842105263157887, "oy": 0.6222222222222222, "term": "languages", "cat25k": 70, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 99, "ncat": 51, "s": 0.9064516129032258, "os": 0.251865671641791, "bg": 8.012105757659441e-06}, {"x": 0.7142857142857141, "y": 0.8, "ox": 0.7142857142857141, "oy": 0.8, "term": "such", "cat25k": 130, "ncat25k": 97, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 185, "ncat": 120, "s": 0.7075268817204301, "os": 0.085052677787533, "bg": 1.601918655263975e-06}, {"x": 0.8646616541353382, "y": 0.9185185185185186, "ox": 0.8646616541353382, "oy": 0.9185185185185186, "term": "as", "cat25k": 266, "ncat25k": 152, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 378, "ncat": 189, "s": 0.6333333333333333, "os": 0.05344600526777876, "bg": 5.045607401026502e-07}, {"x": 0.8872180451127819, "y": 0.6518518518518518, "ox": 0.8872180451127819, "oy": 0.6518518518518518, "term": "etc", "cat25k": 76, "ncat25k": 158, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 196, "s": 0.07634408602150537, "os": -0.23364793678665496, "bg": 1.1784478175359625e-05}, {"x": 0.20300751879699244, "y": 0.3037037037037037, "ox": 0.20300751879699244, "oy": 0.3037037037037037, "term": "concepts", "cat25k": 29, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 27, "s": 0.7322580645161291, "os": 0.09986830553116771, "bg": 5.936897674993005e-06}, {"x": 0.23308270676691725, "y": 0.5555555555555555, "ox": 0.23308270676691725, "oy": 0.5555555555555555, "term": "advanced", "cat25k": 60, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 31, "s": 0.953763440860215, "os": 0.32001755926251096, "bg": 1.337257122404355e-06}, {"x": 0.5187969924812029, "y": 0.11851851851851852, "ox": 0.5187969924812029, "oy": 0.11851851851851852, "term": "aws", "cat25k": 11, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 79, "s": 0.03870967741935484, "os": -0.3973880597014925, "bg": 0.0002304398976361718}, {"x": 0.10526315789473684, "y": 0.014814814814814812, "ox": 0.10526315789473684, "oy": 0.014814814814814812, "term": "emr", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.19462365591397848, "os": -0.08988147497805092, "bg": 4.435727008725907e-05}, {"x": 0.05263157894736841, "y": 0.04444444444444444, "ox": 0.05263157894736841, "oy": 0.04444444444444444, "term": "bigquery", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.5591397849462365, "os": -0.008230904302019316, "bg": 0.0003869969040247678}, {"x": 0.10526315789473684, "y": 0.05925925925925926, "ox": 0.10526315789473684, "oy": 0.05925925925925926, "term": "modern", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 14, "s": 0.3516129032258064, "os": -0.045763827919227396, "bg": 7.706156949687308e-07}, {"x": 0.12030075187969923, "y": 0.42962962962962964, "ox": 0.12030075187969923, "oy": 0.42962962962962964, "term": "visualization", "cat25k": 42, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 16, "s": 0.9505376344086021, "os": 0.30695785776997364, "bg": 3.6348653413915746e-05}, {"x": 0.037593984962406006, "y": 0.11851851851851852, "ox": 0.037593984962406006, "oy": 0.11851851851851852, "term": "deliver", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.6935483870967742, "os": 0.08022388059701493, "bg": 1.8361251324523838e-06}, {"x": 0.007518796992481202, "y": 0.31851851851851853, "ox": 0.007518796992481202, "oy": 0.31851851851851853, "term": "actionable", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 1, "s": 0.9526881720430107, "os": 0.30860403863037755, "bg": 0.00012899291711618744}, {"x": 0.037593984962406006, "y": 0.3259259259259259, "ox": 0.037593984962406006, "oy": 0.3259259259259259, "term": "insights", "cat25k": 31, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 5, "s": 0.9387096774193547, "os": 0.28610623353819137, "bg": 1.2815634027339147e-05}, {"x": 0.5263157894736841, "y": 0.2962962962962963, "ox": 0.5263157894736841, "oy": 0.2962962962962963, "term": "1", "cat25k": 28, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 80, "s": 0.07849462365591398, "os": -0.2283801580333626, "bg": 0.0}, {"x": 0.15037593984962405, "y": 0.23703703703703705, "ox": 0.15037593984962405, "oy": 0.23703703703703705, "term": "year", "cat25k": 22, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 20, "s": 0.7086021505376344, "os": 0.08593064091308164, "bg": 2.3051698891811297e-07}, {"x": 0.33082706766917286, "y": 0.07407407407407408, "ox": 0.33082706766917286, "oy": 0.07407407407407408, "term": "source", "cat25k": 7, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 44, "s": 0.06666666666666665, "os": -0.2549385425812116, "bg": 5.998963145875381e-07}, {"x": 0.6766917293233081, "y": 0.6888888888888889, "ox": 0.6766917293233081, "oy": 0.6888888888888889, "term": "large", "cat25k": 86, "ncat25k": 87, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 123, "ncat": 108, "s": 0.5903225806451613, "os": 0.01207199297629502, "bg": 2.7842870867567284e-06}, {"x": 0.6315789473684209, "y": 0.5777777777777777, "ox": 0.6315789473684209, "oy": 0.5777777777777777, "term": "scale", "cat25k": 62, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 97, "s": 0.3193548387096774, "os": -0.05344600526777876, "bg": 7.97845695866137e-06}, {"x": 0.09022556390977442, "y": 0.06666666666666667, "ox": 0.09022556390977442, "oy": 0.06666666666666667, "term": "6", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 12, "s": 0.4580645161290322, "os": -0.023485513608428446, "bg": 0.0}, {"x": 0.030075187969924807, "y": 0.22222222222222218, "ox": 0.030075187969924807, "oy": 0.22222222222222218, "term": "analyzing", "cat25k": 21, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 4, "s": 0.8548387096774194, "os": 0.19062774363476734, "bg": 1.5396228739053394e-05}, {"x": 0.857142857142857, "y": 0.7333333333333333, "ox": 0.857142857142857, "oy": 0.7333333333333333, "term": "on", "cat25k": 96, "ncat25k": 145, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 137, "ncat": 180, "s": 0.14731182795698924, "os": -0.12291483757682176, "bg": 1.690445472102035e-07}, {"x": 0.18045112781954886, "y": 0.08148148148148147, "ox": 0.18045112781954886, "oy": 0.08148148148148147, "term": "but", "cat25k": 8, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 24, "s": 0.1806451612903226, "os": -0.09833187006145742, "bg": 7.000231987688072e-08}, {"x": 0.6165413533834585, "y": 0.37777777777777777, "ox": 0.6165413533834585, "oy": 0.37777777777777777, "term": "not", "cat25k": 37, "ncat25k": 76, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 94, "s": 0.07311827956989246, "os": -0.23705004389815632, "bg": 1.1163619533109492e-07}, {"x": 0.36842105263157887, "y": 0.28148148148148144, "ox": 0.36842105263157887, "oy": 0.28148148148148144, "term": "required", "cat25k": 27, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 51, "s": 0.2075268817204301, "os": -0.08636962247585606, "bg": 1.1156640791115646e-06}, {"x": 0.0150375939849624, "y": 0.08148148148148147, "ox": 0.0150375939849624, "oy": 0.08148148148148147, "term": "electrical", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.664516129032258, "os": 0.06584723441615453, "bg": 7.592641468870084e-07}, {"x": 0.05263157894736841, "y": 0.28148148148148144, "ox": 0.05263157894736841, "oy": 0.28148148148148144, "term": "applied", "cat25k": 27, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 7, "s": 0.8795698924731183, "os": 0.22706321334503948, "bg": 1.9987818978940476e-06}, {"x": 0.09022556390977442, "y": 0.49629629629629624, "ox": 0.09022556390977442, "oy": 0.49629629629629624, "term": "problems", "cat25k": 51, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 12, "s": 0.9688172043010752, "os": 0.40298507462686567, "bg": 1.3866548911052897e-06}, {"x": 0.15789473684210525, "y": 0.10370370370370371, "ox": 0.15789473684210525, "oy": 0.10370370370370371, "term": "proficient", "cat25k": 10, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 21, "s": 0.3182795698924731, "os": -0.05388498683055312, "bg": 3.088353378217402e-05}, {"x": 0.6616541353383457, "y": 0.31851851851851853, "ox": 0.6616541353383457, "oy": 0.31851851851851853, "term": "one", "cat25k": 30, "ncat25k": 85, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 105, "s": 0.05053763440860214, "os": -0.34064969271290607, "bg": 2.979054179484693e-07}, {"x": 0.30075187969924805, "y": 0.22962962962962963, "ox": 0.30075187969924805, "oy": 0.22962962962962963, "term": "more", "cat25k": 22, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 40, "s": 0.24408602150537634, "os": -0.07067603160667252, "bg": 9.191896878128552e-08}, {"x": 0.3909774436090225, "y": 0.28888888888888886, "ox": 0.3909774436090225, "oy": 0.28888888888888886, "term": "c", "cat25k": 27, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 57, "s": 0.17741935483870966, "os": -0.10140474100087793, "bg": 3.2177485324360094e-07}, {"x": 0.060150375939849614, "y": 0.04444444444444444, "ox": 0.060150375939849614, "oy": 0.04444444444444444, "term": "familiar", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5096774193548388, "os": -0.015693590869183496, "bg": 1.701309686424928e-06}, {"x": 0.3834586466165413, "y": 0.2444444444444444, "ox": 0.3834586466165413, "oy": 0.2444444444444444, "term": "relational", "cat25k": 23, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 54, "s": 0.13010752688172042, "os": -0.13805970149253732, "bg": 5.5682245108122756e-05}, {"x": 0.0150375939849624, "y": 0.11851851851851852, "ox": 0.0150375939849624, "oy": 0.11851851851851852, "term": "ph", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.7419354838709677, "os": 0.10261194029850747, "bg": 1.64808738086045e-06}, {"x": 0.4360902255639097, "y": 0.13333333333333333, "ox": 0.4360902255639097, "oy": 0.13333333333333333, "term": "d", "cat25k": 13, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 63, "s": 0.058064516129032254, "os": -0.3005926251097454, "bg": 4.4805716191577187e-07}, {"x": 0.5714285714285713, "y": 0.19259259259259262, "ox": 0.5714285714285713, "oy": 0.19259259259259262, "term": "highly", "cat25k": 18, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 88, "s": 0.046236559139784944, "os": -0.37609745390693594, "bg": 5.3116446766906715e-06}, {"x": 0.5714285714285713, "y": 0.44444444444444436, "ox": 0.5714285714285713, "oy": 0.44444444444444436, "term": "equivalent", "cat25k": 44, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 88, "s": 0.14623655913978495, "os": -0.12609745390693589, "bg": 1.2438848031649737e-05}, {"x": 0.5789473684210525, "y": 0.17037037037037037, "ox": 0.5789473684210525, "oy": 0.17037037037037037, "term": "hands", "cat25k": 16, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 89, "s": 0.03655913978494623, "os": -0.40561896400351183, "bg": 4.55701260156391e-06}, {"x": 0.0, "y": 0.26666666666666666, "ox": 0.0, "oy": 0.26666666666666666, "term": "scientist", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.9225806451612903, "os": 0.26459613696224754, "bg": 8.30654664322067e-06}, {"x": 0.04511278195488721, "y": 0.14814814814814817, "ox": 0.04511278195488721, "oy": 0.14814814814814817, "term": "graph", "cat25k": 14, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 6, "s": 0.7354838709677419, "os": 0.10217295873573311, "bg": 3.334628066807734e-06}, {"x": 0.12030075187969923, "y": 0.13333333333333333, "ox": 0.12030075187969923, "oy": 0.13333333333333333, "term": "analytic", "cat25k": 13, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 16, "s": 0.5924731182795698, "os": 0.01284021071115013, "bg": 2.5531668843742052e-05}, {"x": 0.33082706766917286, "y": 0.13333333333333333, "ox": 0.33082706766917286, "oy": 0.13333333333333333, "term": "processes", "cat25k": 13, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 44, "s": 0.09462365591397848, "os": -0.1961150131694469, "bg": 3.1378365826348124e-06}, {"x": 0.10526315789473684, "y": 0.12592592592592594, "ox": 0.10526315789473684, "oy": 0.12592592592592594, "term": "use", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 14, "s": 0.5978494623655913, "os": 0.0204126426690079, "bg": 8.610543544741543e-08}, {"x": 0.04511278195488721, "y": 0.037037037037037035, "ox": 0.04511278195488721, "oy": 0.037037037037037035, "term": "demonstrable", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5623655913978495, "os": -0.00812115891132572, "bg": 5.137558118626217e-05}, {"x": 0.20300751879699244, "y": 0.37037037037037035, "ox": 0.20300751879699244, "oy": 0.37037037037037035, "term": "well", "cat25k": 36, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 27, "s": 0.8225806451612904, "os": 0.166044776119403, "bg": 4.307607297444625e-07}, {"x": 0.4661654135338345, "y": 0.05185185185185185, "ox": 0.4661654135338345, "oy": 0.05185185185185185, "term": "ambiguity", "cat25k": 5, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 68, "s": 0.033333333333333326, "os": -0.4113257243195786, "bg": 9.080227441536955e-05}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "prioritizing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 2.7391694381735216e-05}, {"x": 0.12030075187969923, "y": 0.04444444444444444, "ox": 0.12030075187969923, "oy": 0.04444444444444444, "term": "delivering", "cat25k": 4, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 16, "s": 0.22473118279569892, "os": -0.07539508340649693, "bg": 5.4256128137191055e-06}, {"x": 0.09022556390977442, "y": 0.37777777777777777, "ox": 0.09022556390977442, "oy": 0.37777777777777777, "term": "results", "cat25k": 37, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 12, "s": 0.9365591397849462, "os": 0.28533801580333623, "bg": 4.846260241065882e-07}, {"x": 0.05263157894736841, "y": 0.34074074074074073, "ox": 0.05263157894736841, "oy": 0.34074074074074073, "term": "dynamic", "cat25k": 32, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 7, "s": 0.9376344086021505, "os": 0.2858867427568042, "bg": 3.615432329646116e-06}, {"x": 0.04511278195488721, "y": 0.11851851851851852, "ox": 0.04511278195488721, "oy": 0.11851851851851852, "term": "areas", "cat25k": 11, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.6741935483870967, "os": 0.07276119402985075, "bg": 3.604975915483634e-07}, {"x": 0.007518796992481202, "y": 0.0962962962962963, "ox": 0.007518796992481202, "oy": 0.0962962962962963, "term": "completion", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.7161290322580645, "os": 0.08801580333625987, "bg": 1.4757382828752062e-06}, {"x": 0.29323308270676685, "y": 0.711111111111111, "ox": 0.29323308270676685, "oy": 0.711111111111111, "term": "3", "cat25k": 90, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 128, "ncat": 39, "s": 0.9709677419354839, "os": 0.41472783143107994, "bg": 0.0}, {"x": 0.3834586466165413, "y": 0.22222222222222218, "ox": 0.3834586466165413, "oy": 0.22222222222222218, "term": "database", "cat25k": 21, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 54, "s": 0.1086021505376344, "os": -0.1601185250219491, "bg": 1.6185143084901954e-06}, {"x": 0.05263157894736841, "y": 0.14074074074074075, "ox": 0.05263157894736841, "oy": 0.14074074074074075, "term": "solid", "cat25k": 13, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.7096774193548386, "os": 0.08735733099209833, "bg": 1.3869018482360782e-06}, {"x": 0.16541353383458646, "y": 0.17777777777777778, "ox": 0.16541353383458646, "oy": 0.17777777777777778, "term": "linux", "cat25k": 17, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 22, "s": 0.5913978494623656, "os": 0.012181738366988604, "bg": 1.0118624595595969e-06}, {"x": 0.06766917293233082, "y": 0.05185185185185185, "ox": 0.06766917293233082, "oy": 0.05185185185185185, "term": "basic", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5043010752688172, "os": -0.01580333625987708, "bg": 3.7688110930903057e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "advantage", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 4.2384879714890236e-07}, {"x": 0.022556390977443604, "y": 0.0962962962962963, "ox": 0.022556390977443604, "oy": 0.0962962962962963, "term": "goals", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.678494623655914, "os": 0.07309043020193151, "bg": 8.687634997703207e-07}, {"x": 0.09022556390977442, "y": 0.05185185185185185, "ox": 0.09022556390977442, "oy": 0.05185185185185185, "term": "english", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.39032258064516123, "os": -0.038191395961369626, "bg": 2.20368119608578e-07}, {"x": 0.06766917293233082, "y": 0.3333333333333333, "ox": 0.06766917293233082, "oy": 0.3333333333333333, "term": "employment", "cat25k": 32, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 9, "s": 0.9182795698924732, "os": 0.2636084284460053, "bg": 1.364112063271155e-06}, {"x": 0.0150375939849624, "y": 0.08888888888888888, "ox": 0.0150375939849624, "oy": 0.08888888888888888, "term": "social", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.6806451612903226, "os": 0.07320017559262511, "bg": 2.005248594965279e-07}, {"x": 0.14285714285714285, "y": 0.07407407407407408, "ox": 0.14285714285714285, "oy": 0.07407407407407408, "term": "benefits", "cat25k": 7, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 19, "s": 0.24838709677419354, "os": -0.0683713784021071, "bg": 7.567044898447714e-07}, {"x": 0.0150375939849624, "y": 0.17777777777777778, "ox": 0.0150375939849624, "oy": 0.17777777777777778, "term": "toolkits", "cat25k": 17, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.8129032258064517, "os": 0.16143546971027217, "bg": 6.865665335783848e-05}, {"x": 0.030075187969924807, "y": 0.20740740740740743, "ox": 0.030075187969924807, "oy": 0.20740740740740743, "term": "numpy", "cat25k": 20, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 4, "s": 0.8376344086021505, "os": 0.17592186128182616, "bg": 0.0006373359357884045}, {"x": 0.0, "y": 0.2, "ox": 0.0, "oy": 0.2, "term": "natural", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 0, "s": 0.8623655913978494, "os": 0.1984196663740123, "bg": 5.134594406834967e-07}, {"x": 0.5263157894736841, "y": 0.44444444444444436, "ox": 0.5263157894736841, "oy": 0.44444444444444436, "term": "language", "cat25k": 44, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 80, "s": 0.2172043010752688, "os": -0.08132133450395079, "bg": 2.049279322941202e-06}, {"x": 0.44360902255639095, "y": 0.3555555555555555, "ox": 0.44360902255639095, "oy": 0.3555555555555555, "term": "processing", "cat25k": 34, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 64, "s": 0.2064516129032258, "os": -0.08746707638279194, "bg": 4.068102776548546e-06}, {"x": 0.18045112781954886, "y": 0.02962962962962963, "ox": 0.18045112781954886, "oy": 0.02962962962962963, "term": "procedures", "cat25k": 3, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 24, "s": 0.11720430107526882, "os": -0.14980245829675154, "bg": 1.1635062201769721e-06}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "amounts", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 4.046997375825726e-07}, {"x": 0.5639097744360901, "y": 0.0962962962962963, "ox": 0.5639097744360901, "oy": 0.0962962962962963, "term": "structured", "cat25k": 9, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 87, "s": 0.012903225806451611, "os": -0.4642230026338894, "bg": 2.3210690751317296e-05}, {"x": 0.30827067669172925, "y": 0.05925925925925926, "ox": 0.30827067669172925, "oy": 0.05925925925925926, "term": "unstructured", "cat25k": 6, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 41, "s": 0.07204301075268815, "os": -0.24725636523266026, "bg": 0.00011780672724496948}, {"x": 0.32330827067669166, "y": 0.10370370370370371, "ox": 0.32330827067669166, "oy": 0.10370370370370371, "term": "hive", "cat25k": 10, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 43, "s": 0.08494623655913978, "os": -0.21806409130816506, "bg": 0.00010932787333310956}, {"x": 0.8270676691729322, "y": 0.49629629629629624, "ox": 0.8270676691729322, "oy": 0.49629629629629624, "term": "technologies", "cat25k": 51, "ncat25k": 131, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 163, "s": 0.052688172043010746, "os": -0.32835820895522383, "bg": 8.06315695857994e-06}, {"x": 0.037593984962406006, "y": 0.11851851851851852, "ox": 0.037593984962406006, "oy": 0.11851851851851852, "term": "performing", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.6935483870967742, "os": 0.08022388059701493, "bg": 2.0024195903383255e-06}, {"x": 0.20300751879699244, "y": 0.05185185185185185, "ox": 0.20300751879699244, "oy": 0.05185185185185185, "term": "oracle", "cat25k": 5, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 27, "s": 0.1139784946236559, "os": -0.1501316944688323, "bg": 3.934875268215275e-06}, {"x": 0.22556390977443602, "y": 0.07407407407407408, "ox": 0.22556390977443602, "oy": 0.07407407407407408, "term": "mysql", "cat25k": 7, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 30, "s": 0.1129032258064516, "os": -0.1504609306409131, "bg": 3.51328094855775e-06}, {"x": 0.06766917293233082, "y": 0.05925925925925926, "ox": 0.06766917293233082, "oy": 0.05925925925925926, "term": "attention", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 9, "s": 0.556989247311828, "os": -0.00845039508340649, "bg": 7.585951620816655e-07}, {"x": 0.10526315789473684, "y": 0.2444444444444444, "ox": 0.10526315789473684, "oy": 0.2444444444444444, "term": "detail", "cat25k": 23, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 14, "s": 0.7763440860215053, "os": 0.13805970149253732, "bg": 2.0631430641344264e-06}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "resolve", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 1.1418862133226148e-06}, {"x": 0.35338345864661647, "y": 0.06666666666666667, "ox": 0.35338345864661647, "oy": 0.06666666666666667, "term": "quality", "cat25k": 6, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 48, "s": 0.06021505376344086, "os": -0.2846795434591747, "bg": 6.013395281080891e-07}, {"x": 0.16541353383458646, "y": 0.04444444444444444, "ox": 0.16541353383458646, "oy": 0.04444444444444444, "term": "issues", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 22, "s": 0.15161290322580645, "os": -0.120171202809482, "bg": 3.7290207705324897e-07}, {"x": 0.07518796992481203, "y": 0.13333333333333333, "ox": 0.07518796992481203, "oy": 0.13333333333333333, "term": "experienced", "cat25k": 13, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.6397849462365591, "os": 0.05761633011413521, "bg": 2.288384410005258e-06}, {"x": 0.11278195488721804, "y": 0.14074074074074075, "ox": 0.11278195488721804, "oy": 0.14074074074074075, "term": "multi", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 15, "s": 0.6043010752688172, "os": 0.027655838454784906, "bg": 1.2319137845862514e-06}, {"x": 0.007518796992481202, "y": 0.06666666666666667, "ox": 0.007518796992481202, "oy": 0.06666666666666667, "term": "media", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.6483870967741936, "os": 0.058604038630377525, "bg": 9.237888473475927e-08}, {"x": 0.31578947368421045, "y": 0.08888888888888888, "ox": 0.31578947368421045, "oy": 0.08888888888888888, "term": "designing", "cat25k": 8, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 42, "s": 0.08172043010752687, "os": -0.22530728709394204, "bg": 1.1319347510885963e-05}, {"x": 0.0, "y": 0.0962962962962963, "ox": 0.0, "oy": 0.0962962962962963, "term": "experiments", "cat25k": 9, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.7301075268817204, "os": 0.09547848990342406, "bg": 1.8898481645104103e-06}, {"x": 0.06766917293233082, "y": 0.4222222222222222, "ox": 0.06766917293233082, "oy": 0.4222222222222222, "term": "solve", "cat25k": 41, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 9, "s": 0.9623655913978494, "os": 0.3518437225636523, "bg": 1.005962621535633e-05}, {"x": 0.2706766917293233, "y": 0.22962962962962963, "ox": 0.2706766917293233, "oy": 0.22962962962962963, "term": "real", "cat25k": 22, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 36, "s": 0.3870967741935483, "os": -0.040825285338015826, "bg": 4.500544802703521e-07}, {"x": 0.05263157894736841, "y": 0.23703703703703705, "ox": 0.05263157894736841, "oy": 0.23703703703703705, "term": "world", "cat25k": 22, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 7, "s": 0.8440860215053764, "os": 0.18294556628621597, "bg": 1.8055494576466233e-07}, {"x": 0.6165413533834585, "y": 0.13333333333333333, "ox": 0.6165413533834585, "oy": 0.13333333333333333, "term": "good", "cat25k": 13, "ncat25k": 76, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 94, "s": 0.008602150537634409, "os": -0.4796971027216857, "bg": 6.12249897590955e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "experts", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 5.626581596920761e-07}, {"x": 0.0, "y": 0.06666666666666667, "ox": 0.0, "oy": 0.06666666666666667, "term": "outside", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.6666666666666666, "os": 0.06606672519754171, "bg": 2.6673374131148183e-07}, {"x": 0.15789473684210525, "y": 0.5259259259259259, "ox": 0.15789473684210525, "oy": 0.5259259259259259, "term": "your", "cat25k": 54, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 21, "s": 0.964516129032258, "os": 0.3652326602282704, "bg": 9.50471780634434e-08}, {"x": 0.9398496240601503, "y": 0.7851851851851852, "ox": 0.9398496240601503, "oy": 0.7851851851851852, "term": "you", "cat25k": 129, "ncat25k": 387, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 183, "ncat": 480, "s": 0.1118279569892473, "os": -0.1535338015803337, "bg": 4.425533600178169e-07}, {"x": 0.26315789473684204, "y": 0.5555555555555555, "ox": 0.26315789473684204, "oy": 0.5555555555555555, "term": "new", "cat25k": 60, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 35, "s": 0.9419354838709677, "os": 0.2901668129938543, "bg": 1.5470636746281585e-07}, {"x": 0.4661654135338345, "y": 0.014814814814814812, "ox": 0.4661654135338345, "oy": 0.014814814814814812, "term": "go", "cat25k": 1, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 68, "s": 0.01827956989247312, "os": -0.44809043020193157, "bg": 3.324202864048183e-07}, {"x": 0.09022556390977442, "y": 0.23703703703703705, "ox": 0.09022556390977442, "oy": 0.23703703703703705, "term": "create", "cat25k": 22, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 12, "s": 0.7806451612903226, "os": 0.14563213345039508, "bg": 6.416878162960368e-07}, {"x": 0.04511278195488721, "y": 0.17037037037037037, "ox": 0.04511278195488721, "oy": 0.17037037037037037, "term": "masters", "cat25k": 16, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 6, "s": 0.7634408602150538, "os": 0.12423178226514488, "bg": 3.7416103258639397e-06}, {"x": 0.0, "y": 0.42962962962962964, "ox": 0.0, "oy": 0.42962962962962964, "term": "tensorflow", "cat25k": 42, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 0, "s": 0.9752688172043011, "os": 0.4263608428446005, "bg": 0.0017848908985438263}, {"x": 0.0, "y": 0.26666666666666666, "ox": 0.0, "oy": 0.26666666666666666, "term": "keras", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.9225806451612903, "os": 0.26459613696224754, "bg": 0.0010713169759102475}, {"x": 0.05263157894736841, "y": 0.05185185185185185, "ox": 0.05263157894736841, "oy": 0.05185185185185185, "term": "customers", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.578494623655914, "os": -0.0008779631255487266, "bg": 3.318130174394404e-07}, {"x": 0.11278195488721804, "y": 0.05185185185185185, "ox": 0.11278195488721804, "oy": 0.05185185185185185, "term": "medical", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 15, "s": 0.27634408602150534, "os": -0.06057945566286216, "bg": 2.832830366447854e-07}, {"x": 0.5187969924812029, "y": 0.26666666666666666, "ox": 0.5187969924812029, "oy": 0.26666666666666666, "term": "into", "cat25k": 25, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 79, "s": 0.06989247311827956, "os": -0.25032923617208075, "bg": 5.164099592759107e-07}, {"x": 0.060150375939849614, "y": 0.02962962962962963, "ox": 0.060150375939849614, "oy": 0.02962962962962963, "term": "consumer", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.4279569892473118, "os": -0.03039947322212467, "bg": 3.543324942739869e-07}, {"x": 0.07518796992481203, "y": 0.11851851851851852, "ox": 0.07518796992481203, "oy": 0.11851851851851852, "term": "products", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.6150537634408603, "os": 0.04291044776119403, "bg": 1.2546905248032422e-07}, {"x": 0.0, "y": 0.16296296296296298, "ox": 0.0, "oy": 0.16296296296296298, "term": "ready", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.8161290322580645, "os": 0.16165496049165937, "bg": 7.013661289159721e-07}, {"x": 0.0150375939849624, "y": 0.08888888888888888, "ox": 0.0150375939849624, "oy": 0.08888888888888888, "term": "combination", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.6806451612903226, "os": 0.07320017559262511, "bg": 9.888881115129565e-07}, {"x": 0.0, "y": 0.0962962962962963, "ox": 0.0, "oy": 0.0962962962962963, "term": "explain", "cat25k": 9, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.7301075268817204, "os": 0.09547848990342406, "bg": 1.037878617620728e-06}, {"x": 0.11278195488721804, "y": 0.14814814814814817, "ox": 0.11278195488721804, "oy": 0.14814814814814817, "term": "following", "cat25k": 14, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 15, "s": 0.6075268817204301, "os": 0.035008779631255496, "bg": 3.1706182466878784e-07}, {"x": 0.09774436090225562, "y": 0.11851851851851852, "ox": 0.09774436090225562, "oy": 0.11851851851851852, "term": "scientific", "cat25k": 11, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 13, "s": 0.5989247311827957, "os": 0.020522388059701496, "bg": 1.3652631988526329e-06}, {"x": 0.08270676691729321, "y": 0.05925925925925926, "ox": 0.08270676691729321, "oy": 0.05925925925925926, "term": "ideas", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.45913978494623653, "os": -0.02337576821773485, "bg": 5.639919600274557e-07}, {"x": 0.06766917293233082, "y": 0.14074074074074075, "ox": 0.06766917293233082, "oy": 0.14074074074074075, "term": "stakeholders", "cat25k": 13, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 9, "s": 0.6731182795698925, "os": 0.07243195785776999, "bg": 8.36848970728069e-06}, {"x": 0.19548872180451127, "y": 0.20740740740740743, "ox": 0.19548872180451127, "oy": 0.20740740740740743, "term": "all", "cat25k": 20, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 26, "s": 0.589247311827957, "os": 0.011742756804214216, "bg": 5.3398543555614495e-08}, {"x": 0.13533834586466165, "y": 0.08888888888888888, "ox": 0.13533834586466165, "oy": 0.08888888888888888, "term": "end", "cat25k": 8, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 18, "s": 0.3494623655913978, "os": -0.046202809482001755, "bg": 2.716412891300579e-07}, {"x": 0.022556390977443604, "y": 0.11851851851851852, "ox": 0.022556390977443604, "oy": 0.11851851851851852, "term": "customer", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.7258064516129032, "os": 0.09514925373134328, "bg": 2.0599100107712694e-07}, {"x": 0.4060150375939849, "y": 0.5777777777777777, "ox": 0.4060150375939849, "oy": 0.5777777777777777, "term": "5", "cat25k": 62, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 59, "s": 0.8301075268817204, "os": 0.17043459174714665, "bg": 0.0}, {"x": 0.037593984962406006, "y": 0.4666666666666667, "ox": 0.037593984962406006, "oy": 0.4666666666666667, "term": "ml", "cat25k": 47, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 5, "s": 0.9731182795698925, "os": 0.42581211589113255, "bg": 7.242713176688201e-06}, {"x": 0.13533834586466165, "y": 0.06666666666666667, "ox": 0.13533834586466165, "oy": 0.06666666666666667, "term": "preferably", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 18, "s": 0.24946236559139787, "os": -0.06826163301141351, "bg": 1.3588132628226678e-05}, {"x": 0.21804511278195488, "y": 0.15555555555555559, "ox": 0.21804511278195488, "oy": 0.15555555555555559, "term": "security", "cat25k": 15, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 29, "s": 0.27096774193548384, "os": -0.06211589113257243, "bg": 4.3462909014224717e-07}, {"x": 0.30827067669172925, "y": 0.18518518518518517, "ox": 0.30827067669172925, "oy": 0.18518518518518517, "term": "technology", "cat25k": 18, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 41, "s": 0.14838709677419357, "os": -0.12225636523266026, "bg": 5.528991890992004e-07}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "flink", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 0.0001638632397401129}, {"x": 0.15789473684210525, "y": 0.014814814814814812, "ox": 0.15789473684210525, "oy": 0.014814814814814812, "term": "pipeline", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 21, "s": 0.12365591397849461, "os": -0.14212028094820017, "bg": 2.886451636059612e-06}, {"x": 0.14285714285714285, "y": 0.4148148148148148, "ox": 0.14285714285714285, "oy": 0.4148148148148148, "term": "process", "cat25k": 41, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 19, "s": 0.9258064516129032, "os": 0.26986391571553997, "bg": 8.705659257952784e-07}, {"x": 0.09022556390977442, "y": 0.3555555555555555, "ox": 0.09022556390977442, "oy": 0.3555555555555555, "term": "fast", "cat25k": 34, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 12, "s": 0.9172043010752687, "os": 0.2632791922739245, "bg": 1.2977521740965603e-06}, {"x": 0.05263157894736841, "y": 0.31851851851851853, "ox": 0.05263157894736841, "oy": 0.31851851851851853, "term": "paced", "cat25k": 30, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 7, "s": 0.9193548387096774, "os": 0.26382791922739246, "bg": 3.5426546244750674e-05}, {"x": 0.007518796992481202, "y": 0.06666666666666667, "ox": 0.007518796992481202, "oy": 0.06666666666666667, "term": "m", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.6483870967741936, "os": 0.058604038630377525, "bg": 5.8539266350029534e-08}, {"x": 0.07518796992481203, "y": 0.007407407407407408, "ox": 0.07518796992481203, "oy": 0.007407407407407408, "term": "curation", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.26236559139784943, "os": -0.06738366988586479, "bg": 0.00010048690472927914}, {"x": 0.2706766917293233, "y": 0.06666666666666667, "ox": 0.2706766917293233, "oy": 0.06666666666666667, "term": "system", "cat25k": 6, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 36, "s": 0.09139784946236558, "os": -0.20258999122036875, "bg": 2.266761374307601e-07}, {"x": 0.05263157894736841, "y": 0.014814814814814812, "ox": 0.05263157894736841, "oy": 0.014814814814814812, "term": "map", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4032258064516129, "os": -0.03764266900790167, "bg": 5.811255065118164e-08}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "reduce", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 3.508954325094492e-07}, {"x": 0.08270676691729321, "y": 0.11851851851851852, "ox": 0.08270676691729321, "oy": 0.11851851851851852, "term": "office", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 11, "s": 0.6086021505376344, "os": 0.035447761194029856, "bg": 2.0235570520551058e-07}, {"x": 0.007518796992481202, "y": 0.08888888888888888, "ox": 0.007518796992481202, "oy": 0.08888888888888888, "term": "equipment", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.7021505376344086, "os": 0.08066286215978928, "bg": 1.800985280256376e-07}, {"x": 0.060150375939849614, "y": 0.19259259259259262, "ox": 0.060150375939849614, "oy": 0.19259259259259262, "term": "free", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.7709677419354839, "os": 0.13136523266022826, "bg": 6.704960398033902e-08}, {"x": 0.12781954887218044, "y": 0.5703703703703703, "ox": 0.12781954887218044, "oy": 0.5703703703703703, "term": "any", "cat25k": 61, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 17, "s": 0.978494623655914, "os": 0.4392010535557506, "bg": 2.926244974582242e-07}, {"x": 0.06766917293233082, "y": 0.02222222222222222, "ox": 0.06766917293233082, "oy": 0.02222222222222222, "term": "elements", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.3591397849462365, "os": -0.04521510096575943, "bg": 5.42405143736459e-07}, {"x": 0.33834586466165406, "y": 0.5925925925925927, "ox": 0.33834586466165406, "oy": 0.5925925925925927, "term": "information", "cat25k": 63, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 46, "s": 0.9075268817204301, "os": 0.2523046532045654, "bg": 2.9163843840679915e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "ideally", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.9158741158826694e-06}, {"x": 0.21052631578947367, "y": 0.28888888888888886, "ox": 0.21052631578947367, "oy": 0.28888888888888886, "term": "google", "cat25k": 27, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 28, "s": 0.6903225806451613, "os": 0.07769973661106236, "bg": 1.5832521788592426e-06}, {"x": 0.05263157894736841, "y": 0.49629629629629624, "ox": 0.05263157894736841, "oy": 0.49629629629629624, "term": "sas", "cat25k": 51, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 7, "s": 0.9795698924731182, "os": 0.44029850746268656, "bg": 2.5836028538019266e-05}, {"x": 0.0, "y": 0.10370370370370371, "ox": 0.0, "oy": 0.10370370370370371, "term": "spss", "cat25k": 10, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.7440860215053764, "os": 0.10283143107989465, "bg": 2.9421314271123717e-05}, {"x": 0.030075187969924807, "y": 0.0962962962962963, "ox": 0.030075187969924807, "oy": 0.0962962962962963, "term": "manipulation", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.6602150537634409, "os": 0.06562774363476734, "bg": 7.0667397450694416e-06}, {"x": 0.12030075187969923, "y": 0.17777777777777778, "ox": 0.12030075187969923, "oy": 0.17777777777777778, "term": "through", "cat25k": 17, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 16, "s": 0.6365591397849463, "os": 0.05695785776997367, "bg": 2.33617217383338e-07}, {"x": 0.15037593984962405, "y": 0.2444444444444444, "ox": 0.15037593984962405, "oy": 0.2444444444444444, "term": "expertise", "cat25k": 23, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 20, "s": 0.721505376344086, "os": 0.09328358208955223, "bg": 5.93894828397451e-06}, {"x": 0.11278195488721804, "y": 0.16296296296296298, "ox": 0.11278195488721804, "oy": 0.16296296296296298, "term": "product", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 15, "s": 0.6225806451612903, "os": 0.049714661984196676, "bg": 1.8537837440966978e-07}, {"x": 0.21052631578947367, "y": 0.2, "ox": 0.21052631578947367, "oy": 0.2, "term": "teams", "cat25k": 19, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 28, "s": 0.5516129032258065, "os": -0.01053555750658472, "bg": 3.192985741809251e-06}, {"x": 0.04511278195488721, "y": 0.19259259259259262, "ox": 0.04511278195488721, "oy": 0.19259259259259262, "term": "sense", "cat25k": 18, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.7827956989247312, "os": 0.1462906057945566, "bg": 1.2144941681128635e-06}, {"x": 0.15037593984962405, "y": 0.05185185185185185, "ox": 0.15037593984962405, "oy": 0.05185185185185185, "term": "managing", "cat25k": 5, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 20, "s": 0.1817204301075269, "os": -0.09789288849868306, "bg": 2.149542989247628e-06}, {"x": 0.060150375939849614, "y": 0.04444444444444444, "ox": 0.060150375939849614, "oy": 0.04444444444444444, "term": "client", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5096774193548388, "os": -0.015693590869183496, "bg": 4.0429294419867927e-07}, {"x": 0.060150375939849614, "y": 0.10370370370370371, "ox": 0.060150375939849614, "oy": 0.10370370370370371, "term": "independently", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 8, "s": 0.6182795698924731, "os": 0.04312993854258121, "bg": 5.842306585129311e-06}, {"x": 0.07518796992481203, "y": 0.007407407407407408, "ox": 0.07518796992481203, "oy": 0.007407407407407408, "term": "show", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.26236559139784943, "os": -0.06738366988586479, "bg": 8.88496988124446e-08}, {"x": 0.14285714285714285, "y": 0.05925925925925926, "ox": 0.14285714285714285, "oy": 0.05925925925925926, "term": "engineer", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 19, "s": 0.2096774193548387, "os": -0.08307726075504827, "bg": 2.381500714141501e-06}, {"x": 0.0, "y": 0.06666666666666667, "ox": 0.0, "oy": 0.06666666666666667, "term": "simulation", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.6666666666666666, "os": 0.06606672519754171, "bg": 1.108920308982162e-06}, {"x": 0.30075187969924805, "y": 0.22222222222222218, "ox": 0.30075187969924805, "oy": 0.22222222222222218, "term": "familiarity", "cat25k": 21, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 40, "s": 0.21827956989247313, "os": -0.07802897278314311, "bg": 7.644955175989599e-05}, {"x": 0.007518796992481202, "y": 0.06666666666666667, "ox": 0.007518796992481202, "oy": 0.06666666666666667, "term": "forecasting", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.6483870967741936, "os": 0.058604038630377525, "bg": 5.704695306518983e-06}, {"x": 0.030075187969924807, "y": 0.21481481481481485, "ox": 0.030075187969924807, "oy": 0.21481481481481485, "term": "optimization", "cat25k": 20, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 4, "s": 0.8483870967741934, "os": 0.18327480245829675, "bg": 4.090462689588296e-06}, {"x": 0.06766917293233082, "y": 0.26666666666666666, "ox": 0.06766917293233082, "oy": 0.26666666666666666, "term": "javascript", "cat25k": 25, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 9, "s": 0.8612903225806452, "os": 0.19743195785776996, "bg": 3.483856313068928e-06}, {"x": 0.20300751879699244, "y": 0.4222222222222222, "ox": 0.20300751879699244, "oy": 0.4222222222222222, "term": "decision", "cat25k": 41, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 27, "s": 0.8741935483870967, "os": 0.2175153643546971, "bg": 2.4200222056735226e-06}, {"x": 0.0, "y": 0.25925925925925924, "ox": 0.0, "oy": 0.25925925925925924, "term": "word", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.9118279569892472, "os": 0.257243195785777, "bg": 7.089429825213045e-07}, {"x": 0.0, "y": 0.20740740740740743, "ox": 0.0, "oy": 0.20740740740740743, "term": "embeddings", "cat25k": 20, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 0, "s": 0.8655913978494624, "os": 0.20577260755048288, "bg": 0.00030872024433002194}, {"x": 0.037593984962406006, "y": 0.21481481481481485, "ox": 0.037593984962406006, "oy": 0.21481481481481485, "term": "resources", "cat25k": 20, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 5, "s": 0.8354838709677419, "os": 0.17581211589113258, "bg": 3.1287983122636146e-07}, {"x": 0.30827067669172925, "y": 0.5555555555555555, "ox": 0.30827067669172925, "oy": 0.5555555555555555, "term": "like", "cat25k": 60, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 41, "s": 0.8989247311827957, "os": 0.24539069359086918, "bg": 4.840079695062072e-07}, {"x": 0.12030075187969923, "y": 0.2444444444444444, "ox": 0.12030075187969923, "oy": 0.2444444444444444, "term": "platform", "cat25k": 23, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 16, "s": 0.7612903225806451, "os": 0.12313432835820895, "bg": 2.579103947179109e-06}, {"x": 0.05263157894736841, "y": 0.02962962962962963, "ox": 0.05263157894736841, "oy": 0.02962962962962963, "term": "report", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4666666666666666, "os": -0.02293678665496049, "bg": 7.684125066344561e-08}, {"x": 0.19548872180451127, "y": 0.25925925925925924, "ox": 0.19548872180451127, "oy": 0.25925925925925924, "term": "best", "cat25k": 25, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 26, "s": 0.6537634408602151, "os": 0.06321334503950835, "bg": 3.2802755044243656e-07}, {"x": 0.15789473684210525, "y": 0.3111111111111111, "ox": 0.15789473684210525, "oy": 0.3111111111111111, "term": "bs", "cat25k": 30, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 21, "s": 0.789247311827957, "os": 0.15199736611062334, "bg": 9.550386017507223e-06}, {"x": 0.04511278195488721, "y": 0.22962962962962963, "ox": 0.04511278195488721, "oy": 0.22962962962962963, "term": "ba", "cat25k": 22, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 6, "s": 0.8451612903225806, "os": 0.18305531167690958, "bg": 4.120201190537702e-06}, {"x": 0.05263157894736841, "y": 0.21481481481481485, "ox": 0.05263157894736841, "oy": 0.21481481481481485, "term": "hbase", "cat25k": 20, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 7, "s": 0.8086021505376344, "os": 0.1608867427568042, "bg": 0.0010713169759102475}, {"x": 0.12030075187969923, "y": 0.22962962962962963, "ox": 0.12030075187969923, "oy": 0.22962962962962963, "term": "cassandra", "cat25k": 22, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 16, "s": 0.7473118279569892, "os": 0.10842844600526777, "bg": 7.778179558447641e-05}, {"x": 0.21052631578947367, "y": 0.42962962962962964, "ox": 0.21052631578947367, "oy": 0.42962962962962964, "term": "platforms", "cat25k": 42, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 28, "s": 0.8731182795698925, "os": 0.21740561896400354, "bg": 1.330792930283539e-05}, {"x": 0.15037593984962405, "y": 0.2962962962962963, "ox": 0.15037593984962405, "oy": 0.2962962962962963, "term": "expert", "cat25k": 28, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 20, "s": 0.7784946236559139, "os": 0.14475417032484633, "bg": 3.1911129631385233e-06}, {"x": 0.0, "y": 0.3555555555555555, "ox": 0.0, "oy": 0.3555555555555555, "term": "retrieval", "cat25k": 34, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 0, "s": 0.9634408602150538, "os": 0.3528314310798946, "bg": 1.6675503549159725e-05}, {"x": 0.022556390977443604, "y": 0.18518518518518517, "ox": 0.022556390977443604, "oy": 0.18518518518518517, "term": "previous", "cat25k": 18, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 3, "s": 0.810752688172043, "os": 0.16132572431957856, "bg": 2.7755766326126376e-07}, {"x": 0.007518796992481202, "y": 0.17037037037037037, "ox": 0.007518796992481202, "oy": 0.17037037037037037, "term": "ecommerce", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.8150537634408602, "os": 0.16154521510096578, "bg": 4.481545834543195e-06}, {"x": 0.0150375939849624, "y": 0.22962962962962963, "ox": 0.0150375939849624, "oy": 0.22962962962962963, "term": "key", "cat25k": 22, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 2, "s": 0.8709677419354839, "os": 0.21290605794556627, "bg": 4.819979639383584e-07}, {"x": 0.0150375939849624, "y": 0.3259259259259259, "ox": 0.0150375939849624, "oy": 0.3259259259259259, "term": "presentation", "cat25k": 31, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 2, "s": 0.9516129032258065, "os": 0.3084942932396839, "bg": 2.456318710342968e-06}, {"x": 0.037593984962406006, "y": 0.4666666666666667, "ox": 0.037593984962406006, "oy": 0.4666666666666667, "term": "quickly", "cat25k": 47, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 5, "s": 0.9731182795698925, "os": 0.42581211589113255, "bg": 3.5795250273554015e-06}, {"x": 0.022556390977443604, "y": 0.25925925925925924, "ox": 0.022556390977443604, "oy": 0.25925925925925924, "term": "adapt", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 3, "s": 0.8849462365591398, "os": 0.23485513608428446, "bg": 1.6024370537423655e-05}, {"x": 0.06766917293233082, "y": 0.3259259259259259, "ox": 0.06766917293233082, "oy": 0.3259259259259259, "term": "flexible", "cat25k": 31, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 9, "s": 0.9086021505376344, "os": 0.2562554872695347, "bg": 5.354440199843873e-06}, {"x": 0.0, "y": 0.2518518518518518, "ox": 0.0, "oy": 0.2518518518518518, "term": "responsive", "cat25k": 24, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 0, "s": 0.9053763440860214, "os": 0.2498902546093064, "bg": 1.5461197283285743e-05}, {"x": 0.16541353383458646, "y": 0.4, "ox": 0.16541353383458646, "oy": 0.4, "term": "able", "cat25k": 39, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 22, "s": 0.8817204301075268, "os": 0.23276997366110624, "bg": 1.4252264924718303e-06}, {"x": 0.09022556390977442, "y": 0.28148148148148144, "ox": 0.09022556390977442, "oy": 0.28148148148148144, "term": "perform", "cat25k": 27, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 12, "s": 0.853763440860215, "os": 0.1897497805092186, "bg": 3.2844859114440192e-06}, {"x": 0.037593984962406006, "y": 0.3111111111111111, "ox": 0.037593984962406006, "oy": 0.3111111111111111, "term": "meet", "cat25k": 30, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 5, "s": 0.9279569892473118, "os": 0.2714003511852502, "bg": 1.0070375857495182e-06}, {"x": 0.0, "y": 0.25925925925925924, "ox": 0.0, "oy": 0.25925925925925924, "term": "aggressive", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.9118279569892472, "os": 0.257243195785777, "bg": 9.442712680915629e-06}, {"x": 0.022556390977443604, "y": 0.31851851851851853, "ox": 0.022556390977443604, "oy": 0.31851851851851853, "term": "deadlines", "cat25k": 30, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 3, "s": 0.9440860215053763, "os": 0.2936786654960492, "bg": 2.2629654391063055e-05}, {"x": 0.07518796992481203, "y": 0.274074074074074, "ox": 0.07518796992481203, "oy": 0.274074074074074, "term": "members", "cat25k": 26, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 10, "s": 0.8602150537634408, "os": 0.19732221246707637, "bg": 4.146021629150883e-07}, {"x": 0.022556390977443604, "y": 0.3037037037037037, "ox": 0.022556390977443604, "oy": 0.3037037037037037, "term": "equal", "cat25k": 29, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 3, "s": 0.9354838709677419, "os": 0.278972783143108, "bg": 2.521521111048678e-06}, {"x": 0.05263157894736841, "y": 0.3111111111111111, "ox": 0.05263157894736841, "oy": 0.3111111111111111, "term": "opportunity", "cat25k": 30, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 7, "s": 0.9096774193548387, "os": 0.25647497805092184, "bg": 1.6306028345334792e-06}, {"x": 0.04511278195488721, "y": 0.274074074074074, "ox": 0.04511278195488721, "oy": 0.274074074074074, "term": "employer", "cat25k": 26, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 6, "s": 0.8806451612903226, "os": 0.22717295873573312, "bg": 3.474995849198272e-06}, {"x": 0.04511278195488721, "y": 0.3259259259259259, "ox": 0.04511278195488721, "oy": 0.3259259259259259, "term": "do", "cat25k": 31, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 6, "s": 0.9344086021505377, "os": 0.2786435469710272, "bg": 1.0517249444408682e-07}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "discriminate", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 4.286569944656906e-05}, {"x": 0.0150375939849624, "y": 0.26666666666666666, "ox": 0.0150375939849624, "oy": 0.26666666666666666, "term": "against", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 2, "s": 0.9, "os": 0.2496707638279192, "bg": 5.158591599692423e-07}, {"x": 0.10526315789473684, "y": 0.28148148148148144, "ox": 0.10526315789473684, "oy": 0.28148148148148144, "term": "employee", "cat25k": 27, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 14, "s": 0.8311827956989247, "os": 0.17482440737489024, "bg": 2.2713848425903005e-06}, {"x": 0.0, "y": 0.25925925925925924, "ox": 0.0, "oy": 0.25925925925925924, "term": "applicant", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.9118279569892472, "os": 0.257243195785777, "bg": 3.6334338617972563e-06}, {"x": 0.0, "y": 0.25925925925925924, "ox": 0.0, "oy": 0.25925925925925924, "term": "because", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.9118279569892472, "os": 0.257243195785777, "bg": 2.5793025736811677e-07}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "race", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 1.4131732419389639e-06}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "color", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 6.546768441667224e-07}, {"x": 0.007518796992481202, "y": 0.2518518518518518, "ox": 0.007518796992481202, "oy": 0.2518518518518518, "term": "sex", "cat25k": 24, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 1, "s": 0.8946236559139785, "os": 0.24242756804214222, "bg": 2.186316621967163e-07}, {"x": 0.007518796992481202, "y": 0.274074074074074, "ox": 0.007518796992481202, "oy": 0.274074074074074, "term": "age", "cat25k": 26, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 1, "s": 0.921505376344086, "os": 0.264486391571554, "bg": 5.723194274974303e-07}, {"x": 0.0150375939849624, "y": 0.28888888888888886, "ox": 0.0150375939849624, "oy": 0.28888888888888886, "term": "national", "cat25k": 27, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 2, "s": 0.9290322580645161, "os": 0.271729587357331, "bg": 3.0775717112490714e-07}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "origin", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 2.336592385026472e-06}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "religion", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 1.8425115723421085e-06}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "sexual", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 2.198267836240216e-06}, {"x": 0.04511278195488721, "y": 0.28148148148148144, "ox": 0.04511278195488721, "oy": 0.28148148148148144, "term": "orientation", "cat25k": 27, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 6, "s": 0.8838709677419354, "os": 0.23452589991220368, "bg": 6.865335966917818e-06}, {"x": 0.0150375939849624, "y": 0.26666666666666666, "ox": 0.0150375939849624, "oy": 0.26666666666666666, "term": "gender", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 2, "s": 0.9, "os": 0.2496707638279192, "bg": 2.5924248935280864e-06}, {"x": 0.037593984962406006, "y": 0.25925925925925924, "ox": 0.037593984962406006, "oy": 0.25925925925925924, "term": "identity", "cat25k": 25, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.8763440860215054, "os": 0.21992976294995611, "bg": 3.002921504795497e-06}, {"x": 0.037593984962406006, "y": 0.3259259259259259, "ox": 0.037593984962406006, "oy": 0.3259259259259259, "term": "status", "cat25k": 31, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 5, "s": 0.9387096774193547, "os": 0.28610623353819137, "bg": 7.593260757928451e-07}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "veteran", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 8.265023103531813e-06}, {"x": 0.0150375939849624, "y": 0.25925925925925924, "ox": 0.0150375939849624, "oy": 0.25925925925925924, "term": "basis", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.886021505376344, "os": 0.24231782265144863, "bg": 1.2339784366937296e-06}, {"x": 0.04511278195488721, "y": 0.31851851851851853, "ox": 0.04511278195488721, "oy": 0.31851851851851853, "term": "disability", "cat25k": 30, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 6, "s": 0.9268817204301074, "os": 0.27129060579455666, "bg": 4.481102914655657e-06}, {"x": 0.0, "y": 0.26666666666666666, "ox": 0.0, "oy": 0.26666666666666666, "term": "federal", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.9225806451612903, "os": 0.26459613696224754, "bg": 7.35408259137279e-07}, {"x": 0.007518796992481202, "y": 0.25925925925925924, "ox": 0.007518796992481202, "oy": 0.25925925925925924, "term": "state", "cat25k": 25, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.9032258064516129, "os": 0.24978050921861283, "bg": 1.5888030341901144e-07}, {"x": 0.0, "y": 0.3925925925925926, "ox": 0.0, "oy": 0.3925925925925926, "term": "local", "cat25k": 39, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 0, "s": 0.9666666666666667, "os": 0.38959613696224754, "bg": 4.0618859940044866e-07}, {"x": 0.0150375939849624, "y": 0.26666666666666666, "ox": 0.0150375939849624, "oy": 0.26666666666666666, "term": "protected", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 2, "s": 0.9, "os": 0.2496707638279192, "bg": 2.5110704339727743e-06}, {"x": 0.0, "y": 0.2444444444444444, "ox": 0.0, "oy": 0.2444444444444444, "term": "class", "cat25k": 23, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 0, "s": 0.8956989247311827, "os": 0.24253731343283583, "bg": 3.452695908123762e-07}, {"x": 0.05263157894736841, "y": 0.02962962962962963, "ox": 0.05263157894736841, "oy": 0.02962962962962963, "term": "functions", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4666666666666666, "os": -0.02293678665496049, "bg": 4.240260074424273e-07}, {"x": 0.022556390977443604, "y": 0.11851851851851852, "ox": 0.022556390977443604, "oy": 0.11851851851851852, "term": "analyses", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.7258064516129032, "os": 0.09514925373134328, "bg": 3.7836335148680863e-06}, {"x": 0.5563909774436089, "y": 0.31851851851851853, "ox": 0.5563909774436089, "oy": 0.31851851851851853, "term": "level", "cat25k": 30, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 86, "s": 0.07419354838709677, "os": -0.23617208077260748, "bg": 1.273250755553414e-06}, {"x": 0.07518796992481203, "y": 0.06666666666666667, "ox": 0.07518796992481203, "oy": 0.06666666666666667, "term": "creating", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 10, "s": 0.5559139784946237, "os": -0.008560140474100086, "bg": 9.760859958476273e-07}, {"x": 0.28571428571428564, "y": 0.16296296296296298, "ox": 0.28571428571428564, "oy": 0.16296296296296298, "term": "support", "cat25k": 15, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 38, "s": 0.14946236559139783, "os": -0.12192712906057945, "bg": 3.2121651117110723e-07}, {"x": 0.030075187969924807, "y": 0.15555555555555559, "ox": 0.030075187969924807, "oy": 0.15555555555555559, "term": "network", "cat25k": 15, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7655913978494624, "os": 0.12445127304653206, "bg": 2.2193992745769186e-07}, {"x": 0.19548872180451127, "y": 0.0962962962962963, "ox": 0.19548872180451127, "oy": 0.0962962962962963, "term": "integration", "cat25k": 9, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 26, "s": 0.17956989247311828, "os": -0.0985513608428446, "bg": 2.397796388230041e-06}, {"x": 0.15037593984962405, "y": 0.08888888888888888, "ox": 0.15037593984962405, "oy": 0.08888888888888888, "term": "bi", "cat25k": 8, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 20, "s": 0.27419354838709675, "os": -0.06112818261633013, "bg": 4.87164657543798e-06}, {"x": 0.21804511278195488, "y": 0.15555555555555559, "ox": 0.21804511278195488, "oy": 0.15555555555555559, "term": "practices", "cat25k": 15, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 29, "s": 0.27096774193548384, "os": -0.06211589113257243, "bg": 2.626191591759148e-06}, {"x": 0.0, "y": 0.16296296296296298, "ox": 0.0, "oy": 0.16296296296296298, "term": "efficiencies", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.8161290322580645, "os": 0.16165496049165937, "bg": 2.207371517181628e-05}, {"x": 0.15789473684210525, "y": 0.08888888888888888, "ox": 0.15789473684210525, "oy": 0.08888888888888888, "term": "making", "cat25k": 8, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 21, "s": 0.24731182795698922, "os": -0.0685908691834943, "bg": 5.311191608568334e-07}, {"x": 0.007518796992481202, "y": 0.08148148148148147, "ox": 0.007518796992481202, "oy": 0.08148148148148147, "term": "recommendations", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.6849462365591398, "os": 0.0733099209833187, "bg": 6.705858872573565e-07}, {"x": 0.22556390977443602, "y": 0.02962962962962963, "ox": 0.22556390977443602, "oy": 0.02962962962962963, "term": "architecture", "cat25k": 3, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 30, "s": 0.09677419354838708, "os": -0.1945785776997366, "bg": 1.6369120588013474e-06}, {"x": 0.05263157894736841, "y": 0.14814814814814817, "ox": 0.05263157894736841, "oy": 0.14814814814814817, "term": "analyze", "cat25k": 14, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 7, "s": 0.7247311827956989, "os": 0.09471027216856892, "bg": 6.128064982908941e-06}, {"x": 0.593984962406015, "y": 0.21481481481481485, "ox": 0.593984962406015, "oy": 0.21481481481481485, "term": "sets", "cat25k": 20, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 91, "s": 0.04516129032258064, "os": -0.37642669007901663, "bg": 4.68534849163307e-06}, {"x": 0.2706766917293233, "y": 0.10370370370370371, "ox": 0.2706766917293233, "oy": 0.10370370370370371, "term": "monitoring", "cat25k": 10, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 36, "s": 0.1064516129032258, "os": -0.16582528533801583, "bg": 2.3972379599562492e-06}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "alternative", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 2.1594657481739014e-07}, {"x": 0.31578947368421045, "y": 0.08148148148148147, "ox": 0.31578947368421045, "oy": 0.08148148148148147, "term": "sources", "cat25k": 8, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 42, "s": 0.07741935483870968, "os": -0.2326602282704126, "bg": 1.6926140947648324e-06}, {"x": 0.45112781954887204, "y": 0.02962962962962963, "ox": 0.45112781954887204, "oy": 0.02962962962962963, "term": "capable", "cat25k": 3, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 65, "s": 0.02795698924731183, "os": -0.418459174714662, "bg": 8.390253808217645e-06}, {"x": 0.09774436090225562, "y": 0.06666666666666667, "ox": 0.09774436090225562, "oy": 0.06666666666666667, "term": "leading", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 13, "s": 0.42365591397849456, "os": -0.03094820017559262, "bg": 7.819623465936494e-07}, {"x": 0.0, "y": 0.06666666666666667, "ox": 0.0, "oy": 0.06666666666666667, "term": "theory", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.6666666666666666, "os": 0.06606672519754171, "bg": 3.5754130997534493e-07}, {"x": 0.28571428571428564, "y": 0.28888888888888886, "ox": 0.28571428571428564, "oy": 0.28888888888888886, "term": "by", "cat25k": 27, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 38, "s": 0.5827956989247312, "os": 0.0030728709394205467, "bg": 4.596855587380911e-08}, {"x": 0.27819548872180444, "y": 0.13333333333333333, "ox": 0.27819548872180444, "oy": 0.13333333333333333, "term": "implementing", "cat25k": 13, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 37, "s": 0.12150537634408601, "os": -0.1438762071992976, "bg": 8.055454333482921e-06}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "pre", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 2.8209018843638693e-07}, {"x": 0.11278195488721804, "y": 0.08148148148148147, "ox": 0.11278195488721804, "oy": 0.08148148148148147, "term": "their", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 15, "s": 0.4225806451612903, "os": -0.0311676909569798, "bg": 6.641831258738606e-08}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "core", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 2.728421552199411e-07}, {"x": 0.0, "y": 0.08148148148148147, "ox": 0.0, "oy": 0.08148148148148147, "term": "presentations", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.7043010752688171, "os": 0.08077260755048289, "bg": 1.4583096772303304e-06}, {"x": 0.0, "y": 0.08148148148148147, "ox": 0.0, "oy": 0.08148148148148147, "term": "enthusiasm", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.7043010752688171, "os": 0.08077260755048289, "bg": 4.940001437091326e-06}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "contributions", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 5.467906781849338e-07}, {"x": 0.10526315789473684, "y": 0.037037037037037035, "ox": 0.10526315789473684, "oy": 0.037037037037037035, "term": "activities", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 14, "s": 0.2505376344086021, "os": -0.06782265144863917, "bg": 2.86247222083702e-07}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "version", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 6.881331760650835e-08}, {"x": 0.16541353383458646, "y": 0.02222222222222222, "ox": 0.16541353383458646, "oy": 0.02222222222222222, "term": "control", "cat25k": 2, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 22, "s": 0.12258064516129032, "os": -0.14223002633889376, "bg": 2.3188120925994978e-07}, {"x": 0.20300751879699244, "y": 0.07407407407407408, "ox": 0.20300751879699244, "oy": 0.07407407407407408, "term": "query", "cat25k": 7, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 27, "s": 0.13870967741935483, "os": -0.12807287093942055, "bg": 2.360332055341111e-06}, {"x": 0.09022556390977442, "y": 0.02222222222222222, "ox": 0.09022556390977442, "oy": 0.02222222222222222, "term": "postgres", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.25483870967741934, "os": -0.06760316066725197, "bg": 3.6928167328911804e-05}, {"x": 0.7518796992481201, "y": 0.28148148148148144, "ox": 0.7518796992481201, "oy": 0.28148148148148144, "term": "about", "cat25k": 27, "ncat25k": 106, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 132, "s": 0.01182795698924731, "os": -0.4669666374012291, "bg": 2.7714348442103556e-07}, {"x": 0.07518796992481203, "y": 0.3037037037037037, "ox": 0.07518796992481203, "oy": 0.3037037037037037, "term": "full", "cat25k": 29, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 10, "s": 0.8784946236559139, "os": 0.22673397717295873, "bg": 3.290360780672418e-07}, {"x": 0.007518796992481202, "y": 0.26666666666666666, "ox": 0.007518796992481202, "oy": 0.26666666666666666, "term": "student", "cat25k": 25, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 1, "s": 0.910752688172043, "os": 0.2571334503950834, "bg": 5.151145287892741e-07}, {"x": 0.0, "y": 0.274074074074074, "ox": 0.0, "oy": 0.274074074074074, "term": "gpa", "cat25k": 26, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 0, "s": 0.9311827956989247, "os": 0.27194907813871816, "bg": 2.3401510788887066e-05}, {"x": 0.14285714285714285, "y": 0.02962962962962963, "ox": 0.14285714285714285, "oy": 0.02962962962962963, "term": "modelling", "cat25k": 3, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 19, "s": 0.16236559139784945, "os": -0.11248902546093062, "bg": 6.966587338439539e-06}, {"x": 0.07518796992481203, "y": 0.04444444444444444, "ox": 0.07518796992481203, "oy": 0.04444444444444444, "term": "two", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.4268817204301075, "os": -0.030618964003511856, "bg": 7.248582475139299e-08}, {"x": 0.09774436090225562, "y": 0.28888888888888886, "ox": 0.09774436090225562, "oy": 0.28888888888888886, "term": "people", "cat25k": 27, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 13, "s": 0.8526881720430107, "os": 0.18964003511852504, "bg": 2.1649951145323945e-07}, {"x": 0.33082706766917286, "y": 0.10370370370370371, "ox": 0.33082706766917286, "oy": 0.10370370370370371, "term": "nosql", "cat25k": 10, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 44, "s": 0.08064516129032256, "os": -0.22552677787532927, "bg": 0.0017254458641360127}, {"x": 0.07518796992481203, "y": 0.037037037037037035, "ox": 0.07518796992481203, "oy": 0.037037037037037035, "term": "bonus", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.3924731182795699, "os": -0.03797190517998244, "bg": 1.0757826480131693e-06}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "international", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 4.734426104483867e-08}, {"x": 0.09022556390977442, "y": 0.037037037037037035, "ox": 0.09022556390977442, "oy": 0.037037037037037035, "term": "competitive", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.32258064516129026, "os": -0.0528972783143108, "bg": 1.3375548663039164e-06}, {"x": 0.05263157894736841, "y": 0.13333333333333333, "ox": 0.05263157894736841, "oy": 0.13333333333333333, "term": "salary", "cat25k": 13, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.6913978494623656, "os": 0.08000438981562774, "bg": 2.355883869437481e-06}, {"x": 0.0, "y": 0.07407407407407408, "ox": 0.0, "oy": 0.07407407407407408, "term": "industrial", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.6870967741935484, "os": 0.0734196663740123, "bg": 3.0308160188749526e-07}, {"x": 0.06766917293233082, "y": 0.02962962962962963, "ox": 0.06766917293233082, "oy": 0.02962962962962963, "term": "desired", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.3956989247311828, "os": -0.03786215978928884, "bg": 1.6937137553399213e-06}, {"x": 0.07518796992481203, "y": 0.02962962962962963, "ox": 0.07518796992481203, "oy": 0.02962962962962963, "term": "travel", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3548387096774193, "os": -0.04532484635645303, "bg": 9.729435551417965e-08}, {"x": 0.060150375939849614, "y": 0.17037037037037037, "ox": 0.060150375939849614, "oy": 0.17037037037037037, "term": "organization", "cat25k": 16, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 8, "s": 0.7494623655913978, "os": 0.10930640913081652, "bg": 8.193332625358914e-07}, {"x": 0.11278195488721804, "y": 0.11111111111111112, "ox": 0.11278195488721804, "oy": 0.11111111111111112, "term": "provide", "cat25k": 11, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 15, "s": 0.5741935483870968, "os": -0.0017559262510974533, "bg": 3.3129367779299e-07}, {"x": 0.13533834586466165, "y": 0.05185185185185185, "ox": 0.13533834586466165, "oy": 0.05185185185185185, "term": "own", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 18, "s": 0.210752688172043, "os": -0.08296751536435469, "bg": 2.154439542180046e-07}, {"x": 0.12781954887218044, "y": 0.2444444444444444, "ox": 0.12781954887218044, "oy": 0.2444444444444444, "term": "writing", "cat25k": 23, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 17, "s": 0.7548387096774193, "os": 0.11567164179104478, "bg": 1.201433002004543e-06}, {"x": 0.26315789473684204, "y": 0.06666666666666667, "ox": 0.26315789473684204, "oy": 0.06666666666666667, "term": "appropriate", "cat25k": 6, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 35, "s": 0.09569892473118279, "os": -0.19512730465320455, "bg": 1.261858313501127e-06}, {"x": 0.4887218045112781, "y": 0.02962962962962963, "ox": 0.4887218045112781, "oy": 0.02962962962962963, "term": "operating", "cat25k": 3, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 73, "s": 0.015053763440860216, "os": -0.4557726075504829, "bg": 2.4019651568750734e-06}, {"x": 0.030075187969924807, "y": 0.08148148148148147, "ox": 0.030075187969924807, "oy": 0.08148148148148147, "term": "may", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.6301075268817204, "os": 0.05092186128182617, "bg": 3.623673234019579e-08}, {"x": 0.05263157894736841, "y": 0.04444444444444444, "ox": 0.05263157894736841, "oy": 0.04444444444444444, "term": "action", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.5591397849462365, "os": -0.008230904302019316, "bg": 1.60404104827414e-07}, {"x": 0.05263157894736841, "y": 0.04444444444444444, "ox": 0.05263157894736841, "oy": 0.04444444444444444, "term": "hours", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.5591397849462365, "os": -0.008230904302019316, "bg": 1.3110780325002934e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "individuals", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 3.956089168491596e-07}, {"x": 0.12030075187969923, "y": 0.08888888888888888, "ox": 0.12030075187969923, "oy": 0.08888888888888888, "term": "different", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 16, "s": 0.42043010752688165, "os": -0.03127743634767341, "bg": 3.113508114522145e-07}, {"x": 0.15037593984962405, "y": 0.08888888888888888, "ox": 0.15037593984962405, "oy": 0.08888888888888888, "term": "up", "cat25k": 8, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 20, "s": 0.27419354838709675, "os": -0.06112818261633013, "bg": 7.710503581819864e-08}, {"x": 0.20300751879699244, "y": 0.0962962962962963, "ox": 0.20300751879699244, "oy": 0.0962962962962963, "term": "access", "cat25k": 9, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 27, "s": 0.16989247311827957, "os": -0.10601404741000878, "bg": 3.66881270043899e-07}, {"x": 0.060150375939849614, "y": 0.5185185185185185, "ox": 0.060150375939849614, "oy": 0.5185185185185185, "term": "position", "cat25k": 53, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 8, "s": 0.9838709677419354, "os": 0.4548946444249342, "bg": 1.926463184910124e-06}, {"x": 0.037593984962406006, "y": 0.10370370370370371, "ox": 0.037593984962406006, "oy": 0.10370370370370371, "term": "present", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.6580645161290323, "os": 0.06551799824407376, "bg": 3.819558661254339e-07}, {"x": 0.030075187969924807, "y": 0.08148148148148147, "ox": 0.030075187969924807, "oy": 0.08148148148148147, "term": "very", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.6301075268817204, "os": 0.05092186128182617, "bg": 8.955476398298681e-08}, {"x": 0.05263157894736841, "y": 0.02222222222222222, "ox": 0.05263157894736841, "oy": 0.02222222222222222, "term": "components", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.4301075268817204, "os": -0.03028972783143108, "bg": 3.6342546453542586e-07}, {"x": 0.09774436090225562, "y": 0.15555555555555559, "ox": 0.09774436090225562, "oy": 0.15555555555555559, "term": "health", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.6376344086021505, "os": 0.057287093942054446, "bg": 1.5437576891051636e-07}, {"x": 0.18045112781954886, "y": 0.13333333333333333, "ox": 0.18045112781954886, "oy": 0.13333333333333333, "term": "effective", "cat25k": 13, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 24, "s": 0.3473118279569892, "os": -0.04686128182616331, "bg": 1.1048555970176005e-06}, {"x": 0.04511278195488721, "y": 0.11851851851851852, "ox": 0.04511278195488721, "oy": 0.11851851851851852, "term": "translate", "cat25k": 11, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.6741935483870967, "os": 0.07276119402985075, "bg": 4.065319333605463e-06}, {"x": 0.16541353383458646, "y": 0.22222222222222218, "ox": 0.16541353383458646, "oy": 0.22222222222222218, "term": "build", "cat25k": 21, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 22, "s": 0.635483870967742, "os": 0.056299385425812115, "bg": 1.345924665645783e-06}, {"x": 0.07518796992481203, "y": 0.11851851851851852, "ox": 0.07518796992481203, "oy": 0.11851851851851852, "term": "life", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.6150537634408603, "os": 0.04291044776119403, "bg": 1.695874838592666e-07}, {"x": 0.037593984962406006, "y": 0.08888888888888888, "ox": 0.037593984962406006, "oy": 0.08888888888888888, "term": "sciences", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.6279569892473118, "os": 0.05081211589113257, "bg": 7.918200701608476e-07}, {"x": 0.20300751879699244, "y": 0.2518518518518518, "ox": 0.20300751879699244, "oy": 0.2518518518518518, "term": "oriented", "cat25k": 24, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 27, "s": 0.6204301075268818, "os": 0.048397717295873555, "bg": 7.2390517649164225e-06}, {"x": 0.007518796992481202, "y": 0.08148148148148147, "ox": 0.007518796992481202, "oy": 0.08148148148148147, "term": "complete", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.6849462365591398, "os": 0.0733099209833187, "bg": 1.6066470200513566e-07}, {"x": 0.08270676691729321, "y": 0.12592592592592594, "ox": 0.08270676691729321, "oy": 0.12592592592592594, "term": "professional", "cat25k": 12, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 11, "s": 0.6139784946236558, "os": 0.042800702370500446, "bg": 4.5443551213575716e-07}, {"x": 0.09774436090225562, "y": 0.007407407407407408, "ox": 0.09774436090225562, "oy": 0.007407407407407408, "term": "optimizing", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.1967741935483871, "os": -0.08977172958735732, "bg": 1.0044201661741421e-05}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "them", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 4.9619469282462184e-08}, {"x": 0.09022556390977442, "y": 0.02222222222222222, "ox": 0.09022556390977442, "oy": 0.02222222222222222, "term": "ecosystem", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.25483870967741934, "os": -0.06760316066725197, "bg": 6.412901046799215e-06}, {"x": 0.21052631578947367, "y": 0.037037037037037035, "ox": 0.21052631578947367, "oy": 0.037037037037037035, "term": "improve", "cat25k": 4, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 28, "s": 0.10215053763440858, "os": -0.17230026338893764, "bg": 1.0753946478390483e-06}, {"x": 0.12781954887218044, "y": 0.2518518518518518, "ox": 0.12781954887218044, "oy": 0.2518518518518518, "term": "effectively", "cat25k": 24, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 17, "s": 0.7602150537634409, "os": 0.12302458296751534, "bg": 5.210719184994477e-06}, {"x": 0.060150375939849614, "y": 0.11111111111111112, "ox": 0.060150375939849614, "oy": 0.11111111111111112, "term": "levels", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 8, "s": 0.6258064516129032, "os": 0.0504828797190518, "bg": 6.822087969133671e-07}, {"x": 0.04511278195488721, "y": 0.007407407407407408, "ox": 0.04511278195488721, "oy": 0.007407407407407408, "term": "sharing", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.4086021505376343, "os": -0.03753292361720807, "bg": 4.628373815900811e-07}, {"x": 0.037593984962406006, "y": 0.10370370370370371, "ox": 0.037593984962406006, "oy": 0.10370370370370371, "term": "senior", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.6580645161290323, "os": 0.06551799824407376, "bg": 6.666421880333412e-07}, {"x": 0.09774436090225562, "y": 0.04444444444444444, "ox": 0.09774436090225562, "oy": 0.04444444444444444, "term": "others", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.32150537634408594, "os": -0.05300702370500439, "bg": 3.409144819251807e-07}, {"x": 0.0, "y": 0.10370370370370371, "ox": 0.0, "oy": 0.10370370370370371, "term": "recommender", "cat25k": 10, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.7440860215053764, "os": 0.10283143107989465, "bg": 0.00011960700555318242}, {"x": 0.007518796992481202, "y": 0.10370370370370371, "ox": 0.007518796992481202, "oy": 0.10370370370370371, "term": "commerce", "cat25k": 10, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.7279569892473118, "os": 0.09536874451273046, "bg": 7.054370644675654e-07}, {"x": 0.007518796992481202, "y": 0.0962962962962963, "ox": 0.007518796992481202, "oy": 0.0962962962962963, "term": "clickstream", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.7161290322580645, "os": 0.08801580333625987, "bg": 0.00021227881305817956}, {"x": 0.09022556390977442, "y": 0.02962962962962963, "ox": 0.09022556390977442, "oy": 0.02962962962962963, "term": "unit", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.27956989247311825, "os": -0.06025021949078139, "bg": 3.4121850921036193e-07}, {"x": 0.007518796992481202, "y": 0.0962962962962963, "ox": 0.007518796992481202, "oy": 0.0962962962962963, "term": "noisy", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.7161290322580645, "os": 0.08801580333625987, "bg": 7.387681515994462e-06}, {"x": 0.022556390977443604, "y": 0.14074074074074075, "ox": 0.022556390977443604, "oy": 0.14074074074074075, "term": "excel", "cat25k": 13, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.7580645161290323, "os": 0.11720807726075505, "bg": 3.098447318755092e-06}, {"x": 0.10526315789473684, "y": 0.037037037037037035, "ox": 0.10526315789473684, "oy": 0.037037037037037035, "term": "employees", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 14, "s": 0.2505376344086021, "os": -0.06782265144863917, "bg": 6.83562066392297e-07}, {"x": 0.09774436090225562, "y": 0.10370370370370371, "ox": 0.09774436090225562, "oy": 0.10370370370370371, "term": "tasks", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 13, "s": 0.5849462365591398, "os": 0.0058165057067603165, "bg": 2.6547927292699894e-06}, {"x": 0.08270676691729321, "y": 0.02962962962962963, "ox": 0.08270676691729321, "oy": 0.02962962962962963, "term": "limited", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3247311827956989, "os": -0.0527875329236172, "bg": 2.824269569752186e-07}, {"x": 0.037593984962406006, "y": 0.08888888888888888, "ox": 0.037593984962406006, "oy": 0.08888888888888888, "term": "graduate", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.6279569892473118, "os": 0.05081211589113257, "bg": 8.151042656204245e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "12", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 0.0}, {"x": 0.12781954887218044, "y": 0.08888888888888888, "ox": 0.12781954887218044, "oy": 0.08888888888888888, "term": "extensive", "cat25k": 8, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 17, "s": 0.38817204301075264, "os": -0.03874012291483758, "bg": 2.670192302645857e-06}, {"x": 0.08270676691729321, "y": 0.13333333333333333, "ox": 0.08270676691729321, "oy": 0.13333333333333333, "term": "this", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 11, "s": 0.6236559139784946, "os": 0.050153643546971036, "bg": 1.7964793502747223e-08}, {"x": 0.13533834586466165, "y": 0.13333333333333333, "ox": 0.13533834586466165, "oy": 0.13333333333333333, "term": "role", "cat25k": 13, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 18, "s": 0.5709677419354838, "os": -0.002085162423178216, "bg": 9.761890012541182e-07}, {"x": 0.0150375939849624, "y": 0.11851851851851852, "ox": 0.0150375939849624, "oy": 0.11851851851851852, "term": "qualifications", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.7419354838709677, "os": 0.10261194029850747, "bg": 3.2342572528218897e-06}, {"x": 0.0150375939849624, "y": 0.08888888888888888, "ox": 0.0150375939849624, "oy": 0.08888888888888888, "term": "listed", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.6806451612903226, "os": 0.07320017559262511, "bg": 2.3725076457660013e-07}, {"x": 0.007518796992481202, "y": 0.10370370370370371, "ox": 0.007518796992481202, "oy": 0.10370370370370371, "term": "acceptable", "cat25k": 10, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.7279569892473118, "os": 0.09536874451273046, "bg": 2.0466537450045444e-06}, {"x": 0.022556390977443604, "y": 0.0962962962962963, "ox": 0.022556390977443604, "oy": 0.0962962962962963, "term": "considered", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.678494623655914, "os": 0.07309043020193151, "bg": 5.570498708471171e-07}, {"x": 0.0150375939849624, "y": 0.0962962962962963, "ox": 0.0150375939849624, "oy": 0.0962962962962963, "term": "offers", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.6989247311827957, "os": 0.0805531167690957, "bg": 2.110371070414325e-07}, {"x": 0.09774436090225562, "y": 0.19259259259259262, "ox": 0.09774436090225562, "oy": 0.19259259259259262, "term": "education", "cat25k": 18, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 13, "s": 0.7225806451612904, "os": 0.09405179982440737, "bg": 2.923480396815838e-07}, {"x": 0.18045112781954886, "y": 0.11851851851851852, "ox": 0.18045112781954886, "oy": 0.11851851851851852, "term": "take", "cat25k": 11, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 24, "s": 0.27311827956989243, "os": -0.06156716417910449, "bg": 3.025523940191866e-07}, {"x": 0.022556390977443604, "y": 0.11111111111111112, "ox": 0.022556390977443604, "oy": 0.11111111111111112, "term": "account", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.7129032258064516, "os": 0.08779631255487269, "bg": 1.767491084406743e-07}, {"x": 0.022556390977443604, "y": 0.08888888888888888, "ox": 0.022556390977443604, "oy": 0.08888888888888888, "term": "provided", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.6623655913978496, "os": 0.06573748902546092, "bg": 1.8767238294850016e-07}, {"x": 0.0, "y": 0.11111111111111112, "ox": 0.0, "oy": 0.11111111111111112, "term": "hiring", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7516129032258064, "os": 0.11018437225636524, "bg": 3.461270745558872e-06}, {"x": 0.022556390977443604, "y": 0.11111111111111112, "ox": 0.022556390977443604, "oy": 0.11111111111111112, "term": "manager", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.7129032258064516, "os": 0.08779631255487269, "bg": 4.1138676042902286e-07}, {"x": 0.04511278195488721, "y": 0.0962962962962963, "ox": 0.04511278195488721, "oy": 0.0962962962962963, "term": "regarding", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.6268817204301076, "os": 0.05070237050043898, "bg": 7.528195320216245e-07}, {"x": 0.06766917293233082, "y": 0.05185185185185185, "ox": 0.06766917293233082, "oy": 0.05185185185185185, "term": "reporting", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5043010752688172, "os": -0.01580333625987708, "bg": 8.407187851361339e-07}, {"x": 0.23308270676691725, "y": 0.16296296296296298, "ox": 0.23308270676691725, "oy": 0.16296296296296298, "term": "across", "cat25k": 15, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 31, "s": 0.24623655913978496, "os": -0.06968832309043019, "bg": 1.3826500248648737e-06}, {"x": 0.022556390977443604, "y": 0.08888888888888888, "ox": 0.022556390977443604, "oy": 0.08888888888888888, "term": "packages", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.6623655913978496, "os": 0.06573748902546092, "bg": 6.577692269727217e-07}, {"x": 0.20300751879699244, "y": 0.3259259259259259, "ox": 0.20300751879699244, "oy": 0.3259259259259259, "term": "scripting", "cat25k": 31, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 27, "s": 0.7591397849462366, "os": 0.12192712906057945, "bg": 1.9380315768125166e-05}, {"x": 0.08270676691729321, "y": 0.014814814814814812, "ox": 0.08270676691729321, "oy": 0.014814814814814812, "term": "php", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.26021505376344084, "os": -0.06749341527655837, "bg": 5.027108586376872e-07}, {"x": 0.022556390977443604, "y": 0.12592592592592594, "ox": 0.022556390977443604, "oy": 0.12592592592592594, "term": "drive", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.7376344086021506, "os": 0.10250219490781387, "bg": 3.7082854276935235e-07}, {"x": 0.5488721804511277, "y": 0.17037037037037037, "ox": 0.5488721804511277, "oy": 0.17037037037037037, "term": "datasets", "cat25k": 16, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 85, "s": 0.04731182795698924, "os": -0.37576821773485514, "bg": 0.00011246649435688951}, {"x": 0.022556390977443604, "y": 0.10370370370370371, "ox": 0.022556390977443604, "oy": 0.10370370370370371, "term": "organizational", "cat25k": 10, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.6967741935483871, "os": 0.0804433713784021, "bg": 3.020712403036242e-06}, {"x": 0.04511278195488721, "y": 0.02962962962962963, "ox": 0.04511278195488721, "oy": 0.02962962962962963, "term": "responsible", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.5193548387096775, "os": -0.01547410008779631, "bg": 3.058719460980222e-07}, {"x": 0.0150375939849624, "y": 0.07407407407407408, "ox": 0.0150375939849624, "oy": 0.07407407407407408, "term": "classification", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.6473118279569893, "os": 0.05849429323968394, "bg": 1.2001310543111306e-06}, {"x": 0.05263157894736841, "y": 0.02962962962962963, "ox": 0.05263157894736841, "oy": 0.02962962962962963, "term": "willingness", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4666666666666666, "os": -0.02293678665496049, "bg": 5.9106354900346695e-06}, {"x": 0.04511278195488721, "y": 0.014814814814814812, "ox": 0.04511278195488721, "oy": 0.014814814814814812, "term": "number", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.43225806451612897, "os": -0.030179982440737486, "bg": 4.168240785431615e-08}, {"x": 0.04511278195488721, "y": 0.007407407407407408, "ox": 0.04511278195488721, "oy": 0.007407407407407408, "term": "judgment", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.4086021505376343, "os": -0.03753292361720807, "bg": 8.996497278045487e-07}, {"x": 0.060150375939849614, "y": 0.04444444444444444, "ox": 0.060150375939849614, "oy": 0.04444444444444444, "term": "diverse", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5096774193548388, "os": -0.015693590869183496, "bg": 2.0741484361365253e-06}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "sound", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 1.7985968606311157e-07}, {"x": 0.09022556390977442, "y": 0.02222222222222222, "ox": 0.09022556390977442, "oy": 0.02222222222222222, "term": "supporting", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.25483870967741934, "os": -0.06760316066725197, "bg": 1.2220689521447168e-06}, {"x": 0.13533834586466165, "y": 0.05925925925925926, "ox": 0.13533834586466165, "oy": 0.05925925925925926, "term": "internal", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 18, "s": 0.2225806451612903, "os": -0.0756145741878841, "bg": 9.88923260734068e-07}, {"x": 0.17293233082706763, "y": 0.05185185185185185, "ox": 0.17293233082706763, "oy": 0.05185185185185185, "term": "external", "cat25k": 5, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 23, "s": 0.15053763440860216, "os": -0.12028094820017558, "bg": 1.184087451331292e-06}, {"x": 0.10526315789473684, "y": 0.02962962962962963, "ox": 0.10526315789473684, "oy": 0.02962962962962963, "term": "maintaining", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.22688172043010751, "os": -0.07517559262510976, "bg": 2.6805724839085606e-06}, {"x": 0.06766917293233082, "y": 0.05185185185185185, "ox": 0.06766917293233082, "oy": 0.05185185185185185, "term": "positive", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5043010752688172, "os": -0.01580333625987708, "bg": 5.694005192968322e-07}, {"x": 0.21804511278195488, "y": 0.19259259259259262, "ox": 0.21804511278195488, "oy": 0.19259259259259262, "term": "intelligence", "cat25k": 18, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 29, "s": 0.4559139784946236, "os": -0.02535118525021951, "bg": 3.480951631417601e-06}, {"x": 0.0, "y": 0.06666666666666667, "ox": 0.0, "oy": 0.06666666666666667, "term": "conflict", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.6666666666666666, "os": 0.06606672519754171, "bg": 7.265012270000306e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "timely", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 1.8651492917302804e-06}, {"x": 0.09022556390977442, "y": 0.037037037037037035, "ox": 0.09022556390977442, "oy": 0.037037037037037035, "term": "transformation", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.32258064516129026, "os": -0.0528972783143108, "bg": 2.9655791335572113e-06}, {"x": 0.04511278195488721, "y": 0.02962962962962963, "ox": 0.04511278195488721, "oy": 0.02962962962962963, "term": "programs", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.5193548387096775, "os": -0.01547410008779631, "bg": 1.4028648028396622e-07}, {"x": 0.0150375939849624, "y": 0.0962962962962963, "ox": 0.0150375939849624, "oy": 0.0962962962962963, "term": "creativity", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.6989247311827957, "os": 0.0805531167690957, "bg": 3.899798068456016e-06}, {"x": 0.16541353383458646, "y": 0.037037037037037035, "ox": 0.16541353383458646, "oy": 0.037037037037037035, "term": "continuous", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 22, "s": 0.13978494623655913, "os": -0.12752414398595258, "bg": 2.4612398030607068e-06}, {"x": 0.030075187969924807, "y": 0.11851851851851852, "ox": 0.030075187969924807, "oy": 0.11851851851851852, "term": "improvement", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.7107526881720431, "os": 0.08768656716417911, "bg": 1.0752419650595284e-06}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "stakeholder", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 7.449011516171804e-06}, {"x": 0.06766917293233082, "y": 0.02962962962962963, "ox": 0.06766917293233082, "oy": 0.02962962962962963, "term": "engagement", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.3956989247311828, "os": -0.03786215978928884, "bg": 2.2356496871079283e-06}, {"x": 0.007518796992481202, "y": 0.07407407407407408, "ox": 0.007518796992481202, "oy": 0.07407407407407408, "term": "huge", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.6655913978494624, "os": 0.06595697980684812, "bg": 3.111444136205617e-07}, {"x": 0.09022556390977442, "y": 0.11111111111111112, "ox": 0.09022556390977442, "oy": 0.11111111111111112, "term": "make", "cat25k": 11, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 12, "s": 0.6, "os": 0.02063213345039508, "bg": 1.3328336359030233e-07}, {"x": 0.0150375939849624, "y": 0.08888888888888888, "ox": 0.0150375939849624, "oy": 0.08888888888888888, "term": "scaling", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.6806451612903226, "os": 0.07320017559262511, "bg": 7.277833097973409e-06}, {"x": 0.16541353383458646, "y": 0.5407407407407407, "ox": 0.16541353383458646, "oy": 0.5407407407407407, "term": "can", "cat25k": 56, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 22, "s": 0.9655913978494624, "os": 0.37247585601404737, "bg": 1.6419954542289535e-07}, {"x": 0.05263157894736841, "y": 0.037037037037037035, "ox": 0.05263157894736841, "oy": 0.037037037037037035, "term": "current", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5161290322580645, "os": -0.0155838454784899, "bg": 1.1057139982286463e-07}, {"x": 0.08270676691729321, "y": 0.05185185185185185, "ox": 0.08270676691729321, "oy": 0.05185185185185185, "term": "unix", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.4247311827956989, "os": -0.03072870939420544, "bg": 1.530048689974437e-06}, {"x": 0.4887218045112781, "y": 0.037037037037037035, "ox": 0.4887218045112781, "oy": 0.037037037037037035, "term": "mapreduce", "cat25k": 4, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 73, "s": 0.017204301075268817, "os": -0.44841966637401226, "bg": 0.002319737096462401}, {"x": 0.09774436090225562, "y": 0.02962962962962963, "ox": 0.09774436090225562, "oy": 0.02962962962962963, "term": "mongodb", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 13, "s": 0.25268817204301075, "os": -0.06771290605794555, "bg": 0.0005060427457284039}, {"x": 0.037593984962406006, "y": 0.02222222222222222, "ox": 0.037593984962406006, "oy": 0.02222222222222222, "term": "solr", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.524731182795699, "os": -0.01536435469710272, "bg": 0.00010921799912625601}, {"x": 0.022556390977443604, "y": 0.10370370370370371, "ox": 0.022556390977443604, "oy": 0.10370370370370371, "term": "finance", "cat25k": 10, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.6967741935483871, "os": 0.0804433713784021, "bg": 3.5188909383093705e-07}, {"x": 0.037593984962406006, "y": 0.21481481481481485, "ox": 0.037593984962406006, "oy": 0.21481481481481485, "term": "us", "cat25k": 20, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 5, "s": 0.8354838709677419, "os": 0.17581211589113258, "bg": 5.532144158757009e-08}, {"x": 0.060150375939849614, "y": 0.037037037037037035, "ox": 0.060150375939849614, "oy": 0.037037037037037035, "term": "offer", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.4634408602150537, "os": -0.02304653204565408, "bg": 2.0586533784497252e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "only", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 2.719397623694271e-08}, {"x": 0.12030075187969923, "y": 0.06666666666666667, "ox": 0.12030075187969923, "oy": 0.06666666666666667, "term": "insurance", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 16, "s": 0.3204301075268817, "os": -0.053336259877085165, "bg": 2.5861379313872674e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "defined", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.348320510524879e-07}, {"x": 0.0150375939849624, "y": 0.18518518518518517, "ox": 0.0150375939849624, "oy": 0.18518518518518517, "term": "want", "cat25k": 18, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 2, "s": 0.8268817204301074, "os": 0.16878841088674273, "bg": 2.0868956852013391e-07}, {"x": 0.30827067669172925, "y": 0.17037037037037037, "ox": 0.30827067669172925, "oy": 0.17037037037037037, "term": "our", "cat25k": 16, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 41, "s": 0.13118279569892474, "os": -0.1369622475856014, "bg": 1.281505490865075e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "www", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.8473977738522183e-07}, {"x": 0.12781954887218044, "y": 0.08888888888888888, "ox": 0.12781954887218044, "oy": 0.08888888888888888, "term": "opportunities", "cat25k": 8, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 17, "s": 0.38817204301075264, "os": -0.03874012291483758, "bg": 8.273797299070226e-07}, {"x": 0.04511278195488721, "y": 0.037037037037037035, "ox": 0.04511278195488721, "oy": 0.037037037037037035, "term": "cyber", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5623655913978495, "os": -0.00812115891132572, "bg": 2.3513487977981114e-06}, {"x": 0.0, "y": 0.06666666666666667, "ox": 0.0, "oy": 0.06666666666666667, "term": "spatial", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.6666666666666666, "os": 0.06606672519754171, "bg": 1.874044237438906e-06}, {"x": 0.060150375939849614, "y": 0.04444444444444444, "ox": 0.060150375939849614, "oy": 0.04444444444444444, "term": "specific", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5096774193548388, "os": -0.015693590869183496, "bg": 2.596127075376436e-07}, {"x": 0.05263157894736841, "y": 0.037037037037037035, "ox": 0.05263157894736841, "oy": 0.037037037037037035, "term": "elastic", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5161290322580645, "os": -0.0155838454784899, "bg": 5.539759546736874e-06}, {"x": 0.09774436090225562, "y": 0.08888888888888888, "ox": 0.09774436090225562, "oy": 0.08888888888888888, "term": "search", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.553763440860215, "os": -0.008889376646180863, "bg": 4.88204818293711e-08}, {"x": 0.09774436090225562, "y": 0.037037037037037035, "ox": 0.09774436090225562, "oy": 0.037037037037037035, "term": "hdfs", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.2774193548387096, "os": -0.06035996488147497, "bg": 0.000315811635904274}, {"x": 0.030075187969924807, "y": 0.10370370370370371, "ox": 0.030075187969924807, "oy": 0.10370370370370371, "term": "windows", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.6774193548387097, "os": 0.07298068481123793, "bg": 2.1352397006446135e-07}, {"x": 0.0150375939849624, "y": 0.0962962962962963, "ox": 0.0150375939849624, "oy": 0.0962962962962963, "term": "supervised", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 2, "s": 0.6989247311827957, "os": 0.0805531167690957, "bg": 9.349271972191524e-06}, {"x": 0.0, "y": 0.08888888888888888, "ox": 0.0, "oy": 0.08888888888888888, "term": "unsupervised", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.7193548387096773, "os": 0.08812554872695347, "bg": 4.50962523135317e-05}, {"x": 0.030075187969924807, "y": 0.08148148148148147, "ox": 0.030075187969924807, "oy": 0.08148148148148147, "term": "applicants", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.6301075268817204, "os": 0.05092186128182617, "bg": 2.404725574320609e-06}, {"x": 0.15037593984962405, "y": 0.10370370370370371, "ox": 0.15037593984962405, "oy": 0.10370370370370371, "term": "company", "cat25k": 10, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 20, "s": 0.3483870967741935, "os": -0.04642230026338895, "bg": 2.0965688039016084e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "healthy", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 3.9437619545284237e-07}, {"x": 0.08270676691729321, "y": 0.08148148148148147, "ox": 0.08270676691729321, "oy": 0.08148148148148147, "term": "interest", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 11, "s": 0.5752688172043011, "os": -0.0013169446883230795, "bg": 3.65630284578111e-07}, {"x": 0.06766917293233082, "y": 0.05925925925925926, "ox": 0.06766917293233082, "oy": 0.05925925925925926, "term": "either", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 9, "s": 0.556989247311828, "os": -0.00845039508340649, "bg": 3.4270670095117943e-07}, {"x": 0.09774436090225562, "y": 0.02962962962962963, "ox": 0.09774436090225562, "oy": 0.02962962962962963, "term": "service", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 13, "s": 0.25268817204301075, "os": -0.06771290605794555, "bg": 6.543439460030758e-08}, {"x": 0.04511278195488721, "y": 0.037037037037037035, "ox": 0.04511278195488721, "oy": 0.037037037037037035, "term": "successful", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5623655913978495, "os": -0.00812115891132572, "bg": 5.103357132143407e-07}, {"x": 0.14285714285714285, "y": 0.14074074074074075, "ox": 0.14285714285714285, "oy": 0.14074074074074075, "term": "requirements", "cat25k": 13, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 19, "s": 0.5698924731182796, "os": -0.0021949078138717992, "bg": 7.810826629956878e-07}, {"x": 0.06766917293233082, "y": 0.06666666666666667, "ox": 0.06766917293233082, "oy": 0.06666666666666667, "term": "planning", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.5763440860215053, "os": -0.0010974539069358996, "bg": 3.4633417303336304e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "workplace", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 9.080291635618014e-07}, {"x": 0.037593984962406006, "y": 0.02222222222222222, "ox": 0.037593984962406006, "oy": 0.02222222222222222, "term": "events", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.524731182795699, "os": -0.01536435469710272, "bg": 7.948231854473558e-08}, {"x": 0.13533834586466165, "y": 0.15555555555555559, "ox": 0.13533834586466165, "oy": 0.15555555555555559, "term": "implementation", "cat25k": 15, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 18, "s": 0.5967741935483871, "os": 0.019973661106233553, "bg": 1.6602151698457589e-06}, {"x": 0.09022556390977442, "y": 0.014814814814814812, "ox": 0.09022556390977442, "oy": 0.014814814814814812, "term": "administration", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.22903225806451613, "os": -0.07495610184372256, "bg": 3.6090678344922736e-07}, {"x": 0.18796992481203004, "y": 0.037037037037037035, "ox": 0.18796992481203004, "oy": 0.037037037037037035, "term": "storage", "cat25k": 4, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 25, "s": 0.11612903225806451, "os": -0.14991220368744512, "bg": 7.37212387645146e-07}, {"x": 0.0, "y": 0.17777777777777778, "ox": 0.0, "oy": 0.17777777777777778, "term": "improved", "cat25k": 17, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 0, "s": 0.8397849462365592, "os": 0.17636084284460055, "bg": 1.7476996722298495e-06}, {"x": 0.26315789473684204, "y": 0.12592592592592594, "ox": 0.26315789473684204, "oy": 0.12592592592592594, "term": "manage", "cat25k": 12, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 35, "s": 0.13225806451612904, "os": -0.13630377524143986, "bg": 2.8487954169894163e-06}, {"x": 0.10526315789473684, "y": 0.014814814814814812, "ox": 0.10526315789473684, "oy": 0.014814814814814812, "term": "changes", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.19462365591397848, "os": -0.08988147497805092, "bg": 2.4495275236490784e-07}, {"x": 0.0, "y": 0.11111111111111112, "ox": 0.0, "oy": 0.11111111111111112, "term": "necessary", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7516129032258064, "os": 0.11018437225636524, "bg": 4.1281606178128124e-07}, {"x": 0.36090225563909767, "y": 0.11111111111111112, "ox": 0.36090225563909767, "oy": 0.11111111111111112, "term": "it", "cat25k": 11, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 49, "s": 0.07096774193548387, "os": -0.24802458296751537, "bg": 4.549928357841148e-08}, {"x": 0.05263157894736841, "y": 0.037037037037037035, "ox": 0.05263157894736841, "oy": 0.037037037037037035, "term": "personnel", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5161290322580645, "os": -0.0155838454784899, "bg": 7.197465772301571e-07}, {"x": 0.007518796992481202, "y": 0.18518518518518517, "ox": 0.007518796992481202, "oy": 0.18518518518518517, "term": "here", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 1, "s": 0.8387096774193548, "os": 0.17625109745390694, "bg": 8.127814319206574e-08}, {"x": 0.10526315789473684, "y": 0.14814814814814817, "ox": 0.10526315789473684, "oy": 0.14814814814814817, "term": "program", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 14, "s": 0.6129032258064516, "os": 0.04247146619841967, "bg": 2.216758651066893e-07}, {"x": 0.0150375939849624, "y": 0.16296296296296298, "ox": 0.0150375939849624, "oy": 0.16296296296296298, "term": "numbers", "cat25k": 15, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 2, "s": 0.7838709677419354, "os": 0.146729587357331, "bg": 7.368253893450598e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "start", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 1.1112442683564033e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "part", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 4.623567603482155e-08}, {"x": 0.17293233082706763, "y": 0.14074074074074075, "ox": 0.17293233082706763, "oy": 0.14074074074074075, "term": "application", "cat25k": 13, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 23, "s": 0.4193548387096774, "os": -0.03204565408252852, "bg": 5.495806542584964e-07}, {"x": 0.05263157894736841, "y": 0.014814814814814812, "ox": 0.05263157894736841, "oy": 0.014814814814814812, "term": "before", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4032258064516129, "os": -0.03764266900790167, "bg": 6.4838415697951e-08}, {"x": 0.05263157894736841, "y": 0.014814814814814812, "ox": 0.05263157894736841, "oy": 0.014814814814814812, "term": "additional", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4032258064516129, "os": -0.03764266900790167, "bg": 1.3610083596006687e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "several", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 1.3004292289600908e-07}, {"x": 0.06766917293233082, "y": 0.05185185185185185, "ox": 0.06766917293233082, "oy": 0.05185185185185185, "term": "personal", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5043010752688172, "os": -0.01580333625987708, "bg": 1.4901804512596107e-07}, {"x": 0.06766917293233082, "y": 0.02962962962962963, "ox": 0.06766917293233082, "oy": 0.02962962962962963, "term": "automation", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.3956989247311828, "os": -0.03786215978928884, "bg": 2.0537606608938617e-06}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "splunk", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 3.25370283002424e-05}, {"x": 0.07518796992481203, "y": 0.02962962962962963, "ox": 0.07518796992481203, "oy": 0.02962962962962963, "term": "power", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3548387096774193, "os": -0.04532484635645303, "bg": 1.235311086824797e-07}, {"x": 0.7067669172932329, "y": 0.11111111111111112, "ox": 0.7067669172932329, "oy": 0.11111111111111112, "term": "implement", "cat25k": 11, "ncat25k": 93, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 116, "s": 0.0021505376344086017, "os": -0.5913081650570676, "bg": 1.260161737430199e-05}, {"x": 0.060150375939849614, "y": 0.007407407407407408, "ox": 0.060150375939849614, "oy": 0.007407407407407408, "term": "maintenance", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.332258064516129, "os": -0.05245829675153643, "bg": 3.4566326356702865e-07}, {"x": 0.037593984962406006, "y": 0.02222222222222222, "ox": 0.037593984962406006, "oy": 0.02222222222222222, "term": "handling", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.524731182795699, "os": -0.01536435469710272, "bg": 5.048768580099471e-07}, {"x": 0.030075187969924807, "y": 0.0962962962962963, "ox": 0.030075187969924807, "oy": 0.0962962962962963, "term": "providing", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.6602150537634409, "os": 0.06562774363476734, "bg": 5.859559659606459e-07}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "optimize", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 3.242627436158971e-06}, {"x": 0.23308270676691725, "y": 0.02962962962962963, "ox": 0.23308270676691725, "oy": 0.02962962962962963, "term": "infrastructure", "cat25k": 3, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 31, "s": 0.09247311827956989, "os": -0.20204126426690078, "bg": 2.400756169600391e-06}, {"x": 0.12030075187969923, "y": 0.007407407407407408, "ox": 0.12030075187969923, "oy": 0.007407407407407408, "term": "ensure", "cat25k": 1, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.1655913978494624, "os": -0.11215978928884987, "bg": 5.707111635165704e-07}, {"x": 0.10526315789473684, "y": 0.11851851851851852, "ox": 0.10526315789473684, "oy": 0.11851851851851852, "term": "within", "cat25k": 11, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 14, "s": 0.5935483870967743, "os": 0.01305970149253731, "bg": 2.294822686107624e-07}, {"x": 0.0, "y": 0.16296296296296298, "ox": 0.0, "oy": 0.16296296296296298, "term": "concentration", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.8161290322580645, "os": 0.16165496049165937, "bg": 2.7156955041228887e-06}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "geek", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 5.124315278621833e-06}, {"x": 0.04511278195488721, "y": 0.5925925925925927, "ox": 0.04511278195488721, "oy": 0.5925925925925927, "term": "re", "cat25k": 63, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 6, "s": 0.9935483870967742, "os": 0.5433494293239683, "bg": 4.455636849500778e-07}, {"x": 0.022556390977443604, "y": 0.4666666666666667, "ox": 0.022556390977443604, "oy": 0.4666666666666667, "term": "looking", "cat25k": 47, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 3, "s": 0.9806451612903225, "os": 0.44073748902546095, "bg": 9.291372442361419e-07}, {"x": 0.007518796992481202, "y": 0.2, "ox": 0.007518796992481202, "oy": 0.2, "term": "love", "cat25k": 19, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 1, "s": 0.8580645161290323, "os": 0.19095697980684812, "bg": 2.784258844589756e-07}, {"x": 0.060150375939849614, "y": 0.36296296296296293, "ox": 0.060150375939849614, "oy": 0.36296296296296293, "term": "patterns", "cat25k": 35, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 8, "s": 0.9462365591397849, "os": 0.30048287971905185, "bg": 4.482737499586989e-06}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "puzzles", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 4.236337609479068e-06}, {"x": 0.0, "y": 0.16296296296296298, "ox": 0.0, "oy": 0.16296296296296298, "term": "folks", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.8161290322580645, "os": 0.16165496049165937, "bg": 3.181921364164898e-06}, {"x": 0.0150375939849624, "y": 0.17037037037037037, "ox": 0.0150375939849624, "oy": 0.17037037037037037, "term": "see", "cat25k": 16, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.7903225806451613, "os": 0.15408252853380158, "bg": 7.33699856912093e-08}, {"x": 0.037593984962406006, "y": 0.22222222222222218, "ox": 0.037593984962406006, "oy": 0.22222222222222218, "term": "when", "cat25k": 21, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 5, "s": 0.8473118279569892, "os": 0.18316505706760317, "bg": 1.0757837656434943e-07}, {"x": 0.007518796992481202, "y": 0.15555555555555559, "ox": 0.007518796992481202, "oy": 0.15555555555555559, "term": "choose", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.7849462365591398, "os": 0.1468393327480246, "bg": 4.884689846673693e-07}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "technique", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 2.222471750531991e-06}, {"x": 0.022556390977443604, "y": 0.3111111111111111, "ox": 0.022556390977443604, "oy": 0.3111111111111111, "term": "given", "cat25k": 30, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 3, "s": 0.9408602150537634, "os": 0.2863257243195786, "bg": 7.16502424938768e-07}, {"x": 0.007518796992481202, "y": 0.17777777777777778, "ox": 0.007518796992481202, "oy": 0.17777777777777778, "term": "even", "cat25k": 17, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 1, "s": 0.8279569892473119, "os": 0.16889815627743637, "bg": 2.0344646697042336e-07}, {"x": 0.11278195488721804, "y": 0.17777777777777778, "ox": 0.11278195488721804, "oy": 0.17777777777777778, "term": "solution", "cat25k": 17, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 15, "s": 0.6559139784946236, "os": 0.06442054433713786, "bg": 1.0530892108728653e-06}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "doesn", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 2.837127309974701e-07}, {"x": 0.04511278195488721, "y": 0.15555555555555559, "ox": 0.04511278195488721, "oy": 0.15555555555555559, "term": "t", "cat25k": 15, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.7505376344086021, "os": 0.1095258999122037, "bg": 1.3899069528334767e-07}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "involve", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 3.4873730938703687e-06}, {"x": 0.060150375939849614, "y": 0.16296296296296298, "ox": 0.060150375939849614, "oy": 0.16296296296296298, "term": "gcp", "cat25k": 15, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 8, "s": 0.7344086021505376, "os": 0.10195346795434593, "bg": 0.0002094562515709219}, {"x": 0.4285714285714285, "y": 0.20740740740740743, "ox": 0.4285714285714285, "oy": 0.20740740740740743, "term": "mindset", "cat25k": 20, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 62, "s": 0.08279569892473118, "os": -0.2196005267778753, "bg": 0.00014336746275432792}, {"x": 0.04511278195488721, "y": 0.16296296296296298, "ox": 0.04511278195488721, "oy": 0.16296296296296298, "term": "should", "cat25k": 15, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.7559139784946237, "os": 0.11687884108867429, "bg": 1.3927048206525092e-07}, {"x": 0.06766917293233082, "y": 0.17777777777777778, "ox": 0.06766917293233082, "oy": 0.17777777777777778, "term": "lead", "cat25k": 17, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 9, "s": 0.7483870967741936, "os": 0.10919666374012293, "bg": 9.650160763636522e-07}, {"x": 0.7293233082706765, "y": 0.18518518518518517, "ox": 0.7293233082706765, "oy": 0.18518518518518517, "term": "excited", "cat25k": 18, "ncat25k": 101, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 125, "s": 0.004301075268817204, "os": -0.5401668129938543, "bg": 3.099663159604445e-05}, {"x": 0.030075187969924807, "y": 0.16296296296296298, "ox": 0.030075187969924807, "oy": 0.16296296296296298, "term": "why", "cat25k": 15, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7731182795698924, "os": 0.13180421422300265, "bg": 2.707376318107637e-07}, {"x": 0.07518796992481203, "y": 0.2, "ox": 0.07518796992481203, "oy": 0.2, "term": "what", "cat25k": 19, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 10, "s": 0.7623655913978494, "os": 0.12379280070237049, "bg": 9.108109431079298e-08}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "kinds", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 1.2156530966012943e-06}, {"x": 0.037593984962406006, "y": 0.3333333333333333, "ox": 0.037593984962406006, "oy": 0.3333333333333333, "term": "challenges", "cat25k": 32, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 5, "s": 0.943010752688172, "os": 0.293459174714662, "bg": 4.498124259693087e-06}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "talk", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 4.289302024247241e-07}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "intelligently", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 5.131394236466865e-05}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "passionately", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 5.446058091286307e-05}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "interesting", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 8.926591030952402e-07}, {"x": 0.0, "y": 0.16296296296296298, "ox": 0.0, "oy": 0.16296296296296298, "term": "presented", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.8161290322580645, "os": 0.16165496049165937, "bg": 9.402344059933447e-07}, {"x": 0.007518796992481202, "y": 0.16296296296296298, "ox": 0.007518796992481202, "oy": 0.16296296296296298, "term": "humor", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.7913978494623656, "os": 0.1541922739244952, "bg": 2.5340598299238197e-06}, {"x": 0.0, "y": 0.16296296296296298, "ox": 0.0, "oy": 0.16296296296296298, "term": "perspective", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.8161290322580645, "os": 0.16165496049165937, "bg": 2.0350628376527697e-06}, {"x": 0.030075187969924807, "y": 0.15555555555555559, "ox": 0.030075187969924807, "oy": 0.15555555555555559, "term": "preference", "cat25k": 15, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7655913978494624, "os": 0.12445127304653206, "bg": 4.591829518961776e-06}, {"x": 0.0, "y": 0.15555555555555559, "ox": 0.0, "oy": 0.15555555555555559, "term": "mass", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7946236559139784, "os": 0.15430201931518878, "bg": 1.0387777464028426e-06}, {"x": 0.0150375939849624, "y": 0.20740740740740743, "ox": 0.0150375939849624, "oy": 0.20740740740740743, "term": "there", "cat25k": 20, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 2, "s": 0.8559139784946237, "os": 0.1908472344161545, "bg": 8.556303398338679e-08}, {"x": 0.060150375939849614, "y": 0.19259259259259262, "ox": 0.060150375939849614, "oy": 0.19259259259259262, "term": "no", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.7709677419354839, "os": 0.13136523266022826, "bg": 7.255813660675098e-08}, {"x": 0.037593984962406006, "y": 0.15555555555555559, "ox": 0.037593984962406006, "oy": 0.15555555555555559, "term": "relocation", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.7569892473118279, "os": 0.11698858647936788, "bg": 5.779992799462816e-06}, {"x": 0.030075187969924807, "y": 0.19259259259259262, "ox": 0.030075187969924807, "oy": 0.19259259259259262, "term": "being", "cat25k": 18, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 4, "s": 0.8096774193548386, "os": 0.16121597892888498, "bg": 2.4706579311010257e-07}, {"x": 0.007518796992481202, "y": 0.16296296296296298, "ox": 0.007518796992481202, "oy": 0.16296296296296298, "term": "offered", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.7913978494623656, "os": 0.1541922739244952, "bg": 9.124351099952427e-07}, {"x": 0.060150375939849614, "y": 0.02222222222222222, "ox": 0.060150375939849614, "oy": 0.02222222222222222, "term": "dental", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.39892473118279564, "os": -0.03775241439859526, "bg": 9.438864240343689e-07}, {"x": 0.060150375939849614, "y": 0.014814814814814812, "ox": 0.060150375939849614, "oy": 0.014814814814814812, "term": "401k", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.36129032258064514, "os": -0.04510535557506584, "bg": 0.0}, {"x": 0.007518796992481202, "y": 0.06666666666666667, "ox": 0.007518796992481202, "oy": 0.06666666666666667, "term": "focus", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.6483870967741936, "os": 0.058604038630377525, "bg": 3.044422091282643e-07}, {"x": 0.23308270676691725, "y": 0.014814814814814812, "ox": 0.23308270676691725, "oy": 0.014814814814814812, "term": "career", "cat25k": 1, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 31, "s": 0.08709677419354837, "os": -0.21674714661984196, "bg": 9.2545837988611e-07}, {"x": 0.037593984962406006, "y": 0.0962962962962963, "ox": 0.037593984962406006, "oy": 0.0962962962962963, "term": "validation", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.6419354838709678, "os": 0.05816505706760316, "bg": 4.138849668817297e-06}, {"x": 0.06766917293233082, "y": 0.02222222222222222, "ox": 0.06766917293233082, "oy": 0.02222222222222222, "term": "#", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.3591397849462365, "os": -0.04521510096575943, "bg": 0.0}, {"x": 0.12781954887218044, "y": 0.007407407407407408, "ox": 0.12781954887218044, "oy": 0.007407407407407408, "term": "docker", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15483870967741936, "os": -0.11962247585601404, "bg": 0.00021200913994958895}, {"x": 0.14285714285714285, "y": 0.007407407407407408, "ox": 0.14285714285714285, "oy": 0.007407407407407408, "term": "stream", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.13548387096774195, "os": -0.1345478489903424, "bg": 1.0346715058942776e-06}, {"x": 0.13533834586466165, "y": 0.02222222222222222, "ox": 0.13533834586466165, "oy": 0.02222222222222222, "term": "batch", "cat25k": 2, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 18, "s": 0.16344086021505377, "os": -0.11237928007023704, "bg": 5.31494732760652e-06}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "ingest", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 3.882703526465478e-05}, {"x": 0.14285714285714285, "y": 0.007407407407407408, "ox": 0.14285714285714285, "oy": 0.007407407407407408, "term": "storm", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.13548387096774195, "os": -0.1345478489903424, "bg": 1.7929698101984575e-06}, {"x": 0.32330827067669166, "y": 0.02222222222222222, "ox": 0.32330827067669166, "oy": 0.02222222222222222, "term": "kafka", "cat25k": 2, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 43, "s": 0.05913978494623655, "os": -0.2989464442493415, "bg": 0.00016333139229359882}, {"x": 0.037593984962406006, "y": 0.02222222222222222, "ox": 0.037593984962406006, "oy": 0.02222222222222222, "term": "get", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.524731182795699, "os": -0.01536435469710272, "bg": 2.6400388519997635e-08}, {"x": 0.07518796992481203, "y": 0.037037037037037035, "ox": 0.07518796992481203, "oy": 0.037037037037037035, "term": "extracting", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.3924731182795699, "os": -0.03797190517998244, "bg": 1.5955537236235024e-05}, {"x": 0.0, "y": 0.08888888888888888, "ox": 0.0, "oy": 0.08888888888888888, "term": "marketing", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.7193548387096773, "os": 0.08812554872695347, "bg": 2.188515117660591e-07}, {"x": 0.07518796992481203, "y": 0.02962962962962963, "ox": 0.07518796992481203, "oy": 0.02962962962962963, "term": "improving", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3548387096774193, "os": -0.04532484635645303, "bg": 1.5622256515554995e-06}, {"x": 0.4210526315789473, "y": 0.014814814814814812, "ox": 0.4210526315789473, "oy": 0.014814814814814812, "term": "possessing", "cat25k": 1, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 61, "s": 0.03763440860215053, "os": -0.4033143107989465, "bg": 8.927299895635613e-05}, {"x": 0.09022556390977442, "y": 0.08148148148148147, "ox": 0.09022556390977442, "oy": 0.08148148148148147, "term": "cross", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 12, "s": 0.5548387096774193, "os": -0.008779631255487266, "bg": 6.191269416426559e-07}, {"x": 0.11278195488721804, "y": 0.06666666666666667, "ox": 0.11278195488721804, "oy": 0.06666666666666667, "term": "functional", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.35053763440860214, "os": -0.04587357330992098, "bg": 1.1447614808367403e-06}, {"x": 0.16541353383458646, "y": 0.037037037037037035, "ox": 0.16541353383458646, "oy": 0.037037037037037035, "term": "stack", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 22, "s": 0.13978494623655913, "os": -0.12752414398595258, "bg": 4.839673709198526e-06}, {"x": 0.15789473684210525, "y": 0.14074074074074075, "ox": 0.15789473684210525, "oy": 0.14074074074074075, "term": "methodologies", "cat25k": 13, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 21, "s": 0.5, "os": -0.017120280948200173, "bg": 2.543461396614653e-05}, {"x": 0.05263157894736841, "y": 0.11111111111111112, "ox": 0.05263157894736841, "oy": 0.11111111111111112, "term": "model", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.6408602150537634, "os": 0.05794556628621598, "bg": 2.7282941521253294e-07}, {"x": 0.4736842105263157, "y": 0.07407407407407408, "ox": 0.4736842105263157, "oy": 0.07407407407407408, "term": "etl", "cat25k": 7, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 70, "s": 0.039784946236559135, "os": -0.396729587357331, "bg": 0.00030016340145166525}, {"x": 0.6090225563909772, "y": 0.007407407407407408, "ox": 0.6090225563909772, "oy": 0.007407407407407408, "term": "dimensional", "cat25k": 1, "ncat25k": 75, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 93, "s": 0.001075268817204301, "os": -0.5972344161545216, "bg": 1.7753730077576246e-05}, {"x": 0.09774436090225562, "y": 0.014814814814814812, "ox": 0.09774436090225562, "oy": 0.014814814814814812, "term": "together", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.21290322580645163, "os": -0.08241878841088673, "bg": 3.1853575145997413e-07}, {"x": 0.11278195488721804, "y": 0.007407407407407408, "ox": 0.11278195488721804, "oy": 0.007407407407407408, "term": "automated", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.17204301075268816, "os": -0.10469710272168568, "bg": 2.4836741115315587e-06}, {"x": 0.06766917293233082, "y": 0.014814814814814812, "ox": 0.06766917293233082, "oy": 0.014814814814814812, "term": "ad", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3268817204301075, "os": -0.052568042142230015, "bg": 2.790413878118349e-07}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "hoc", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 4.480508555643099e-06}, {"x": 0.04511278195488721, "y": 0.014814814814814812, "ox": 0.04511278195488721, "oy": 0.014814814814814812, "term": "space", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.43225806451612897, "os": -0.030179982440737486, "bg": 1.3160876714434508e-07}, {"x": 0.08270676691729321, "y": 0.007407407407407408, "ox": 0.08270676691729321, "oy": 0.007407407407407408, "term": "apis", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.23225806451612901, "os": -0.07484635645302896, "bg": 1.1910031621133954e-05}, {"x": 0.09774436090225562, "y": 0.037037037037037035, "ox": 0.09774436090225562, "oy": 0.037037037037037035, "term": "principles", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.2774193548387096, "os": -0.06035996488147497, "bg": 1.1569042373802957e-06}, {"x": 0.12781954887218044, "y": 0.05185185185185185, "ox": 0.12781954887218044, "oy": 0.05185185185185185, "term": "10", "cat25k": 5, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 17, "s": 0.22365591397849463, "os": -0.07550482879719052, "bg": 0.0}, {"x": 0.04511278195488721, "y": 0.02222222222222222, "ox": 0.04511278195488721, "oy": 0.02222222222222222, "term": "metadata", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.46989247311827953, "os": -0.0228270412642669, "bg": 2.717284237441731e-06}, {"x": 0.08270676691729321, "y": 0.02222222222222222, "ox": 0.08270676691729321, "oy": 0.02222222222222222, "term": "integrity", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.2806451612903225, "os": -0.06014047410008779, "bg": 2.1030141976741715e-06}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "strongly", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 8.484732783946885e-07}, {"x": 0.06766917293233082, "y": 0.014814814814814812, "ox": 0.06766917293233082, "oy": 0.014814814814814812, "term": "objects", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3268817204301075, "os": -0.052568042142230015, "bg": 6.487312718285171e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "tune", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 1.1115942429052035e-06}, {"x": 0.12781954887218044, "y": 0.007407407407407408, "ox": 0.12781954887218044, "oy": 0.007407407407407408, "term": "ssis", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15483870967741936, "os": -0.11962247585601404, "bg": 0.00018580453363062057}, {"x": 0.14285714285714285, "y": 0.007407407407407408, "ox": 0.14285714285714285, "oy": 0.007407407407407408, "term": "owners", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.13548387096774195, "os": -0.1345478489903424, "bg": 6.595446833278646e-07}, {"x": 0.4360902255639097, "y": 0.04444444444444444, "ox": 0.4360902255639097, "oy": 0.04444444444444444, "term": "growing", "cat25k": 4, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 63, "s": 0.04193548387096774, "os": -0.38882791922739246, "bg": 3.557016120216628e-06}, {"x": 0.08270676691729321, "y": 0.007407407407407408, "ox": 0.08270676691729321, "oy": 0.007407407407407408, "term": "membership", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.23225806451612901, "os": -0.07484635645302896, "bg": 4.5450007340176186e-07}, {"x": 0.08270676691729321, "y": 0.02962962962962963, "ox": 0.08270676691729321, "oy": 0.02962962962962963, "term": "initiative", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3247311827956989, "os": -0.0527875329236172, "bg": 1.3701117609297469e-06}, {"x": 0.31578947368421045, "y": 0.037037037037037035, "ox": 0.31578947368421045, "oy": 0.037037037037037035, "term": "azure", "cat25k": 4, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 42, "s": 0.06344086021505377, "os": -0.27677787532923614, "bg": 8.472475720498433e-05}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "ibm", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.975685819949484e-07}, {"x": 0.037593984962406006, "y": 0.02222222222222222, "ox": 0.037593984962406006, "oy": 0.02222222222222222, "term": "global", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.524731182795699, "os": -0.01536435469710272, "bg": 1.7109047099431603e-07}, {"x": 0.04511278195488721, "y": 0.014814814814814812, "ox": 0.04511278195488721, "oy": 0.014814814814814812, "term": "root", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.43225806451612897, "os": -0.030179982440737486, "bg": 3.8184018666066603e-07}, {"x": 0.15037593984962405, "y": 0.02222222222222222, "ox": 0.15037593984962405, "oy": 0.02222222222222222, "term": "existing", "cat25k": 2, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 20, "s": 0.14193548387096774, "os": -0.1273046532045654, "bg": 7.813791106889927e-07}, {"x": 0.24812030075187963, "y": 0.05925925925925926, "ox": 0.24812030075187963, "oy": 0.05925925925925926, "term": "maintain", "cat25k": 6, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 33, "s": 0.09784946236559139, "os": -0.18755487269534682, "bg": 2.4841499870384932e-06}, {"x": 0.060150375939849614, "y": 0.014814814814814812, "ox": 0.060150375939849614, "oy": 0.014814814814814812, "term": "speed", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.36129032258064514, "os": -0.04510535557506584, "bg": 2.4037615021490226e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "matter", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 2.098951059702669e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "happy", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 2.2033676901309588e-07}, {"x": 0.07518796992481203, "y": 0.05185185185185185, "ox": 0.07518796992481203, "oy": 0.05185185185185185, "term": "8", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 10, "s": 0.4612903225806451, "os": -0.023266022827041266, "bg": 0.0}, {"x": 0.060150375939849614, "y": 0.037037037037037035, "ox": 0.060150375939849614, "oy": 0.037037037037037035, "term": "doing", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.4634408602150537, "os": -0.02304653204565408, "bg": 3.214276133270317e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "visa", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 5.2239753302989e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "fluency", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 1.2395708192633438e-05}, {"x": 0.060150375939849614, "y": 0.04444444444444444, "ox": 0.060150375939849614, "oy": 0.04444444444444444, "term": "higher", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5096774193548388, "os": -0.015693590869183496, "bg": 3.3905373082555795e-07}, {"x": 0.04511278195488721, "y": 0.02962962962962963, "ox": 0.04511278195488721, "oy": 0.02962962962962963, "term": "purpose", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.5193548387096775, "os": -0.01547410008779631, "bg": 3.1064519095693835e-07}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "accuracy", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 8.107230829042384e-07}, {"x": 0.06766917293233082, "y": 0.014814814814814812, "ox": 0.06766917293233082, "oy": 0.014814814814814812, "term": "json", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3268817204301075, "os": -0.052568042142230015, "bg": 0.00013596781271051834}, {"x": 0.04511278195488721, "y": 0.007407407407407408, "ox": 0.04511278195488721, "oy": 0.007407407407407408, "term": "xml", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.4086021505376343, "os": -0.03753292361720807, "bg": 2.4429003879273465e-07}, {"x": 0.12781954887218044, "y": 0.007407407407407408, "ox": 0.12781954887218044, "oy": 0.007407407407407408, "term": "jenkins", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15483870967741936, "os": -0.11962247585601404, "bg": 8.883254054403518e-06}, {"x": 0.44360902255639095, "y": 0.02962962962962963, "ox": 0.44360902255639095, "oy": 0.02962962962962963, "term": "semi", "cat25k": 3, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 64, "s": 0.034408602150537634, "os": -0.41099648814749784, "bg": 8.233496138611391e-06}, {"x": 0.21052631578947367, "y": 0.02962962962962963, "ox": 0.21052631578947367, "oy": 0.02962962962962963, "term": "structures", "cat25k": 3, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 28, "s": 0.09999999999999999, "os": -0.17965320456540823, "bg": 2.5066294473970887e-06}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "000", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 0.0}, {"x": 0.20300751879699244, "y": 0.05185185185185185, "ox": 0.20300751879699244, "oy": 0.05185185185185185, "term": "microsoft", "cat25k": 5, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 27, "s": 0.1139784946236559, "os": -0.1501316944688323, "bg": 6.65187700072931e-07}, {"x": 0.04511278195488721, "y": 0.037037037037037035, "ox": 0.04511278195488721, "oy": 0.037037037037037035, "term": "above", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5623655913978495, "os": -0.00812115891132572, "bg": 1.5497126473500243e-07}, {"x": 0.15037593984962405, "y": 0.014814814814814812, "ox": 0.15037593984962405, "oy": 0.014814814814814812, "term": "object", "cat25k": 1, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 20, "s": 0.13440860215053763, "os": -0.134657594381036, "bg": 6.295926818665031e-07}, {"x": 0.08270676691729321, "y": 0.05925925925925926, "ox": 0.08270676691729321, "oy": 0.05925925925925926, "term": "test", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.45913978494623653, "os": -0.02337576821773485, "bg": 2.450557155772961e-07}, {"x": 0.09022556390977442, "y": 0.02222222222222222, "ox": 0.09022556390977442, "oy": 0.02222222222222222, "term": "share", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.25483870967741934, "os": -0.06760316066725197, "bg": 2.51337477726368e-07}, {"x": 0.08270676691729321, "y": 0.037037037037037035, "ox": 0.08270676691729321, "oy": 0.037037037037037035, "term": "paid", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.353763440860215, "os": -0.04543459174714661, "bg": 5.301338430570234e-07}, {"x": 0.16541353383458646, "y": 0.04444444444444444, "ox": 0.16541353383458646, "oy": 0.04444444444444444, "term": "efficient", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 22, "s": 0.15161290322580645, "os": -0.120171202809482, "bg": 2.431517609137365e-06}, {"x": 0.060150375939849614, "y": 0.02222222222222222, "ox": 0.060150375939849614, "oy": 0.02222222222222222, "term": "always", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.39892473118279564, "os": -0.03775241439859526, "bg": 1.7227614905078702e-07}, {"x": 0.05263157894736841, "y": 0.02222222222222222, "ox": 0.05263157894736841, "oy": 0.02222222222222222, "term": "great", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.4301075268817204, "os": -0.03028972783143108, "bg": 6.632297855992658e-08}, {"x": 0.18045112781954886, "y": 0.007407407407407408, "ox": 0.18045112781954886, "oy": 0.007407407407407408, "term": "airflow", "cat25k": 1, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 24, "s": 0.10322580645161289, "os": -0.1718612818261633, "bg": 4.507586719207097e-05}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "near", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 2.936859359311894e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "foundation", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 1.851260734014699e-07}, {"x": 0.09022556390977442, "y": 0.014814814814814812, "ox": 0.09022556390977442, "oy": 0.014814814814814812, "term": "pig", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.22903225806451613, "os": -0.07495610184372256, "bg": 3.945870547824938e-06}, {"x": 0.22556390977443602, "y": 0.02222222222222222, "ox": 0.22556390977443602, "oy": 0.02222222222222222, "term": "streaming", "cat25k": 2, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 30, "s": 0.0935483870967742, "os": -0.2019315188762072, "bg": 6.961747727780475e-06}, {"x": 0.060150375939849614, "y": 0.007407407407407408, "ox": 0.060150375939849614, "oy": 0.007407407407407408, "term": "parties", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.332258064516129, "os": -0.05245829675153643, "bg": 3.404787721699915e-07}, {"x": 0.04511278195488721, "y": 0.014814814814814812, "ox": 0.04511278195488721, "oy": 0.014814814814814812, "term": "while", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.43225806451612897, "os": -0.030179982440737486, "bg": 7.071442831119315e-08}, {"x": 0.4210526315789473, "y": 0.007407407407407408, "ox": 0.4210526315789473, "oy": 0.007407407407407408, "term": "humble", "cat25k": 1, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 61, "s": 0.03548387096774193, "os": -0.41066725197541704, "bg": 3.297228679295052e-05}, {"x": 0.09022556390977442, "y": 0.007407407407407408, "ox": 0.09022556390977442, "oy": 0.007407407407407408, "term": "lambda", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.21397849462365592, "os": -0.08230904302019315, "bg": 2.781726878928119e-06}, {"x": 0.060150375939849614, "y": 0.02962962962962963, "ox": 0.060150375939849614, "oy": 0.02962962962962963, "term": "architectures", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.4279569892473118, "os": -0.03039947322212467, "bg": 6.974201846187465e-06}, {"x": 0.037593984962406006, "y": 0.02222222222222222, "ox": 0.037593984962406006, "oy": 0.02222222222222222, "term": "message", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.524731182795699, "os": -0.01536435469710272, "bg": 4.2878380557317164e-08}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "line", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 4.2845394955884494e-08}, {"x": 0.060150375939849614, "y": 0.037037037037037035, "ox": 0.060150375939849614, "oy": 0.037037037037037035, "term": "plan", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.4634408602150537, "os": -0.02304653204565408, "bg": 1.6167804096558404e-07}, {"x": 0.11278195488721804, "y": 0.037037037037037035, "ox": 0.11278195488721804, "oy": 0.037037037037037035, "term": "practical", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.22580645161290322, "os": -0.07528533801580334, "bg": 1.3695766689875284e-06}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "developer", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 3.337281801215532e-07}, {"x": 0.08270676691729321, "y": 0.02222222222222222, "ox": 0.08270676691729321, "oy": 0.02222222222222222, "term": "delivery", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.2806451612903225, "os": -0.06014047410008779, "bg": 2.389349991244312e-07}, {"x": 0.04511278195488721, "y": 0.02962962962962963, "ox": 0.04511278195488721, "oy": 0.02962962962962963, "term": "neo4j", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.5193548387096775, "os": -0.01547410008779631, "bg": 0.0}, {"x": 0.06766917293233082, "y": 0.014814814814814812, "ox": 0.06766917293233082, "oy": 0.014814814814814812, "term": "ruby", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3268817204301075, "os": -0.052568042142230015, "bg": 2.0187179195900607e-06}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "restful", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 3.436020229569102e-05}, {"x": 0.12781954887218044, "y": 0.014814814814814812, "ox": 0.12781954887218044, "oy": 0.014814814814814812, "term": "api", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 17, "s": 0.16451612903225807, "os": -0.11226953467954345, "bg": 2.6585789895300963e-06}, {"x": 0.11278195488721804, "y": 0.014814814814814812, "ox": 0.11278195488721804, "oy": 0.014814814814814812, "term": "interfaces", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.18494623655913978, "os": -0.0973441615452151, "bg": 2.7661731840825607e-06}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "stores", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 1.6987829126448453e-07}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "rest", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 3.7467785432258997e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "mssql", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.645193573061349e-05}, {"x": 0.09774436090225562, "y": 0.02222222222222222, "ox": 0.09774436090225562, "oy": 0.02222222222222222, "term": "postgresql", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.22795698924731184, "os": -0.07506584723441614, "bg": 6.829138997093989e-06}, {"x": 0.12030075187969923, "y": 0.014814814814814812, "ox": 0.12030075187969923, "oy": 0.014814814814814812, "term": "compensation", "cat25k": 1, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 16, "s": 0.1709677419354839, "os": -0.10480684811237928, "bg": 1.4482897207033617e-06}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "coverage", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 3.2044873076666714e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "off", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 4.9277925107627645e-08}, {"x": 0.04511278195488721, "y": 0.007407407407407408, "ox": 0.04511278195488721, "oy": 0.007407407407407408, "term": "facing", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.4086021505376343, "os": -0.03753292361720807, "bg": 1.040714768850188e-06}, {"x": 0.09022556390977442, "y": 0.02222222222222222, "ox": 0.09022556390977442, "oy": 0.02222222222222222, "term": "tech", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.25483870967741934, "os": -0.06760316066725197, "bg": 3.209625281062874e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "snacks", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 3.0616129933105943e-06}, {"x": 0.13533834586466165, "y": 0.014814814814814812, "ox": 0.13533834586466165, "oy": 0.014814814814814812, "term": "enable", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 18, "s": 0.15376344086021507, "os": -0.11973222124670763, "bg": 1.0050201257792739e-06}, {"x": 0.07518796992481203, "y": 0.05185185185185185, "ox": 0.07518796992481203, "oy": 0.05185185185185185, "term": "financial", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 10, "s": 0.4612903225806451, "os": -0.023266022827041266, "bg": 2.2911445678865968e-07}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "actively", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 2.3536276050391637e-06}, {"x": 0.49624060150375937, "y": 0.02962962962962963, "ox": 0.49624060150375937, "oy": 0.02962962962962963, "term": "growth", "cat25k": 3, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 75, "s": 0.013978494623655913, "os": -0.4632352941176471, "bg": 1.973378622789051e-06}, {"x": 0.15789473684210525, "y": 0.007407407407407408, "ox": 0.15789473684210525, "oy": 0.007407407407407408, "term": "continuously", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 21, "s": 0.1193548387096774, "os": -0.14947322212467076, "bg": 7.749767066660325e-06}, {"x": 0.30827067669172925, "y": 0.02222222222222222, "ox": 0.30827067669172925, "oy": 0.02222222222222222, "term": "redshift", "cat25k": 2, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 41, "s": 0.06129032258064516, "os": -0.2840210711150132, "bg": 0.00012680535522070616}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "columnar", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 4.5068826536525064e-05}, {"x": 0.060150375939849614, "y": 0.007407407407407408, "ox": 0.060150375939849614, "oy": 0.007407407407407408, "term": "finding", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.332258064516129, "os": -0.05245829675153643, "bg": 4.842655400139199e-07}, {"x": 0.08270676691729321, "y": 0.02222222222222222, "ox": 0.08270676691729321, "oy": 0.02222222222222222, "term": "around", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.2806451612903225, "os": -0.06014047410008779, "bg": 1.5653195143050581e-07}, {"x": 0.12030075187969923, "y": 0.02222222222222222, "ox": 0.12030075187969923, "oy": 0.02222222222222222, "term": "enterprise", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 16, "s": 0.18387096774193548, "os": -0.09745390693590869, "bg": 6.72778636318071e-07}, {"x": 0.06766917293233082, "y": 0.014814814814814812, "ox": 0.06766917293233082, "oy": 0.014814814814814812, "term": "interface", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3268817204301075, "os": -0.052568042142230015, "bg": 3.705257791121347e-07}, {"x": 0.04511278195488721, "y": 0.014814814814814812, "ox": 0.04511278195488721, "oy": 0.014814814814814812, "term": "five", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.43225806451612897, "os": -0.030179982440737486, "bg": 1.535023263469436e-07}, {"x": 0.23308270676691725, "y": 0.014814814814814812, "ox": 0.23308270676691725, "oy": 0.014814814814814812, "term": "server", "cat25k": 1, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 31, "s": 0.08709677419354837, "os": -0.21674714661984196, "bg": 4.2897324837028515e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "semantic", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.5755533200706646e-06}, {"x": 0.09774436090225562, "y": 0.007407407407407408, "ox": 0.09774436090225562, "oy": 0.007407407407407408, "term": "tuning", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.1967741935483871, "os": -0.08977172958735732, "bg": 3.7096789365622377e-06}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "schema", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 3.6082065048746868e-06}, {"x": 0.33082706766917286, "y": 0.007407407407407408, "ox": 0.33082706766917286, "oy": 0.007407407407407408, "term": "warehouse", "cat25k": 1, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 44, "s": 0.05376344086021505, "os": -0.3211150131694469, "bg": 6.165473076749179e-06}, {"x": 0.4285714285714285, "y": 0.007407407407407408, "ox": 0.4285714285714285, "oy": 0.007407407407407408, "term": "massive", "cat25k": 1, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 62, "s": 0.029032258064516127, "os": -0.4181299385425812, "bg": 6.454265331979235e-06}, {"x": 0.15037593984962405, "y": 0.02222222222222222, "ox": 0.15037593984962405, "oy": 0.02222222222222222, "term": "apple", "cat25k": 2, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 20, "s": 0.14193548387096774, "os": -0.1273046532045654, "bg": 9.08761079106368e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "millions", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 5.748479587029226e-07}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "users", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 9.371606223421289e-08}, {"x": 0.07518796992481203, "y": 0.02962962962962963, "ox": 0.07518796992481203, "oy": 0.02962962962962963, "term": "elasticsearch", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3548387096774193, "os": -0.04532484635645303, "bg": 0.00041675969338393984}, {"x": 0.12781954887218044, "y": 0.007407407407407408, "ox": 0.12781954887218044, "oy": 0.007407407407407408, "term": "workflow", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15483870967741936, "os": -0.11962247585601404, "bg": 6.766804089179712e-06}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "authoring", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 2.700141217385669e-06}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "post", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 3.053251663024125e-08}, {"x": 0.060150375939849614, "y": 0.02222222222222222, "ox": 0.060150375939849614, "oy": 0.02222222222222222, "term": "culture", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.39892473118279564, "os": -0.03775241439859526, "bg": 3.2035595157505265e-07}, {"x": 0.05263157894736841, "y": 0.014814814814814812, "ox": 0.05263157894736841, "oy": 0.014814814814814812, "term": "architecting", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4032258064516129, "os": -0.03764266900790167, "bg": 8.844556912266909e-05}, {"x": 0.04511278195488721, "y": 0.007407407407407408, "ox": 0.04511278195488721, "oy": 0.007407407407407408, "term": "schemas", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.4086021505376343, "os": -0.03753292361720807, "bg": 1.0134411248617376e-05}, {"x": 0.060150375939849614, "y": 0.02222222222222222, "ox": 0.060150375939849614, "oy": 0.02222222222222222, "term": "robust", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.39892473118279564, "os": -0.03775241439859526, "bg": 3.043220789956486e-06}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "microservices", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 0.00017863256769430017}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "contribute", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 1.0751580258306716e-06}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "between", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 4.696601959559322e-08}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "answer", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 1.8542344572852754e-07}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "fundamentals", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 2.879790351262428e-06}, {"x": 0.12030075187969923, "y": 0.007407407407407408, "ox": 0.12030075187969923, "oy": 0.007407407407407408, "term": "tables", "cat25k": 1, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.1655913978494624, "os": -0.11215978928884987, "bg": 9.403505499626363e-07}, {"x": 0.17293233082706763, "y": 0.02222222222222222, "ox": 0.17293233082706763, "oy": 0.02222222222222222, "term": "apache", "cat25k": 2, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 23, "s": 0.1182795698924731, "os": -0.14969271290605793, "bg": 3.128653793820632e-06}, {"x": 0.037593984962406006, "y": 0.014814814814814812, "ox": 0.037593984962406006, "oy": 0.014814814814814812, "term": "patient", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47956989247311826, "os": -0.022717295873573306, "bg": 3.284251215472051e-07}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "workflows", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 2.8009144985837883e-05}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "particular", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 1.6637989935596696e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "exhibits", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 1.5787246851496736e-06}, {"x": 0.04511278195488721, "y": 0.014814814814814812, "ox": 0.04511278195488721, "oy": 0.014814814814814812, "term": "100", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.43225806451612897, "os": -0.030179982440737486, "bg": 0.0}, {"x": 0.09774436090225562, "y": 0.007407407407407408, "ox": 0.09774436090225562, "oy": 0.007407407407407408, "term": "kinesis", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.1967741935483871, "os": -0.08977172958735732, "bg": 0.00011401672788279081}, {"x": 0.08270676691729321, "y": 0.014814814814814812, "ox": 0.08270676691729321, "oy": 0.014814814814814812, "term": "active", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.26021505376344084, "os": -0.06749341527655837, "bg": 3.089649214062163e-07}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "extract", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 2.176024417895335e-06}, {"x": 0.15789473684210525, "y": 0.02222222222222222, "ox": 0.15789473684210525, "oy": 0.02222222222222222, "term": "50", "cat25k": 2, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 21, "s": 0.13333333333333333, "os": -0.13476733977172958, "bg": 0.0}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "case", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 5.092726302200248e-08}, {"x": 0.09022556390977442, "y": 0.014814814814814812, "ox": 0.09022556390977442, "oy": 0.014814814814814812, "term": "availability", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.22903225806451613, "os": -0.07495610184372256, "bg": 3.471744685118461e-07}, {"x": 0.13533834586466165, "y": 0.007407407407407408, "ox": 0.13533834586466165, "oy": 0.007407407407407408, "term": "assist", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.14408602150537633, "os": -0.12708516242317822, "bg": 1.2653603086839814e-06}, {"x": 0.08270676691729321, "y": 0.007407407407407408, "ox": 0.08270676691729321, "oy": 0.007407407407407408, "term": "dynamics", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.23225806451612901, "os": -0.07484635645302896, "bg": 1.7644711817376642e-06}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "participate", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 4.898413236590217e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "pertains", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 1.296454951971746e-05}, {"x": 0.08270676691729321, "y": 0.007407407407407408, "ox": 0.08270676691729321, "oy": 0.007407407407407408, "term": "governance", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.23225806451612901, "os": -0.07484635645302896, "bg": 1.747845725523114e-06}, {"x": 0.060150375939849614, "y": 0.007407407407407408, "ox": 0.060150375939849614, "oy": 0.007407407407407408, "term": "thereof", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.332258064516129, "os": -0.05245829675153643, "bg": 1.7433871629947868e-06}, {"x": 0.04511278195488721, "y": 0.007407407407407408, "ox": 0.04511278195488721, "oy": 0.007407407407407408, "term": "pyspark", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.4086021505376343, "os": -0.03753292361720807, "bg": 0.00020840156003453512}, {"x": 0.15789473684210525, "y": 0.014814814814814812, "ox": 0.15789473684210525, "oy": 0.014814814814814812, "term": "s3", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 21, "s": 0.12365591397849461, "os": -0.14212028094820017, "bg": 0.0}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "shell", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 7.741999043185693e-07}, {"x": 0.09774436090225562, "y": 0.007407407407407408, "ox": 0.09774436090225562, "oy": 0.007407407407407408, "term": "visualisation", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.1967741935483871, "os": -0.08977172958735732, "bg": 3.95780974808541e-05}, {"x": 0.05263157894736841, "y": 0.014814814814814812, "ox": 0.05263157894736841, "oy": 0.014814814814814812, "term": "volume", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4032258064516129, "os": -0.03764266900790167, "bg": 2.420960644984803e-07}, {"x": 0.030075187969924807, "y": 0.014814814814814812, "ox": 0.030075187969924807, "oy": 0.014814814814814812, "term": "reliable", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5311827956989248, "os": -0.015254609306409126, "bg": 5.876884845526816e-07}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "saas", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 3.067617969083524e-05}, {"x": 0.037593984962406006, "y": 0.007407407407407408, "ox": 0.037593984962406006, "oy": 0.007407407407407408, "term": "offerings", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4387096774193548, "os": -0.030070237050043896, "bg": 1.4243533079893404e-06}, {"x": 0.07518796992481203, "y": 0.014814814814814812, "ox": 0.07518796992481203, "oy": 0.014814814814814812, "term": "compute", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.28387096774193543, "os": -0.0600307287093942, "bg": 4.643677111199427e-06}, {"x": 0.10526315789473684, "y": 0.007407407407407408, "ox": 0.10526315789473684, "oy": 0.007407407407407408, "term": "scrum", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 14, "s": 0.1860215053763441, "os": -0.09723441615452151, "bg": 6.084501557632399e-05}, {"x": 0.05263157894736841, "y": 0.007407407407407408, "ox": 0.05263157894736841, "oy": 0.007407407407407408, "term": "framework", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36344086021505373, "os": -0.04499561018437226, "bg": 4.924740727790498e-07}, {"x": 0.07518796992481203, "y": 0.007407407407407408, "ox": 0.07518796992481203, "oy": 0.007407407407407408, "term": "bash", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.26236559139784943, "os": -0.06738366988586479, "bg": 4.5937889884789855e-06}, {"x": 0.08270676691729321, "y": 0.007407407407407408, "ox": 0.08270676691729321, "oy": 0.007407407407407408, "term": "native", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.23225806451612901, "os": -0.07484635645302896, "bg": 6.812797579095089e-07}, {"x": 0.06766917293233082, "y": 0.007407407407407408, "ox": 0.06766917293233082, "oy": 0.007407407407407408, "term": "aggregation", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.29247311827956984, "os": -0.059920983318700605, "bg": 7.369490016551875e-06}, {"x": 0.15037593984962405, "y": 0.007407407407407408, "ox": 0.15037593984962405, "oy": 0.007407407407407408, "term": "warehousing", "cat25k": 1, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.12580645161290324, "os": -0.1420105355575066, "bg": 1.29273489146875e-05}, {"x": 0.15037593984962405, "y": 0.0, "ox": 0.15037593984962405, "oy": 0.0, "term": "architect", "cat25k": 0, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 20, "s": 0.12043010752688171, "os": -0.14936347673397718, "bg": 4.433859835714409e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "presto", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.2198717101584816e-05}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "parquet", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 3.601128696622913e-05}, {"x": 0.09774436090225562, "y": 0.0, "ox": 0.09774436090225562, "oy": 0.0, "term": "devops", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1870967741935484, "os": -0.09712467076382791, "bg": 0.0003869969040247678}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "integrate", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.5759675455750114e-06}, {"x": 0.10526315789473684, "y": 0.0, "ox": 0.10526315789473684, "oy": 0.0, "term": "snowflake", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.17311827956989248, "os": -0.1045873573309921, "bg": 2.982839087549523e-05}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "dataflow", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 4.995738011009358e-05}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "scripts", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 6.895127127469928e-07}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "azkaban", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.7093944042974175e-05}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "luigi", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 8.187694713614907e-06}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "preparation", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 6.315086338676345e-07}, {"x": 0.09774436090225562, "y": 0.0, "ox": 0.09774436090225562, "oy": 0.0, "term": "mentoring", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1870967741935484, "os": -0.09712467076382791, "bg": 6.384618765818814e-06}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "detailed", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 3.0497082429896974e-07}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "oozie", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 0.00020840156003453512}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "avro", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 5.221484169578888e-05}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "cloudera", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 0.00020840156003453512}, {"x": 0.07518796992481203, "y": 0.0, "ox": 0.07518796992481203, "oy": 0.0, "term": "usage", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.23763440860215052, "os": -0.07473661106233538, "bg": 7.840804384985533e-07}, {"x": 0.07518796992481203, "y": 0.0, "ox": 0.07518796992481203, "oy": 0.0, "term": "rdbms", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.23763440860215052, "os": -0.07473661106233538, "bg": 3.5143887862882606e-05}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "beam", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 1.176970058722399e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "debugging", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 2.79612540902072e-06}, {"x": 0.07518796992481203, "y": 0.0, "ox": 0.07518796992481203, "oy": 0.0, "term": "definition", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.23763440860215052, "os": -0.07473661106233538, "bg": 4.33988004962132e-07}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "each", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 4.106053086562046e-08}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "eg", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.8557702190263523e-07}, {"x": 0.08270676691729321, "y": 0.0, "ox": 0.08270676691729321, "oy": 0.0, "term": "gym", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2150537634408602, "os": -0.08219929762949955, "bg": 2.213350487470324e-06}, {"x": 0.10526315789473684, "y": 0.0, "ox": 0.10526315789473684, "oy": 0.0, "term": "rds", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.17311827956989248, "os": -0.1045873573309921, "bg": 2.0821373437281562e-05}, {"x": 0.10526315789473684, "y": 0.0, "ox": 0.10526315789473684, "oy": 0.0, "term": "ec2", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.17311827956989248, "os": -0.1045873573309921, "bg": 0.0}, {"x": 0.06766917293233082, "y": 0.0, "ox": 0.06766917293233082, "oy": 0.0, "term": "jira", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.26559139784946234, "os": -0.0672739244951712, "bg": 1.0497665144310818e-05}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "designs", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 3.633846922078663e-07}, {"x": 0.09022556390977442, "y": 0.0, "ox": 0.09022556390977442, "oy": 0.0, "term": "mpp", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.2010752688172043, "os": -0.08966198419666374, "bg": 4.270827401628609e-05}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "vertica", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 0.00020840156003453512}, {"x": 0.06766917293233082, "y": 0.0, "ox": 0.06766917293233082, "oy": 0.0, "term": "mapping", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.26559139784946234, "os": -0.0672739244951712, "bg": 9.101425834427569e-07}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "perl", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 6.563982190767619e-07}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "organisation", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 7.087515354991275e-07}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "third", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 1.465103055662886e-07}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "party", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 1.1051656346618459e-07}, {"x": 0.09022556390977442, "y": 0.0, "ox": 0.09022556390977442, "oy": 0.0, "term": "impala", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.2010752688172043, "os": -0.08966198419666374, "bg": 2.1742911810749693e-05}, {"x": 0.09774436090225562, "y": 0.0, "ox": 0.09774436090225562, "oy": 0.0, "term": "informatica", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1870967741935484, "os": -0.09712467076382791, "bg": 3.872118808519257e-05}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "athena", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 7.973790151771127e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "kanban", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 7.951074388926804e-05}, {"x": 0.05263157894736841, "y": 0.0, "ox": 0.05263157894736841, "oy": 0.0, "term": "kubernetes", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3376344086021505, "os": -0.05234855136084284, "bg": 0.00020840156003453512}, {"x": 0.09774436090225562, "y": 0.0, "ox": 0.09774436090225562, "oy": 0.0, "term": "ci", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1870967741935484, "os": -0.09712467076382791, "bg": 2.9497407291350655e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "cd", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 6.285150259322943e-08}, {"x": 0.09774436090225562, "y": 0.0, "ox": 0.09774436090225562, "oy": 0.0, "term": "teradata", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1870967741935484, "os": -0.09712467076382791, "bg": 0.00011159612503916595}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "lake", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.4066566886255934e-07}, {"x": 0.09774436090225562, "y": 0.0, "ox": 0.09774436090225562, "oy": 0.0, "term": "add", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1870967741935484, "os": -0.09712467076382791, "bg": 6.713160934867872e-08}, {"x": 0.06766917293233082, "y": 0.0, "ox": 0.06766917293233082, "oy": 0.0, "term": "usability", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.26559139784946234, "os": -0.0672739244951712, "bg": 3.9038465250452365e-06}, {"x": 0.08270676691729321, "y": 0.0, "ox": 0.08270676691729321, "oy": 0.0, "term": "ssrs", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2150537634408602, "os": -0.08219929762949955, "bg": 0.00018318525858264568}, {"x": 0.07518796992481203, "y": 0.0, "ox": 0.07518796992481203, "oy": 0.0, "term": "ssas", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.23763440860215052, "os": -0.07473661106233538, "bg": 0.00014934623684819702}, {"x": 0.09022556390977442, "y": 0.0, "ox": 0.09022556390977442, "oy": 0.0, "term": "exchange", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.2010752688172043, "os": -0.08966198419666374, "bg": 3.11095896217372e-07}, {"x": 0.16541353383458646, "y": 0.0, "ox": 0.16541353383458646, "oy": 0.0, "term": "load", "cat25k": 0, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 22, "s": 0.1075268817204301, "os": -0.16428884986830553, "bg": 1.1590105284779819e-06}, {"x": 0.25563909774436083, "y": 0.0, "ox": 0.25563909774436083, "oy": 0.0, "term": "enhance", "cat25k": 0, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 34, "s": 0.06774193548387096, "os": -0.2538410886742757, "bg": 3.038749643896526e-06}, {"x": 0.4210526315789473, "y": 0.0, "ox": 0.4210526315789473, "oy": 0.0, "term": "digging", "cat25k": 0, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 61, "s": 0.030107526881720432, "os": -0.41802019315188765, "bg": 4.545618824525327e-05}, {"x": 0.4210526315789473, "y": 0.0, "ox": 0.4210526315789473, "oy": 0.0, "term": "petabyte", "cat25k": 0, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 61, "s": 0.030107526881720432, "os": -0.41802019315188765, "bg": 0.0011264276547222247}, {"x": 0.45864661654135325, "y": 0.0, "ox": 0.45864661654135325, "oy": 0.0, "term": "pragmatic", "cat25k": 0, "ncat25k": 53, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 66, "s": 0.016129032258064516, "os": -0.45533362598770855, "bg": 7.839052377341101e-05}, {"x": 0.45112781954887204, "y": 0.0, "ox": 0.45112781954887204, "oy": 0.0, "term": "letting", "cat25k": 0, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.01935483870967742, "os": -0.44787093942054435, "bg": 1.546301377944842e-05}, {"x": 0.45112781954887204, "y": 0.0, "ox": 0.45112781954887204, "oy": 0.0, "term": "perfect", "cat25k": 0, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.01935483870967742, "os": -0.44787093942054435, "bg": 2.157808610575915e-06}, {"x": 0.45112781954887204, "y": 0.0, "ox": 0.45112781954887204, "oy": 0.0, "term": "enemy", "cat25k": 0, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.01935483870967742, "os": -0.44787093942054435, "bg": 9.186358568448758e-06}, {"x": 0.45112781954887204, "y": 0.0, "ox": 0.45112781954887204, "oy": 0.0, "term": "directed", "cat25k": 0, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.01935483870967742, "os": -0.44787093942054435, "bg": 5.3247486370486664e-06}, {"x": 0.4210526315789473, "y": 0.0, "ox": 0.4210526315789473, "oy": 0.0, "term": "amidst", "cat25k": 0, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 61, "s": 0.030107526881720432, "os": -0.41802019315188765, "bg": 8.083361989514157e-05}, {"x": 0.4285714285714285, "y": 0.0, "ox": 0.4285714285714285, "oy": 0.0, "term": "continually", "cat25k": 0, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 62, "s": 0.024731182795698924, "os": -0.4254828797190518, "bg": 2.3195111667627518e-05}, {"x": 0.4285714285714285, "y": 0.0, "ox": 0.4285714285714285, "oy": 0.0, "term": "awareness", "cat25k": 0, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 62, "s": 0.024731182795698924, "os": -0.4254828797190518, "bg": 6.019906081697505e-06}, {"x": 0.4285714285714285, "y": 0.0, "ox": 0.4285714285714285, "oy": 0.0, "term": "extras", "cat25k": 0, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 62, "s": 0.024731182795698924, "os": -0.4254828797190518, "bg": 8.862775150751159e-06}, {"x": 0.14285714285714285, "y": 0.0, "ox": 0.14285714285714285, "oy": 0.0, "term": "categories", "cat25k": 0, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.12688172043010754, "os": -0.14190079016681298, "bg": 2.1850860082629143e-07}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "messaging", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.0264473570648533e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "rapidly", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 9.978121306016341e-07}, {"x": 0.12030075187969923, "y": 0.0, "ox": 0.12030075187969923, "oy": 0.0, "term": "traditional", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 16, "s": 0.15913978494623657, "os": -0.11951273046532046, "bg": 6.678523550414985e-07}, {"x": 0.21052631578947367, "y": 0.0, "ox": 0.21052631578947367, "oy": 0.0, "term": "analysts", "cat25k": 0, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 28, "s": 0.09032258064516129, "os": -0.2090649692712906, "bg": 7.498831655246571e-06}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "profiling", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 5.945795894948191e-06}, {"x": 0.18045112781954886, "y": 0.0, "ox": 0.18045112781954886, "oy": 0.0, "term": "loads", "cat25k": 0, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 24, "s": 0.1010752688172043, "os": -0.1792142230026339, "bg": 5.193038774797706e-06}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "speech", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 5.130368927074083e-07}, {"x": 0.10526315789473684, "y": 0.0, "ox": 0.10526315789473684, "oy": 0.0, "term": "optimising", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.17311827956989248, "os": -0.1045873573309921, "bg": 7.153295114299437e-05}, {"x": 0.06766917293233082, "y": 0.0, "ox": 0.06766917293233082, "oy": 0.0, "term": "olap", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.26559139784946234, "os": -0.0672739244951712, "bg": 2.358348422395925e-05}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "summary", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 1.9852643505422954e-07}, {"x": 0.12781954887218044, "y": 0.0, "ox": 0.12781954887218044, "oy": 0.0, "term": "indexing", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 17, "s": 0.14516129032258063, "os": -0.12697541703248463, "bg": 9.668157553432922e-06}, {"x": 0.07518796992481203, "y": 0.0, "ox": 0.07518796992481203, "oy": 0.0, "term": "further", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.23763440860215052, "os": -0.07473661106233538, "bg": 1.8248954093131822e-07}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "supplement", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 1.2255455784230446e-06}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "context", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 3.9374353337768853e-07}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "exports", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 1.7274097717195597e-06}, {"x": 0.06766917293233082, "y": 0.0, "ox": 0.06766917293233082, "oy": 0.0, "term": "monitor", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.26559139784946234, "os": -0.0672739244951712, "bg": 4.098791626132274e-07}, {"x": 0.09022556390977442, "y": 0.0, "ox": 0.09022556390977442, "oy": 0.0, "term": "troubleshoot", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.2010752688172043, "os": -0.08966198419666374, "bg": 2.4180385677151547e-05}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "meta", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 1.2501455247524906e-06}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "whom", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 5.566040778136675e-07}, {"x": 0.060150375939849614, "y": 0.0, "ox": 0.060150375939849614, "oy": 0.0, "term": "archiving", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30107526881720426, "os": -0.05981123792800702, "bg": 6.464309536997869e-06}, {"x": 0.14285714285714285, "y": 0.0, "ox": 0.14285714285714285, "oy": 0.0, "term": "ownership", "cat25k": 0, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.12688172043010754, "os": -0.14190079016681298, "bg": 2.045566390956616e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "wellness", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 1.269821920173915e-06}, {"x": 0.04511278195488721, "y": 0.0, "ox": 0.04511278195488721, "oy": 0.0, "term": "programmes", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3709677419354838, "os": -0.04488586479367866, "bg": 6.721806391362389e-07}], "docs": {"categories": ["data scientist", "data engineer"], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "texts": ["2+ years of relevant experience preferred.,Healthcare industry knowledge/experience preferred.", "Pursuing a PhD or MS in CS, Math, Statistics, Physics, Economics or other quantitative field with an expected graduation date of Winter 2018 or Spring/Summer 2019. We will also consider candidates with quantitative backgrounds who are currently enrolled in business school.,Ability to think and execute at multiple altitudes: from strategy and vision to execution.,Interpersonal skills with demonstrated ability to influence outcomes and communicate technical content to general audiences, including ability to \u201cstory tell\u201d with data.,Ability to write and mentor code development in SQL and Python (or R).,Experience with dashboard design and data viz tools (i.e. Tableau) is a plus,Experience with building data pipelines is a plus.,Stock,$2,000 yearly employee travel coupon,Competitive salary,Paid time off,Medical, dental, & vision insurance,Life & disability coverage,401K,Flexible Spending Accounts,Apple equipment,Daily breakfast, lunch, and dinner", "PhD or MS in computational biology, computer science, statistics, or a related quantitative science field with 7+ years (9+ for MS) of project-based work in computational/quantitative data analysis in biology related fields,Proficiency in analysing data with R, Matlab or Python,Strong interested in biology and immunological diseases, liaise with scientists from Immunology and Inflammation to help define and address biological questions with computational and analytical approach,Proficiency in biostatistics, linear/non-linear regression models, dimensionality reduction, clustering, AI/machine learning methods,Ability to develop, benchmark and apply predictive algorithms to identify novel biomarkers, dissect gene/disease relationships and generate hypotheses,Experience with Bayesian analysis and causal inference,Excellent written and oral communication skills,Excellent interpersonal and team skills", "Bachelor\u2019s degree from an accredited college/university in Computer Science, Computational Linguistics, Statistics, Mathematics, Engineering, Bioinformatics, Physics, Operations Research or related fields.,Minimum 2 years relevant work experience (if bachelor\u2019s degree) or minimum 0 years relevant work experience (if master\u2019s degree) with a proven track record in driving value in a commercial setting using data science skills.,In-depth knowledge of various modeling algorithms e.g. Linear, GLMs, trees-based models, neural networks, clustering, PCA, and time series models.,Proficiency in R (e.g. ggplot2, cluster, dplyr, caret), Python (e.g. pandas, scikit-learn, bokeh, nltk), Spark \u2013 MLlib, H20, or other statistical tools.,In-depth knowledge of databases, data modeling, Hadoop, and distributed computing frameworks.,Experience in software development environment, Agile, and code management/versioning (e.g. git).,Strong SQL skills and experience/knowledge.,Ability to understand complex and ambiguous business needs and applying the right tools and approaches.,Must be curious, self-motivating, driven and have a passion for problem solving.,Collaborative team player.,Excellent communication skills, both written and verbal.,Experience developing and testing machine learning and/or statistical projects.,Master\u2019s degree from an accredited college/university in Business Analytics, Computer Science, Machine Learning, Computational Linguistics, Statistics, Mathematics, Engineering, Bioinformatics, Physics, Operations Research, or related fields.,Experience in agriculture or commodity businesses.,Experience in deep learning neural networks.,Experience with weather and geospatial data.,Experience with \u201ccomputer vision\u201d applications with images and video,Experience with back testing strategies,Experience working in a cloud environment e.g. Amazon Web Services.,Experience with Big Data development in Hadoop and Spark frameworks.", "Strong working knowledge of a variety of machine learning and analytical techniques and understanding of how and where they are applicable.,Software engineering programing skills with coding experience using Java/Scala or Python, in addition to machine learning and data mining libraries.,Experience working with high performance computing in distributed and parallel systems, e.g. Hadoop, Tez & Spark,Strong mathematical and statistical background, with thorough understanding of probability,Experience in data science and machine learning with a degree in Computer Science, Mathematics or another empirical science,Commercial experience building and deploying scalable software solutions that employ machine learning algorithms in production environments,Self-starter, comfortable juggling multiple projects, strong communication skills,Experience with NLP techniques and tools a plus.,Have developed production data science solutions in healthcare, ad tech or web search,Prior experience building recommendation systems and/or natural language processing solutions,Healthcare Domain expertise", "Master\u2019s degree in a relevant technical discipline (Math, Engineering, Computer Science, Statistics or similar field) with at least 2 years of job experience or Bachelor\u2019s degree in a relevant technical discipline with 4 years of job experience.,Minimum 2 years of experience with programming languages for data science such as Python, Java, R, etc.,Strong mathematical and statistics skills.,Strong interpersonal and communication skills (both written and oral).,Ability to communicate complex technical or statistical concepts to a non-technical audience.,PhD in a relevant technical discipline is preferred.,Credit risk management/fraud detection industry experience in developing scoring models.,Experiences with advanced machine learning algorithms including Deep learning, boosting, and tree based methods.,Experience using cloud or distributed computing frameworks such as Hadoop, SPARK, AWS/EMR, BigQuery, etc.,Demonstrated ability to apply modern data exploration and visualization techniques to deliver actionable insights.", "Bachelor\u2019s Degree plus 2 years of experience in data analytics, or Master\u2019s Degree, or PhD,At least 1 year of experience in open source programming languages for large scale data analysis,At least 1 year of experience with machine learning,Master\u2019s Degree or PhD,Experience working with AWS,At least 2 years\u2019 experience in Python, Scala, or R,At least 2 years\u2019 experience with machine learning", "PhD or MS in computational biology, computer science, statistics, or a related quantitative science field with 4+ years (6+ for MS) of project-based work in computational/quantitative data analysis in biology related fields,Proficiency in analyzing data with R, MatLab or Python,Strong interested in biology and immunological diseases, liaise with scientists from Immunology and Inflammation to help define and address biological questions with computational and analytical approache,Proficiency in biostatistics, linear/non-linear regression models, dimensionality reduction, clustering, AI/machine learning methods,Ability to develop, benchmark and apply predictive algorithms to identify novel biomarkers, dissect gene/disease relationships and generate hypotheses,Experience with Bayesian analysis and causal inference,Excellent written and oral communication skills,Excellent interpersonal and team skills", "Bachelors degree in Computer Science or related degree.,2-4 years related work experience.,Experience in working on an Open Source project a plus but not required.,Good English language skills", "PhD or MS degree in Computer Science, Statistics, Electrical Engineering, Applied Math, Operations Research, Econometrics, or other related fields.,Deep understanding of statistical modeling, machine learning, deep learning, or data mining concepts, and a track record of solving problems with these methods.,Proficient in one or more programming languages such as Python, Java, Scala, and C,Familiar with one or more machine learning or statistical modeling tools such as R, scikit learn, and Spark MLlib,Knowledge and experience working with relational databases and SQL,Strong analytical and quantitative problem solving ability.,Experience with big data techniques (such as Hadoop, MapReduce, Hive, Pig, Spark),3+ years of experience in machine learning, data mining, information retrieval, or statistical analysis,Knowledge of cloud platforms (such as AWS or Azure) and experience of developing applications on the cloud platforms using various cloud services", "Ph.D degree in highly quantitive field (Computer Science, Machine Learning, Operational Research, Statistics, Mathematics, etc.) or equivalent experience,2+ years of hands-on experience as a Principal Data Scientist,Experience with building DMP, DSP, ID Graph, MarTech, or AdTech is highly desirable,Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations,Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.,Job function\nEngineeringInformation Technology", "Upstream oil & gas industry experience preferred, particularly in the areas of Drilling, Completion, and Production Operations", "Bachelor\u2019s Degree plus 2 years of experience in data analytics, or Master\u2019s Degree plus 1 year of experience in data analytics, or PhD,At least 1 year of experience in open source programming languages for large scale data analysis,At least 1 year of experience with machine learning,At least 1 year of experience with relational databases,Master\u2019s Degree or PhD,At least 1 year experience working with AWS,At least 3 years\u2019 experience in Python, Scala, or R,At least 3 years\u2019 experience with machine learning,At least 3 years\u2019 experience with SQL", "Experience in Data Visualizaiton using tools such as Tableau", "Experience in data mining,Understanding of machine-learning and operations research,Analytical mind and business acumen,Strong math skills (e.g. statistics, algebra),Problem-solving aptitude,Excellent communication and presentation skills,BSc/BA in Computer Science, Engineering or relevant field; graduate degree in Data Science or other quantitative field is preferred", "PhD in bioinformatics, computer science or similar experience,Experience with relational database management systems,Solid skills in R, Python and Linux,Experience with systems biology research data,Solid data visualization skills,Basic knowledge on web development,Experience with data work\ufb02ow management tools is an advantage,Experience in big data and cloud computing is an advantage,Ability to work in a team and pursue goals in a focused way,Excellent written / oral communication skills in English", "Employment, payment and social benefits are consistent with those at other research institutes", "Master or PhD degree in Engineering, Neuroscience, Bioinformatics, or other quantitative fields with strong analysis and programming experience.,Excellent understanding of machine learning techniques and algorithms,Experience with common data science toolkits and libraries, such as scikit-learn, Pandas, NumPy, SciPy, Matlab, etc.,Programming languages: Python,Working knowledge of Linux OS, SQL,2+ years of Python programming and product development experience,Great verbal and written skills,Be a team player while being able to work independently", "Bachelor\u2019s or Master\u2019s degree in a quantitative field,Knowledge of Python and/or R,Knowledge of algorithms for data mining, machine learning, and/or natural language processing,Possess some understanding of statistical procedures used in advanced analytics,Experience processing large amounts of structured and unstructured data using Spark, Hive or other Big Data technologies,Experience building scalable data models and performing complex relational database queries using SQL (Oracle, MySQL), etc.,Attention to detail and demonstrated ability to detect and resolve data/analytics quality issues,Outstanding verbal and written communication skills,Experienced user of data visualization tools (e.g. Tableau, matplotlib, ggplot2, etc.)", "Experience with multi-touch attribution modeling, media mix modeling, and data visualization a big plus.", "Strong experience designing quantitative modeling experiments to solve \u201cfuzzy\u201d real world problems.,Good communication skills to clearly understand the problems of experts outside your field as well as collaborate with other data scientists.,Deep understanding deep learning models and methods allow you to design and redesign models to solve new applications.,Strong experimental design that will allow you to verify the utility of models in practice.,Passion for research and curiosity that calls you to go beyond \u201cgood enough\u201d to create something innovative and exciting.,Masters in a quantitative field such as computer science, electrical engineering, statistics, biostatistics, applied math, etc. PhD preferred,3+ years working in data science/ deep learning is required.,Experience with deep learning libraries (pytorch, tensorflow, keras, etc.) as well as statistical modeling software (scikit-learn, statsmodels, python or R),Experience directly interfacing with customers, particularly medical professionals.,Experience turning research projects into consumer products,Ability to write beautiful, production ready code.", "Social environment with built-in bars", "Ph.D. in life sciences or related field with expertise in molecular / cellular biology and experience with \u201cOmics\u201d data analysis,Experience with data analysis of compound screens is a strong plus,Experience in toxicogenomics is a plus,Experience with R-based data analysis is a plus,Excellent communication skills with the ability to interact professionally with all levels of staff, collaborators and customers,Experience in working on and with interdisciplinary teams,Fluency in English,A permanent position within a vigorous and exciting professional environment promoted by an open culture and a spirit of community,A diverse, international workforce with a dynamic working environment that fosters creativity, innovations and teamwork,Capital-forming benefits, holiday pay and annual bonus payment depending on performance", "PhD required in computer science or related discipline; or equivalent combination of educational training, relevant experience, and accomplishments.,Strong coding/algorithm prototyping skills, and ability to explain and document work.,Proficiency in one or more of the following: Python, C, C++, SQL.,Experience working in data analysis/statistics/machine learning/scientific computing to address basic research questions; or commensurate achievements.,Strong problem-solving skills; a passion for answering hard questions with data.,The ability to communicate complex ideas to relevant stakeholders.,Experience in a collaborative, multi-disciplinary research environment.,Eagerness to collaborate with both technical and non-technical colleagues.,Experience in database design and building data-driven web applications.", "First of all, must have true startup spirit. Be willing to wear multiple hats and deliver end-to-end;,Ability of thinking out-of-box and evaluating results based on customer value;,5+ years of industry experience in applying AI/ML, preferably on well-known security products or services, such as malware detection, anomaly detection, security analytics and data security;,Experience of applying AI/ML in more than one domains highly desirable;,Hands-on experience with relevant technology stacks such as CUDA, Python, R, Spark, Flink, Tensorflow;,Hands-on experience using modern big data pipeline;,Natural language process (NLP) and data mining experience highly desirable;,Security research experience and strong security domain knowledge highly desirable;,Energetic self-starter, with the desire to work in a dynamic fast-paced environment;,Excellent verbal and written communication skills;,Ability to influence without authority,PhD in Computer Science, Statistics, Electrical Engineering or equivalent technical degree.", "Bachelors or M.S. in a quantitative discipline such as Mathematics, Statistics, Physics, Computer Science, or Engineering,Background in linear algebra and multivariable calculus,Experience with Python a plus,Experience cleaning and visualizing data a plus", "PhD in Bioinformatics or related field,Proficiency with database management, scientific curation, and bioinformatics software engineering,Proficiency with MySQL, Oracle, and graph databases,Python/R and Java programming skills,Demonstrated understanding of posttranslational modifications and cell signaling", "MS Statistics, Machine Learning, Operations Research, Applied Math; or equivalent.,2 \u2013 5 years of data science experience.,Experience with statistical and mathematical software,Proven ability to develop system prototypes.,SQL skills,Excellent oral and written communication skills.,Strong documentation skills.,Understanding of machine learning techniques and the ability to invent.,Big data experience- Hadoop, Spark, Map-reduce, Hive, etc.,Light lifting (20-25 lbs.), office environment,Office: Standard office equipment; work usually performed in an office setting free from any disagreeable elements.", "Ph.D. degree in Bioinformatics, Computer Science, Biostatistics, Applied Mathematics, Applied Physics or related discipline.,Experience in analyzing multimodal OMICs data from cancer immunotherapy patients,Knowledge about immunology, tumor immunology and / or tumor biology and genetics,Proficiency in Python and R", "A degree in computer science, statistics, econometrics, mathematics or information science or related field.,2 years of experience in data science, 4 years of experience in digital analytics, ideally in media.,Proven ability with digital metrics systems such as Google Analytics or Adobe Analytics. BigQuery experience is a plus.,Facile with statistical computing using R, SAS, SPSS, or S-Plus. Data Science Studio experience a plus.,Hands-on knowledge of database manipulation through environments such as MySQL.,Proven ability in Python or similar.,Solid experience with machine learning and data mining.,Expertise in decomposing a problem and working through a technical approach and attacking a problem in a systematic way.,Experience working with digital product teams in an Agile environment.,Strong analysis and experimental design skills with a keen sense for data gaps and inconsistencies.,Experience managing data-driven research from project inception to client communication.,Experience in developing data visualization independently or with tools.,Excellent oral, written and visual communication skills, particularly at explaining complex quantitative information to non-technical audiences.,Strong collaboration skills,Not just a tolerance but a zeal for multitasking. At the same time show a clear sense for priorities and commitment to follow-through.,Ability to work independently with little supervision as well as in collaborative team environment.,Self-starter with the ability to work in a constantly changing environment.", "Minimum 2 years of experience in upstream O&G as a reservoir engineer, data scientist or petrophysicist,MSc in petroleum engineering or a related major,Algorithmic thinking,Working knowledge of the upstream data,Experience with analytical simulation tools in the field,Familiarity with reservoir simulation,PhD in petroleum engineering or a related major,Demonstrated experience with statistical analysis, quantitative analytics, forecasting/predictive analytics, multivariate testing, and optimization algorithms", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Restaurant or Retail industry experience (nice-to-have)", "The 2018 Jobs Rated Report,The Best Jobs of 2018,The Toughest Jobs to Fill in 2017,The Best Jobs in Retail (in Time for Holiday Shopping),Jobs Rated Report 2017,The Best Jobs of 2017,The Best Jobs in Advertising for 2017", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Managing functions in a complex environment with multiple constituencies.,Complex modeling and analytical methodology including longitudinal analyses, multi-level modeling, and multivariate analyses of variance.,Summarizing and presenting the findings of any complex analytical or modeling exercises.,Creating standard reports that presents complex data.,Developing, selling and driving suitable partnerships, alliances, or other transactions to closure.,Performing ongoing analytic projects in support of business strategies and tactics, including network optimization, incorporation of unstructured information, and ROI analysis.,Working with multi-source data integration and development in BI tool (Cognos or Tableau).,Applying industry best practices to analytic processes to create time efficiencies.,Making recommendations for data architecture strategy.,Developing and deploying analytical tools and data science techniques to analyze complex data sets.,Monitoring trends in data science and alternative sources.", "Strong quantitative skills and knowledge of Deep/Machine Learning,Highly motivated, infinitely curious self-starter with a passion for AI,Python proficiency and expertise in PyTorch and Tensorflow is essential,Perseverant and capable of thinking outside-the-box,Excellent communication skills,Team player capable of leading independent research efforts,Master\u2019s degree or PhD in a quantitative field such as data science/machine learning, mathematics, physics, statistics, engineering, computational neuroscience/biology or computer science.,Deep understanding of AI/Machine Learning theory, algorithms, and techniques as evidenced by completion of foundational Deep Learning and other relevant coursework (linear algebra, advanced calculus, mathematical optimization, advanced statistics, signal processing, information theory, AI/Machine Learning classes, etc),Substantial experience designing and implementing neural networks in Python using PyTorch or Tensorflow,Experience working with a range of real-world data types at all stages of the data science/ Machine Learning pipeline (data fetching, pre-processing, visualization, modeling, interpretation, etc.),Ability to distill complex ideas/results to their core essence and communicate clearly to non-specialists,Presentations at Top-tier AI, Deep/Machine Learning conferences,Publications in AI, Machine/Deep Learning peer-reviewed journals,Enthusiasm for AI/Deep learning demonstrated by Kaggle contributions, github commits, and other relevant community activities,Working knowledge of high performance distributed computing or code-optimization through parallelization (GPU, clusters); Git, or other method of version control.,Proficient in C/C++,Solid understanding of SQL query development (MySQL, SQLite, and/or Postgres).", "You are passionate about subjects such as experimental design, data visualization, and statistical analysis techniques. Experience with Looker, Tableau, or other data visualization software a plus.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "MSc or PhD level in the field of Computer Science, Machine Learning, Applied Statistics, or Mathematics,Experience in statistical modelling and machine learning techniques,Programming experience in at least two of the following languages: R, Python, Scala, SQL,Experience in applying data science methods to business problems,Experience in applying advanced analytical and statistical methods in the commercial world,Strong presentation and communication skills, with the ability to explain complex analytical concepts to people from other fields,Knowledge of distributed computing or NoSQL technologies is a bonus,Technical and non-technical training courses,Attendance at one international ML conference per year (e.g. NIPS, ICML),McKinsey benefits (competitive salary, annual bonus, generous pension scheme etc., excellent healthcare),Office events", "Bachelor\u2019s Degree in a STEM field (Science, Technology, Engineering, Math) from an accredited college or university,Minimum 3 years of experience in analytics development for industrial applications in a commercial and/or industrial setting)\nDesired Characteristics:,Master\u2019s Degree in a STEM field (Science, Technology, Engineering, Math) from an accredited college or university,Ph.D. in a STEM field (Science, Technology, Engineering, Math) from an accredited college or university,Demonstrated skill in data management methods,Demonstrated skill in feature extraction and realtime analytics development and deployment", "Subject to call back at all times. Ability to travel to any area of the organization, both local and remote as needed. Must provide own transportation.,Required to sit for extended periods.,Ability to understand complex verbal and written communications, and respond verbally or in writing as appropriate. Typical mediums of communication include face-to-face dialog, telephone, memos, and electronic mail.,Ability to interpret equipment status indicators to determine appropriate operating condition. Indicators may include visual and/or auditory techniques or cues.,Ability to read and understand technical manuals and other documentation to determine correct action, safety precautions, and other conditions of proper hardware and software operation.,Ability to work varying hours due to the accessibility of individuals or equipment involved in different projects, the need to minimize system downtime or user interruption, or to recover from hardware or software failures.,Will occasionally experience stressful working conditions due to tight project schedules and hardware or software problems.,Ability to occasionally lift and/or move equipment up to 25 pounds without assistance. Must occasionally lift and/or move equipment up to 40 pounds with assistance.,Ability to occasionally crouch, kneel, bend and/or crawl to access, inspect, connect, position or perform other operations on equipment. Some locations, such as user or equipment locations, may present very close quarters.,Ability to occasionally use small hand tools and be able to manipulate small equipment components such as screws, nuts, or other fastening devices as usually found on computer equipment.,Subject to regular periods of repetitive hand motion in the operation of computer terminals and other equipment.", "PhD in a computational and quantitative discipline (e.g. statistics, computer science, biomedical informatics, genetics, physics, epidemiology, health economics) or Master\u2019s Degree in a similar field of study,Deep understanding in ML, including strong knowledge of the mathematical underpinnings behind various methods (e.g. regression techniques, neural networks, decision trees, clustering, pattern recognition, dimensionality reduction),Proven experience in applied statistics and ML in a business setting,Deep understanding of the tools of the trade, including a variety of modern programming languages (R, Python, JavaScript) and open-source technologies (Linux, TensorFlow, Hadoop, and Spark),Experience in effective data visualization approaches and a keen eye for detail in the visual communication of findings,Comfort working and communicating with non-technical teams to translate business questions to analytically actionable questions,A strong desire to build meaningful solutions for a life sciences business,Task oriented with ability to set goals and complete deliverables,Domain knowledge of clinical data, real-world data, or life sciences-related research data,Expertise in other data science-related tools (e.g. SQL, Tableau, D3)", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "2 years of professional experience as a Data Scientist or Machine Learning Engineer working on developing, optimizing, and implementing machine learning models for at least 2 projects,Professional experience with machine learning libraries such as scikit-learn or mllib,Professional experience implementing multiple machine learning models in a production environment,Bachelor's in Data Science, Analytics, Statistics, Mathematics, Physics, Economics, Computer Science or equivalent experience,Deep understanding of statistical concepts and applying them to real-world problems,Master's degree or PhD in Data Science, Analytics, Statistics, Mathematics, Physics, Economics, Computer Science or another quantitative discipline,Experience with petabytes of data,Experience with deep learning frameworks such as tensorflow or keras", "Familiarity with AWS ecosystem is a plus.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Bachelor\u2019s degree, preferably in a mathematically intensive field,Enthusiasm for analytically intensive work,Excitement to learn about technology, and customer operations analytics,Enthusiasm to improve the business performance of Journey Analytics clients,Understanding of statistical and advanced analytic methods (descriptive statistics, predictive analytics, machine learning),Ability to work collaboratively in a team environment and effectively with people at all levels in an organization,Confidence sharing your views and perspectives with senior clients and colleagues", "Bachelors degree in statistics, computer science, economics, physics or other quantitative field,Experience using applied statistics or machine learning,Proficiency with Python or R,Experience working with imperfect data,Passion and eagerness to constantly learn and teach others,Masters or PhD student in statistics, computer science, economics, physics or other quantitative field,Internship or work experience in applied statistics or machine learning,2+ years of experience", "Strong data mining and machine learning background,Experience with big data environment,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Experience with big data environment including Spark, Hadoop, Hive, etc.,Proficiency in at least one statistics/data analysis package, such as Python or R,Proficiency in at least one programming language, such as Java or Python, etc.,Solid coding practices including good design documentation, unit testing, and integration testing,Experience working with real-world noisy data,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Experience with MS Office (Word, Access, Excel, PowerPoint, Outlook) required,Experience managing 2+ employees,Experience with SAS, SQL, logistic regression, multiple regression required,Excellent written and oral presentation skills,Ability to explain/present complicated/advanced analytical methodology and results to non-technical audiences,Ability to provide strategic insights to improve client campaign performance,Ability to execute advanced analytic tasks (including but not limited to modeling, segmentation, DOE, forecasting, etc.) and create meaningful analytical outputs,Must have experience with modeling and statistical concepts,Experiences with applying statistical techniques such as regression, ANOVA, cluster analysis, factor analysis, time series forecasting, experimental and design, etc. to solve business problems,Possess core database marketing knowledge such as understanding of Merkle and client data, relational database concepts, and direct marketing concepts", "Graduate degree in Computer Science, Electrical Engineering, Applied Math, or other related STEM majors with machine learning and algorithms development experience,12+ years industry experience,Extensive knowledge about the details of the algorithm used in Machine learning.,Experienced in building large scale data analysis system.,Extensive knowledge and experience in successfully developing and implementing Machine Learning projects,Familiar with Python, PHP, R, HTML, CSS, SQL, MongoDB, Apache Hadoop, Spark and AWS,Good written/verbal presentation and communication skills,Strong project management and leadership skills", "MS with 3 years of experience or a new Ph.D,Deep technical skills including computer programming (R, Python or similar),Data management (SQL) and data visualization techniques,Multivariate analysis, data science,Unit operation model building (mechanistic),Machine learning,Bioprocess economic modelling packages such as Biosolve, SuperPro, Bio-G or similar,Total Cost of Ownership / Net Present Value analysis,Approaches for systems biology and proteomics analysis,Understanding of scripting approaches to build automation across platforms such as DeltaV, Pi, SIPAT, Tecan and similar,Approaches for chemometric modeling of spectral data sources,Approaches for complex Residence Time Distribution Modeling", "Experience to turn research results into commercialization is highly valued", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "M.B.A, B.S. or M.S. in Statistics, Economics, Engineering, Computer Science, Mathematics or a related quantitative field is required for this position;,Proven track record of end-to-end experience in building analytical frameworks to inform business strategy;,Deep knowledge and understanding of statistical and econometric modeling methods for significance testing and causal inference. These methods include (but not limited to) regression analysis, forecasting, bootstrapping, outlier detection, feature selection, and decision trees;,At least 3 years of industrial experience working in an analytics and business strategy role;,Exceptional programming skills in Python and R. Knowledge of Tableau and other data visualization tools is preferred.,Strong knowledge of databases and related languages/tools such as SQL, NoSQL, Hive, etc;,Ability to work in a dynamic, cross-functional environment, with a strong attention to detail;,Effective communication and presentation skills and ability to explain complex analyses in simple terms to business leaders;,Strong relationship building and collaborative skills;,Exceptional problem-solving skills;,Inspiring company mission,Amazing work environment in San Francisco, CA,Competitive compensation, including equity", "The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.", "Degrees in Physics, Mathematics, Computer Science or Engineering are a plus", "Degree in an analytical field (e.g. Computer Science, Engineering, Mathematics, Statistics, Operations Research, Management Science),3+ years of experience in a role with data analysis and metrics development,3+ years of hands-on experience analyzing and interpreting data, drawing conclusions, defining recommended actions, and reporting results across stakeholders,3+ years of SQL development experience writing queries,3+ years of hands-on project management experience,3+ years of experience with data visualization tools,3+ years of experience with packages such as R, Tableau, SPSS, SAS, STATA, etc.,2+ years of experience with scripting in Python or PHP,Experience leveraging data driven models to drive business decisions,Experience using data access tools and building visualizations using large datasets and multiple data sources,Experience thinking analytically,Experience communicating data to all organizational levels,Experienced with packages such as NumPy, SciPy, pandas, scikit-learn, dplyr, ggplot2,Knowledge of statistics and optimization techniques,Hands-on experience with medium to large datasets (i.e. data extraction, cleaning, analysis and presentation)", "Master\u2019s Degree required in Mathematics, Information Technology, Statistics, Data Science or related field.,At least 12 plus years of progressively responsible hands-on quantitative modeling skills and SQL, SAS, R and/or Python experience.,Significant experience working with relational databases and associated query/extraction languages.,Strong knowledge of predictive models (classification and regression models), decision trees, time series data mining, etc.,Ability to stay organized and meet deadlines.,Willingness to work as a team member.,A self-starter able to administer a number of open, ongoing assignments at any one time, where some assignments are routinely unstructured, requiring autonomy and independent judgment.,In-depth experience successfully harmonizing diverse and competing interests.,Ability to clearly articulate a position with sound logic, supporting empirical evidence, and impartiality.,Ability to effectively represent the organization to a variety of both internal and external constituencies.,Superior verbal and written communication skills.,Ensures that own behavior and the behavior of others is consistent with the highest ethical standards and aligns with the values of the organization.,Ability to promote collaboration by unifying teams, setting common goals and incentivizing collaborative behavior.,Demonstrated success in establishing and maintaining positive working relationships with others, both internally and externally, to achieve the goals of the organization.,Strong ability to build credibility, organize effectively, solve problems quickly and communicate clearly.,Possesses the balance and emotional intelligence required to meet the diverse needs of the divisions/offices.,Proven ability to navigate and resolve various types of conflict in a timely and productive manner.,Proven transformation skills that include the ability to consistently execute at a high level, drive positive change and desire to build established programs and teams.,Demonstrated agility and ability to navigate complex environments.,Ability to foster an environment of creativity and innovation, focusing on the empowerment and support of staff through tools and continuous process improvement.,Supports individuals and teams in process excellence, project management, problem solving, and value creation to drive toward required outcomes.,Surfaces capacity, pacing, resourcing and any other issues requiring leadership attention. Ensures organizational alignment, effective stakeholder engagement and communication.,Demonstrated ability to think broadly and strategically, including the ability to translate long-term goals and objectives into short-term tactical plans and operational activities.,Effectively assesses progress by identifying and articulating clear, consistent key performance indicators.", "2-4 years of relevant industry experience,A graduate degree in statistics, applied mathematics, computer science, physical sciences, or a similar technical field,Experience developing and deploying machine learning / deep learning solutions,The versatility to communicate clearly with both technical and non-technical audiences,Python (numpy, pandas, sklearn, xgboost, TensorFlow),MySQL, Hive,Java,Google Cloud Platform", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Pursuing or has graduated with a Masters in a quantitative field (Operation Research, Computer Science, Engineering, Applied Math, Statistics, Physics, Analytics, etc.) or have equivalent work experience.,Proven leadership in applying and scaling Analytic techniques to deliver impactful insights from data (in academics or in industry.,Strong written and verbal communication skills to influence others to take action.,Able to balance multiple priorities.,Good social skills, self-motivated, dynamic with a can-do mentality.,Strong enthusiasm and curiosity about the intersection of business and technology,2-4 years of Data Science Experience,Experience in disrupting current business practices in CPG or related industries to help crafting a new go-to-market models.,Experience with Analytical Tools/Applications including:,Unix/Linux,Big Data Ecosystem: Hadoop, Spark, MapReduce, SQL, HIV,Scientific Computing: R, Python, C++, Java, Scala, etc.,High-Performance Parallel and Distributing Computing,Deep Learning frameworks: Keras, Tensorflow,Data Visualization,Data Management Systems,Business Intelligence tools: such as KNIME, Tableau", "Experience with SQL and/or querying MongoDB or Solr Indexes", "Be pursuing an M.S. (or BS) in Computer Science, Information systems, Mechanical Engineering, Materials Engineering, Chemical Engineering, Electrical Engineering or Chemistry/Physics,Expertise in engineering analysis tools, data analysis and scripting methods in order to automate and train standard analytical tasks is required.,Prior experience with machine learning algorithms, artificial intelligence, image processing or numerical computing is a plus.", "Bachelor's degree,Experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,GPA of at least 3.0 on a 4-point scale,Experience with real world data through thesis research, internships, or work experience,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Advanced degree in a data science equivalent field or sub-field,Experience working with data rich problems through research or programs,Experience with computer programming or user experience/user interface,Ability to successfully complete projects with large or incomplete data and provide solutions,Strong written and verbal communication skills,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "PhD in a computational and quantitative discipline (e.g. statistics, computer science, biomedical informatics, genetics, physics, epidemiology, health economics) or Master\u2019s Degree in a similar field of study,Deep understanding in ML, including strong knowledge of the mathematical underpinnings behind various methods (e.g. regression techniques, neural networks, decision trees, clustering, pattern recognition, dimensionality reduction),Proven experience in applied statistics and ML in a business setting,Deep understanding of the tools of the trade, including a variety of modern programming languages (R, Python, JavaScript) and open-source technologies (Linux, TensorFlow, Hadoop, and Spark),Experience in effective data visualization approaches and a keen eye for detail in the visual communication of findings,Comfort working and communicating with non-technical teams to translate business questions to analytically actionable questions,A strong desire to build meaningful solutions for a life sciences business,Task oriented with ability to set goals and complete deliverables,Domain knowledge of clinical data, real-world data, or life sciences-related research data,Expertise in other data science-related tools (e.g. SQL, Tableau, D3)", "B.A./B.S. in Statistics, Math, Economics, Finance, Computer Science, or a similar field,Database experience including proficiency in SQL,Excellent communication, organization, and analytical skills with experience presenting to various business contacts at all levels in the company,Real passion for working with product management and engineering,Ability to thrive in a fun and dynamic start-up environment,A roll up your sleeves attitude,An excellent sense of humor,Coding experience with scripting languages such as R and Python,Experience with visualization tools like Tableau,Contributions to the data science community,Background in online advertising,Masters degree in statistics, math, or a related field", "Master's or PhD preferably in an engineering, statistics, technology or science role.,Analytics - Familiarity with common advanced analysis tools - SQL, Python, R, SAS are preferred.,Demonstrate familiarity (work experience, Github account) with OOP concepts. Python, Java, Scala skills is a big plus.,Machine Learning, Deep Learning, NLP experience and also working in Hadoop and/or Spark environment.,Ability to ask and tackle the most important analytical questions with a view on driving product impact.", "US citizenship required,We DO NOT offer sponsorships or contracting work. This position is for Charlottesville only.,Bachelor\u2019s Degree in a technical field,Prior Work experience in a related field and/or a working portfolio that clearly demonstrates your abilities.,Casual Work Environment,Intellectually Challenging Work,Health Insurance,Short Term Disability Insurance,Generous Defined Benefit Retirement,Very Flexible Vacation Policy,Want to know more? Check out our recruitment video: https://www.youtube.com/watch?v=W_b2EY1tlRM,Commonwealth Computer Research, Inc. does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity or any other reason prohibited by law in provision of employment opportunities and benefits.", "You have worked with some seriously huge datasets", "Master\u2019s degree in behavioral sciences or relevant field, with training in research methodology and statistics,Two or more years of experience with various data analysis and visualization tools,Strong communication and data presentation skills,Experience with SPSS, Excel, PowerPoint and Word,Ph.D. in behavioral sciences or relevant field, with rigorous training in research methodology, statistics and psychometrics,Expertise in Python and R,Expertise in using and applying machine learning or deep learning models and/or systems,Professional experience in human capital consulting", "Qualifications may warrant placement in a different job level.*", "You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "1-7 years experience working in the Intelligence Community on teams working in any of the fields related to Data Science such as: Statistical Modeling, Information Retrieval, Text Analysis, Data Mining, Machine Learning, Intelligence Analysis, Cyber Threat Analysis, Image Analysis, Network Security, Statistical Modeling, Geo-spatial analytics, and Data Munging and Cleaning,Bachelor's Degree,Ability to work with datasets of different sizes and formats across multiple databases.,Knowledge and experience with specific techniques such as: neural networks, cluster analysis, feature engineering, extraction, and reduction, web-scraping, decision trees (CART), collaborative filtering, geo-spatial analysis.,Experience with: PCAP data, Elastic Search, Hadoop, HDFS, git, Spark, MLLib, SQL OS Experience/ Windows, Linux/ Windows,Ability to compile results and deliver presentations to senior-level leadership. Strong communication skills and ability to present material to audiences of differing technical aptitude.,R, R-shiny, R-studio, Python, Sci-kit, Tensorflow, Intelligence Community Experience, Machine Learning, Statistical modeling,,Experience with multi-TB dataset manipulation, cleaning, querying, and modeling.,Experience with scikit-learn, tensorflow, R-caret.,Experience curating datasets for supervised and unsupervised machine learning methods.,Experience with C, Java, R, Javascript, PhP, MatLab, Pig, Hive, Impala, PySpark, Scala, Ruby, Pytorch", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "You have a bachelor\u2019s degree in computer science, information systems, mathematics, statistics, or a related quantitative discipline. Applicants with some data-related expertise and a professional background in media are also encouraged to apply.,1-2 years professional experience at a media company is preferred. An enthusiasm for The Wall Street Journal and an understanding of the product is a must.,You have an entrepreneurial attitude toward work and sweat the details, as well as a healthy skepticism of the status quo.,You have experience using analytics tools (SQL, Tableau, Excel) and an interest in more advanced topics in analytics (artificial intelligence, data engineering).,You have worked with visualization and machine learning libraries in either R or Python.,Experience in building web applications and agile development is a plus.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]", "You must be a U.S. Citizen or National to apply.,Required to pass a background investigation and fingerprint check.,Registered for Selective Service, if applicable.,Successful completion of a one year probationary period.,Meet education and/or experience requirements.,Complete the Occupational Questionnaire/submit resume/supporting documents.,Organizational Performance Analysis,Problem Solving,Database Management Systems,Planning and Evaluating,An applicant with a disability needs an accommodation to have an equal opportunity to apply for a job.,An employee with a disability needs an accommodation to perform the essential job duties or to gain access to the workplace.,An employee with a disability needs an accommodation to receive equal access to benefits, such as details, training, and office-sponsored events.", "Conduct the planning, development, implementation, and administration of systems for the acquisition, storage, and retrieval of data.,Consult with customers and apply analytical processes to the planning, design, and implementation of new and improved information systems to meet the business requirements of customer organizations.,Identify, adapt, and manage changes to data analysis tools in response to evolving user needs.,Provide technical advice to the Group Director and Deputy Director in the collecting, analyzing,interpreting and communicating insights from data.,Develop database system proposals and coordinate the efforts necessary to translate business requirements into effective IT data system solutions.,You must be a U.S. Citizen or National to apply for this position.,You will be subject to a background and suitability investigation.,Optional Form 306, Declaration of Federal Employment and the Background/Suitability Investigation - A background and suitability investigation will be required for all selectees. Appointment will be subject to the successful completion of the investigation and favorable adjudication. Failure to successfully meet these requirements may be grounds for appropriate personnel action. In addition, if hired, a reinvestigation or supplemental investigation may be required at a later time. If selected, the Optional Form 306 will be required prior to final job offer. Click here to obtain a copy of the Optional Form 306.,Form I-9, Employment Verification and the Electronic Eligibility Verification Program - CMS participates in the Electronic Employment Eligibility Verification Program (E-Verify). E-Verify helps employers determine employment eligibility of new hires and the validity of their Social Security numbers. If selected, the Form I-9 will be required at the time of in-processing. Click here for more information about E-Verify and to obtain a copy of the Form I-9.,Standard Form 61, Appointment Affidavits - If selected, the Standard Form 61 will be required at the time of in-processing. Click here to obtain a copy of the Standard Form 61.,Best Qualified - for those who are superior in the evaluation criteria,Well Qualified - for those who excel in the evaluation criteria,Qualified - for those who only meet the minimum qualification requirements,Official Position Title (include series and grade if Federal job),Duties (be specific in describing your duties),Employer's name and address,Supervisor name and phone number,Start and end dates including month, day and year (e.g. June 18, 2007 to April 05, 2008),Full-time or part-time status (include hours worked per week),Salary,To begin, click Apply to access the online application. You will need to be logged into your USAJOBS account to apply. If you do not have a USAJOBS account, you will need to create one before beginning the application.,Follow the prompts to select your resume and/or other supporting documents to be included with your application package. You will have the opportunity to upload additional documents to include in your application before it is submitted. Your uploaded documents may take several hours to clear the virus scan process.,After acknowledging you have reviewed your application package, complete the Include Personal Information section as you deem appropriate and click to continue with the application process.,You will be taken to the online application which you must complete in order to apply for the position. Complete the online application, verify the required documentation is included with your application package, and submit the application.,First week of September: The announcement will be open during this period or until midnight the day that 200 applications are received.,Early September: First round reviews (Written Assessment),Mid September: Second round reviews (1 hour phone calls),Early October: Hiring manager's interviews with qualifying applicants,Mid-to-late October: Tentative job offers are sent to applicants,An applicant with a disability needs an accommodation to have an equal opportunity to apply for a job.,An employee with a disability needs an accommodation to perform the essential job duties or to gain access to the workplace.,An employee with a disability needs an accommodation to receive equal access to benefits, such as details, training, and office-sponsored events.", "Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Gated dog run located outside - in our LA location", "Bachelor's Degree in Data Science, Computer Science, Mathematics, or Statistics,Master\u2019s Degree in Data Science, Computer Science, Mathematics, or Statistics preferred,2 years\u2019 experience in leading data analytics initiatives and using statistical modeling, analytics platforms tools and producing high quality data analytics products,Excellent verbal and written communication skills and the ability to effectively communicate and collaborate with internal stakeholders and analytics vendor partners,Demonstrated ability to analyze large complex data sets and demonstrated aptitude for conducting quantitative and qualitative analysis,Understanding of how analytical techniques support cybersecurity program goals and objectives,Deep understanding of statistics, artificial intelligence and machine learning models and the application of these models to support cybersecurity objectives,Ability to develop intuitive analytics, reports and visualizations that improves risk management decision making and optimizes cybersecurity operations orchestration/automation,Experience with working with large scale analytics and event management platforms (Hadoop, Splunk, Qradar),Experience with statistical, analytics reporting and visualization tools, (Power BI, Excel, Tableau, SSPS statistics/modeler),Mature, self-starting, self-motivating, and capable of making decisions independently,Proficiency with data management languages (e.g., SAS, R, Python, etc.),Develop and implement cybersecurity analytics product and tools. This includes the development, maintenance and continuous improvement of cybersecurity statistical models, predictive analytics, reports and visualizations. Partner with stakeholders to define analytics requirements and program needs.,Provide direct support to incident handling, vulnerability management teams and IT risk teams by providing expertise in exploratory data analysis, pattern discovery and advanced analytical techniques to anticipate or detect undiscovered threats. Work with internal stakeholders, vendors and partners to optimize analytics systems to detect and manage external and internal threats. Coordinate data infrastructure needs with appropriate stakeholders.,Develop high quality, compelling and intuitive data dashboards, research papers, visualizations, stories and presentations.,Coordinate cybersecurity analytics program activities. This includes coordinating development activities and initiatives with analytics stakeholders, architects and engineers. Communicate analytics initiatives status. Ensure data analytics initiatives are aligned and support cyber analytics program objectives and requirements.,Effectively manage shifting priorities and timelines. Mentor and coach cybersecurity and risk management staff team members. Communicate analytics gaps and needs to leadership. Effectively work within a team to support the goals and objectives of analytics program.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "You have a background in engineering mechanics, inertial sensing and signal processing,You have working knowledge of system identification, statistical inference and modeling,You have working knowledge of MATLAB", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Experience in digital sound processing,Excellent medical, dental, vision, life, disability benefits, and a 401k program,Ability to work closely with customers who are hungry for our product, and where can make a positive impact on their livelihood and the world,A focus on community involvement and career development,We are an equal opportunity employer and value diversity at our company. We are committed to creating an inclusive environment for all employees.", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "A Bachelor's or Master Degree in Computer Science or related field, with a focus on data science / machine learning is preferred.,A minimum of 5 years of hands on experience in data science or machine learning,3+ years' experience in data engineering, analytics and /or business intelligence,At least 3 years of experience in C++, C# or Java,At least 2 years of current experience in Deep Learning.,Hands-on experience with deep learning and neural networks,Highly proficient in Python and SQL,Proficient in Tensor Flow, Keras, Theano,Experience with Jupyter notebooks and docker containers,Strong mathematics, statistics, and data analytics abilities,Have solid understanding of both relational and NoSQL database technologies. Skilled working with stream & batch data, extremely large data sets,Experience leading small technical teams (2-5 people) and project management,Track record of successful projects in data engineering, and machine learning product,Experience with advanced data analytics and visualization techniques.,Experience with advanced machine learning techniques, complex data pipelines, ingest and configurations systems.,Experience developing for and leveraging distributed computing or GPU systems / frameworks,Experience developing / implementing predictive solutions and anomaly detection in large scale systems.,Prior experience in a research or labs environments, exploring and evaluating new technologies or products, publishing research, creating white papers etc.,Experience in complex networks, big data, mobile, media and /or wireless environments,Experience with data processing and storage frameworks like Hadoop, Scala, Spark, Storm, Cassandra, Kafka, etc.", "Degree in computer science, mathematics, statistics, or equivalent experience,2+ years of work experience in data science and analytics,Senior-level SQL and Python, R, or alternate analytic programming language experience,Methodical and detail oriented approach: you know your information is accurate, and you are able to keep yourself to a schedule,Tenacious and unstoppable attitude: want to get the job done quickly and efficiently,Ability to effectively work with a wide variety of personalities and skill levels", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Bachelor degree,Minimum of 1 year of experience in predictive/statistical modeling using SAS or R or Python,Proficient in MS Office applications,Excel proficiency (Pivots, V-Lookups, Formulas),Bilingual (Spanish / English),Master\u2019s Degree,Experience performing data analysis,Experience extracting data using SQL and/or data exploration tools (such as SAS or R),Experience with data analytics design,Experience working with large databases,Health care industry experience,Demonstrated ability to effectively gather requirements, probe for deeper understanding, and translate deep technical concepts to non-technical as well as technical senior stakeholders, marketing customers, and data scientists,Demonstrated ability to manage people and prioritize deliverables,Proven organizational skills with ability to be flexible and work with ambiguity", "4 + years of recent experience in a data science or data analyst role.,Familiarity with measuring UX, customer engagement, planning and analyzing AB experiments.,Comfortable and well versed working with predictive and causal problems,A passion for improving the customer experience through refining the product.,Excellent presentation, communication and social skills, with strong attention to detail.,Strong business mindset, possessing an ability to condense complex analysis and technical concepts into clear and concise takeaways for business leaders.,Ability to operate comfortably and effectively in a dynamic, highly cross-functional, fast-paced environment.,Excellent time management skills with the ability to manage work to tight deadlines and handle the pressure of product launches and executive requests.,Well-versed in SQL languages and experienced with big data technologies such as Hadoop and Spark.,Familiarity with Python or R and data visualization tools such as Tableau for full-stack data analysis, insight synthesis and presentation.,Ability to comprehensively understand data elements, sources and relationships in business and technical terms.", "United States (Preferred)", "Must have at least 3+ years of actual working experience performing advanced quantitative analyses.,Must have the actual working knowledge of Python/SQL.,Working knowledge of SAS/R is plus.,Working knowledge of big data manipulation tool is plus.,Ability to apply advanced statistical methodologies such as mixed model (random and fixed effects), simultaneous equations, ARIMA, neural networks, and multinomial discrete choice. Ability to apply mathematical operations to such tasks as cluster analytics, sampling theory and design of experiments, analysis of variance, correlation techniques, and factor analysis.,Ability to apply advanced optimization methodologies such as linear and mixed integer optimization.,Ability to apply advanced simulation modeling methodologies and techniques.,Utilize complex computer operations (intermediate programming in 3rd and 4th generation languages, relational databases, and operating systems) and advanced features of software packages (word-processing, spreadsheet, graphics, etc.).,Must have relational database experience.,A strong passion for empirical research and for answering tough questions with data,Demonstrated experience in organizing, prioritizing, and coordinating complex team efforts,Experience in working with executives or strategic planning departments to set and/or manage to corporate level strategies a plus,Experience with business support software applications such as MS Office (Word, PowerPoint, Excel, Project) required", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "BS or graduate degree in Computer Science, Statistics, Mathematics, Economics or similar quantitative field, with an emphasis on predictive analytics, data mining, statistics, machine learning, algorithms, etc.,3-6 years of data science experience working closely with product and engineering,Ability to translate business objectives and problems into analytical problems, and use quantitative/qualitative skills to deliver simple, logical and actionable solutions.,Experience with data visualization (such as Tableau) and presentation,Advanced proficiency in R, Python, and Excel,Proficiency with SQL, Hadoop, or other Map/Reduce (or other scripting language),Proven ability to own data science initiatives end-to-end,Ability to build predictive modeling such as time series, k-nearest neighbors, random forests, ensemble methods,Understanding of applied math topics, such as probability and statistics, linear algebra, basic optimization techniques, etc.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "4+ years of experience in a quantitative analysis/consulting role,Advanced data modeling experience, SQL skills, and ETL design,Experience with advanced data analytics, data transformation and data management projects,Dimensional modeling and BI support,In-depth experience of analyzing data and creating reports, working with data to identify trends and make recommendations", "Ability to tie together disparate and unique vintage data sources into a cohesive, automated and stable data output for utilization in models,Expertise in SQL, MS Access, VBA-Excel, SAS and other query and modeling languages/tools,Ability to leverage industry-standard operations modeling techniques (Regression, Six Sigma) to address both ongoing and ad hoc modeling needs (e.g. working with actuaries to remove operations impact from reserving models),Expertise with one or more analytics packages (SAS, R, Python) required,Experience building advanced analytics models (Artificial Neural Nets, k-Nearest Neighbors),Ability to creatively problem solve in a collaborative fashion,Ability to explain highly technical or complicated issues into simple non-technical language,Keen analytical thinker with strong research capabilities,Ability to perform independent research on industry trends around analytics,Excellent written and verbal skills,5+ years of work experience", "15+ years of proven top performance in data science and analytics space,Must have at least 5+ years of direct experience with models built and maintained in production focused on large commercial business problems that tend to be in the billion dollar plus in scope of opportunity,Must have expert level understanding and ability to explain both the code and the underlying math used in algorithms/models. Must have excellent coding proficiency with demonstrated expertise in python, java, R, etc. Expert level knowledge of AI/ML models, frameworks(keras, pytorch, etc.) and libraries/packages/apis (e.g., scikit),Must have at least 5+ years of developing models and algorithms independently, writing your own code, developing strategy for algorithmic experimentation, and deploying in production,Must have worked with both batch and streaming models in production,Advanced knowledge of math, probability, statistics, and models,Experience with Hadoop, AWS or other distributed compute services,Exposure to data structures, data modeling and software architecture skills,Excellent communication skills,Excellent cross-functional collaboration skills,Outstanding analytical and problem solving skills", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Ability to work in a team-oriented, collaborative environment", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "3+ years of working experience", "Knowledge of GIS principles, concepts, and methods.,Working knowledge and experience with ArcGIS Desktop 10.6, address processing, and geocoding.,Demonstrable expertise in cartographic design and production.,Ability to work with and identify patterns in very large data repositories and cross correlate metadata across multiple data sources.,Proficiency in required programming languages and scripting languages (Python, JavaScript); SQL databases and SQL query language.,Ability to develop and implement GIS automation applications, quality assurance protocols, and metadata standards.,Proven ability to conceptualize and complete complex projects with thorough documentation and demonstration of applied logic (i.e. flow charts).,Ability to communicate effectively with supervisors, project leader, and co-workers both orally and in writing.,Ability to work in a team environment as well as a goal-oriented individual who functions with the highest level of integrity and professionalism.,Ability to multitask and work in a fast-paced environment with deadlines.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Visualization to tell a story and explain a point", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Healthcare Data analysis experience in clinical environment \u2013 strongly preferred,3+ year or equivalent exposure to Business Objects or Tableau,Understands data integration processes and how to tune for performance,5+ years SQL experience (Oracle preferred, SSIS ok),Ability to develop work plans and follow through on assignments with minimal guidance,Ability to work with business system owners to obtain requirements and manage expectations,2+ years\u2019 experience with SAS,Oncology data experience preferred.", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "2+ years of experience performing statistical and predictive analytics within video games industry,Expert in machine learning,Expertise in R and Python,Must have more than a passing familiarity with Behavioral Science or Psychology or Social Science\nEligibility Requirements:,Interested candidates must submit a resume/CV online to be considered,Must have unrestricted work authorization to work in the United States,Must be covered by Solutions, NBCU\u2019s Alternative Dispute Resolution Program\nDesired Characteristics:,Expertise in Java and C/C++ a plus,Understanding of Big Data technologies (Cassandra, Spark, Hadoop) strongly preferred\nAre you a fan of The Fast and the Furious, Jurassic World, or Back to the Future? Have you ever dreamed about working on innovative games based on your favorite movies and TV shows? If so, The Universal Games and Digital Platforms group is looking for you! Join a diverse group of creative talent, crafting groundbreaking interactive experiences inspired by some of the world\u2019s most valuable brands. Operating with a start-up mindset, the business unit is nimble, and explores new and emerging platforms, such as AR and VR, to stay at the forefront of innovation.They\u2019re currently looking for even more movers and shakers, because this group is GROWING. Could you be the next member of the team that really is all \u2018fun and games\u2019?\nNBCUniversal\u2019s policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.", "Experience managing small teams or projects.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "PhD in a computational and quantitative discipline (e.g. statistics, computer science, biomedical informatics, genetics, physics, epidemiology, health economics) or Master\u2019s Degree in a similar field of study,Deep understanding in ML, including strong knowledge of the mathematical underpinnings behind various methods (e.g. regression techniques, neural networks, decision trees, clustering, pattern recognition, dimensionality reduction),Proven experience in applied statistics and ML in a business setting,Deep understanding of the tools of the trade, including a variety of modern programming languages (R, Python, JavaScript) and open-source technologies (Linux, TensorFlow, Hadoop, and Spark),Experience in effective data visualization approaches and a keen eye for detail in the visual communication of findings,Comfort working and communicating with non-technical teams to translate business questions to analytically actionable questions,A strong desire to build meaningful solutions for a life sciences business,Task oriented with ability to set goals and complete deliverables,Domain knowledge of clinical data, real-world data, or life sciences-related research data,Expertise in other data science-related tools (e.g. SQL, Tableau, D3)", "Technical Expertise \u2013 5+ years of experience applying advanced analytics techniques (data mining, descriptive statistics, visualization) to solve complex business problems including 2+ years of deep technical experience in predictive analytics, machine learning, and/or optimization. Fluent in multiple technologies such as with Python, Azure ML, IBM SPSS Modeler, R or comparable technologies required. Strong database skills required. Experience with visualization techniques preferred. Experience with optimization software preferred.,Technology Leadership - Strong working knowledge of contemporary analysis technology, software platforms, and methodologies with the ability to apply it to our manufacturing processes. The ability to educate senior leaders on the impact and benefit of analytics (descriptive, predictive, prescriptive and cognitive) in our operations.,Project Leadership \u2013 Small to medium scale project management experience, including, but not limited to scope, schedule, cost, risk, resource, and change management. Possibly including initiatives with global reach, technology, processes, cross-functional teams and partner team members.,Consulting skills - Proven track record of influencing the decision and problem-solving processes. The ability to understand business and economic drivers and align goals across functional lines and organizational boundaries for execution.,Global Experience \u2013 understands, communicates with, and effectively interacts with people across cultures to achieve business results. Prior experience participating or collaborating with cross-functional and cultural teams is beneficial.,Analytics - Has demonstrated experience in applying statistical techniques to solve business problems.,Visualization \u2013 Has experience in the effective utilization of visualization techniques to explore data to find root causes as well as presentation of results.,Optimization - Demonstrated experience in various optimization techniques including linear programming, integer programming, non-linear programming and dynamic programming.,Leadership - Recognized expert in their field with the ability to help other define the problem and move quickly to resolution. Someone sought out to bring resolution to an issue in timely, cost effective manner.,Consultative Skills - Ability to influence business partners in their decision-making. Shape solutions by helping partners articulate what they need.,Problem Solving \u2013 Strong intrinsic problem-solving skills. Ability to structure and solve problems and conduct and interpret analysis independently with demonstrated analytical and quantitative skills.,Diversity - Understands, communicates with, and effectively interacts with people across cultures. Effectively achieves business results working across and with multi-national teams.,Communication \u2013 Strong presentation and communication skills with ability to explain complex analytical concepts to people from other fields.,External Technology Knowledge - Keeps abreast of the latest technological developments in areas that can push our technology roadmaps to realization. Constantly looks for opportunities to incorporate new solution methods to solve existing problems.,Unconditional commitment to safety,Competes on Analytics,Adaptability - Ability to respond quickly to the demands of the moment. A flexible person who can stay productive when the demands of work pull in many different directions at once. Shifts focus as necessary to maintain effectiveness in a variety of environments. Can quickly come up to speed on a project and be a contributor.,Accountability - Knows what needs to be done and gets it done. Willingly takes responsibility for the organization as a whole; unafraid of owning the results, actions, and decisions of self or organization. Committed to follow through to completion \u2013 excuses and rationalizations are totally unacceptable.,Curiosity - Naturally curious, leading one to seek knowledge about people and things that stretch beyond one\u2019s own work environment. Thrives in a dynamic work environment, where new subject matter is learned and quickly put into practice. An information \u201csponge\u201d \u2013 constantly absorbing new methods, technologies, and approaches that will deliver business outcomes.,Decisive - Is able to make a decision both competing through analytics and under conditions of high uncertainty. Weighs risks and prioritizes actions to deliver organizational effectiveness with speed.,Risk Taker / Innovator - Is willing to push the envelope to meet stretch goals. Not satisfied with the status quo but willing to be aggressive to implement the next breakthrough technology and get our operations to the next level.", "Team outings to sports games, happy hours, game nights and more!", "3-8+ years of professional industry experience doing quantitative analysis,A proven track record of using analysis to impact key business or product decisions,The ability to clearly and effectively communicate the results of complex analyses,Experience writing production datasets in SQL/Hive OR building internal/production data tools for ETL, experimentation, or exploration in a scripting language (Python, R, etc.),A solid grasp of basic statistical applications and methods (experimentation, probabilities, regression),Experience in software engineering, data engineering, consulting, or academic research a plus", "Background in image processing concepts (e.g. filtering, morphology, transforms, compression, etc.) is desired", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Never-ending ping-pong tournaments", "Master\u2019s Degree from an accredited Institution,Minimum 3 years of experience with data analytics.,Employees must be legally authorized to work in the United States. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position.,MS or PhD, from an accredited institution, in Applied Mathematics, Operations Research, Industrial Engineering with Mathematics focus, Machine Learning, or Physics, or similar quantitative discipline,3+ years of experience delivering computation approach to outcomes solving complex analytical problems using quantitative approaches with your unique blend of analytical, mathematical and engineering skills.,Experience in Manufacturing,Understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms.,Accomplished in the use of statistical analysis environments such as R, MATLAB, SPSS or SAS.,Experience with BI tools such as Tableau or MicroStrategy.,Comfortable with relational databases and Hadoop-based data mining open-sources frameworks.,Familiar with SQL, Python, Java and C/C++.,Experience with isogeometric analysis", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Bachelor\u2019s degree in Statistics, Computer Science, or a related field,2+ years of experience working with large datasets for drawing business insights,Fluency in Python programming,Experience using machine learning to solve complex business problems,Strong understanding of statistics and modeling techniques,Knowledge in predictive modeling,Experienced at leveraging both structured and unstructured data sources,Experience with relational databases and SQL,Willingness and ability to learn new technologies on the job,Demonstrated ability to communicate complex results to technical and non-technical audiences,Demonstrated ability to work with minimal supervision,Experience with distributed computing and big data technologies, such as Spark, or related technologies,Experienced with statistical methodologies and tools (R, SAS, SPSS, etc.),Experience with Natural Language Processing, Information Retrieval, or Recommender Systems", "Possess a Bachelor\u2019s degree or higher (completed and verified prior to start) from an accredited institution,Minimum of two (2) combined years of experience in one or more of the following areas: data analytics, data visualization, statistical analysis, predictive modeling, and/or application development in a private, public, government or military environment,Project Management experience,Extensive knowledge and experience using Excel, Access, Power Point, and MS Word.,Experience working across functions and influencing teams,Continuous improvement mindset,Experience with data modeling tools (e.g., SAP PA, Python, R, SAS, MS-Azure),Experience with business intelligence and data visualization tools (such as from Power BI, Google Analytics, Tableau, Domo, Qlikview),Data integration experience including extract, transform, load (ETL) processes,Experience with databases and complex data queries,Greenbelt certified,Strong organizational skills,Self-motivated and independent,Excellent oral and written communication skills,Ability to work in a rapidly changing environment", "Master\u2019s degree or higher in Statistics/Math/Computer Science or related field,5+ years of industry work experience in SQL, R, Python to implement statistical models, machine learning, and analysis (Recommenders, Prediction, Classification, Clustering, etc.) in big data environment,Experience on large scale computing systems like COSMOS, Hadoop, MapReduce and/or similar systems preferred,Experience with programming skills, e.g. Java, C# is a plus,Familiarity with deep learning toolkits, e.g. CNTK, TensorFlow, etc. is a plus,Exceptional written and verbal communication to educate and work with cross functional teams", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Experience with big data and distributed computing platforms such Hadoop and Spark and NoSQL databases,Strong knowledge of standard machine learning techniques and concepts,5+ years of hand-on experience (either in industry or academia) with a scripting language such as Python/R or a general purpose programming language such as Java or Scala,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Creates statistical approaches and analyze data generated by multiple applications/processes and trends data to understand opportunities for focused investigation.,Develops reports, emphasizing clarity and accuracy of methodologies, degree of statistical support for conclusions.,Communicates effectively through various channels, including written reports and oral presentations.,Assists in new business development, proof of concepts, as required.", "Currently pursuing a bachelors or masters in Computer Science, Computer Science & Engineering, Engineering & Computer Science, Math & Computer Science, Mathematical Engineering, Mathematical Science, Mathematics, Statistics,Strong data wrangling skills,Strong Python and/or SQL skills,Ability to manipulate JSON/XML data,Experience with Splunk,Experience with other data manipulation platforms,Experience with Ansible, Puppet, Jenkins, and Chef", "As a member of an analytics group, there may be opportunities to get your hands dirty developing analytics solutions from time to time.", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Expertise in natural language processing and machine learning, such as classification, feature engineering, information extraction, structured prediction, clustering, semi-supervised learning, topic modeling, and ranking.,Proficiency in data science and analytics, including statistical analyses and A/B testing Experience designing, conducting, analyzing, and interpreting experiments and investigations. Strong programming skills, expert knowledge of algorithms and data structures (Python, Java, or equivalent),Excellent problem solving, critical thinking, creativity, organizational, design, and interpersonal skills; ability to work well with all levels of engineers. Confirmed ability to handle multiple projects with strict deadlines", "1+ years of experience in related work building statistical models, and advanced data analysis,Master\u2019s degree or PhD candidate in Mat, Economics, Statistics and/or Data Science,Experience with Logistic Regression, Linear Regression, Time Series Analysis, Decision Trees, and Cluster Analysis,Advanced programming skills to include knowledge of statistical programs (e.g. SQL, SAS, SPSS, R, Python),Must be eligible for full-time employment,Salary range: $80,000 - $130,000,Full Benefits: Cigna Healthcare, MetLife Dental, VSP Vision, 401K with Voya and Paid Time Off", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Strong communication and data presentation skills; ability to communicate with data-driven stories,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "We connect everything \u2013 people, process, data and things \u2013 and we use those connections to change our world for the better.", "Minimally, candidates will have a bachelor\u2019s degree in statistics, math, computer science, informatics, economics, or related fields. Advanced degrees in these subject areas preferred.,General knowledge of statistical techniques and concepts (regression, properties of distributions, statistical tests, etc.);,Novice understanding of Python and SQL,Understanding of Hadoop and Java preferred,Strong problem-solving skills with an emphasis on data driven solutions,Excellent written and verbal communication skills for coordinating across teams,Ability to analyze policies and procedures and be able to recommend improvements,Excellent oral and written communication skills,Ability to work independently, anticipate problems and initiate corrective actions;,Ability to effectively prioritize a variety of projects and functions;,Ability to research and document findings;,Ability to establish and maintain effective working relationships.", "Advanced statistics, modeling, and data visualization knowledge,Strong data visualization skills,Substantial data analysis experience working with large-scale data,Ability to learn new technologies and to quickly grasp complex problems,Significant experience using relational databases, MySQL preferred,Excellent written and verbal communication skills,Strong scripting language skills (e.g. R, Python),Experience working with Hadoop Map/Reduce,Experience working with Hadoop Map/Reduce", "At this point, a successful candidate will have shown prowess as a data scientist in their own right and have been promoted as such. They'd be leading projects and making a major impact on the business.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Working knowledge of Microsoft Office Suite,Working knowledge of Tableau,Working knowledge of presentation software (e.g. Microsoft PowerPoint),Working knowledge of analytical/statistics software (e.g. SPSS, SAS, R, Stata),Demonstrated ability to collaborate and work effectively with cross-functional teams,Ability to develop and apply advanced mathematical/statistical techniques,Ability to convey complex or technical ideas and processes in easy-to-understand terms to diverse audiences,Excellent written and verbal communication skills", "At least 2+ years of experience generating or implementing short term trading alphas into production trading strategies,Advanced knowledge of modern statistical and machine learning techniques,Proficiency in a scripting language such as Python, R, or MATLAB.,Proficiency in at least one statistical package and one machine learning package in one of the above languages.,Demonstrated experience working with tick data.,Degree in a quantitative discipline such as Statistics, Computer Science, Mathematics, Engineering, etc.,Experience developing software systems an in object oriented language is a plus.", "Demonstrated energy and passion that extends beyond your field of study \u2013 Are you a computer scientist who writes poetry? A mathematician who loves psychology? An engineer passionate about public policy? We want to build something with you.", "Ability to apply common sense understanding to carry out instructions furnished in written, oral, or diagram form", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Strong experience implementing real time analytics, dashboard and business intelligence reporting,Strong experience in data visualization with tools such as Tableau and/or d3.js,Experience building an end-to-end data science workflow,Experience in programming with Python and R,A software engineering mindset,Documentation,Source control and release cycles,Repeatability and sharing,Experience implementing ETL, and report generation from large volume of data,Ability to work within a small high achieving team, as well as independently,Self-driven, highly motivated, innovative,Strong communication skills, both written and verbal,Computer Science degree or another highly quantitative degree such as engineering, physics, or mathematics,Graph Databases such as Janus or Neo4J,Apache Spark, Scala,AWS,Linux,Machine learning tools", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Must be eighteen years of age or older.,Must be legally permitted to work in the United States.,The knowledge, skills and abilities typically acquired through the completion of a master's degree program or equivalent degree in a field of study related to the job.,Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.,Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,2-3+ years of experience in Data Science / Natural Language Processing,Previous work experience in Ecommerce, Conversational AI, or Search Science,Experience with Deep Learning & Machine Learning,Experience with Information Retrieval, Dialogue Systems, Coreference Resolution, NER, NLU and/or Knowledge Graph,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Bachelor\u2019s degree in computer science, computer engineering, or related field, or the equivalent combination of education and related experience.,Fundamental knowledge of one or more of the following: high performance computing, scientific data analysis, statistical analysis, knowledge discovery, computer security, systems programming, large-scale data management, and big data technologies.,Skilled in all aspects of the software project life cycle: feasibility, requirements, design, implementation, integration, test and deployment.,Fundamental experience developing software with C++, C, Java, Python, R, or Matlab, software applications in Linux, UNIX, Windows environments, data analysis algorithms, data management approaches, relational databases, or machine learning algorithms.,Ability to effectively handle concurrent technical tasks with conflicting priorities, to approach difficult problems with enthusiasm and creativity and to change focus when necessary, and to work independently and implement research concepts in a multi-disciplinary team environment, where commitments and deadlines are important to project success.,Sufficient interpersonal skills necessary to interact with all levels of personnel.,Included in 2020 Best Places to Work by Glassdoor!,Work for a premier innovative national Laboratory,Comprehensive Benefits Package,Flexible schedules (*depending on project needs)", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Access to opportunities to expand your skill set and share your knowledge with others across the organization", "Many other unique benefits including floating holidays, lunch every Wednesday, paid volunteer time, fuel efficient vehicle purchase assistance, transit fare contribution and first-time homebuyer\u2019s down payment assistance", "As always, the interviews and screening call will be conducted via a video call.", "An eye for great data visualization with Matplotlib, Plotly, ggplot, or Tableau", "2+ years Data Science experience outside of academia. Title will be commensurate with experience,Advanced degree in a quantitative discipline (applied mathematics, statistics, computer science, physics, or other related field) from a leading academic institution.,Expert in Python and SQL. (Must have experience writing production level code.),Proficiency working with Spark to process large data sets.,Excellent communication skills with demonstrated success presenting complex data and analysis (qualitative and quantitative) in a clear and compelling manner that inspires action,Strong understanding of statistical analysis,A strong passion for empirical research and for answering hard questions with data. A passion for problem solving, comfort with ambiguity, and creativity.,A flexible analytic approach that allows for results at varying levels of precision.,Quick learner, with the ability to initiate and drive projects to completion with minimal guidance.,Ability to thrive in a dynamic and fast-paced environment. Drive change and collaborate effectively with a variety of individuals and organizations", "Interest in politics and/or educational policy", "4+ years Professional industry experience in a quantitative analysis role,Proficiency in SQL and some experience with a programming language like Python, R, etc.,Ability to identify complex business problems and provide sound analytical/modeling solutions,Ability to communicate clearly and effectively to cross- functional partners of varying technical levels,Ability to define relevant metrics that can guide and influence stakeholders to the appropriate and accurate insights,Experience or willingness to learn tools to create data pipelines using Airflow,Ability to build clear and easy to understand dashboards and presentations,7+ years industry experience,Experience with Python or R,Experience with Tableau,Ability to model and run experiments,Ability to ramp up to data science manager role in the near future,Stock,$2,000 yearly employee travel coupon,Competitive salary,Paid time off,Medical, dental, & vision insurance,Life & disability coverage,401K,Flexible Spending Accounts,Apple equipment,Daily breakfast, lunch, and dinner", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Undergraduate or Graduate degree preferred with a major in Computer Science, Mathematics, Statistics, Physics, Engineering or related STEM major,At least 2+ years of relevant experience, preferably in a digital domain,Experience in productionising machine-learning models,Programming experience with either Python or R, plus one other general purpose programming language such as Java or C/C++,SQL & relational database experience a must, experience with noSQL, Big Data stack preferred,Strong foundation in inferential statistics and machine learning algorithms,Experience in visualization tools such as Tableau, PowerBI, etc.,Strong capacity to communicate complex concepts in easy-to-understand terminology,Ability to translate data and insight into a value-driving business plan,Experience with off-the-shelf models from AWS, GCP and Natural Language Processing is desired", "At least 2 year of experience working within a Retail or eCommerce company", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Bachelor\u2019s degree in computer science, computer engineering, or related field, or the equivalent combination of education and related experience.,Fundamental knowledge of one or more of the following: high performance computing, scientific data analysis, statistical analysis, knowledge discovery, computer security, systems programming, large-scale data management, and big data technologies.,Skilled in all aspects of the software project life cycle: feasibility, requirements, design, implementation, integration, test and deployment.,Fundamental experience developing software with C++, C, Java, Python, R, or Matlab, software applications in Linux, UNIX, Windows environments, data analysis algorithms, data management approaches, relational databases, or machine learning algorithms.,Ability to effectively handle concurrent technical tasks with conflicting priorities, to approach difficult problems with enthusiasm and creativity and to change focus when necessary, and to work independently and implement research concepts in a multi-disciplinary team environment, where commitments and deadlines are important to project success.,Sufficient interpersonal skills necessary to interact with all levels of personnel.,Included in 2020 Best Places to Work by Glassdoor!,Work for a premier innovative national Laboratory,Comprehensive Benefits Package,Flexible schedules (*depending on project needs)", "3+ years of experience as a Data Scientist, preferably in Big Data Environment,2+ years of programming experience in Java/Scala and/or Python,Hadoop stack (HIVE, Pig, Hadoop streaming) and MapReduce,HBase or comparable NoSQL,SQL & database experience,Experience with Google products: Google Cloud Storage, Google Analytics and Google Big Query (a plus),Bachelor\u2019s degree in quantitative or related field,Design and build predictive customer behavior models for targeting and personalization,Implement Machine Learning and statistics-based algorithms for prediction and optimization, then deliver to production,Build and maintain code to populate HDFS, Hadoop with log from Kafka or data loaded from SQL production systems,Design, build and support algorithms of data transformation, conversion, computation on Hadoop, Spark and other distributed Big Data Systems", "Parties. All-night LAN.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "University student in their final year in a STEM related field (Ops Research, Statistics, Applied Math, Engineering, Business Analytics); expected graduation date of December 2018 or May 2019. Or, currently completing your first year in a non-business, 2-year masters program and have less than 2 years of work experience,Deep understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms,Experience in one or more programming languages (R, Python, C++, etc.),Ability to easily understand the complex algorithm and logic to process data,Experience working with a large volume of data with ability to solve performance issues,Practitioner of statistical data quality procedures or test driven approach for quality assurance,Basic business intuition and clear expertise in analyses with the ability to describe analytic processes, including when and why specific approaches are favored,Excellent communication and presentation skills with an ability to visualize and report insights creatively in variety of formats to various stakeholders,Ability to deliver in deadline-driven environment,Team player with a passion for coaching colleagues and clients", "4 + years of recent experience in a data science or data analyst role.,Familiarity with measuring UX, customer engagement, planning and analyzing AB experiments.,Comfortable and well versed working with predictive and causal problems,A passion for improving the customer experience through refining the product.,Excellent presentation, communication and social skills, with strong attention to detail.,Strong business mindset, possessing an ability to condense complex analysis and technical concepts into clear and concise takeaways for business leaders.,Ability to operate comfortably and effectively in a dynamic, highly cross-functional, fast-paced environment.,Excellent time management skills with the ability to manage work to tight deadlines and handle the pressure of product launches and executive requests.,Well-versed in SQL languages and experienced with big data technologies such as Hadoop and Spark.,Familiarity with Python or R and data visualization tools such as Tableau for full-stack data analysis, insight synthesis and presentation.,Ability to comprehensively understand data elements, sources and relationships in business and technical terms.", "Designing data architecture from table to dashboard", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "2+ years Data Science experience outside of academia. Title will be commensurate with experience,Advanced degree in a quantitative discipline (applied mathematics, statistics, computer science, physics, or other related field) from a leading academic institution.,Expert in Python and SQL. (Must have experience writing production level code.),Proficiency working with Spark to process large data sets.,Excellent communication skills with demonstrated success presenting complex data and analysis (qualitative and quantitative) in a clear and compelling manner that inspires action,Strong understanding of statistical analysis,A strong passion for empirical research and for answering hard questions with data. A passion for problem solving, comfort with ambiguity, and creativity.,A flexible analytic approach that allows for results at varying levels of precision.,Quick learner, with the ability to initiate and drive projects to completion with minimal guidance.,Ability to thrive in a dynamic and fast-paced environment. Drive change and collaborate effectively with a variety of individuals and organizations", "Minimum 3 years of relevant data science experience,Ph.D. or Master\u2019s Degree in operations research, applied statistics, data mining, machine learning, physics or a related quantitative discipline preferred,Experience with Hive SQL and Spark and/or Python and/or Scala,Deep understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms,Strong analytical and problem-solving skills", "Collaborative team player who values the contribution of others.", "Being passionate in Data Science, disciplined at work, and confident while being humble", "Dog friendly office", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Experience with big data and distributed computing platforms such Hadoop and Spark and NoSQL databases,Strong knowledge of standard machine learning techniques and concepts,5+ years of hand-on experience (either in industry or academia) with a scripting language such as Python/R or a general purpose programming language such as Java or Scala,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Currently pursuing a degree or graduated within last 12 months with a degree in the following field: Electrical Engineering, Computer Science, Data Science, Statistics, or other relevant fields.,PhD in the aforementioned fields.,MS (or BS with 2+ years\u2019 experience) in the aforementioned fields.,Strong background in machine learning and/or statistics.,1.5+ years\u2019 experience in probabilistic graphical models/Bayesian networks/deep learning/other modeling paradigms.,Experience in Python/R/Scala or similar.,Research publications are a plus.,BS/MS in the aforementioned fields.,Experience in developing in C/C++/Java/C# and 1+ scripting language.,Experience in database systems and systems engineering.,Experience in designing and developing high-scale distributed systems a plus.,Knowledge of lambda architectures a plus.,Knowledge of machine learning, data visualization, and AI a plus.,BS/MS in the aforementioned fields or business management, marketing, communication, or similar.,Experience in project/product/program management or customer design.,Excellent storytelling, team work, written and oral communication skills.", "Previous message: [Jobs] Fwd: [inedinfo] Fwd: JOB: Statistician 0.6 fte,Next message: [Jobs] NPD Group", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "B.S. in CS, statistics, applied math, physics or other quantitative discipline,2 - 7 years of experience in a role developing predictive or explanatory models and/or experimentation processes,Experience working with data and analytics,Experience with moderate to large-scale data sets (>100GB) preferred,Core mathematical ability to understand, utilize and innovate on state-of-the art machine learning algorithms and/or statistical modeling,Expertise in at least one production-quality programming languages (e.g. java, python, scala, C++),Exceptional communication skills,Expertise with Hadoop ecosystem (especially spark) is a plus", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Bachelor\u2019s in Computer Science, Statistics, Quantitative social sciences or related field.,Minimum of 2 years of experience in technical projects with database components.,R and/or Python programming proficiency.,Proficiency in data integration and data quality development.,Programming experience in mySQL, shell programming, and Django,Some data visualization experience, including R Shiny and Python DASH,Experience with technologies like Github and Amazon AWS,Excellent problem solving skills with proven track record,Experience communicating technical topics to non-technical audience,Interest in educational applications", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Masters or PhD in computer science, analytics or other relevant\nline of study required,At least 10+ years\u2019 experience performing advanced quantitative\nanalyses,Experience with data visualization tools,Excellent understanding of machine learning techniques and algorithms, such as k-NN, Kmeans, NLP, Naive Bayes, SVM, etc. Experience with recommendation engines.,Applied statistics skills, such as distributions, statistical testing, regression, etc.,Experience with common data science toolkits in R and Python,Experience working in with AWS environments,Experience with relational databases and proficiency in query languages such as SQL", "Nordstrom Stock Purchase Plan", "BS/MSc/PhD in machine learning, data science, math, physics, computer science or equivalent degree plus two to four year experience in Game economy optimization, knowledge of IAP a plus,Experience in reinforcement learning, mobile and video games,Exceptional understanding of machine learning concepts, data science and programming,Excellent communication skills with the ability to collaborate with other data scientists, engineers and product managers,Open mind, motivation to learn, and spirit to excel", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Pursuing Ph.D. or M.S. in a quantitative discipline; graduating in 2020,Passion for machine learning,Proficiency in Python/R, SQL,Authorization to work in the United States", "Experience with Spark or other distributed computing frameworks", "An open work environment where everyone, even the new folks, have a voice", "BS degree in Computer Science or related technical field, or equivalent practical experience.,4+ years of proven working experience as a software developer or data engineer.,Strong track record of delivery using graph databases such as Neo4j, TitanDB or OrientDB.,Fluency with graph query languages such as Gremlin, Cypher, SPARQL.,Experience in designing, creating and maintaining recommendation engines.,Experience in Java / Scala development.,Knowledge of a scripting language like Python or Ruby.,Strong experience with RESTful API interfaces and microservice architectures.,Knowledge of statistics and experience using statistical packages for analyzing large datasets (R, Excel, SPSS, SAS etc).,Experience with Microsoft Azure or Amazon Web Services (AWS).,Experience with Agile software development.,Experience in developing knowledge-based systems in different contexts such as information retrieval, intelligent agents, dialog systems and recommendation systems.,Experience in scalability and performance issues concerning very large knowledge stores.,Experience with information extraction through creation of an application layer.,Experience developing REST / JSON applications and multi-threaded applications.,Strong background in computer science: algorithms, data structures, concurrency, and distributed systems.,Strong OO Programming and OO Design knowledge.,Knowledge of professional software engineering practices & best practices for the full software development life cycle including coding standards, code reviews, source control management, build processes, testing, and operations.,Technical expertise regarding data models, database design development, data mining and segmentation techniques.,Technical capabilities with Cloud Services and Micro Services Patterns, API Management, Azure and AWS Services.,Strong communication (verbal and written) and collaboration abilities in addition to technical depth.,Comfortable delivering within an agile program.", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Strong communication and data presentation skills; ability to communicate with data-driven stories,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Self-starter and results orientated, able to work under minimal guidance.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Bachelor's degree in Data Science & Analytics, Engineering, Mathematics, Industrial Engineering, Computer Science, Information Technology, or Economics/Finance,At least 10 years of related experience in Data Science or Information Technology working with a combination of big data / advanced data analytics, or machine learning, programming and data bases. With a master\u2019s degree, at least 8 years of relevant experience is acceptable.,Experience in complex business operations analysis, including engineering design, manufacturing, logistics, finance, and/or market forecasting.,Ability to generate and effectively communicate analytical insights through visualization and reporting tools (SAS, JMP, MS PowerBI, HANA, Qlik, R Markdown).,Proficiency in data science object-oriented programming languages (python, R, ruby).,Proficiency in database languages (MSSQL, Oracle, PostgreSQL, MySQL, neo4j, HANA, Hadoop).,Familiarity with operational business systems preferred (SAP, Salesforce, Enovia).,Excellent communication and customer interfacing skills (verbal and written).,Ability to work independently multi-tasking in a deadline-driven environment.,Manages time and prioritizes tasks effectively.,Master\u2019s degree in Data Science & Analytics, Engineering, Mathematics, Industrial Engineering, Computer Science, Information Technology, Economics/Finance or equivalent professional experience,Experience in stochastic process modeling, design of experiments, non-linear regression, simulation, and optimization methods.,Competency in Natural Language algorithms for processing/generation (NLP/NLG).,Competency in machine learning development.,Knowledge of Aerospace industry,Certification in Data Science & Analytics", "Bachelor\u2019s degree in Computer Science, Mathematics, Engineering, Statistics, or other related field,At least 1 year of experience with R or Python,US Citizenship,Advanced degree in computer science, data science, business analytics or similar analytics concentration that is business-oriented,Experience working with federal agencies and clients,Big data experience - Hive, Spark, Pig, MapReduce,Knowledge of industry leading analytics and big data technologies, approaches, and tools,Experience working in fast-paced collaborative environments,Strong written and oral presentation skills", "Proven ability to perform complex queries and analyses using SAS and SQL (experience in BigQuery Python and R is a plus),Statistical modeling experience,3+ years experience in a marketing role or decision support role,High degree of problem solving and reasoning abilities,Exceptional organizational and communication skills,Understanding of efficient database/data design principles,An excellent total compensation package,Enhanced 401(k) retirement package,Health care, including domestic partner and family coverage, across medical, dental, and vision,Flexible spending accounts for health and dependent care,Life insurance, including domestic partner and dependent coverage,Outdoor experience days in addition to Paid Time Off (vacation, holiday and personal time),Discounts on L.L.Bean merchandise and Outdoor Discovery School adventures,Employee Store,Equipment loan program from the Employee Use Room,Tuition reimbursement", "Bachelor's degree; Master's degree or 1-3 years of work experience after completing your undergraduate degree,2-5 years of advanced analytics experience,Deep familiarity with the analytics or actuarial market landscape and corresponding information needs; knowledge of the insurance industry a plus,Very strong problem-solving skills, and quantitative/analytical thinking capabilities (including experience and familiarity doing multivariate predictive modeling and in analytics software such as SAS, SPSS, R, etc.),Proven record of leadership and ability to work collaboratively in a team environment,Ability to work effectively with people at all levels in an organization,Willingness to travel 30% of the time", "2-5 years of experience in leveraging data for business impact,A deep understanding of statistical analysis (e.g. hypothesis testing, experimentation, regressions) and machine learning algorithms (supervised and unsupervised models),Demonstrated programming experience with at least one analytic tool (R, Python, Scala, etc.).,Ability to write and optimize complex SQL queries.,Experience in building and deploying cloud-based data pipelines and applications (GCP, AWS) is a plus,Ability to communicate clearly and effectively to cross-functional partners of varying technical levels,Ability to work independently and proactively as well as collaborating in a team,A good sense of humor is always a big plus!", "Strong experience with Microsoft Suite (e.g., Excel, Word and PowerPoint),Experience facilitating meetings/workshops to quickly understand business processes and related client business questions.,1 - 2 years implementing BI solutions (working with BI Tool i.e. OBIEE, Cognos, Business Objects, SAP) 4 years of relevant experience delivering Visualizations and Analytic Insight in a client facing role.,Experience, including one or more of the following: use of multi-tier architectures, session management, web-based or web-enabled applications, and Public Key Infrastructure technology preferred.", "Strong data mining and machine learning background,Experience with big data environment,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Experience with big data environment including Spark, Hadoop, Hive, etc.,Proficiency in at least one statistics/data analysis package, such as Python or R,Proficiency in at least one programming language, such as Java or Python, etc.,Solid coding practices including good design documentation, unit testing, and integration testing,Experience working with real-world noisy data,Strong communication and data presentation skills; ability to communicate with data-driven stories,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Bachelor\u2019s degree in computer science, computer engineering, or related field, or the equivalent combination of education and related experience.,Comprehensive knowledge of one or more of the following: high performance computing, scientific data analysis, statistical analysis, knowledge discovery, computer security, systems programming, large-scale data management, and big data technologies.,Skilled in all aspects of the software project life cycle: feasibility, requirements, design, implementation, integration, test and deployment.,Experience developing software with C++, C, Java, Python, R, or Matlab, software applications in Linux, UNIX, Windows environments, data analysis algorithms, data management approaches, relational databases, or machine learning algorithms.,Ability to effectively handle concurrent technical tasks with conflicting priorities, to approach difficult problems with enthusiasm and creativity and to change focus when necessary, and to work independently and implement research concepts in a multi-disciplinary team environment, where commitments and deadlines are important to project success.,Effective interpersonal skills necessary to interact with all levels of personnel.,Effective advanced analytical, problem-solving, and decision-making skills to develop creative solutions to complex problems.,Significant experience with demonstrated expertise in the following technical languages, concepts, or constructs and in one or more of the following advanced areas: high performance computing, scientific data analysis, statistical analysis, knowledge discovery, computer security, systems programming, large-scale data management or big data technologies.,Included in 2020 Best Places to Work by Glassdoor!,Work for a premier innovative national Laboratory,Comprehensive Benefits Package,Flexible schedules (*depending on project needs)", "PhD in Statistics, Machine Learning or related quantitative discipline,Strong research background,Passion for solving real-world problems,Significant experience with R/Matlab,Hadoop/Scripting knowledge is a plus,Competitive salary and bonus program in an entrepreneurial environment,Top notch health, dental, and vision insurance,Stock options in a fast growing tech company,401k plan with matching contribution,Generous paid time off plan plus paid holidays,Frequent company sponsored lunches, happy hours, fun events, and plenty of snacks and drinks,Supplyframe is an equal opportunity employer", "Data Science: 2 years (Preferred),Master's (Required),United States (Required)", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Bachelor\u2019s degree or 2 years of equivalent work-related experience,Strong in working with stakeholders to both gather requirements and present results,Strong in data mining and data visualization,Ability to handle multiple competing priorities in a fast-paced environment,Experience with various math/statistics methodologies and algorithms,Strong in at least 2 code bases with 1 being Python,Healthcare and Population Health knowledge is a preferred,Experience deploying and maintaining machine learning models in a production environment is a plus", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Must have basic knowledge of statistical concepts such as regression, time series, mixed model, Bayesian methods, clustering, etc., to analyze data and provide insights.", "Experience working with structured and unstructured clinical data,Solid understanding of machine learning algorithms and use cases,Experience building and deploying supervised and unsupervised learning models, including clustering for anomaly detection,Experience with bioinformatics and/or NLP is a plus,Experience working with Amazon AWS and/or Microsoft Azure services,Proficient in Jupyter, Python, Spark and SQL,Able to explain technical concepts and results to a non-technical audience,BSc/BA in Computer Science, Engineering or relevant field; graduate degree in Data Science or other quantitative field is preferred,Experience with bio-statistics and research methodologies would be an additional asset", "Leveraging your educational background in Science, Mathematics, Statistics, Computer Science, Data Science, or a related discipline, along with your relevant professional work experience, you will lead, execute, and coordinate innovative data analytics initiatives and establish sustainable solutions.,Your experience with statistical toolkits and programming languages (e.g., R, Python) will be a valued asset as you drive the diffusion of AI within BASF and work creatively on new applications for AI.,Your familiarity with different analytical areas of expertise in AI (e.g., machine/reinforcement learning, statistics, analytics) will enable you to explore technical possibilities and create ideas together with internal non-AI-expert business partners.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Programming languages such as Python, R, SAS, Java, JavaScript, PHP, D3.JS,Relational databases (SQL) and no-SQL databases,Machine learning models including probability and statistical models in addition to time series analysis.,Modern ML techniques such as support vector machines, categorical and regression trees, neural networks and recommendation systems.,Microsoft office suite of tools and Windows OS,Linux OS and command line tools such as grep (regex),Big data technologies including HDFS (Hadoop), Hive, HBase, and Spark,Modern versioning systems such as git or subversion", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "1-7 years experience working in the Intelligence Community on teams working in any of the fields related to Data Science such as: Statistical Modeling, Information Retrieval, Text Analysis, Data Mining, Machine Learning, Intelligence Analysis, Cyber Threat Analysis, Image Analysis, Network Security, Statistical Modeling, Geo-spatial analytics, and Data Munging and Cleaning,Bachelor's Degree,Ability to work with datasets of different sizes and formats across multiple databases.,Knowledge and experience with specific techniques such as: neural networks, cluster analysis, feature engineering, extraction, and reduction, web-scraping, decision trees (CART), collaborative filtering, geo-spatial analysis.,Experience with: PCAP data, Elastic Search, Hadoop, HDFS, git, Spark, MLLib, SQL OS Experience/ Windows, Linux/ Windows,Ability to compile results and deliver presentations to senior-level leadership. Strong communication skills and ability to present material to audiences of differing technical aptitude.,R, R-shiny, R-studio, Python, Sci-kit, Tensorflow, Intelligence Community Experience, Machine Learning, Statistical modeling,,Experience with multi-TB dataset manipulation, cleaning, querying, and modeling.,Experience with scikit-learn, tensorflow, R-caret.,Experience curating datasets for supervised and unsupervised machine learning methods.,Experience with C, Java, R, Javascript, PhP, MatLab, Pig, Hive, Impala, PySpark, Scala, Ruby, Pytorch", "Degree in Statistics, Information Systems, Mathematics or Finance preferred,A minimum of 3-5 years of experience in applied data science,Experience doing hands-on, practical casework at an agency, or on the corporate side,Strong knowledge and experience with a wide variety of tools including SQL, SAS, R, Alteryx and Tableau,Proficient Excel, PowerPoint presentation and communication skills, both written and oral,Hands-on experience manipulating and deriving insight from very large datasets using advanced analytic techniques including time-series, regression, cluster analysis, decision trees, etc.,Comfort with efficient data acquisition and warehousing practices for both structured and unstructured datasets", "Data Visualization skills are a plus.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Fun, puzzle-loving office in the SF Financial District", "Experience in processing and performing multi-faceted analysis with customer/consumer behavior data,Experience in developing or enhancing models applying machine learning and artificial intelligence algorithms, statistical analysis, and natural language processing,Strong coding skills in SQL; and Python, R, Julia, or Scala.,Experience in working with distributed computing i.e. Hive, Apache Spark,Experience in working with Unix/Linux environment,Knowledge of descriptive analytics and data visualization,Exceptional standards for quality and strong attention to detail,Experience with full lifecycle agile application development in supporting analytic requirement,Modern machine learning models expert level: 4 years (Required),Statistical Algorithms: 7 years (Required),expert usage of R and/or Python: 5 years (Required),Unix: 2 years (Required),Master's (Preferred),Dallas, TX (Preferred),Authorized to work in US w/o sponsorship now or in future (Required),United States (Required)", "Has familiarity with marketing data such as email engagement metrics,Has experience developing, testing, and implementing customer segments,Is fluent in at least one statistical computer language: Python, R, etc.,Has utilized statistical analysis or machine learning to gain insight from very large data sets,Has experience querying databases with SQL,Has created, used, and leveraged both supervised and unsupervised machine learning algorithms and their components: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.,Has visualized/presented data for key stakeholders in an accessible format,Values: Aligned to Simple Energy's mission, actively understands the need for and supports diversity, openness to different points of view, self-aware, and has a positive, proactive attitude.,Outcome orientation: Driven to help deliver critical company outcomes through quantitative means.,Thought Process: An appetite for problem solving, curiosity, knack for structured thinking and process creation, ability to spot unusual patterns and an emphasis on product development.,Growth Mindset: Comfortable with learning and growing from mistakes. Drive to continuously learn and master new technologies and techniques.,Great communicator: Communicates proactively across the team and external stakeholders. Excellent written and verbal communication skills with the ability to communicate results and explain your solutions to technical and non-technical stakeholders", "2 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key insights from data to solve business problems,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Experience in working with large data sets and distributed computing tools (Hive, Redshift) is a plus", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "SQL experience", "You've had 2+ years doing quantitative data analysis.,2+ years SQL or Hive experience,2+ years of experience in Python, R or other scripting languages.,Experience in designing analytic solutions to open-ended problems.,PREFERRED QUALIFICATIONS:,Experience designing data quality metrics and implementing ETL validations.,Experience with Druid or other columnar data stores.,Experience with BI tools such as Tableau or Looker.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Master\u2019s degree in a quantitative discipline (e.g. Statistics, Operations Research, Economics, Computer Science, Mathematics, Physics, Electrical Engineering, Industrial Engineering), or equivalent practical experience.,15 years of experience working in role focused on statistical data analysis such as; linear models, multivariate analysis, stochastic models, sampling methods and Machine Learning.,PhD in Data Science, Statistics, or a similar technical/quantitative field.,Ability to initiate and drive multiple successful improvement initiatives inside and across the organization.,Ability to communicate clearly and persuasively, creating commitment and a drive for success in his/her teams and peers.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines", "Passionate about empirical research, asking and answering questions in large datasets, finding patterns and insights within structured and unstructured data,Strong communication and interpersonal skills and ability to work in team environment,Quick learner who adapts well to a fast-moving environment and gets things done,Combines creativity, problem-solving skills, and a can-do attitude to overcome any obstacle,Understanding of statistical, predictive and descriptive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms,Familiar with data visualization using tools like Python and R. Experience with a web-based visualization tool such as (Shiny, D3 or Plotly),Knowledge of machine learning algorithms, both using and creating,Knowledge of geospatial,Experience designing and evaluating the results of complex controlled experiments,Experience computer programming using technologies and languages like C++, Java, R, Python, Scala or related,Knowledge of relational database and multi-dimensional concepts with ability to perform complex queries in a SQL Server environment", "Bachelor of Science or Arts degree or higher in Computer Science or Masters of Science or PhD candidate in a discipline requiring strong mathematics and statistical methods,1+ years developing software in Java, python, C++ and other high level languages.,Relevant experience as described above with a solid background in machine learning, statistical analysis, and software development.,Solid background in machine learning, statistical analysis, and clustering algorithms,Strong software development skills,Knowledge and experience with predictive modeling including: Multivariate Regression, Logistic Regression, Combinatorial Optimization, Stochastic Processes, Complex Analysis, Principal Component Analysis and Time Series Analysis.,Experience with Matlab, R , or Weka.,Experience with using the Agile software development methodology to develop and deliver software so as to support a continuous integration/continuous deployment process.,Working knowledge of Linux,Strong communication and presentation skills \u2013 you must be able to explain and present hypotheses and analysis results to a wide audience in a clear and concise manner.", "Data science is a dynamic and evolving profession \u2013 we\u2019re looking for people who love to learn and find unique solutions without being micro-managed: you\u2019ll have the freedom to try new things, test solutions and technologies, and tell us when there\u2019s a better path.,You know your way around a cloud console and have handled some pretty large volumes of data (time-series a plus!).,You are familiar with a few machine learning frameworks, like TensorFlow or PyTorch.,You've got experience with deep learning.,Maybe you have used enterprise products in the past for high throughput data ingestion and analysis.", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Bachelor's degree.,Computational social science.,Computer science.,Data analytics.,Economics.,Engineering.,Geospatial analysis.,Mathematics.,Operations research.,Quantitative finance.,Statistics.,GPA of at least 3.0 on a 4.0 scale.,Experience with real world data through thesis research, internships, or work experience.,Creativity.,Initiative.,Integrity.,Leadership abilities.,Problem solving skills.,Advanced degree in a data science equivalent field or sub-field.,Experience working with data rich problems through research or programs.,Experience with computer programming or user experience/user interface.,Ability to successfully complete projects with large or incomplete data and provide solutions.,Bachelor's degree.,Computational social science.,Computer science.,Data analytics.,Economics.,Engineering.,Geospatial analysis.,Mathematics.,Operations research.,Quantitative finance.,Statistics.,GPA of at least 3.0 on a 4.0 scale.,Resume.,Cover letter in which you specify your qualifications for this position. Please address why you want to work in this role and what differentiates you from other applicants.,Unofficial transcripts for all degrees.,A writing sample, five (5) pages MAXIMUM, single spaced, technical or analytic paper that focuses on your current area of expertise or interest and is related to your interest in positions at CIA. You can excerpt longer papers.", "Bachelor of Science and 2 years' data science experience OR Master of Science and 1 years' data science experience.,4 years experience with SQL and relational databases (for example, DB2, Oracle, SQL Server).,4 years experience with statistical programming languages (for example, SAS, R).,Bachelor's degree in Statistics, Economics, Analytics, Mathematics and 7 years experience in an analytics related field.,Certificate in business analytics, data mining, or statistical analysis.,Doctoral degree in Statistics, Economics, Analytics, or Mathematics and 1 year experience in an analytics related field.,Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans,Develops analytical models to drive analytics insights,Leads small and participates in large data analytics project teams,Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity,Participates in the continuous improvement of data science and analytics,Presents data insights and recommendations to key stakeholders", "Commuter Benefits through a Flexible Spendi", "Bachelor\u2019s or Master\u2019s degree in math, statistics, economics or related analytical field,Solid SQL skills, with ability to build and manage databases and \u201cdata labs\u201d,Knowledge of at least one scripting language \u2013 R and Python preferred \u2013 with ability/interest to learn Python,Demonstrable knowledge of hypothesis testing, distributions, and Bayesian methods,Demonstrable knowledge of healthcare data (e.g. claims, electronic health records),Experience relating data science approaches to lay businesspeople,Expert in a data visualization package (e.g. matplotlib, seaborn, ggplot, Tableau),1 - 2 years experience working on a data science team focused on healthcare problems,Proficient using Microsoft Word, Excel, and PowerPoint,Strong interpersonal skills with a demonstrated ability to influence / motivate teams,Highly detail oriented, with the ability to coordinate initiatives with little supervision,Strong oral, written and presentation skills at all levels of an organization,Ability to apply independent thought and judgment, organize work priorities and meet specific objectives under tight project deadlines,Ability to organize and manage up to 2 or 3 concurrent projects", "M.S. or PhD in Computer Science, Statistics, Applied Math, or a quantitative field with a strong background in machine learning or data mining,Strong knowledge and experiences in machine learning and statistical modeling (e.g. neural networks, decision trees, clustering and regression analysis) is required,Expertise in data warehouse and SQL programming is required,Expertise in one of the statistical programing languages (Python, R, SAS/Base, SAS/Stats, SAS Enterprise Miner) is required,Experience in payment fraud detection and prevention is a plus,Familiarity with open source Python based ML libraries is a plus", "A graduate degree in machine learning, computer science, artificial intelligence, applied mathematics, statistics, physics, or a related technical field.,Python (numpy, pandas, sklearn, xgboost, TensorFlow, etc.), Java", "Expert programming skills in Java, Python or Scala with 5+ years of relevant work experience,Experience in building/using large scale knowledge graphs (including Linked Data) and ontologies (RDF(S)/OWL, SPARQL),Strong command over linear algebra and statistics having the ability to quickly translate ideas to efficient, elegant code,Development experience in Python or Java/Scala with good command over respective data pipelining, matrix algebra and statistics libraries,Experience in NLP methods such as LSA, LDA, Semantic Hashing, Word2Vec, LSTM, BiDAF etc.,Experience in information retrieval tools such as Elastic Search, Lucene, Solr and graph databases such as Neo4J, OrientDB, or a triple store,Tuning and optimization of sequential deep learning models,MS in Computer Science with emphasis on Data Science, Analytics and/or Machine Learning. PhD preferred.", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Strong communication and data presentation skills; ability to communicate with data-driven stories,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "6+ years\u2019 practical experience with SAS, ETL, data processing, database programming and data analytics,Experience in predictive modeling (Python, SAS, Alteryx, Angos, R),Experience with programming languages such as Java/Python an asset,Extensive background in data mining and statistical analysis,Experience with analytics reporting tools (Tableau, MS Power BI, Tibco, Sisense, Qlik or SAP BO),Experience in mining Claims and EMR data, preferably Oncology related data.,Able to understand various data structures and common methods in data transformation", "Education: Bachelor\u2019s degree in Engineering / Finance / Mathematics / Stats / Econometrics or any other quantitative field,Intermediate-Advanced English communications skills,3+ years of experience in data analytics,Understanding and applying the structure/schema of the source operational data in the PLAN, TRACK, VIEW & SHARE databases (PostgreSQL & MySQL) is a *MUST*,Pentaho data integration experience is a *MUST*,Create and maintain the ETL (or ELT) processes,Create and maintain the data warehouse or data cubes,Domo knowledge or experience is a PLUS", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Ph.D. or MS degree in Computer Science, Applied Mathematics, or related field", "2+ years of experience in Data Science and analytics fields,Experience with programming languages such as R, Python, or Scala,Experience in processing and analyzing large scale data volumes, semi-structured and unstructured data and near real-time throughput,Experience applying the right ML model to solve business problems,Ability and passion to learn new techniques to stay on the cutting edge.,Experience working in the AWS and Hadoop ecosystems,Strong database knowledge and expertise in SQL,Good storytelling and presentation skills i.e be able to present business side of the story that data presents to c-team and business leaders,Competitive health and insurance benefits.,Competitive salary.,Annual target bonus or commission.,Parental leave for up to 20 weeks (dependent on eligibility).,Paid vacation and sick time.,Employee Stock Purchase Program.,Free snacks and beverages.,Frequent company update talks with our leadership team.,Free listing on HomeAway.com.,Electronic, adjustable stand-up desk.,Discounted Metro & Rail pass.", "Ability to explain complex analytical concepts to people from other fields", "PhD degree in Statistics/Biostatistics, Mathematics, Physics, Operations Research, Econometrics or related field,Exceptional interpersonal and communication skills,Ability to work independently and drive your own projects,3+ years relevant experience with a proven track record of leveraging massive amounts of data to drive product innovation,Strong statistical knowledge and intuition - ability to tease out incrementality vs. correlations; understanding of predictive modeling; time series; probabilistic graphical models;,Strong skills in SQL, Python and/or R,Experience with distributed analytic processing technologies (Hive, Pig, Presto, Spark),Data visualization skills to convey information and results clearly,Deep product sense", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "You have a bachelor\u2019s degree in computer science, information systems, mathematics, statistics, or a related quantitative discipline. Applicants with some data-related expertise and a professional background in media are also encouraged to apply.,1-2 years professional experience at a media company is preferred. An enthusiasm for The Wall Street Journal and an understanding of the product is a must.,You have an entrepreneurial attitude toward work and sweat the details, as well as a healthy skepticism of the status quo.,You have experience using analytics tools (SQL, Tableau, Excel) and an interest in more advanced topics in analytics (artificial intelligence, data engineering).,You have worked with visualization and machine learning libraries in either R or Python.,Experience in building web applications and agile development is a plus.", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Bring your 7+ Years industry experience working with Data Science to our team,You have hands-on experience with NLP, mining of structured, semi-structured and unstructured data,You have a rich history of crafting new solutions to evolving problems,You bring an affinity for Apple Media Products and digital content, in general.,You enjoy paying attention to details almost as much as the big technical wins,You are passionate about working with large scale data sets,You bring experience with Solr/ Lucene, Cassandra and related technologies,You have experience with machine learning tools and libraries such as Spark,You have an intuitive understanding of machine learning algorithms, supervised and unsupervised modeling techniques,You are highly technical, detail-oriented, creative, motivated, and focused on achieving results.,You are excited about reaching millions of users across our many platforms,You share our obsession with quality,We value your strong interpersonal skills as well as experience driving decisions across diverse organizations,You love collaborating under tight deadlines and tackles problems with imaginative and elegant solutions.,Your creative problem solving skills will be utilized daily", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "We expect you would be in a technical Masters or PhD program.", "Education,Computer science.,Statistics.,Physics.,Mathematics.,Economics.,Specialization,Certification", "Strong programming skills with the ability to explore large data sets,Excellent written and verbal communication skills to report research results/methodologies,Publications in top tier journals focusing on any of the above topics a plus,Preferred candidates will be pursuing a masters in Computer Science or have at least 2-3 years of working experience within a quantitative trading/research role,Experience from top tier quantitative investment firms - such as AQR Capital, QMS Capital, Two Sigma, D.E. Shaw - preferred,Development experience,Experience working for a startup,Fast paced development environment,Master's", "Bachelor's degree in math, statistics, operations research, computer science engineering, econometrics, quantitative social science, or other quantitative field OR an equivalent combination of education and work related experience,5+ years of industry working experience performing advanced quantitative analyses,Professional experience working with Python/R and SQL,Professional experience with big data manipulation,Proven ability to apply advanced statistical methodologies such as multiple regression model, mixed models, time series models (Bayesian preferred), neural networks, cluster analysis, text mining, and have prior experience in optimization, simulation, marketing mix, multivariate testing, ensemble modeling, graph algorithms,Ability to apply advanced optimization methodologies such as linear and mixed integer optimization,Ability to apply advanced simulation modeling methodologies and techniques.,Must have relational database experience,A strong passion for empirical research and for answering hard questions with data,Curiosity, humility, and empathy,Masters\u2019 degree or Ph.D.,Digital experience,Professional experience with software development practices", "Bachelor\u2019s degree in computer science, computer engineering, or related field, or the equivalent combination of education and related experience.,Comprehensive knowledge of one or more of the following: high performance computing, scientific data analysis, statistical analysis, knowledge discovery, computer security, systems programming, large-scale data management, and big data technologies.,Skilled in all aspects of the software project life cycle: feasibility, requirements, design, implementation, integration, test and deployment.,Experience developing software with C++, C, Java, Python, R, or Matlab, software applications in Linux, UNIX, Windows environments, data analysis algorithms, data management approaches, relational databases, or machine learning algorithms.,Ability to effectively handle concurrent technical tasks with conflicting priorities, to approach difficult problems with enthusiasm and creativity and to change focus when necessary, and to work independently and implement research concepts in a multi-disciplinary team environment, where commitments and deadlines are important to project success.,Effective interpersonal skills necessary to interact with all levels of personnel.,Effective advanced analytical, problem-solving, and decision-making skills to develop creative solutions to complex problems.,Significant experience with demonstrated expertise in the following technical languages, concepts, or constructs and in one or more of the following advanced areas: high performance computing, scientific data analysis, statistical analysis, knowledge discovery, computer security, systems programming, large-scale data management or big data technologies.,Included in 2020 Best Places to Work by Glassdoor!,Work for a premier innovative national Laboratory,Comprehensive Benefits Package,Flexible schedules (*depending on project needs)", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Nice to have: Experience working with Database/storage systems like: Postgres, Druid, Aerospike, Elasticsearch", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "PhD or Masters in Statistics, Mathematics, Computer Science, Engineering, or a related discipline,at Least one year research experience with applying quantitative research (e.g. artificial intelligence or operations research) in solving real world problems,At least one year experience with Python,Ability to communicate complex ideas in a clear precise and actionable manner,Ability to execute or propose and execute an analytics plan by collaborating with colleagues in as well as outside the team,Ability to solve complicated problems by decomposing the problem and gradually demonstrating progess,Demonstrated industrial experience of applying artificial intelligence, optimization and/or statistics to drive key decisions,Familiarity with operations research, CPLEX and solving integer problems,Experience with basic machine learning techniques, statistics and mixed models, as well as with R or other programing languages", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "3+ years of experience working with SQL queries and accessing data,2+ years of consulting experience, preferably at a \"Big 4\" or similar firm,Demonstrated expertise in business analysis and data analysis,Experience writing requirements and specifications for IT systems", "Master\u2019s degree in computer science, computer engineering, or a related field or the equivalent combination of education and related experience.,Experience developing software in Python, C++, or C.,Comprehensive experience implementing a deep learning workflow using one or more of the following frameworks: Theano, Tensorflow, Pytorch, or Keras.,Fundamental knowledge and/or experience in applying algorithms in one or more of the following Machine Learning areas: anomaly detection, one/few-shot learning, deep learning, unsupervised feature learning, ensemble methods, probabilistic graphical models, reinforcement learning.,Broad knowledge of network protocols such as DNS, or HTTPS.,Knowledge and/or experience of computer vulnerabilities, such as, buffer overflows, code injection, format string, etc.,Ability to effectively manage concurrent technical tasks with contending priorities, as well as approaching difficult problems with enthusiasm and creativity to change focus when necessary.,Lead multidisciplinary teams in the areas of machine learning and deep learning algorithms.,Pursue program development opportunities by co-authoring proposals and proposing ideas that will address sponsor needs. Identify program growth opportunities for existing customers, understanding the customer space and needs.,Ph.D. degree in computer science, computer engineering, or a related field.,Experience with high performance computing, parallel programing, and/or cloud computing. Experience with Modbus, DNP3, IEC 104, or IEC 61850 protocols. Familiarity with full-stack software development.,Included in 2020 Best Places to Work by Glassdoor!,Work for a premier innovative national Laboratory,Comprehensive Benefits Package,Flexible schedules (*depending on project needs)", "Preferred\nBachelors of Science in Computer Science, Math or Scientific Computing;\nData Analytics, Machine Learning or Business Analyst nanodegree; or\nequivalent experience.,Requires 5-8 years experience or PhD or Masters in an approved field with no minimum years of relevant experience.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Post Grad Degree or equivalent in Statistics, Mathematics, Data Science, Analytics or related area;,At least 5 years of solid experience in the marketing analytics arena;,Advanced knowledge of R;,Experience with SAS/SPSS;,Experience writing SQL queries;,Exposure to Big Data technologies as Hadoop, and Hive.,Advanced knowledge of Excel .,Experience with handling, integrating and modelling digital behavioural data. Including segmentation and predictive analytics;,Retrieving information from webpages: product characteristics, reviews etc.;,Ability to transform the scraped data into a usable format;,Integrating scraped data with other data sources;", "Multiple years proven track record in the application of ML and NLP As a global business we rely on diversity of culture and thought to deliver on our goals.", "Bachelor\u2019s Degree in Economics, Math/Science Finance, engineering or similar discipline with least 1 year of experience.,Comfortable in developing statistical models using Python, SAS or R statistical packages.,Master\u2019s Degree in Economics, Math/Statistics, Finance, Engineering or similar discipline,1+ year of experience in business application of machine learning techniques and statistical analysis.,Experience in Python development is highly desired.,Strong data analysis and communication skills", "5+ years experience as a multi-faceted software engineer,Strong back-end orientation and experience architecting schemas and building databases,Experience with scripting languages and building and maintaining data pipelines,Possess good organizational, communication, analytical and technical writing skills,Experience working with scientists on R&D systems would be beneficial,Background in machine learning / data science / statistics,Exceptional multitasking skills, attention to detail and ability to work independently Excellent communication and presentation skills. \u200b Self-motivated, passionate and comfortable working in a fast-paced environment. \u200b", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "BS, MS, or PhD in healthcare related field,Minimum 3 years of experience solving data science problems in a healthcare domain,Masters or PhD,Application of data science solutions in industrial projects", "A love of games!", "A Bachelor\u2019s degree in business, economics, statistics or related field and a minimum of 4 years general management experience in the business or marketing analytics field or an equivalent combination of experience and training that provides the required knowledge, skills and abilities.,Master\u2019s Degree Preferred.,Strong analytical skills. Curiosity.,Attention to detail and data accuracy.,Proven experience working with large datasets and relational database analytical tools, such as Alteryx, Tableau, SAS, JMP or similar.,Highly skilled in Excel; proficient with PowerPoint or Keynote.,Advanced statistical skills a plus (multi-variate analysis, cluster analysis, etc.),CRM, retail, consumer and/or loyalty analytics experience highly desired.,Excellent project coordination and management skills.,Comfortable with reaching out to business partners and adept at cross-functional collaboration.,Must be able to develop analytic plans and manage multiple projects simultaneously.,Adept in translating data into concise, insightful and business friendly executive summaries and presentations.,Strong written and verbal skills.,Travel by air and overnight, as required 10% amount of time.,Lifting and/or bending 5 pounds amount of weight.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Master\u2019s Degree in Computer Science, Engineering, Statistics or other quantitative fields,Theoretical knowledge of Machine Learning, Algorithms, Statistics, etc.,Basic coding ability in Python or R,Programming skills with Java or Scala is a plus,Full cycle machine learning engine implementation via internship or school project", "Bachelor degree,Minimum of 1 year of experience in predictive/statistical modeling using SAS or R or Python,Proficient in MS Office applications,Excel proficiency (Pivots, V-Lookups, Formulas),Bilingual (Spanish / English),Master\u2019s Degree,Experience performing data analysis,Experience extracting data using SQL and/or data exploration tools (such as SAS or R),Experience with data analytics design,Experience working with large databases,Health care industry experience,Demonstrated ability to effectively gather requirements, probe for deeper understanding, and translate deep technical concepts to non-technical as well as technical senior stakeholders, marketing customers, and data scientists,Demonstrated ability to manage people and prioritize deliverables,Proven organizational skills with ability to be flexible and work with ambiguity", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "You find yourself excited to tell people about what we're building here at Fetch. You wake up truly excited because you know what you do today will directly impact a young growing company.", "External Technology Knowledge: Keeps abreast of the latest technological developments in areas that can push our technology roadmaps to realization. Constantly looks for opportunities to incorporate new solution methods to solve existing problems", "Bonus points: Building and analyzing web and mobile applications, and business intelligence tools", "Bachelor\u2019s degree or higher in the studies of Data Analytics, Statistics, Mathematics, Computer Science or other related field,Completion of the MHS Lean Six Sigma Green Belt curriculum is required within 2 years of job placement.,1-3 years\u2019 experience in data analysis, operations improvement work, computer science/business intelligence work, or other related experience.,Must have strong problem solving skills with a keen drive to learn about and explore datasets.,Understanding of application and interpretation of statistical tools such as Hypothesis Testing, Non-Parametric Tests, Analysis of Variation, Various forms of Regression and factor analysis, as well as various forecasting and prediction techniques.,Basic understanding of Structured Query Language (SQL) code to process and ETL techniques for extracting data from systems and transforming that data into the forms necessary for analysis.,Experience with statistical computer packages (Minitab, SPSS, SAS) to manipulate data and draw insights from large data sets.,Basic understanding of relational database (RDB) systems and how to explore various data architectures using standard tools (Ex: Microsoft SQL Management Studio, Oracle SQL Developer, etc.).,Basic understanding of data mining concepts.", "And More! (Like an on-site cafeteria, free parking, access to Crown Center and a fitness center.)", "Bachelor or Master's degree in highly quantitative field (CS, machine learning, mathematics, statistics) or equivalent experience.,MS + 2 years or BS + 5 years of experience with machine learning, statistical modeling, data mining, and analytics techniques.,Experience with Python, statistical/machine learning software.,Experience applying various machine learning techniques, and understanding the key parameters that affect their performance.,Experience developing experimental and analytic plans for data modeling processes, use of strong baselines, and the ability to accurately determine cause and effect relationships.,Experience in one or more natural language processing topics: tagging, syntactic parsing, word sense disambiguation, topic modeling; contextual text mining, and application of deep learning to NLP.,Previous experience in a ML or data scientist role with a large technology company.,Fluency in a language other than English", "Deep expertise in search, preferably in e-commerce. Excellent knowledge on improving search incrementally as well as making step-changes such as applying NLP, text mining, etc.,Proven track record building robust search systems and achieving strong results,Expertise in personalization and recommender systems,Able to guide a team of engineers to identify and break down necessary architecture and services to support search and personalization services,Able to guide and work well with a team of engineers to implement Machine Learning algorithms and models, as well as implementing necessary software,Solid understanding of search metrics and implementing tracking to measure performance,Solid understanding of search engines and utilizing features effectively. Familiarity with ElasticSearch is a plus,Hands-on experience developing and implementing Machine Learning algorithms and models.Background in Machine Learning,Statistics & Information Retrieval,Design, implement, test robust technical solutions that our high traffic site/apps can rely on,Write clean code that\u2019s testable, maintainable, solves the right problem and does it well. Code you can be proud of.,Phd, Masters, or equivalent experience in a quantitative field (computer science, physics, mathematics, bioinformatics, etc.) a plus, but by no means a must,Experience programming in functional languages (Scala,Golang,Haskell, Clojure etc.) a plus but not a must,Knowledge of scripting languages like Python/R, familiarity with web frameworks a plus,Experience with Java and/or Scala and microservices is a plus,Understanding of A/B testing,Able to be a key influencer in the team\u2019s strategy and contribute significantly to team planning, showing good judgement making technical trade-offs between the team\u2019s short term and long term business needs, and the needs of the company as a whole,Strong team player: Superb communication skills, thrives in a collaborative environment and be committed to the success of the team as a whole.,Critical thinking: ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems,Creativity: you can conceive of new data driven products, features, and technologies,Results: you prioritize, focusing on ideas and features that will have significant, measurable impact,Planning & estimation: ability to set and meet your own project objectives & milestones,Communicate results and progress internally and externally in meetings, presentations, and tech talks,Passion for technology. Our developers are always evaluating new tools and technologies that can make us better. What has attracted your interest lately?", "5+ years of experience as a data scientist,Tools: R, Python,Analytical Methods: Regression modeling, forecasting, machine learning algorithms including ensemble models and neural nets, Bayesian models, model selection and validation,The ability to take digital marketing data and translate that into valuable insights for our clients,Education: Master's Degree in Statistics, Quantitative Analysis, Business Analytics, Biostatistics, or related discipline,You are in love with data,You enjoy taking on new challenges and are not afraid of chasing down an answer.,You are a lifelong learner. You enjoy reading about the new trends in data science, learning the new tool that is trending, and connecting with other data junkies.,You have experience working with clients and enjoy helping their business grow through answering complicated business needs through the implementation of a data science project,You have impeccable attention to detail", "New York, NY", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Master\u2019s degree in a quantitative or technical field (Math, Statistics, Engineering, Economics, Physics, Computer Science, etc.),Experience with data exploration and machine learning tools such as those found in R or Python,Demonstrated ability applying advanced statistical modeling techniques to solve problems,Ability to craft rigorous research / evaluation design approaches, based upon understanding of clients' research needs,Advanced knowledge and experience with querying languages (SQL, etc.),Understanding of modern software development and engineering practices including scrum/agile, Git, and DevOps,Competitive Compensation,Full Health Benefits - Medical/Dental/Vision,401k, Paid Time Off and Tuition Reimbursement,Full Service Gym, Game and Lounge Area, Basketball Court,Free Healthy Snacks and Refreshments,Subsidized Public Transit", "BS or Master's degree in Computer Science, Electrical Engineering, other related degree, or equivalent experience.,3+ years experience with software engineering and infrastructure design skills.,Write well-structured, maintainable, idiomatic code with good documentation.,Strong work-ethic and passion for problem solving in the real world.,Experience with Python. We make generous use of tools like Celery, MongoDB, Pandas, Scikit Learn, and Django.,Experience with data science fundamentals, machine learning and natural language processing.,Development and deployment of software within Linux environments.", "5+ years work experience, 2+ years of work experience applying scientific methods to solve-real-world problems,Degree computer science, applied statistics, economics, etc,Ability to write structured and efficient SQL queries on large data sets,A proven track record of using analysis to impact key business or product decisions,Familiar with data pipelines and knowledge of how to transform raw production and external data into user-friendly tables,Fluent with data science libraries such as Python or R,Comfortable with Amazon Web Services, Apache Spark, Tensorflow, etc,Ability to organize and clearly communicate insights to stakeholders", "Huge technical problems to solve \u2013 you will constantly be learning and pushing boundaries, working with some of the smartest people around!", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Masters degree in Data Science, Statistics, Operations Research, or in highly quantitative field (e.g. Computer Science, Operations Research, Systems Engineering, Physics), or equivalent experience,2+ years of industry experience in predictive modeling, data science and analysis,Programming experience in Python, R or equivalent,Demonstrated experience in data science and data analysis,Desire to pursue challenging questions through extensive operational and data analysis (experience to do data analysis, regression analysis),Demonstrated outstanding written and verbal communication skills,Comfortable in a Linux environment,Experience with Amazon Web Services (AWS), e.g. DynamoDB, AuroraDB (MySQL), S3, SQS, SNS, EC2,Interest in and experience with experimental design,Background in applied statistics or machine learning", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Three to five years of experience in positions of increasing responsibility, working with large datasets and conducting statistical and quantitative modeling, melding analytics with strong programming, data mining, clustering and segmentation,Bachelor\u2019s degree and a combination of 6 years of education and work experience in predictive modelling, machine learning, and or a related quantitative field (Computer Science, Physics, Mathematics, Statistics, Bioinformatics, etc.),Strong data and statistical programming skills. e.g. SQL, Python, R, Stata, SAS, SQL, Hadoop, Hive, and/or other large data systems,Data scientist with in-depth knowledge and experience with machine learning and predictive analytics, proficiency in R or Python programming languages, or other tools used for data manipulation and visualization, and experience working with data warehouses. In-depth knowledge of all relevant clinical informatics software and systems, and of the highly-complex concepts, principles, policies, methodologies, techniques, best practices, regulations, and standards practices involved with patient care and electronic medical data management in the UC health care system,Broadly encompassing and highly in-depth knowledge of machine learning, and deployment of predictive analytics in operational environments. Knowledge of controlled terminology, clinical workflows, user interface optimization, clinical decision support, rules development, data integration and mining, clinical ontologies and adoption of technology. (clinical domain knowledge is preferred but not required, and may be developed or expanded after hiring),Advanced organizational and project management skills, with the ability to lead a team, prioritize tasks, and see projects through from inception to completion on schedule,Advanced interpersonal communications skills, to convey highly technical information and instructions to all levels of clinical users in a clear and concise manner, to provide technical support, and to develop and deliver training materials as needed,Ability to apply advanced problem-resolution skills to highly complex issues, quickly diagnose problems, and develop, test, and implement appropriate and effective solutions in a timely manner,Advanced analytical skills and expertise in documentation and reporting, with the ability to apply metrics, design and run queries, collect and analyze performance data, and produce sophisticated reports and analyses for management use,Advanced ability to serve as a technical leader and information resource, and to work collaboratively with senior staff and management across departments, providing advice, counsel, and analysis on issues of policy, functionality, system efficiency, upgrades, business analytics, and industry advances and trends,Strong interest in working with health care data and understanding the challenges that face complex health care delivery systems,The flexibility to orient and work at all UCSF Medical Center locations,Completion of one to two years of undergraduate or graduate level coursework in Statistics,Master\u2019s degree or Doctorate in computer science or related area,Background in STATA and/or SAS,Prior experience with healthcare data, in particular Epic-derived healthcare data,Demonstrates service excellence by following the Everyday PRIDE Guide with the UCSF Medical Center standards and expectations for communication and behavior. These standards and expectations convey specific behavior associated with the Medical Center\u2019s values: Professionalism, Respect, Integrity, Diversity and Excellence, and provide guidance on how we communicate with patients, visitors, faculty, staff, and students, virtually everyone, every day and with every encounter. These standards include, but are not limited to: personal appearance, acknowledging and greeting all patients and families, introductions using AIDET, managing up, service recovery, managing delays and expectations, phone standards, electronic communication, team work, cultural sensitivity and competency.,Uses effective communication skills with patients and staff; demonstrates proper telephone techniques and etiquette; acts as an escort to any patient or family member needing directions; shows sensitivity to differences of culture; demonstrates a positive and supportive manner in which patients / families/ colleagues perceive interactions as positive and supportive. Exhibits team work skills to positively acknowledge and recognize other colleagues, and uses personal experiences to model and teach Living PRIDE standards.,Exhibits tact and professionalism in difficult situations according to PRIDE Values and Practices,Demonstrates an understanding of and adheres to privacy, confidentiality, and security policies and procedures related to Protected Health Information (PHI) or other sensitive and personal information.,Demonstrates an understanding of and adheres to safety and infection control policies and procedures.,Assumes accountability for improving quality metrics associated with department/unit and meeting organizational/departmental targets.,Keeps working areas neat, orderly and clutter-free, including the hallways. Adheres to cleaning processes and puts things back where they belong. Removes and reports broken equipment and furniture.,Picks up and disposes of any litter found throughout entire facility.,Posts flyers and posters in designated areas only; does not post on walls, doors or windows.,Knows where the Environment of Care Manual is kept in department; corrects or reports unsafe conditions to the appropriate departments.,Protects the physical environment and equipment from damage and theft.", "Previous message: [Jobs] Fwd: OMB seeking applications for Chief Statistician,Next message: [Jobs] Tenure-track position at University of Hawaii,Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]", "6+ years\u2019 experience of applying machine learning techniques, optimization, and statistics to drive key decisions,Extensive hands-on experience with development of predictive models and machine learning/AI-based solutions,Solid programming skills in Python, R or similar data science language,Advanced proficiency in data visualization,Prefer financial services industry experience,Prefer experience in CRM, financial analysis, financial advisory,Bachelors or Master\u2019s degree in Computer Science, Math, Data Science, or Statistics,Be a United States citizen", "FORTUNE \u201cWorld\u2019s Most Admired Companies\u201d \u2013 2016,Corporate Responsibility Magazine \u201c100 Best Corporate Citizens\u201d \u2013 2016,InformationWeek \u201cElite 100\u201d \u2013 2016,Women\u2019s Business Enterprise National Council \u201cAmerica\u2019s Top Corporations for Women\u2019s Business Enterprises\u201d - 2016,Reputation Institute \u201cWorld\u2019s Most Reputable Companies\u201d \u2013 2015", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Master's degree in Mathematics, Physics, Statistics, Computer Science, Engineering, Economics, Operations Research, Bioinformatics, Computational Biology, a similar quantitative field or equivalent practical experience.,Experience in statistical software and database languages (e.g., SQL, R, Python, MATLAB).,Experience using applied statistics to analyze data.,Experience training and deploying Machine Learning models.,PhD degree in a scientific field leveraging statistics.,Experience with Machine Learning libraries (e.g., TensorFlow, Scikit-learn, Keras, Theano, Torch).,Experience with the Google Cloud Platform.,Ability to draw conclusions from data and recommend actions.,Ability to break down technical concepts into simple terms to present to diverse, technical, and non-technical audiences.,Effective written and verbal communication skills.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Doctorate (Preferred)", "Master\u2019s Degree, or Bachelor\u2019s Degree with 3 years of experience, in engineering, computer science, statistics, bioinformatics, or other related field.,Strong background in statistical analysis, machine learning, and data mining, including regression analysis, classification, predictive modeling, feature engineering, hypothesis testing etc.,Solid understanding of probability theory and statistics.,Experience in manipulating large data sets of time-series and intermittent data.,Strong communication and organization skills.,Proven ability to contribute to emerging or cross-disciplinary fields.,Experience performing independent research.,Experience working with Python as a development language with an emphasis on data science.,Experience with Python data analysis packages such as Scikit Learn, Pandas, and SciPy.,Comfortable working in a Linux-based environment.,Experience working with database types such as MongoDB, MySQL, and Postgres.,Knowledge of Bayesian data analysis methods such as model comparison, Markov chain Monte Carlo sampling, Hierarchical modeling, Generalized linear models, and Bayesian Neural Networks.,Experience working with clinical data.", "MS or PhD degree in Computer Science, Artificial Intelligence, Machine Learning, or related technical field.,Prior experience with:,Numerical and topic modeling,Technologies:,Linux,Python", "1-7 years experience working in the Intelligence Community on teams working in any of the fields related to Data Science such as: Statistical Modeling, Information Retrieval, Text Analysis, Data Mining, Machine Learning, Intelligence Analysis, Cyber Threat Analysis, Image Analysis, Network Security, Statistical Modeling, Geo-spatial analytics, and Data Munging and Cleaning,Bachelor's Degree,Ability to work with datasets of different sizes and formats across multiple databases.,Knowledge and experience with specific techniques such as: neural networks, cluster analysis, feature engineering, extraction, and reduction, web-scraping, decision trees (CART), collaborative filtering, geo-spatial analysis.,Experience with: PCAP data, Elastic Search, Hadoop, HDFS, git, Spark, MLLib, SQL OS Experience/ Windows, Linux/ Windows,Ability to compile results and deliver presentations to senior-level leadership. Strong communication skills and ability to present material to audiences of differing technical aptitude.,R, R-shiny, R-studio, Python, Sci-kit, Tensorflow, Intelligence Community Experience, Machine Learning, Statistical modeling,,Experience with multi-TB dataset manipulation, cleaning, querying, and modeling.,Experience with scikit-learn, tensorflow, R-caret.,Experience curating datasets for supervised and unsupervised machine learning methods.,Experience with C, Java, R, Javascript, PhP, MatLab, Pig, Hive, Impala, PySpark, Scala, Ruby, Pytorch", "Must have Master\u2019s degree or a foreign equivalent in Statistical Science, Mathematics or a related quantitative field plus three (3) years of experience in the position offered or as a Vice President, Data Scientist; Senior Statistician or a related position.,Must have three (3) years of experience within the financial industry: extracting useful insights from large, messy data sets; developing and applying statistical methods for solving complex problems; performing quantitative research and programming in multiple languages including Python, Java, C/C++, SQL and R; writing well-structured robust code for both research and production; utilizing large scale distributed computing technology including Spark and Hadoop; analyzing financial datasets and utilizing natural language processing techniques.", "You\u2019ll have experience with data mining, machine learning, statistical modeling, and underlying methods and algorithms.,You\u2019ll have extensive experience in analyzing large data sets using software such as R, Python and Power BI.,You are proficient in working with SQL and databases. Experience with Microsoft Azure cloud computing platform a plus.,You have a proven track record of using data to provide actionable and impactful business results,Prior experience within the insurance, financial services or healthcare industry preferred,Effective communications skills that instill confidence with internal and external audiences and translate complex concepts to non-technical stakeholders to help enable understanding and drive informed business decisions.,A low-ego, team-oriented and collaborative approach in keeping with the corporate culture.,Solutions-oriented mind-set; able to work effectively through complex problems.,Easily establish yourself as a trusted partner and the ability to build effective partnerships across all areas of the organization and with colleagues at all levels.,The ability to influence a variety of stakeholders to drive cultural and organizational change,Intellectual curiosity, a passion for data and a results orientation.", "Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]", "At least 5 years of progressive experience in data science, statistical analysis and data modeling.,5 years of experience with statistical software.,Insurance industry experience preferred.,Professional experience building sophisticated models via regression, segmentation, decision tree, time series, design of experiments and other multivariate analysis.,Experience in machine learning techniques and algorithms, such as SVM, Random Forests, and Neural etc.,Experience with statistical packages (one or more) such as R, SAS, SPSS, Statistica, STATA, Alteryx, KNIME etc. are required,Python (scikit) and any other computer language experience a plus.,Experience with BI tools like Tableau, MSBI etc. is plus.", "Advanced Degree (including MBA) preferred", "Work with engineers to define and manage data sources,Design and implement new distributed machine learning methods,Implement data warehouses, real-time ETL, and batch processing data to support modeling needs,Build and maintain internal data processing and visualization tools,5+ years of data science work and/or academic experience,Deep experience with python,Experience shipping products and features early/often,Experience working with very large datasets, especially using Dask and Kinesis,Background in time series analysis, hidden Markov models, Gaussian processes, and variational inference.,Familiarity with node.js and both front-end/back-end JavaScript", "Project portfolio on GitHub or personal website.,Model deployment experience (data engineering, web development, or business aspects).,Entrepreneurial mindset. Translate between business, math, and tech. Apply analytics to make better, faster, more consistent business decisions, with measurable ROI.,Prescriptive analytics experience (active learning, causal inference, cost-sensitive classification,design of experiments, interactive machine learning, reinforcement learning).,Hadoop experience.,Typically requires:,Bachelor\u2019s degree and at least 2 years of experience in a quantitative discipline like Statistics, Math, Computer Science, Engineering, Operations Research OR,,Master\u2019s Degree and no experience OR,,Normal office environment.", "At least 8 years experience in Java, Spring and MySQL (or any relational database) and Python,At least 5 years of experience as a Data Scientist.,Experience with databases (including NoSQL),Experience in machine learning frameworks and libraries,Supervised and Unsupervised learning,Machine learning concepts and techniques: Regularization, Boosting, Random Forests, Decision Trees, Bayesian models, Neural networks, Support Vector Machines (SVM),Experience with the whole ETL data cycle (extract, validate, transform, clean, aggregate, audit, archive),Computer Science or Mathematics or Physics degree,Excellent communication and analytical skills,Willingness to work hard (50 hrs per week),Very good English,Experience with Apache Spark,Natural Language Processing (tokenization, tagging, sentiment analysis, entity recognition, summarization),R programming language,Modeling complex problems, discovering insights and identifying opportunities through the use of statistical, algorithmic, mining and visualization techniques,Participating in the areas of architecture, design, implementation, and testing,Proposing innovative ways to look at problems by using data mining approaches on the set of information available,Designing experiments, testing hypotheses, and building models,Conducting advanced data analysis and designing highly complex algorithm,Applying advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple real-time decision systems,Very competitive salary based on prior experience and qualifications,Potential for stock options after the first year,Raise and advancement opportunities based on periodic evaluations,Visa sponsorship (if working from outside the US, sponsorship can be granted after 18 months with the company, based on performance).,Health benefits (in case you will be working from our office in Washington DC),This position does not have a location requirement and can be performed either remotely (including from outside the U.S.) or from WalletHub\u2019s offices in downtown Washington DC.,If you're intending to work from outside the US please be aware this position entails working at least 50 hour per week and requires an overlap with EST business hours (8am - 7pm ET, including 1 hour break).", "Experience querying databases and using statistical computer languages: R, Python, SLQ, etc.,Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc.,Experience analyzing data from 3rd party providers: Google Analytics, Adobe Analytics,Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark,Experience visualizing/presenting data", "BS in Computer Science, Statistics, Applied Mathematics, Physics or Engineering. MS/PhD is preferred.,Excellent communication and collaborative skills,5+ years professional experience in applications of machine learning, data science and analysis,Deep expertise in at least one of the ML frameworks like Scikit Learn, Keras, TensorFlow etc,Proficient in at least one of programming language like Python, R, C#, Java", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "United States (Preferred)", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Strong communication and data presentation skills; ability to communicate with data-driven stories,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "1-7 years experience working in the Intelligence Community on teams working in any of the fields related to Data Science such as: Statistical Modeling, Information Retrieval, Text Analysis, Data Mining, Machine Learning, Intelligence Analysis, Cyber Threat Analysis, Image Analysis, Network Security, Statistical Modeling, Geo-spatial analytics, and Data Munging and Cleaning,Bachelor's Degree,Ability to work with datasets of different sizes and formats across multiple databases.,Knowledge and experience with specific techniques such as: neural networks, cluster analysis, feature engineering, extraction, and reduction, web-scraping, decision trees (CART), collaborative filtering, geo-spatial analysis.,Experience with: PCAP data, Elastic Search, Hadoop, HDFS, git, Spark, MLLib, SQL OS Experience/ Windows, Linux/ Windows,Ability to compile results and deliver presentations to senior-level leadership. Strong communication skills and ability to present material to audiences of differing technical aptitude.,R, R-shiny, R-studio, Python, Sci-kit, Tensorflow, Intelligence Community Experience, Machine Learning, Statistical modeling,,Experience with multi-TB dataset manipulation, cleaning, querying, and modeling.,Experience with scikit-learn, tensorflow, R-caret.,Experience curating datasets for supervised and unsupervised machine learning methods.,Experience with C, Java, R, Javascript, PhP, MatLab, Pig, Hive, Impala, PySpark, Scala, Ruby, Pytorch", "Experience with distributed software packages.,This is a full time, 1 year, term appointment with the possibility of extension or conversion to Career appointment based upon satisfactory job performance, continuing availability of funds and ongoing operational needs.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "masters level degree in statistics, biostatistics, or related fields.,experience with SAS, SPSS,5 years of experience in a research setting using quantitative methods and principles of statistics", "At least MSc. (PhD preferred) degree in Math, Applied Maths, Electrical Engineering, Computer Science, or related field,Experience leveraging Software Development and Machine Learning,3+ years in Advanced Analytics (AI, Graph Theory, Numerical Analysis, etc.),Strong mathematical background,Strong software skills in Python & its numeric/scientific computation libraries (or an equivalent) & query languages like SQL or derivatives used for other datastores (e.g. HiveQL, Bigquery, etc.),Experience using complex data structures and algorithms,Experience with implementation of optimization strategies,Experience applying machine learning and computational math to topics such as ecommerce, sales and marketing,PhD degree in Computer Science, Computer Engineering, Electrical Engineering, or related field,Excellent communication and management skill.,Can lead a team towards execution on a product vision,Experience evaluating and making decisions around the use of new or existing tools for a project,Ready and able to coordinate with cross-disciplinary teams throughout all phases of development,Ability to make the right trade-offs between schedule, resources and scope in order to deliver on a project,Competitive Base Salary,Equity Stake,Competitive benefits", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Apply data science techniques to support early warning on conflict and instability and diagnostics related to fragility and the relationship with international development challenges;,Assist USAID subject matter experts with data science focused analytical thinking for improved planning, monitoring, evaluation, and reporting of programs in conflict-affected and fragile environments.,Design, develop and deliver data analysis and visualization products to support DCHA/CMM s conflict, fragility and violence work, with an emphasis on early warning.,Acquire, process, manage, and analyze data from a range of sources for improved decision making in the areas of conflict, fragility and violence;,Collaborate with a team of interdisciplinary experts, particularly with USAID s GeoCenter, interagency partners and regional bureaus.,Develop and manage an interim early warning tool in coordination with technical staff in DCHA/CMM.,Serve as technical advisor on team responsible for adapting and evolving early warning tools;,Provide consultation to non-technical audiences inside and outside USAID and develop and implement guidance on integrating data and analysis into work related to conflict, fragility and violence.,Prepare concept papers, background analyses, and briefings to build support for the use of data analytics and data science techniques in early warning on conflict and instability, and the dynamics related to fragility.,Participate in discussions among key USAID stakeholders to articulate a vision and plan for integrating data science into development work related to conflict, fragility and violence;,Provide training and capacity building services to non-technical audiences on accessing, analyzing, and visualizing data.,An advanced quantitative degree such as Statistics, Physics, Math, Computer Science, Economics, Engineering, or related technical field,Strong quantitative background and experience in data collection, cleaning, processing and applied analysis and visualization using statistical software or programming languages (Python, Stata, SAS, R, SPSS, MATLAB, Tableau, PowerBI),Ability to produce compelling data visualization products using Adobe applications, such as Illustrator, Photoshop, and InDesign,Ability to create interactive web-based products using HTML, CSS, and JavaScript,Curious, self-motivated individual with excellent communications skills - must be able to distill highly technical quantitative methods into policy relevant snippets,Experience translating statistical, regression and visualization results into briefing documents or presentations for senior leadership,Experience working on data science as it pertains to conflict, fragility and violence within the foreign policy realm.,Experience applying data science analytical techniques to foreign policy and programming.,Ability to apply data science analytical techniques to problems and data sets spanning diverse sectors as agriculture, democracy & governance, economic growth, education, environment, and health,Strategic project leadership: experience leading and facilitating projects through concept, iterative design and development, delivery, and ongoing support,Strong interpersonal skills, and experience working across different offices and agencies.", "Bachelor or Master\u2019s Degree in Engineering, Math, Statistics, Finance, Computer Science, Logistics/Transportation or related industry experience,3+ years of hands on experience with statistical analysis, applying various machine learning techniques, predictive modeling and data mining.,3+ years of experience with data querying languages (e.g. SQL), scripting languages (e.g. Python), or statistical/mathematical software (e.g. R, SAS, Matlab).,Experience articulating business questions and using quantitative techniques to arrive at a solution using available data.,Master\u2019s degree or higher in Engineering, Math, Finance, Statistics, Computer Science, or other technical field from an accredited university,Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations.,Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.,Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences.,Experience processing, filtering, and presenting large quantities (Millions to Billions of rows) of data.", "Interest in causal inference", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Bachelor\u2019s or graduate degree in Computer Science, Applied Statistics or an equivalent quantitative field,Demonstrated knowledge of multivariate statistical and modelling techniques,Strong background with R, Python, and SQL. Experience using SAS including SAS Enterprise Guide to manipulate data,SMART \u2013 Well rounded, with emotional intelligence and clear SME areas,ENGAGED \u2013 Small business owner mentality,CURIOUS \u2013 Always learning and questioning what he/she can do more,FUN \u2013 Good attitude under stress and pressure, pleasant to work with,TEAM PLAYER \u2013 Recognizes others, helps when possible, looks to contribute to a broader goal", "Data science is a dynamic and evolving profession \u2013 we\u2019re looking for people who love to learn and find unique solutions without being micro-managed: you\u2019ll have the freedom to try new things, test solutions and technologies, and tell us when there\u2019s a better path.,You know your way around a cloud console and have handled some pretty large volumes of data.,You are familiar with a few machine learning frameworks, like TensorFlow or PyTorch.,Maybe you have used enterprise products in the past for high throughput data ingestion and analysis.", "Master's or degree or above in computer science or mathematics,3+ years of experiences as a software engineer, data engineer or data scientist,Proficiency in R, Python or Spark,Proficiency in SQL or HiveQL,Knowledge of data visualization,Experience with advanced analytics,Strong interest in the gaming industry", "The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Bachelor degree,Minimum of 1 year of experience in predictive/statistical modeling using SAS or R or Python,Proficient in MS Office applications,Excel proficiency (Pivots, V-Lookups, Formulas),Bilingual (Spanish / English),Master\u2019s Degree,Experience performing data analysis,Experience extracting data using SQL and/or data exploration tools (such as SAS or R),Experience with data analytics design,Experience working with large databases,Health care industry experience,Demonstrated ability to effectively gather requirements, probe for deeper understanding, and translate deep technical concepts to non-technical as well as technical senior stakeholders, marketing customers, and data scientists,Demonstrated ability to manage people and prioritize deliverables,Proven organizational skills with ability to be flexible and work with ambiguity", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Advanced Degree in Mathematics, Statistics, Economics, or Computer Science (or related fields)", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Experience working with Amazon Web Services (AWS)", "M.S. in a quantitative field,1+ years of professional experience as a Data Scientist,NLP experience is a must,Experience with unstructured text data and performing tasks such as text mining, sentiment analysis, language modeling, classification, and information retrieval tasks,Proficient in R and Python", "In process of receiving a graduate degree in an analytical area such as Machine learning, Computer Science, Physics, Mathematics, Statistics, Engineering or similar.,Experience in Analytics or other quantitative disciplines.,Experience with modeling and analysis including machine learning, statistical analysis, operations research and management science and data mining.,Strong interpersonal and communication skills. Must be able to explain technical concepts and analyses implications clearly to a wide audience, and be able to translate business objectives into actionable analyses.,Experience with SQL (any variations thereof), Python/PySpark and/or Scala preferred; though experience with other analytics software (SAS, STATA, MATLAB, Mathematica, R/SparklyR) is acceptable.,Familiarity with AWS solutions such as Glue, S3, and Redshift.,Proven analytical and quantitative skills to use hard data and metrics to back up assumptions, develop business cases, and complete root cause analyses.,Capable of taking responsibility for an initiative and working with minimal direction; self-starter even when assignments are vague or undefined.,In process of receiving a PhD in a quantitative field such as Mathematics, Statistics, Analytics or Economics.,Knowledge and experience with Agile development practices.,Experience in Demand Planning & Forecasting, Supply Chain or Inventory Management is a plus.,Demonstrated ability to manage multiple competing priorities simultaneously and drive projects to completion.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Minimum of 5 - 8 years of experience since obtaining a bachelor's degree.,Experience in data science projects.,Experience in signal processing and digital communications.,Familiarity with more than one of the following programming languages: Python, C/C++ and MATLAB within Unix/Linux programming environments.,Working knowledge of Machine Learning libraries in any language.,Strong written and verbal communication skills.,This position requires the ability to obtain and maintain a security clearance, which is issued by the U.S. government. U.S. citizenship is required to obtain a security clearance.,Advanced degree in the area of digital communications, computer science, machine learning or signal processing.,Experience working with Linux, and development on a Linux platform.,Hands-on experience implementing data-science solutions in Python.,Familiarity with TensorFlow, Keras, PyTorch and scikit-learn.,Familiarity with GNU Radio.,Experience in implementing data science algorithms on GPUs.,Familiarity with Agile methodology.,Current and active secret and special access clearances.", "Academic background in a technical/quantitative field (graduate degree a plus) or equivalent experience,Working knowledge of statistics as pertains to machine learning, such as distributions, statistical testing, regression, etc.,Proficiency in using SQL with several major DBMS and DW engines,Experience with a variety of Big Data technologies, distributed machine learning and computing frameworks (S3, Spark, Hadoop, Elasticsearch, TensorFlow, etc.),Good scripting and programming skills in Python and UNIX shell,Data manipulation skills - extract data from relational and non-relational databases, files of multiple formats, clean it, join it, slice/dice it, organize it, analyze it, and explain it,Experience in the Python data science ecosystem: Pandas, NumPy, SciPy, scikit-learn, NLTK, Gensim, etc. You should be able to hit the ground running with these tools fast,Experience with partitioning and clustering techniques (K-Means, DBSCAN, etc.),Experience with text mining, parsing, and classification using state-of-the-art techniques,Experience with information retrieval, Natural Language Processing, Natural Language Understanding, Neural Language Modeling, and Chat and Dialog Modeling technologies,Strong background in machine learning (unsupervised and supervised techniques). In particular, excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, logistic regression, MLPs, RNNs, etc.,Ability to evaluate quality of ML models and to define the right performance metrics for models in accordance with the requirements of the business", "Bachelor's or higher degrees in business, mathematics, computer science, industrial engineering or other related fields, and at least 5+ years\u2019 experience performing advanced quantitative analyses.,Ability to manipulate, analyzes, and interprets terabytes of data. Ability to organize findings and translate into actionable insights using original or innovative techniques or style.,Ability to apply advanced statistical methodologies and mathematical operations to such tasks as cluster analytics, sampling theory and design of experiments, analysis of variance, correlation techniques, and factor analysis.,Intermediate to advanced experience in Statistical Software (e.g. R, SAS, SPSS) and database applications.,Working knowledge of SAS required.,Must have relational database experience.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc.,Great communication skills.,Experience with data visualisation tools, such as D3.js, GGplot, etc.,Proficiency in using query languages such as SQL.,Experience with NoSQL databases.,Good applied statistics skills, such as distributions, statistical testing, regression, etc.,Good scripting and programming skills in Python or Java.,B.S. in Computer Science, Software Engineering, Information Science, Mathematics, Statistics, Electrical Engineering, Physics or related fields.", "Excellent understanding of machine learning techniques and algorithms (clustering, decision trees, neural networks, etc.),Proficiency in using Python (numpy, pandas, scikit-learn, tensorflow, keras, etc.) or R to manipulate/visualize data and build machine learning pipelines,Experience with SQL / noSQL databases, ElasticSearch,Excellent understanding of math (especially probability theory and statistics),Good understanding of general computer science algorithms (sorting, hashing, etc.),Good communication and presentation skills and the ability to explain technical concepts to a non-technical audience,Ability to work with minimum supervision,Experience with cloud technologies (Hadoop, Spark etc) would be a plus,Knowledge of Java and Scala would be a plus,Minimum of a Bachelor\u2019s degree in Statistics, Mathematics, Computer Science, MIS or related degree and seven (7) years of relevant experience or combination of education, training, and experience.,Master\u2019s degree or Ph.D. in Statistic, Mathematics, or Computer Science and ten (10) years of experience highly preferred.,An equivalent combination of education, experience, and training.,Analysis: identify and understand issues, problems, and opportunities; compare data from different sources to draw conclusions,Communication: clearly convey information and ideas through a variety of media to individuals or groups in a manner that engages the audience and helps them understand and retain the message,Exercise judgment and decision making: use effective approaches for choosing a course of action or developing appropriate solutions; recommend or take action that is consistent with available facts, constraints and probable consequences,Technical and professional knowledge: demonstrate a satisfactory level of technical and professional skill or knowledge in position-related areas; remains current with developments and trends in areas of expertise,Building effective relationships: develop and use collaborative relationships to facilitate the accomplishment of work goals,Client Focus: make internal and external clients and their needs a primary focus of actions; develop and sustain productive client relationships,Opportunity to work on bleeding-edge projects,Work with a highly motivated and dedicated team,Competitive salary,Flexible schedule,Medical insurance,Benefits program,Corporate social events,Professional development opportunities", "Bachelor's degree in Computer Science, Management Information Systems, Statistics, Healthcare Administration or other related field,3-5 years hands on experience in machine learning,Command of principles of machine learning, statistical analysis, data mining algorithms, and mathematical segmentation and modeling,Demonstrated ability to use knowledge of current techniques, and develop new methodologies,1+ years of experience in the healthcare industry,Proven ability and experience in design and development of solutions for increasing yield,Proven analytical skills and experience in handling large volume of data,Experience in dealing with imperfections in data,Experience in implementing Data Visualization solutions,Working knowledge in statistical programming languages, \u2018R\u2019 or Python", "You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day", "You must have deep hands-on experience. You think strategically but love getting your hands dirty.,Bachelor\u2019s degree in mathematics/statistics, engineering, computer science, or other technical discipline,Experience with ETL development, patterns and tooling,Experience in real time, incremental, and batch data ingestion,Expertise in schema design and developing data models for complex data sets,Expert knowledge in a mathematical programming language (R & Python preferred),Strong SQL experience with the ability to develop, tune and debug complex SQL applications,Experience with PowerBI or equivalent BI tool to create impactful reports, visualizations, and interactive dashboards,Ability to communicate complex findings in a clear, precise, and actionable manner. Able to translate high-level ideas into well-defined problems", "Bachelor\u2019s degree in statistics, actuarial science or related field of study,2+ years of PROFESSIONAL experience with PYTHON and/or R/SAS (not including academic experience/Internships),Experience working with large data sets,Experience in data wrangling/cleansing, statistical modeling, and programming,Experience with Tableau or similar visualization tool,Up to 15% Travel,Master's Degree in statistics, actuarial science or related field of study,Experience in data mining and predictive modeling experience,Extensive knowledge of tools for data mining and statistics,Experience in HR Analytics,Strong knowledge of MS Office products,Solid statistical understanding,Good oral and written communication skills,Ability to effectively work in a team environment and as an individual,Strong work ethic and desire to help clients improve their businesses,Strong desire for success,Business analytics experience in one or more of the following industries: Insurance, Consumer Products and Packaged Goods, and Human Resources", "Ability to work cross functionally with measurement teams, product teams, and other members of the wider analytical teams.", "Extensive experience in software development with expertise in architecting and delivering new technologies and product features at scale in highly reliable cloud services.,Experience developing scalable SaaS monitoring, automation, and logging solutions for highly reliable service offerings.,Prior technical paper publications and public speaking engagements.,Extensive software development experience in one or more of the following C, JAVA, C++, and Python. Experience doing so across Windows and Linux is a plus.,Strong algorithmic and problem solving skills,Distributed systems experience,Ability to see and present \"the big picture\" and offer solutions to make it better.,An extraordinarily intelligent, rigorous thinker who can operate successfully among very bright and charismatic people.,Strong customer facing and relationship building skills,You are effective in working both independently and in a team setting,Ability to uncover business challenges and develop custom solutions to solve those challenges,15+ years of work experience in technology industry", "5+ years of revenue forecasting experience needed, preferably within a retailing domain,Excellent statistical skills in applied regression, spatial and time series modeling,Deep understanding of design of experiments principles,Proven working knowledge of SQL,Proven working knowledge of SAS and R,Experience with scripting and automation of data extraction, transformation and modeling outputs,Experience with Data Visualization such as Tableau,Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.", "Prior experience in finance", "A solid understanding of how ad networks & media campaigns work.", "You will be proud to say that you work for Stitch Fix and will know that the work you do brings joy to our clients every day", "Ph.D. degree in Computer Science, Engineering, or Applied Mathematics,Expert knowledge of a scientific computing language (such as R or Python) and SQL,Data Science prototyping experience of physical systems,Experience with distributed storage and compute tools (e.g. Hive, Spark),Experience bringing prototypes to production on Hadoop/Spark platforms and/or as containerized services.,Experience with Natural Language Processing, Deep Learning using Tensorflow (CNN, RNN, LSTM, GANs), Streaming Analytics (i.e. Spark Streaming),Experience with Data Visualization Tools (Tableau, Plotly, Bokeh etc.),Experience working with remote and global teams,Results driven with a positive can-do attitude", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "BS in Statistics, Computer Science, Engineering, Operational Research, Marketing, Business, Economics, Applied Mathematics, or another quantitative field of study.,Experience with Google Analytics & Google Tag Management,Experience performing advanced data transformations in SQL and Excel and excited to learn Google BigQuery,Advanced knowledge of at least one scripting language,Experience with A/B testing and collecting and leveraging user event data for analysis,Experience analyzing, visualizing and presenting data,Excellent oral and written communication skills. Must be able to interact cross-functionally with both technical and non-technical people.,Ability to handle a large workload and efficiently prioritize,Forward thinking, dynamic, and innovative,2+ years applicable experience,Experience with Digital Media,Experience using BI or visualization software,Familiarity with a Unix/Linux environment for automating processes with shell scripting,Familiarity with R, SAS, or SPSS for statistical modeling, clustering, classification, machine learning, and data/text mining", "M.S, or Ph.D. in math, statistics, operations research, computer science, econometrics, or other quantitative field,Must have at least 3+ years of actual working experience performing advanced quantitative analyses.,Advanced hands-on working knowledge of Python and SQL required.,Ability to apply advanced statistical methodologies such as mixed model (random and fixed effects), simultaneous equations, ARIMA, neural networks, and multinomial discrete choice. Ability to apply mathematical operations to such tasks as cluster analytics, sampling theory and design of experiments, analysis of variance, correlation techniques, and factor analysis.,Ability to apply advanced optimization methodologies such as linear and mixed integer optimization.,Ability to apply advanced simulation modeling methodologies and techniques.,Utilize complex computer operations (intermediate programming in 3rd and 4th generation languages, relational databases, and operating systems) and advanced features of software packages (word-processing, spreadsheet, graphics, etc.).,Experience with AWS Data and Machine Learning services is a plus,Must have relational database experience.,A strong passion for empirical research and for answering hard questions with data,Working knowledge of SAS/R is preferred.,Working knowledge of big data manipulation is a plus.,Experience with deploying models in highly scalable production environment is preferred,Experience with AWS Data and Machine Learning services is a plus", "Bachelors Degree in a quantitative,discipline and 4+ years of analytics experience or;,Masters Degree in a quantitative discipline,and 2+ years of analytics experience,2+ Hands on experience in applied machine learning with either Python or R,2+ years using data mining methods, such as clustering and anomaly detection, to understand data patterns and select appropriate predictive techniques.,Proficient understanding of relational SQL (e.g. Oracle, SQL Server, PostgreSQL) and NoSQL (Mongo, Neo4j) databases and data structures.,Excellent communication skills to be able to interact directly with non-technical client stakeholders and act in a business-to-technical translation role.,Experience working in an onsite client technical consulting environment preferred.,Experience working within the Agile Scrum Framework.,Self-motivated and self-managing.,Proficient in creating reasonable and accurate time estimates for assigned tasks.,Masters Degree or PhD,Experience with modern natural language processing techniques (embeddings, deep learning for NLP),Experience with advanced deep learning methods,Experience with deploying machine learning models in AWS", "BA/BS in computer science, statistics, healthcare informatics or similar degree,5+ years of applicable analytics or data science experience,Deep knowledge of machine learning techniques, such as statistical modeling, predictive modeling, natural language processing,Demonstrated ability to apply machine learning to solve complex business problems,Proficiency in Python or R programming,Extensive experience in exploratory data analysis, feature engineering, and data visualization,Experience with SQL relational databases,Ability to communicate and collaborate effectively with technical and non-technical audiences,High degree of personal initiative and strong problem-solving skills,Masters degree or in computer science, statistics, healthcare informatics or similar degree,Familiarity with industry and/or experience working with healthcare data, especially in oncology or population health,Experience with model explanation/interpretation methodologies, such as LIME or Shapley values,Strong data visualization skills", "Strong data mining and machine learning background,Experience with big data environment,3+ years experience working with real-world noisy data,PhD in Computer Science, Math or related quantitative field,Experience with recommender systems, information retrieval, graph analysis, statistical modeling, neural network, and natural language processing,Experience with large-scale data analysis and building high-performance computational software,Experience with e-Commerce and analyzing clickstream data,Proficiency in Python, SQL, and statistics,Experienced in software engineering practices and providing solutions for the development, implementation, scaling, execution, validation, monitoring, and improvement of data science solutions,Strong communication and data presentation skills; ability to communicate with data-driven stories,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "LI-BSTEWARD", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Fluent English", "Development languages including Python, R and Javascript,Machine learning frameworks such as Tensorflow and Keras,Data Science algorithms such as decision trees, linear regression, clustering, word embeddings,Cloud ML resources like Google Cloud Platform,.NET Experience is a plus,OCR Experience is a plus,Experience implementing successful machine learning systems", "1+ year of experience developing machine learning models such as using deep learning methods for classification and regression,Experience with Python, R, Scala, Java, C/C++ or other related languages,Comfort with SQL and NoSQL DB, MapReduce, Apache Spark, Kafka and other large scale data processing tools for performing various tasks related to machine learning and data science activities including data cleanup, data transformation, data mashing and algorithm parallelization,Experience with: Caffe, Torch, TensorFlow, Theano, Matlab or a similar deep learning toolkit,Machine Learning: 1 year,,Python: 2 years,AWS: 1 year,Linux: Various flavors", "Bachelor\u2019s Degree with a concentration in Mathematics, Statistics or Computer Science or equivalent work experience (Master\u2019s is a plus),MUST HAVE a minimum of 5 years of experience as a Data Scientist,A Data Geek \u2013 we\u2019re looking for people who love data, and who are comfortable working with numbers and patterns. If you like to solve puzzles in your free time, you\u2019re on the right track!,Detail Oriented & Process Driven\u2013 we\u2019re looking for folks who see data patterns quickly and can help create a new process when efficiencies can be improved,Knowledge of a variety of machine learning and statistical modeling techniques in a business setting,Ability to choose the best technique for a given problem, even if that solution doesn\u2019t involve ML,Proficiency in Python, R, or other scripting languages (as well as toolkits like pandas, NumPy, etc.),Experience writing production-ready code is a plus,Experience with GCP or other cloud platforms is a plus,A mindset that research should lead to actionable results,Excited to tell us why you want to work here and what kinds of challenges you're looking for,Can talk intelligently and passionately about the interesting challenges your projects presented,A sense of humor and perspective,Preference given to local candidates to Mass as there is no relocation being offered for the position.", "Master's in Computer Science, Math or related quantitative field (or BS/BA in Computer Science, Math or related quantitative field and 5 years of data mining experience).,Work experience with Hadoop, SAS, HBASE, Cassandra or other similar development platforms.,Expert knowledge in large scale information retrieval and statistical analysis.,3 years of experience in data mining and statistical analysis,Previous work experience in ecommerce,Experience with large-scale data analysis and a demonstrated ability to identify key,Insights from data to solve business problems,Experience with big data and distributed computing platforms such Hadoop and Spark and NoSQL databases,Strong knowledge of standard machine learning techniques and concepts,5+ years of hand-on experience (either in industry or academia) with a scripting language such as Python/R or a general purpose programming language such as Java or Scala,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.", "Experience in applying a wide variety of unsupervised, semi-supervised, and supervised machine learning techniques, and the ability to turn big data into actionable intelligence,Familiarity with network and endpoint security concepts and technologies,Ability to analyze, retrain, and improve machine learning models,Ability to work as part of a remote team,Ability to provide and receive scientific critiques and work towards data-driven solutions,Significant development experience with Python, Matlab, R, or Scala,Ability to document and explain technical details clearly and concisely (peer-reviewed publications preferred),Minimum of 8 years of experience in data science or data analytics required,B.S. in Computer Science, or equivalent experience (M.S. and Ph.D are preferred),Strong written and verbal communication skills,Experience with sklearn, pandas, numpy or similar packages,Familiarity with malware, host forensics, or network traffic analysis concepts,Experience with Linux command line and bash scripting,Experience with reverse engineering malware,Experience with AWS infrastructure,Experience with a deep learning frameworks such as TensorFlow, Theano, or MXNet,Experience with GPU-accelerated computing and hardware (e.g., NVIDIA DGX-1),Experience using Hadoop and Spark,Experience using relational and non-relational databases,Experience with web frameworks to visualize large datasets", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Provide consultation on data science and machine learning.,Strong interests in machine learning and a fast learner.,Bachelor\u2019s degree or above necessary.,We\u2019re using rich customer insights, advanced technology and data science to build our cloud-native InsurTech solution.,This person will work closely with a growing inter-disciplinary team of Physicians, Research scientists and Software developers to challenge the status quo and\u2026,Experience using statistics, propensity model building, and algorithms to manipulate data and draw insights from large data sets.,Exciting experience with data science\u2019s and analytics\u2019 hottest topics: HP\u2019s Analytics team is covering a wide range of exciting, fast-growing and business\u2026,You'll report directly to Triplebytes' Head of Machine Learning and will work alongside a team of 6-8 machine learning engineers and data scientists.,Use data to understand business patterns and trends.,You will analyze data to understand business and market trends in order to increase company revenue and\u2026,Experience in data mining, advanced data manipulation, machine learning and statistical analysis.,Identifies meaningful insights from large data and metadata\u2026,The Data Scientist, Advanced Analytics will be responsible for leveraging data analytics, visualization, advanced analytics and data science to help driving\u2026,Possess end-to-end data expertise including data sourcing, consolidation, integration, manipulation and visualization.,Fluent/highly adept in SQL, Python, R.,4+ years of industry experience with proven ability to apply scientific methods to solve real-world problems on web scale data.,You will prototype new ideas and build-out existing systems, collaborating with other data scientists, product managers, front-end developers, and a dedicated\u2026,Past experience in clinical science and/or AI assisted diagnostics a big plus.,5+ years proven track record of successfully applying ML predictive modeling to\u2026,Skilled data scientist with machine learning, image analysis, statistical analysis experience.,Machine Learning: 3 years (Required).", "Work-from-home Wednesdays", "BS/BA in quantitative field - Math, Statistics,Computer Sciences, or related field.,5+ years of professional experience in data science / advanced analytics,Experience in a business environment with large-scale, complex datasets,Proficient in SQL and experience with efficient processing of large data sets. Ability to write sophisticated and optimized queries against large databases,Familiarity with columnar databases like Redshift; Understand database optimization,Experience with math/stats software \u2013 R, SAS, Python,Tableau \u2013 build clear and actionable dashboards,Experience conveying key insights from complex analysis in summarized business terms,A Graduate degree (MS, MBA, PhD, etc.) in a quantitative field will be highly valued but is not required,Experience with Big Data solutions as well as other AWS solutions,Business Acumen \u2013 understand business drivers and have a framework-driven process of analyzing business problems and developing solutions,Communication \u2013 share insights in a way that is easy to grasp and actionable and build relationships to help drive adoption of data- and insights-driven decision making", "Minimum 1-2 years of experience solving data science problems,Masters or PhD,Application of data science solutions in industrial projects,Domain expertise in energy, healthcare, finance, and/or logistics", "Masters or PhD degree in quantitative discipline: Statistics, Computer Science, Engineering, Math, Economics, or evidence of exceptional ability in related fields,2 year plus experience in quantitative analysis,Strong passion and curiosity for data and data driven decision making to solve complex business problems,Solid communication skill and acute attention to detail,Proficiency in SQL, R, and Python,Experience with Java, Tableau, Hive, Spark, MongoDB is a plus,Deep knowledge of applied statistics including predictive modeling, Bayesian statistics, Time Series analysis and Machine Learning,Experience and interest in data visualization techniques. Ability to convey complex analyses with the most efficient and intuitive visual methods", "Experience with AWS cloud computing technologies (e.g. EMR, etc)", "Proven ability to solve business problems by developing and implementing machine learning algorithms and/or statistical models,Must have previous predictive analytics and segmentation experience,Experience building web/mobile interfaces for displaying interactive visual insights,Demonstrated ability to listen, quickly learn, innovate and drive insights from disparate data sets in a fluid environment,Must be able to work independently and identify customer opportunities, trends and patterns,Ability to prototype and transform ideas into actionable insights quickly,Curiosity, ability to work under pressure, creativity, and positive \u201ccan do\u201d attitude are a must,Proven ability to manage multiple priorities while keeping a \u201cteam first\u201d perspective,Demonstrated ability in assimilating real-life experiences with objective data that accelerates learning and behavio,Responsible for ensuring that all security, availability, confidentiality and privacy policies andcontrols are adhered to,Masters Degree or equivalent experience in Maths, Statistics or Computer Science,3 \u2013 5 years of experience creating, analyzing, interpreting and presenting complex data,Extensive experience with D3 or another front-end JS data visualization library,Experience analyzing data using Python, R, or another programming language,Strong skillsin machine learning, data & text mining, advanced statistical analysis, model evolution, validation and testing,Experience with relational SQL and NoSQL databases. SalesForce, Ticketing systems is a plus,Experience working in an Agile environment", "PhD in a computational and quantitative discipline (e.g. statistics, computer science, biomedical informatics, genetics, physics, epidemiology, health economics) or Master\u2019s Degree in a similar field of study,Deep understanding in ML, including strong knowledge of the mathematical underpinnings behind various methods (e.g. regression techniques, neural networks, decision trees, clustering, pattern recognition, dimensionality reduction),Proven experience in applied statistics and ML in a business setting,Deep understanding of the tools of the trade, including a variety of modern programming languages (R, Python, JavaScript) and open-source technologies (Linux, TensorFlow, Hadoop, and Spark),Experience in effective data visualization approaches and a keen eye for detail in the visual communication of findings,Comfort working and communicating with non-technical teams to translate business questions to analytically actionable questions,A strong desire to build meaningful solutions for a life sciences business,Task oriented with ability to set goals and complete deliverables,Domain knowledge of clinical data, real-world data, or life sciences-related research data,Expertise in other data science-related tools (e.g. SQL, Tableau, D3)", "Bachelor\u2019s degree in a quantitative field such as Statistics, Computer Science, Economics, or Mathematics; or equivalent work experience,5+ years of experience in business intelligence, statistical modeling, data collection, aggregation, and analysis,1+ year of experience in machine learning. Experience designing or implementing machine learning solutions a plus.,Demonstrated experience with the following technologies:,R or Python programming,SQL Server, MySQL, or PostgreSQL,Self-driven passion for finding and collaboratively solving problems", "Bachelor's degree and 1 year of work experience in an advanced mathematical, statistical, engineering, physics or related quantitative field; OR Actuarial credential or Master's degree in an advanced mathematical, statistical, engineering, physics or related quantitative field without experience; OR 6 years experience in an advanced mathematical, statistical, engineering, physics or related quantitative field.,Learning and growth mindset.,Customer-focused.,Interpersonal, verbal and written communication skills.,Experience in at least three of the following six areas: 1) data analysis and relational-style query languages; 2) machine learning and/or statistical modeling; 3) data visualization; 4) a high-level programming language; 5) distributed computing; 6) understanding of healthcare.,Masters or Ph.D. in a quantitative field, or bachelors degree with significant healthcare experience.", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Bachelor's degree,Enrolled in a graduate program of study as a full-time student,GPA of at least 3.0 on a 4-point scale,Expertise in working with data using a combination of mathematics, computation, or visualization and interaction,Experience through thesis research, internships, or work experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Physics,Quantitative finance,Statistics,Availability to work at least one 90-day tour prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.,The qualifications listed are the minimum acceptable to be considered for the position. Salary offers are based on candidates' education level and years of experience relevant to the position and also take into account information provided by the hiring manager/organization regarding the work level for the position.", "Full-time student pursuing a Bachelor's degree in a technical field,GPA of at least 3.0 on a 4-point scale,Availability to work two 90-day tours prior to graduation,Attending school on a full-time basis before/following this internship,Creativity,Initiative,Integrity,Leadership abilities,Problem solving skills,Ability to work in a diverse team environment,Interest or experience in a science, technology, engineering or mathematics (STEM) related field, such as:,Computational social science,Computer science,Data analytics,Economics,Engineering,Geospatial analysis,Mathematics,Operations research,Quantitative finance,Statistics,A thorough medical and psychological exam,A polygraph interview,A comprehensive background investigation", "Strong programming skills (e.g., Python, R, and/or JavaScript),Proficiency in writing SQL queries,Ability and desire to present complex findings in a simple, approachable way for non-technical audiences (e.g., in writing, through reporting tools, and at in-person presentations),Experience with cleaning, structuring, and transforming data via ETL processes,Ability to design and deploy machine learning algorithms and models", "US citizenship required. We do not offer sponsorships.,3+ years of experience as a data scientist, machine learning engineer, or similar role,Understanding of structure and theory of common machine learning models,Familiarity with common machine learning libraries for implementation (sklearn, weka, tensorflow, torch, \u2026),Proficiency in one or more of: Python, Scala, Java, R, Julia, Matlab,Expertise in a subfield of machine learning (computer vision, natural language processing, ...),Ability to understand and implement new models from the literature,Project team leadership,Experience presenting team progress and results,Technical writing experience, including reports and proposals,Experience with distributed analytics and processing (Spark, Hadoop, \u2026),Experience with geospatial data and analytics,Active TS security clearance,Familiarity with Agile development", "Experience with Google Cloud Platform (such as BigQuery, Compute Engine, Data Flow, ..),Experience with relational (SQL) and NoSQL Databases,Experience with developing products deployed to production,Experience in developing Search, Recommendation, Visual and/or Language applications,50%-Design and develop algorithms and models to use against large datasets to create business insights,20%-Establish scalable, efficient processes for large scale data analyses, model development and model implementation,20%-Present analysis and resulting recommendations to senior management; Leverage data to present a compelling business case to optimize investments and operations,10%-Communicate and educate technical and non-technical employees on analytics and data-driven decision making,This position reports to Director of Data Science, or Sr. Manager, Data Science,This position has no direct reports,Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.,Typically requires overnight travel less than 10% of the time.,Must be eighteen years of age or older.,Must be legally permitted to work in the United States.,The knowledge, skills and abilities typically acquired through the completion of a master's degree program or equivalent degree in a field of study related to the job.,Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.,Ability to build scalable systems that analyze huge data sets and make actionable recommendations,Strong communication and data presentation skills,Ability to quickly adapt to new technologies, tools and techniques,Flexible and responsive; able to perform in a fast paced, dynamic work environment and meet aggressive deadlines,Ability to work with technical and non-technical team members", "Extensive experience solving analytical problems using quantitative approaches.,Comfort manipulating and analyzing complex, high-volume, high-dimensionality data from varying sources.,A strong passion for empirical research and for answering hard questions with data.,A flexible analytic approach that allows for results at varying levels of precision.,Ability to communicate complex quantitative analysis in a clear, precise, and actionable manner.,Familiarity with relational databases and SQL,Expert knowledge of an analysis tool such as R, Matlab, or SAS,Strong working knowledge of financials,Acquired skills would include Data Warehousing, ETL, BI, Data Mining, and Machine Learning.,Strong written and verbal skills \u2013 able to explain the work in plain language", "PhD in computer science, computer engineering or MS with 3+ years of experience in related field.,Demonstrated history of driving and delivering analytics models and solutions,Deep knowledge of fundamentals of machine learning, data mining and statistical predictive modeling, and extensive experience applying these methods to real world problems,Strong skills in software prototyping and engineering with expertise in applicable programming and analytics languages (Python, R, C/C++) and various open source machine learning and analytics packages to generate deliverable modules and prototype demonstrations of their work,Desired interdisciplinary skills include big data technologies, ETL, statistics and causal inference, Deep Learning, modeling and simulation,Breadth of skills and experience in machine learning \u2013 diverse types of data, diverse data sources, different types of learning models, diverse learning settings,Ability and inclination to work in multi-disciplinary environments, and desire to see ideas realized in practice,Experience and knowledge in services domains such as business process outsourcing systems, transportation systems, healthcare systems and financial services is valued,Demonstrated ability to propose novel solutions to problems, performing experiments to show feasibility of their solutions and working to refine the solutions into a real-world context,Prior experience in a similar role is required,Please include other requirements as appropriate,Must be currently eligible to work in the US for any employer without sponsorship", "Ping pong, scooters, foosball, and beautiful trails nearby, to name a few!", "Masters degree in computer science or related field,Experience in machine learning, text analysis, and NLP,Experience in algorithm design and modeling,Experience designing and working with ontologies,Programming skills in Java, Python, or similar language", "Experience with end-to-end ETL work using Python and SQL,Strong systems design knowledge; you know how to architect data pipelines and how storage and compute fits together,Prior experience making large datasets accessible; familiarity with Hive, Presto and parquet is a plus,Experience with compute frameworks and job orchestration systems,Experience with Google Cloud Platform a plus,4+ years of industry experience,Health insurance with 100% premium covered for you and your dependent children,Flexible vacation & paid time off,Up to 20 weeks of paid family leave,Equity plan for all employees,Retirement benefits with employer match,Fertility and adoption benefits,Free lunch and snacks at all offices,Education reimbursement,Dog-friendly workplace in New York office,Commuter benefit in the form of reduced tax (Ireland) and pretax (US)", "Bachelor\u2019s degree in computer science/software engineering or related fields with 3+ years of experience in data engineering,3+ years of experience working with Python, NodeJS,2+ years of experience working with Kafka based pipeline development,Strong knowledge of SQL is required,1+ year of hands-on experience with Apache Airflow,Experience with distributed computing using Hadoop ecosystem, Spark, Presto.,Knowledge of Java, Scala,Knowledge of Kafka connect ecosystem", "Must have a minimum of 2 years of Cyber Security Engineering experience, to include application of cyber security methodologies in an Enterprise environment,Must have experience with the following Security Frameworks: NIST 800-171r2,Candidates must have the ability to obtain, an maintain, a DOD Secret level security clearance as a condition of continued employment Preferred Qualifications: Certifications/Degrees,The ideal candidate will have a Master's degree in an STEM related discipline, and 6 years of experience with Cyber Security Engineering,Professional/technical certifications: CISSP, CISM, CCSP, Security Plus, AWS Certified Security - Specialty, AWS Certified Solutions Architect, Azure Security Engineer, Azure Solutions Architect,DoD 8570 IAT/IAM Level II or III certificationSecurity Skills,Senior-level cyber security engineering and architecture experience,Experience with interpreting and implementing security compliance standards and guidance including Governance, Risk & Compliance (GRC) policies and procedures, NIST 800-171 security control framework,Experience in areas such as system security, network, and application security,Knowledge of current and emerging cyber security threats, vulnerabilities, and controls,Contributor for architectural/industry changes in the area of cyber securityTechnical Skills,Experience with evaluating, designing, configuring, implementing cloud services models such as SaaS, PaaS, and IaaS,Experience with Linux and Windows operating systems,Experience with operating in an Agile/DevOps environment,Experience with Scripting,Knowledge of application program interface (API) and ability to manipulate API's to integrate different toolsets,Advanced knowledge in cybersecurity principles, networking, architecture, servers, systems design, virtual hosts, configuration management, Identity and Access Management, encryption, intrusion detection systems (IDS) and intrusion prevention systems (IPS),Experience in supporting the deployment, configuring, managing, and maintaining any of the below technologies:,Directory Services and Centralized Authentication, such as Active Directory or Red Hat,Identity Manager Vulnerability scanning and management of databases, operating systems, and/or web applications,IDS/IPS and anti-malware tools/technologiesProcess Skills,Experience with Agile, Scrum and Application Lifecycle Management (ALM),Experience operating in an Agile/DevOps environmentSoft skills,Exceptional verbal and written communications.,Quickly learn and adapt to new and changing business/technical concepts, requirements, skills, tools,Goal-oriented team player committed to quality and detail,Proven track record of driving decisions collaboratively, resolving conflicts and ensuring follow-through,Innovative and strategic thinker who is positive, proactive and readily embraces change", "Experience with Data Warehouses/Lakes (Big Query, RedShift, Snowflake), NoSQL (Cassandra, Redis), Stream Processing Engines (Dataflow, Flink), Workflow Management Tools (Airflow, Pachyderm), or other Big Data solutions are highly appreciated.", "Great perks, which vary by location and can include: employee discounts, transportation reimbursements, subsidized cafes and fitness facilities, conveniences such as dry cleaning and car washes, and recycling programs.", "knowledge in relevant engineering best practices, data management fundamentals, data storage principles, and be current with recent advances in distributed systems as it pertains to data storage and computing,2+ years of experience in designing, building and maintaining data architecture(s) and infrastructure(s), both relational and non-relational,2+ years of maintaining data warehouse systems and working on large scale data transformation using SQL, Hadoop, Hive, or other Big Data technologies; experience with ETL tools is a plus,2+ years of data modeling experience, and able to use data models to improve the performance of software services,experience with Cloud Based Solution (AWS Redshift, GCP Big Query) and programming language (Python, Java) is a plus,experience communicating with colleagues from engineering, analytics, and business backgrounds,degree in Engineering, Math, Statistics, Computer Science, or related discipline or equivalent experience is a plus.,be able to legally work in Europe (you are the holder of a EU Passport or you are the holder of EU residency permit or you are the holder of a Schengen Work Visa)", "Education: B.Sc. Computer Science or related fields,Knowledge & Experience: 5+ years in deployment, integration and server management as well as experience with writing scripts using Python and Bash,Advanced knowledge of SQL and non-SQL databases.,Strong analytic skills related to working with unstructured datasets.,A strong knowledge of manipulating, processing and extracting value from large disconnected datasets.,Experience with big data tools: Hadoop, Spark, Kafka, etc,Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.,Experience with stream-processing systems: Storm, Spark-Streaming, etc.,Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.,Experience with AWS cloud services: EC2, EMR, RDS, Redshift", "Lead implementation teams from concept to completion by leveraging the best practices of Big Data,Participate in the analysis, architecture and design of data hub,Builds robust Big Data solution systems with an eye on the long term maintenance and support of the Application,Work as part of a team to design and develop code, scripts, and data pipelines that leverage structured and unstructured data integrated from multiple sources,Looks to leverage reusable code modules to solve problems across the team, including Data Preparation and Transformation and Data export and synchronization,Design and develop automated test cases that verify solution feasibility and interoperability, to include performance assessments,Act as a Big Data delivery liaison with Infrastructure, security, application development and testing team.,Helps drive cross team design / development via technical leadership / mentoring,Keep current on latest Big Data technologies and products, including hands-on evaluations and in-depth research,Consult and advise solution architects on overall enterprise-wide analytics solutions that include a Data Hub Component,Works with Project Manager to perform detailed planning, risks/issues escalation.,Requirements for Big Data Engineer, Healthcare Analytics Senior:,4+ years of experience working with batch-processing and tools in the Hadoop tech stack (e.g., MapReduce, Yarn, Pig, Hive, HDFS, Oozie)*,4+ years of experience working with tools in the stream-processing tech stack (e.g., Spark, Storm, Samza, Kafka, Avro)* Experience developing applications that work with NoSQL stores (e.g., ElasticSearch, Hbase, Cassandra, MongoDB, CouchDB)* Experience developing for TB-level data stores and/or 10Gbps+ ingest speeds,High-capacity data ingest into Hadoop or Spark is highly desired,Hands-on experience with at least one major Hadoop Distribution such as Cloudera or Horton Works or MapR or IBM Big Insights,System usage and optimization tools such as Splunk is a plus,At least 4 years of experience delivering enterprise IT solutions as a solutions architect,8+ years of experience with SQL and at least two major RDBMS's,5+ years as a systems integrator with Linux systems and shell scripting,8+ years of doing Data related benchmarking, performance analysis and tuning,5+ years of Java experience,Solid programming experience with a preference towards Java or Python,DBA and/or Data Modeling experience,Experience with operational and business-level metadata management,Bachelor's degree in Computer Science, Information Systems, Information Technology or related field and 6+ years of software development/DW & BI experience.,Health care experience is plus,Excellent verbal and written communication skills,Hands-on experience with Cloudera 4.5 and higher, Horton Works 2.1 and higher or MapR 4.01 and higher,Experience with Map/Reduce solution design and development,ETL Solution experience, preferable on Hadoop,Experience with industry leading Business Intelligence: Qualifications", "We offer the ability to work with the newest tech-stack and the most talented engineers in IoT industry!", "Fluency in English written/spoken", "Roughly 5 (or more!) years of Industry experience on a data or machine learning team,Proficiency with modern programming languages (Go, Python, Java, Scala, etc.) and SQL,Some practical experience with probability, statistical modeling, or machine learning,Backend developer experience optimizing the data access layer in mature web applications,Experience building and working with real-time compute and streaming infrastructures (Kafka, Kinesis, Flink, Storm, Beam, etc.),Experience writing and debugging ETL jobs using a distributed data framework.,A deep and abiding appreciation for agile software processes, data-driven development, reliability, and responsible experimentation,A collaborative attitude and a helpful personality,Health, dental, vision, and life insurance,401k matching program,Commuter benefits,Catered lunch and unlimited snacks,Unlimited reimbursement for work related books", "You're aware of the challenges associated to building ML datasets and computer vision models: definition and coverage of target data distribution, bias,You have creativity and a good sense of product, which enable you to have an intuition of how each model is expected to work (eg face detector), where a model could potentially fail (eg low light, pale/dark skin etc...), what data can be used to test failure patterns,You have strong Python coding skills which enable you to manipulate data at scale & run ML models,You come up with data-driven solutions when facing situations of trade-offs/decision under uncertainty", "Experience with search technologies (Solr, ElasticSearch, Lucene)", "BS in computer science, systems engineering, or a similar technical field with relevant work experience.,An understanding of data model design, database schemas, and optimizing database applications.,A breadth of understanding of database technologies including both relational and non-relational solutions.,Experience manipulating large data sets of time-series and intermittent data.,Experience using version control software.,Experience designing and developing database access layers for schemas that contain multiple databases containing unique data types and access requirements.,Experience working with Python as a primary development language with an emphasis on data management and processing.,Experience with MySQL and MongoDB,Experience producing software for a clinical setting that utilizes clinical patient data, e.g., labs, physiologic signals, and administrative data.,An understanding of clinical (or any data-driven) research from a data aggregation and methodologies standpoint (study design, subject protections, and statistical analysis).,Experience with Agile software development methodologies, and continuous integration and delivery.", "Minimum four (4) years experience in the planning, design, and implementation of security solutions, including Minimum two (2) years experience leading the design, implementation, troubleshooting, and operation of security technologies.,Minimum two (2) years in a technical leadership role with or without direct reports.,Bachelor's degree in Computer Science, CIS, or related field and Minimum eight (8) years experience in an IT operations environment with technical experience in distributed technologies, systems development, and/or networking. Additional equivalent work experience may be substituted for the degree requirement.\nPreferred Qualifications:,Three (3) years of experience building technology solutions to meet corporate or industry IT regulatory requirements.,Three (3) years experience in the design and implementation of complex data infrastructure solutions.,Three (3) years experience in IT infrastructure consulting.\nPrimary Location:\nCalifornia,Pleasanton,Pleasanton Tech Cntr Building F 5810 Owens Dr.\nScheduled Weekly Hours:\n40\nShift:\nDay\nWorkdays:\nMon, Tue, Wed, Thu, Fri\nWorking Hours Start:\n8:00 AM\nWorking Hours End:\n5:00 PM\nJob Schedule:\nFull-time\nJob Type:\nStandard\nEmployee Status:\nRegular\nEmployee Group/Union Affiliation:\nSalaried, Non-Union, Exempt\nJob Level:\nIndividual Contributor\nJob Category:\nInformation Technology\nSpecialty:\nIT ENG Infrastructure\nDepartment:\nDCSS A2O\nTravel:\nYes, 5 % of the Time", "Gym membership compensation", "BS or MS degree in Computer Science, Math, Statistics or other technical field.,3-5+ years of applied software engineering experience (especially startups, big data, Python).,1+ years as a team lead or managerial role.,Python Expertise: classes & inheritance, generators, decorators, docstrings, pylint, pytest, etc.. Numpy/pandas experience preferred.,SQL/Hive Expertise: where clauses, joins, group bys, windowing functions, exploding.,Spark Expertise: SparkSQL, pyspark, Caching, Checkpointing, Dataframes, RDDs.,Expertise in building, monitoring and maintaining reliable ETL pipelines.,Ability to write well-abstracted, extensible, object-oriented code components.,Enjoy working in a fast-paced, highly collaborative and ambitious startup work environment.,Basic understanding of probability & statistics; experience evaluating data quality at scale.,Experience with Amazon Web Services (RDS, S3, EC2, EMR, Data Pipeline), PyCharm, Github, JIRA (or equivalents).,Experience with open source search platforms such as Solr, ElasticSearch or the like.,Experience with Unix/OSX CLI (bash),Background in data wrangling various structured & unstructured data sets, consuming APIs (e.g. rate limiting and exponential back-offs) and the like.,Knowledge of graph storage and computation frameworks (e.g. GraphX, TitanDB, Neo4J).,Familiarity with Scala and/or Java, Apache Spark internals and job optimization.,Significant interest or background in big data, politics, advertising or finance technology,Experience working directly with data scientists; basic knowledge of common machine-learning techniques,Experience with agile development or similar methodologies for continuous development of product and technology.", "Developer tools (Git, SDLC, OOP, etc) (experience required)", "Experience with or advanced courses on data science and machine learning.", "Strong communication skills, including the ability to identify and communicate data driven insight", "Experience in business intelligence, analytics, or an equivalent analyst position with experience in SQL and an additional object-oriented programming language (e.g., Python, Java).,High level of expertise in data modeling.,Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders.,Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs.,Attention to detail and effective verbal/written communication skills.,Bachelor\u2019s degree in Engineering, Computer Science, Statistics, Economics, Mathematics, Finance, a related quantitative field, or equivalent practical experience.,2-6 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position with experience in SQL and Python", "Gym membership compensation", "Knowledge in the digital and AdTech landscape", "Creative, analytic problem solver with diligent attention to detail", "Demonstrated fluency in modern programming languages for data science, covering a wide gamut from data storage and engineering frameworks through to machine learning libraries", "Experience with Cloudera", "Experience with one or more of the following languages and functional programming in general: Scala, Haskell, Java, JavaScript,Experience with one or more of the following technologies:\nDistributed logging systems (Kafka, Pulsar, Kinesis, etc)\nStream processing. Flink, Spark, Storm, Beam, etc\nBatch processing: Spark, Hadoop, \u2026\nIDL: Avro, Protobuf or Thrift\nMPP databases (Redshift, Vertica, \u2026)\nQuery execution (Columnar storage, push downs): Hive, Presto, Parquet, ...\nWorkflow management (Airflow, Oozie, Azkaban, ...)\nCloud storage: S3, GCS, ...,Distributed logging systems (Kafka, Pulsar, Kinesis, etc),Stream processing. Flink, Spark, Storm, Beam, etc,Batch processing: Spark, Hadoop, \u2026,IDL: Avro, Protobuf or Thrift,MPP databases (Redshift, Vertica, \u2026),Query execution (Columnar storage, push downs): Hive, Presto, Parquet, ...,Workflow management (Airflow, Oozie, Azkaban, ...),Cloud storage: S3, GCS, ...,Understanding of distributed systems concepts and principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms),Eager to learn new things and passionate about technology,Experience with contributing to open source software,Experience with the following Cassandra, DynamoDB, RocksDB/LevelDB, Graphite, StatsD, CollectD\nAbout WeWork\nWeWork Technology is bridging the gap between physical and digital platforms, providing a delightful, flawless & powerful experience for members and employees. We build software and hardware that enables our members to connect with each other and the space around them like never before.\nWe augment our community and culture teams through the tools we build. We believe there\u2019s a macro shift toward a new way of working\u2014one focused on a movement towards meaning and purpose. WeWork Technology is proud to be shaping this movement.\nWe are a team of passionate, fearless and collaborative problem-solvers distributed globally with one goal in mind - to humanize technology across the world.\nWe are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.", "Computer Science, Engineering, or Bioinformatics (Master level) plus 5 years relevant experience,Excellent programming skills (Python, C++, R),Experience in designing and implementing RESTful APIs and webservices,An ability to interact with various data sources, both structured and unstructured (e.g. HDFS, SQL, noSQL),Experience working across multiple scientific compute environments to create data workflows and pipelines (e.g. HPC, cloud, Unix/Linux systems),Expertise with biological/health data,Experience modelling data and information for graph/network representation,,Experience of working with metadata models, controlled vocabularies and ontologies,Ability to understand, map, integrate, and document complex data relationship and business rules,Familiarity with data quality, cleaning and masking techniques,Modern frameworks and concepts for scalable and distributed computation (containerization and orchestration e.g. k8s, specialized frameworks such as Spark, Hadoop, ...),Experience with image processing and computer graphics,Experience with cloud computing", "Bachelor's degree,Experience with designing and developing medium-to-large data environments and associated services (e.g., data pipelines, data cataloging) for enterprise applications and analytics projects,Proven experience fusing, mining and preparing data sets using SQL, Oracle, or Elastic technologies,Experience performing machine learning (ML) functions for data preparation and automation of other data services (e.g., data quality measures, utilization, mapping, etc.),Familiarity with hybrid cloud infrastructures,Experience with Python, Java, or other relevant scripting languages", "As always, the interviews and screening call will be conducted via a video call.", "Bachelor\u2019s degree in math, statistics, computer science, or finance or equivalent experience.,5+ years of experience as a Data Engineer, BI Engineer, Business/Financial Analyst or Systems Analyst,SQL writing experience and experience with ETL,Expert understanding of best practices to handle extremely large volume of data,Ability to create extensible and scalable data schema that lay the foundation for downstream analysis,A clear passion for learning new BI skills and techniques independently and continuously,Ability to prioritize multiple concurrent projects while still delivering timely and accurate results,Experience working in a lean, successful start-up or on a new product team where continuous innovation is desired and ambiguity is the norm,Experience mentoring others in SQL, modeling, forecasting and the use of large datasets,Proficiency with scripting languages and Unix systems (Python, perl, bash, etc.),Experience with the following is a plus: Looker, Tableau, Microstrategy", "Willingness to travel,", "Python Django/Flask", "Gym membership compensation", "Gym membership compensation", "Responsible for staying current with enterprise standards, industry standards and technologies, methodologies and best practices", "You are a data engineer with previous experience in business intelligence and data warehousing,You know how to work with high volume heterogeneous data, preferably with distributed systems such as Kafka, Spark, and MPP databases such as Snowflake.,You are comfortable building and deploying applications in public clouds, in particular in the AWS cloud.,You know how to write distributed, high-volume services in Golang, Scala, or Java.,You have hands-on experience with a large number of technologies and programming languages. It allows you to choose the right tool for job, and to not be afraid of contributing to systems across the whole FindHotel organisation.,You are knowledgeable about data modeling, data access, and data storage techniques.,You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.,You strive for excellence, clarity and transparency, shipping business value as early as possible, and building incrementally afterwards.,(For remote candidates) Based in a time zone between UTC-4 and UTC+6,Experience with e-commerce, clickstream data and event tracking,Experience or interest in data analysis,We continue to hire and grow, look at how we not only survived the Corona-crisis but also thrived.,This year we will be helping +1M customers around the world find better hotel deals, using data, transparency & industry-leading features.,We are in fast growth mode and have been growing bookings by +100% Year-Over-Year for the past 2 years and plan to continue doing so in the coming years.,From our beginnings as an organisation that excelled in User Acquisition, we have grown into an Engineering and Product-driven organisation. Our independence and the maturity that we have achieved in terms of User Acquisition, Engineering and Product enables us to be disruptive and build customer features that nobody else in the industry will.,Plenty of chances to learn and grow \u2013 you'll be surrounded by some of the brightest minds in the city, be part of a culture which values sharing knowledge every day and has a budget to attend conferences and develop yourself.,A profitable company with fast growth and a great scale opportunity.,A competitive compensation package + perks and benefits (including Stock Appreciation Rights).,Flexible time off (take as many holidays as you need) and a chance to work remotely - we measure results, not time spent in the office.,You will be part of a highly international team in a fun work environment.,We value good food and offer catered lunches from various cuisines, great coffee, ice-cream in the fridge and the occasional bbq in our garden.,How to survive, refocus & thrive again as a travel startup in corona times,Our hiring process", "Final meeting with engineering leadership via video or in person. 1 hour.", "Gym membership compensation", "Competitive compensation, visa and relocation support when needed.", "We offer the ability to work with the newest tech-stack and the most talented engineers in IoT industry!", "Willingness to travel,", "Due to business requirements, the successful candidate must be based in Ireland.,Candidates based in the EU but willing to relocate to Ireland will also be considered.,Python experience (3+ years),5+ years of software development experience,Good command of Linux,Front-end development experience required for creating and supporting internal tools,Back-end development experience required for creating and supporting internal tools: Python web frameworks (like twisted, aiohttp, django, flask), databases.,Understanding of the web technologies: JavaScript, HTML, CSS, HTTP,Strong analytics skills related to working with unstructured datasets,Excellent written English,Strong web crawling and web scraping skills: Scrapy knowledge, browser automation experience. Splash experience is a plus.,Experience handling mid-size and large datasets, organizing their parallel processing,Good spoken English,Strong record of open source activity", "pension plans and employee benefits", "Weekly catered lunches and a stocked kitchen full of fruit, energy bars, popcorn, coconut water, and other healthy snacks", "Gym membership compensation", "Significant competence with SQL", "Understanding of data visualization tools", "Minimum 5 years experience in data engineering,Extensive knowledge in different programming and scripting languages such as Java, C++, Python, PHP, Ruby, Perl, Scala, Bash, etc.,Proficient in SQL,Solid understanding of both relational and NoSQL database technologies,Experience designing, building, and maintaining end-to-end data pipelines and ETL infrastructure, using tools such as Hadoop, Spark, Kafka, Samza, Amazon Kinesis, etc.,Experience with AWS,Strong familiarity working in a Linux environment,Excellent critical thinking, problem-solving, and analytical skills,Must be authorized to work in the United States,Gaming industry experience,Experience with Redshift,Machine learning experience", "Strong understanding of security, including threat propagation and malware analysis", "Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.,Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets.,Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.,Strong analytic skills related to working with unstructured datasets.,Build processes supporting data transformation, data structures, metadata, dependency and workload management.,A successful history of manipulating, processing and extracting value from large disconnected datasets.,Working knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data stores.,Strong project management and organizational skills.,Experience supporting and working with cross-functional teams in a dynamic environment.,Experience with big data tools: Hadoop, Spark, Kafka, etc.,Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.,Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.,Experience with AWS cloud services: EC2, EMR, RDS, Redshift,Experience with stream-processing systems: Storm, Spark-Streaming, etc.", "Knowledge of commonly used third-party analytics tools like Periscope, Tableau, Segment, Heap", "You will design and develop applications for data processing on one of languages: Python, Java, Scala.,Create chains for data ingestion from relational DBs to noSQL storage.,Utilize Unix/Linux OS and write data preprocessing scripts.,Develop a data processing systems with Hadoop, Spark, Storm, Impala, etc.", "Bachelor's degree in Computer Science, Engineering, Technical Science or 12 years of experience in programming and building large scale data/analytics solutions operating in production environments.,Minimum 2+ years of expertise in designing, implementing large scale data pipelines for data curation, feature engineering and machine learning, using Spark in combination with pySpark, Java, Scala or Python; either on premise or on Cloud (AWS, Google or Azure).,Minimum 1 year of designing and building performant data tiers (or refactoring existing ones), that supports scaled AI and Analytics, using different Cloud native data stores on AWS, Azure and Google (Redshift, S3, Big Query, SQLDW etc.) as well as using NoSQL and Graph Stores.,Minimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies.,Minimum 1 year of designing and building secured and governed Big Data ETL pipelines, using Talend or Informatica technologies; for data curation and analysis of large production deployed solutions.,Experience implementing smart data preparation tools such as Palate, Trifacta, Tamr for enhancing analytics solutions.,Minimum 1 year of building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions.", "Sense of humor is a must-ash. (Mustache\u2026 Get it? Mustache.)", "The curiosity and determination to understand and improve data flows.", "Python,SQL,Relational databases (e.g., Postgres, MySQL),Distributed computation (e.g., Spark, Hive, Athena),Cloud infrastructure (we use AWS),General application (e.g., web API) development,Git collaboration", "Pet friendly office environment", "Fixed term contract with an option for perm", "Components/Frameworks including: Hadoop/HDFS, Spark, Storm, HBase, Pig, Hive, Scala, Kafka, PyScripts, Unix Shell scripts", "\ud83c\udf7a, \ud83c\udfd3, \u26bd\ufe0f and \ud83d\udd7a\ud83d\udc83", "Experience with Agile Methodologies such as Scrum or Kanban", "We have a strong preference for someone experienced with Python rather than Java", "Gym membership compensation", "A Bachelor\u2019s degree in computer science, information technology, or other quantitative field.,5 or more years of relevant experience, ideally in a scientific computing environment.,Ability to design informative and accessible technical training materials.,Proven deep technical background including Linux administration.,Ability to work in a demanding environment with minimal supervision.,Strong track record of implementing cloud-based processing techniques.,Administration experience with Azure, AWS, Google Cloud or one of the other major IaaS providers. Google Cloud Platform preferred.,A passion for data science including knowledge of at least one high-level programming language. Python and/or R are preferred.,Familiarity with database usage and basic administration tasks. PostgreSQL preferred.,Experience leveraging established distributed computing frameworks such as Apache Spark and Hadoop.,Experience handling large volumes of spatiotemporal data.", "Master\u2019s degree in areas like Mathematics, Physics, Computer Science, Engineering or Econometrics, Business Analytics, or Information Management,Minimum 3+ years of hands on relevant Big Data technologies or implementations,Experienced in distributed computing, distributed storage, and containerisation; Spark/PySpark, Kafka, SQL & NoSQL (e.g., Elastic, Hive, BigQuery, Kafka KTables, AWS S3), Docker, Kubernetes,Experienced in programming with Python or Scala and familiar with micro services development (REST API\u2019s/IoT),Fluent in English. Fluent Dutch is a strong preference, but not mandatory,Good business communication skills, able to transform business requirements into use cases,A flexible transport arrangement that suits your personal situation (electric car or bicycle, flexible budget including NS business card)", "Minimum 5 years of experience with a programming language such as Scala, R, Python or Java, and the experience writing reusable and efficient code to automate analyses and data processes,Minimum 2 years of experience processing large amounts of structured and unstructured data in a cluster-computing environment or similar experience in academia,Interested candidates must submit a resume/CV through www.nbcunicareers.com to be considered,Must be willing to work in New York, NY,Experience formulating opinions on constructing data processing systems and good knowledge of the principles of systems at scale using big data technologies, like Spark, Hive, Impala, Hadoop, and Databricks,Good understanding of AWS, Azure and other cloud technologies including AWS services, such as Athena, Glue, S3, and Lambda,Experience with open source and Enterprise software,Experience building and maintaining production data pipelines,Familiarity with relational databases and SQL,Team-oriented and collaborative approach with a demonstrated aptitude and willingness to learn new methods and tools,Ability to communicate insights and findings through data visualization tools such as Tableau, DOMO, Shiny,Ability to work effectively across functions, disciplines, and levels,Experience in media and entertainment industry a plus,Experience with television ratings and digital measurement tools (Nielsen, Rentrak, comScore, Omniture, etc.),Familiarity with NoSQL and Graph databases,Experience with large-scale video assets,Experience with computer vision and metadata generation from video,Master\u2019s Degree with a specialization in Computer Science, Engineering, Physics or other quantitative field or equivalent", "Extensive experience in software development with expertise in architecting and delivering new technologies and product features at scale in highly reliable cloud services.,Experience developing scalable SaaS monitoring, automation, and logging solutions for highly reliable service offerings.,Prior technical paper publications and public speaking engagements.,Extensive software development experience in one or more of the following C, JAVA, C++, and Python. Experience doing so across Windows and Linux is a plus.,Strong algorithmic and problem solving skills,Distributed systems experience,Ability to see and present \"the big picture\" and offer solutions to make it better.,An extraordinarily intelligent, rigorous thinker who can operate successfully among very bright and charismatic people.,Strong customer facing and relationship building skills,You are effective in working both independently and in a team setting,Ability to uncover business challenges and develop custom solutions to solve those challenges,15+ years of work experience in technology industry", "Experience with large-scale data and query optimization techniques,Experience with ETL to data warehouse systems,Experience with AWS cloud services: EC2, RDS, Redshift, AuroraExpert in SQL, NoSQL, and RDBMS,Knowledge in multiple scripting languages (e.g. Python),Knowledge of cloud, distributed systems, and stream-processing systems,Passionate about learning new technologies and solving hard problems in a fast-paced environment,Has a Computer Science degree,Has 5+ years experience in enterprise SaaS environments,Is a \"student of the game\" and thrives on new challenges,Enjoys learning from teammates, and isn't afraid to teach others at the same time,Sees the glass half-full. This is a new industry space...your vision could make all the difference!,Wants to make a lasting impact and lifelong connections, this is not just another paycheck", "CI/CD (Drone, Gitlab CI)", "Tuition reimbursement and learning & development programs", "Professional Development Program", "Knowledge of Tableau, QlikSense or similar technologies.", "Do you have strong communication skills and the ability to present deep technical findings to a business audience?", "Bachelor degree or equivalent experience", "Roughly 5 (or more!) years of Industry experience on a data or machine learning team,Proficiency with modern programming languages (Go, Python, Java, Scala, etc.) and SQL,Some practical experience with probability, statistical modeling, or machine learning,Backend developer experience optimizing the data access layer in mature web applications,Experience building and working with real-time compute and streaming infrastructures (Kafka, Kinesis, Flink, Storm, Beam, etc.),Experience writing and debugging ETL jobs using a distributed data framework.,A deep and abiding appreciation for agile software processes, data-driven development, reliability, and responsible experimentation,A collaborative attitude and a helpful personality,Health, dental, vision, and life insurance,401k matching program,Commuter benefits,Catered lunch and unlimited snacks,Unlimited reimbursement for work related books", "AWS, Docker, Kubernetes, Terraform, Vault", "High quality swag", "Gleaming new 100,000 square foot headquarters complete with a 70-foot climbing wall, showers, lockers, and bike parking", "Proficient in English. You read and write proficiently, and speak at a conversational level in English.", "Gym membership compensation", "Relocation is simplified \u2014 paid accommodation, as well as experienced \u201crelocation buddies,\u201d guide you through your visa application.", "Always improve: we value personal progress and want you to look back proudly on what you\u2019ve done.", "Transparency \u2013 regular All-Team meetings, so you can stay in-the-know with what\u2019s going on in all areas our business", "Bachelor's degree in Computer Science or a related field,Experience with relational databases, SQL, and map-reduce languages (Pig, Hive),Understanding of how different data storage engines work and what are the limitations (SQL, NoSQL, key-value stores),Knowledge of Java, C++, C and GO or Node.js,Experience with Druid or other time series based data storage solution,Deep knowledge building high-performance, high-availability, distributed systems,Experience with Kafka, Spark, Cassandra,Extensive experience working with big data and designing ETL pipelines from end to end,Expert with one RDMS, familiarity with PostgreSQL and Redshift,Knowledge of ad serving platforms and online advertising systems,Experience in game development", "Excellent medical, dental, and vision benefits", "Total 2+ years in IT (preferably DW/ETL/BI projects),Experience with gathering end user requirements and writing technical documentation,Time management and multitasking skills to effectively meet deadlines,Basic understanding of data management concepts such as 3NF, Dimensional and their specific applications,SQL, PL/SQL experience in analyzing, transforming and integrating data (preferably in one of the database technologies such as Oracle, MSSQL, DB2, Teradata),Basic understanding of DWH-related terms such as for example procedures, functions, triggers, views, indexes etc.,Ability to analyze data from various sources to find meaningful insights as well as identify gaps and inconsistencies,Client facing and team player skills,Good command of written and spoken English,Willingness to develop competences in the area of data analytics,Availability and mobility to work on projects in Poland and across Europe,Previous experience with ETL tools (for example ODI, SAS DI, IBM DataStage, Informatica, MS SSIS),Experience in Agile development methodologies,Access to leading edge Cloud technologies,Career and competence support, including ongoing mentoring and financing of certificates in different technologies and tools.,Full work comfort: private healthcare, additionally life insurance, sport packages.,Opportunity to work with global top clients on innovative and large international projects on European level", "Knowledge and experience of financial markets, banking or exchanges", "https://www.showtimeanytime.com/", "https://www.showtimeanytime.com/", "Exceptionally detail-oriented", "A deep understanding of distributed computing frameworks such as Spark (particularly SparkML, SparkSQL, tune/optimize and debug Spark jobs), Hadoop and/or Flink,Experience with big data at AWS, in particular using EMR and S3,Experience with Docker and container orchestration like Kubernetes, Swarm or similar,Experience with pipeline management tools like Airflow, Luigi or NiFi,Experience with programming languages such as Python, Go and/or Scala,Good knowledge of SQL/RDBMS,Experience with the command line, shell scripting and version control (Git),Excellent communication skills in English, both oral and written; German is nice to have,Preferably experience with automatic configuration management like Terraform and Puppet,Preferably experience with modern agile software development practices like microservices, test-driven development, pair programming, CI/CD etc.,Training opportunities for your individual management or specialist career,The provision of all necessary equipment that you need to deliver top performances,Employees are self-reliant and free to schedule their own work day,Onboarding program including Welcome Day, Trainings and individual incorporation,The chance to work together on our goals, while also working to create something new,700 motivated colleagues and a cool office loft in the trendy district of Kreuzberg", "Strong programming skills in C++, Java, Scala, or Python,Deep understanding of basic data structures and algorithms,Experience with scaling data platforms to hundreds of terabytes or petabytes using Spark or Hadoop,Familiarity with modern machine learning techniques,Creative, collaborative, & product focused", "Minimum of 2 years development experience.,Experience in data analytics, business intelligence, or data science,Proficient in SQL,Strong programming skills,Experience with designing and building RESTful API services,Strong experience with ETL, must be able to interact with various data sources and connecting them to a data warehouse,Must be technically proficient and able to problem solve how data should be extracted, transformed and loaded into a warehouse.,This role will work collaboratively with the business departments it supports and must be able to understand the requirements of the data analytics group, and how their requirements can be technologically executed.,Bachelor's degree in a related field of study.,Experience with Amazon AWS , Azure, and PowerBI preferred, not required,Big Data Tools \u2013 MapReduce, Hadoop, Spark,Communication protocols JSON, XML,Amazon Redshift or SnowflakeBottom of Form", "Little to no travel", "We are offering an annual salary in the range of $120,000 - $140,000 USD.", "Roughly 5 (or more!) years of Industry experience on a data or machine learning team,Proficiency with modern programming languages (Go, Python, Java, Scala, etc.) and SQL,Some practical experience with probability, statistical modeling, or machine learning,Backend developer experience optimizing the data access layer in mature web applications,Experience building and working with real-time compute and streaming infrastructures (Kafka, Kinesis, Flink, Storm, Beam, etc.),Experience writing and debugging ETL jobs using a distributed data framework.,A deep and abiding appreciation for agile software processes, data-driven development, reliability, and responsible experimentation,A collaborative attitude and a helpful personality,Health, dental, vision, and life insurance,401k matching program,Commuter benefits,Catered lunch and unlimited snacks,Unlimited reimbursement for work related books", "Experience with micro-services and kubernetes", "Strong Clevertech Community", "Experience with streaming data", "Provide report predicting the future data, based on existing information.,U-SQL is the new big data query language of the Azure Data Lake Analytics service.,Experience providing technical leadership and mentor other engineers for the best practices on the data engineering space.,Able to use data to frame out and solve problems.,Be comfortable with algorithms and data structures, such as dynamic array, linked list, stack, queue, binary\u2026,Experience with ingesting external data sources into an existing platform.,Create error reporting, install metrics, add data quality measures.,Follow best practices and security standards for all data, including personally identifiable data.,The Data Engineer I builds robust, fault-tolerant data\u2026,Advanced knowledge of application, data, and infrastructure architecture disciplines.,BS/BA degree or equivalent experience.,The data engineer designs and builds platforms, tools, and solutions that help the bank manage, secure, and generate value from its data.,\uf0b7 Designs data solutions for data distributions and.,Verification, data modeling, and data mining.,Automation of data, data platforms, and tools.,Improve data usability and data quality in our data warehouse.,Experience with dimensional data modeling and schema design in a database or data warehouse.,Around 4 years of experience working as data engineer / data warhousing projects managing petabytes of data.,Data engineering: 3 years (Preferred).,Data Vault 2.0 data architecture.,Identify anomalies or inconsistencies in data.,Large scale data flows (Kafka).,Strong written and verbal communication skills.,Location: DE (the position that would be remote to start and then sit onsite in Delaware post-pandemic).,Experience building and optimizing 'big data' data pipelines, architectures and data sets.,Optimize and maintain data pipelines and architecture for the data\u2026,Work with data engineers and data scientists to drive efficient solutions from the platform.,Help define the data story and enable data-driven solutions at\u2026", "Gym membership compensation", "Company events: happy hours, bowling, bocce league, etc.", "BS/MS degree in Computer Science, Engineering or related field,Experience building processes that extract, process and add value to data sets from multiple source systems.,Experiencing with data modeling and tuning of relational as well as NoSQL datastores (Oracle, Red-shift, Impala, HDFS/Hive, Athena, etc.),Experience working with distributed computing tools (Spark, Hive, etc.),Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Lambda,Experience with data pipeline and workflow management tools: Airflow, etc.,Experience with one or more general purpose programming languages, including but not limited to: Java, Scala, C, C++, C#, Swift/Objective C, Python, or JavaScript.,Experience working with agile development methodologies such as Sprint and Scrum", "Expert level SQL skills,7-10 years experience in database technologies (i.e., Postgres, MySQL, SQL Server, Oracle, RedShift etc.),Minimum 5 years of experience in Data Warehousing,Working knowledge of dimensional modeling techniques,Working knowledge of data quality approaches and techniques,Experience with Redshift is highly desired,Experience with AWS tools (S3/Redshift/DynamoDB/IAM) is highly desired,Experience working with a standard ETL tool (i.e., Informatica, SSIS, Talend, Pentaho, etc.),Architectural insight on where to store data and modeling experience to recommend how it should be structured to make it accessible, performant, and resilient to change,An entrepreneurial spirit, a drive to ship quickly, and familiarity with agile software development practices,The ability to deal with ambiguity, communicate well with partner teams \u2013 both technical and non-technical, and a strong empathy for the customer experience,Experience working with Linux is a plus,Programming language experience (Python, Java, etc) is a plus,API development experience is a plus,Working with the Agile/Scrum development process is a plus", "Masters' Degree,4+ years of experience with AWS", "Work with awesome companies around the world. We partner with great software companies all over the world and you'll constantly get to interact with people from these great companies", "Working-knowledge of data integration, data movement, global database administration, traffic shadowing, test driven development, and software development life cycle.,Big data dimensional design leveraging star or snowflake schema.,SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), and SQL Server Analysis Services (SSAS) or other reporting services.,Python, R, Tableau, or other programming languages and analysis tools preferred but not required.,Knowledge of Agile methodology and frameworks like Scrum, Kanban, XP,Build and execute Big Data strategies, systems, and platforms addressing the capture of data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source.,Cluster enterprise relational and document-oriented databases with RESTful services and traffic mirroring.,Integrate database strategies with data larger than 1TB using relational databases, especially MS SQL Server.,Database Administrator 1,Database Administrator 2", "2+ years field experience implementing ETL/data integration in an enterprise IT environment.,Bachelor's Degree (or above) in computer science, data sciences, information sciences, software engineering, or a similar field is required,Commitment: Full time, 40-50 hours/week.,Travel: the basic expectation for travel within North America is approx. 25%.,Candidates are required to pass a background screen including a criminal history, reference check, bankruptcy and drug screen.", "Independent worker able to manage project timelines and priorities;,Advanced analytical thinking and problem-solving skills;,Strong communication skills with the ability to compile and present information;,Good interpersonal skills and demonstrated ability to work within a team environment,Experience working in an international organisation or multicultural environment will be an added advantage.,Understanding of WFP\u2019s Core mission and values.,Language skills: Fluent English (spoken and written).", "Bachelor's degree in Computer Science, Software Engineering, MIS or equivalent combination of education and experience,Development experience building ETL graphs using the Ab Initio GDE, EME and Co-Operating system,Strong SQL development skills,Development experience with at least two different programming languages (Python, Java, Scala, etc.),Development experience with Unix tools and shell scripts,Development experience with at least two different database platforms (Teradata, Oracle, MySQL, MS SQL, etc.),Minimum of 4-5 years experience designing, developing, and testing software aligned with defined requirements,Experience tuning SQL queries to ensure performance and reliability,Competitive salary with performance-based bonus opportunities,Single and Family Health Insurance plans, including Dental coverage,Short-Term and Long-Term disability,Matching 401(k),Competitive Paid Time Off,Training and Certification opportunities eligible for expense reimbursement,Team building and social activities", "5-10 years of technologies experience (Tech Lead experience a plus),Bachelor in Computer Science or related field, or equivalent experience,3-5 years experience with Spark, Scala, Java (Python is a plus),3-5 years experience working with Hadoop, SQL, no-SQL platforms (MongoDB a plus),3-5 years experience with various data types (AVRO, JSON, PARQUET a plus),Experience with development, management, and manipulation of large, complex datasets,Demonstrated knowledge of data management competencies and implementation", "Be a fan of working in agile scrum environments", "BS/MS degree with 6+ experience or Ph.D. degree with 3+ years of experience in a drug development setting.,4+ years of biologics downstream process development experience using a FPLC system (e.g. AKTA),Expertise in method/protocol development and scale-up for biologics purification, including, but not limited to affinity, cation exchange (CEX), anion exchange (AEX), viral filtration, and tangential flow filtration (TFF) is required.,Familiarity with HPLC based analytical methods (SEC, RP-HPLC, ion-exchange, hydrophobic interaction chromatography, etc),Prior experience with IND regulatory requirements, GMP environments, and quality documentation is highly desired.,Demonstrated ability to work independently with minimal supervision.,Familiarity with Design of Experiment (DOE) and comfortable working with analytical tools to create high-quality unbiased analyses.,Prior experience with a successful technology and process transfers to third party vendors (CRO/CDMO/CMO) is highly desired.,Experience mentoring, coaching, and training junior staff members are preferred.,Excellent interpersonal skills, including clear, succinct, and timely communication and proven ability to foster strong relationships with other team members and stakeholders.,Highly self-motivated and detail-oriented, with proven ability to work in a dynamic, fast-moving team.,Comfortable working with a data-driven team and collaborating with research scientists, data scientists, and platform engineers.,Familiarity or experience with scripting and statistical analysis of your data.", "Bachelor's or Master's degree in Life Sciences, Computer Science or Engineering,Experience in Software Engineering and Development,Strong learning agility, ability to pick up new technologies used to support Commercialization data analysis needs,Experience in Planisware,Strong experience working with agile methodology & DevOps (Jenkins, JIRA, Github) frameworks with successful experience working in a collaborative team environment,Experience working with container technologies (e.g., Docker) and developing microservices,Proficiency in software development languages including but not limited to Java and C#,Experience in Cloud (AWS, databricks platforms) & HPC (high performance computing) environments,Experience in developing and supporting web applications including familiarity with web technologies and frameworks (EXTJS, D3 JS, React.js),Data analysis and reporting experience by using analytics, visualization and database technologies (Oracle, PL SQL, Spotfire, Tableau, Python - NumPy, SciPy, Pandas),Expert in R scripting and R development in R Shiny;,Experience processing and analyzing large NGS data;,Expertise with translating business requirements to technical requirements and recommend solutions,Working with leading agile development methodologies such as Sprint and Scrum,Knowledge of or experience in Life, Physical or Computational Sciences,Strong written and oral communication skills", "Bachelor's Degree is required,At least 2 years of employment as a data engineer in a professional setting, or other relevant development experience,Expert in python,Expert working with cloud platforms (AWS, Google Cloud, etc),Experience with Airflow or other workflow management software,Ability to define data model and data storage strategies, including knowledge of distributed data systems,Ability to manage multiple/competing priorities and make the right tradeoffs and timely delivery of features,Experience or familiarity with geography, geometry and GIS systems,Experience working with satellite/remote imagery,Relevant education (Coding Bootcamp, and/or Bachelors in Computer Science) or equivalent experience,Be part of a well-funded early-stage start-up,Market competitive comp and equity incentives to give you a stake in our future,Medical, Vision and Dental for you and your dependents,Pre-tax commuter & parking benefits,Flexible Time Off,An upbeat and collaborative work culture,Company-sponsored outings", "Experience with Informatica tools (PowerCenter, Big Data Management, Master Data Management), Cloudera CDH and ecosystem tools (SOLR, Spark, Impala, Hive, Hue, etc...), MarkLogic, SAS Analytics, python, R and Amazon Web Services preferred.", "BS/MS degree in Computer Science, Engineering or related field,5 or more years of experience designing complex and inter - dependent data models for analytic , Machine learning use cases.,5 or more years of experience architecting and building processes that extract, process and add value to data sets from multiple source systems.,Experiencing with data modeling and tuning of relational as well as NoSQL datastores (Oracle, Red-shift, Impala, HDFS/Hive, Athena, etc.),Experience working with distributed computing tools (Spark, Hive, etc.),Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Lambda,Experience with data pipeline and workflow management tools: Airflow, etc.,5 or more years of experience with one or more general purpose programming languages, including but not limited to: Java, Scala, C, C++, C#, Swift/Objective C, Python, or JavaScript.,5 or more years experience working with and leading agile development methodologies such as Sprint and Scrum,Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops.,Experience with Semantic technologies and approaches is a plus.,Biotech / Pharma experience is a plus,Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc) is a plus.", "Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.,Experience building and optimizing big data' data pipelines, architectures and data sets.,Strong analytic skills related to working with unstructured datasets.,Build processes supporting data transformation, data structures, metadata, dependency and workload management.,We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a degree in Computer Science, Statistics, Informatics, Information Systems or relevant experience. Below is a list of the type of tech we use, though we don't expect you to have experience in all of it:\nBig data tools: Hadoop, Storm, Cassandra, etc.\nRelational SQL and NoSQL databases, including Postgres, MySQL, MSSQL.\nData pipeline and workflow management tools such as Google Cloud Dataflow\nCloud services: AWS Redshift, Google BigQuery, etc\nStream-processing systems: Kafka, Storm, Spark-Streaming, etc.\nObject-oriented/object function scripting languages: Python, Java, C++, Scala, etc.,Big data tools: Hadoop, Storm, Cassandra, etc.,Relational SQL and NoSQL databases, including Postgres, MySQL, MSSQL.,Data pipeline and workflow management tools such as Google Cloud Dataflow,Cloud services: AWS Redshift, Google BigQuery, etc,Stream-processing systems: Kafka, Storm, Spark-Streaming, etc.,Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.", "Clearance TS//SCI or TS/SCI with current CI scope Polygraph (with one year of currency minimum) OR willing to undergo CI scope Polygraph (based on specific analytical position) PLUS,Education Bachelors Degree OR 10+ years direct relevant experience PLUS,Experience 10+ years of analytical experience (with 8+ years of Identity Intelligence or functional/regional all-source analysis experience) at the operational/strategic level within DoD or equivalent Government agencies,Strong briefing skills to include the ability to clearly articulate information to senior members of the intelligence community,Ability to gather, analyze and collate and fuse available intelligence products to produce IIRs, reports, and briefings including the ability to clearly articulate information,Education: Masters Degree in related field,Certification: Counter Terrorism/Counter Insurgency, Global Regional Issues, HUMINT, CI, POL/MIL/Geopolitical analysis; Senior Intelligence Analysis with familiarity of ICD 203, 204 and 206.\nCompeted OS301 Fundamentals Course\nCompleted OS302 OSINT Analytic Tools Course\nCompleted a Basic Social Media Analysis Course\nComplete an Advanced Social Media Analysis Course,Competed OS301 Fundamentals Course,Completed OS302 OSINT Analytic Tools Course,Completed a Basic Social Media Analysis Course,Complete an Advanced Social Media Analysis Course,Experience: 12 years of experience related to the specific labor category with at least a portion of the experience within the last 2 years,Equivalency Chief Warrant Officer 3-5; Field Grade Officer (O4- O5) JISE/ACE Director or Deputy Director", "Connections to recruiters and industry experts through online and live Devex events", "8 - 10 years' application development and support experience.,Deep knowledge of distributed data architecture, commonly-used BI tools, and approaches/packages deployed for machine learning build,Experience creating production software/systems and a proven track record of identifying and resolving performance bottlenecks for production systems.,Experience in machine learning algorithm design, feature engineering, validation, prediction, recommendation, and measurement.,Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets.,Good understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant,Experience planning, organising, and managing multiple large projects with diverse cross-functional teams,Demonstrated ability to incorporate new techniques to solve business problems,Post Graduate Degree in Information Technology,Qualification in Computer Science or Engineering ideal.,Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark.,Working knowledge of Hadoop ecosystem and associated technologies, e.g., Apache Spark, MLlib, GraphX, iPython, sci-kit, and Pandas,Advanced experience in writing and optimizing efficient SQL queries and Python scripts; Scala and C++ experience is ideal,Deliver results within committed scope, timeline and budget,Very strong people/project management skills and experience,Ability to travel within CEMEA on short notice,Results-oriented with strong problem solving skills and demonstrated intellectual and analytical rigor,Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred,Team oriented, collaborative, diplomatic, and flexible style,Very detailed oriented, is expected to ensure highest level of quality/rigor in reports and data analysis,Proven skills in translating analytics output to actionable recommendations and delivery,Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels,Exhibits intellectual curiosity and a desire for continuous learning,Exhibits intellectual curiosity and a desire for continuous learning,Demonstrates integrity, maturity and a constructive approach to business challenges,Role model for the organization and implementing core Visa Values,Respect for the Individuals at all levels in the workplace,Strive for Excellence and extraordinary results,Use sound insights and judgments to make informed decisions in line with business strategy and needs,Leadership skills include an ability to allocate tasks and resources across multiple lines of businesses and geographies. Leadership extends to ability to influence senior management within and outside Analytics groups,Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions", "Be wary of Google Hangout or Skype interviews as these are not publicly-listed numbers that can be used to verify the legitimacy of the interviewer.", "Connections to recruiters and industry experts through online and live Devex events", "BS/MS degree in Computer Science, Engineering or related field,3 or more years of experience architecting and building processes that extract, process and add value to data sets from multiple source systems.,Experiencing with data modeling and tuning of relational as well as NoSQL datastores (Oracle, Red-shift, Impala, HDFS/Hive, Athena, etc.),Experience working with distributed computing tools (Spark, Hive, etc.),Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Lambda,Experience with data pipeline and workflow management tools: Airflow, etc.,3 or more years of experience with one or more general purpose programming languages, including but not limited to: Java, Scala, C, C++, C#, Swift/Objective C, Python, or JavaScript.,3 or more years experience working with and leading agile development methodologies such as Sprint and Scrum,Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops.,Experience with Semantic technologies and approaches is a plus.,Biotech / Pharma experience is a plus", "Minimum of 3 years of professional software development experience in a hands-on data-centric role in data engineering, architecture, streaming or warehousing is REQUIRED.,Experience with at least one modern server-side language (such as Python, Java, or similar) is REQUIRED.,Experience with at least one relational database technology (MySQL, Postgres, MS SQL, etc.) is REQUIRED.,Proficient with SQL is REQUIRED.,Familiarity with cloud services and infrastructure, preferably AWS,Familiarity with columnar store databases (Vertica, Redshift, Snowflake, etc.),Experience building out data ingest pipelines,Experience building out or working within Extract, Load, Transform / Data Lake architectures,Comfortable working with a mix of structured & unstructured data from a variety of sources is preferred,Experience with schema design,Experience with Spark, DataFrames, pandas a big plus,Experience transforming partially or fully unstructured data into more easily queryable formats,Experience tuning databases for performance,Experience working in an agile environment such as Scrum or Kanban,Knowledge of machine learning tools & concepts a plus,Fun, collaborative environment,Optional work-from-home Wednesdays,Competitive compensation and benefits package, including medical, dental, and vision insurance,5 weeks of PTO and 10 paid holidays (total 7 weeks),401k,Stock options,Commuter benefits,Stocked kitchens, with coffee, soda, and snacks,Regular team activities, including Mariners games, ping pong tournaments, movies, etc.", "A competitive salary", "BS in Computer Science, Computer Engineering, Data Analytics/Data Science, Physics, Mathematics, Information Systems, Engineering or other quantitative disciplines.,Experience in extracting, processing, curating, integrating, and analyzing data using Python, Spark, SQL,Hands on experience with AWS services \u2013 Kinesis, S3, Glue, Lambda, Cloudformation, RDS, EC2, EMR or HDFS, Hadoop Yarn, Hbase, Hive, Pig,Hands on experience in ELT/ETL and dimensional data modeling,Proficiency in Python and at least one SQL language such as T-SQL or PL/SQL,Excellent knowledge of relational and non-relational database systems,Ability to think creatively, and solve problems,Ability to work in a highly collaborative and dynamic work environment,Effective written and oral communication skills,Experience in building production data pipelines using Python, SQL, Spark and AWS environment (Kinesis, S3, Glue, Lambda, Cloudformation, RDS) or HDFS (Hadoop Yarn, Hbase, Hive, Pig),Strong programming experience in Python, Spark, Scala,Experience in ETL/ELT and dimensional data modeling,Experience in working with relational/non-relational databases and advanced SQL/NoSQL scripting,Familiarity with data pipeline and workflow management tools such as Airflow, AWS Step functions, Nifi", "Understand the business use of data to support work processes and strategic business objectives. Leverage data, analytics and data science techniques to create business value through solution delivery and/or self-service enablement.,Identifies, acquires, and cleanses/prepares data (including data architecture) aligned with defined architecture patterns.,Collaborates with data scientists to identify, build and integrate advanced analytics models into solutions.,Enables the data scientists to develop the most advanced analytics models.,Uses an agile approach to develop analytic solutions and technical components of the platform (data ingestion, data preparation, analytics processing, visualization).,Deploys solution including model, documentation, training, integration.,B.S or M.S. in Computer Science or related field,5+ years industry experience developing data products and/or analytics solutions (business intelligence and data warehousing, data science\u2026),Data Architecture, modeling and Integration\nExperience in Analytics/Data Product solution architecture (Hadoop, Microsoft Azure Analytics)\nExperience in Data Extract, Load and transformation technics and tools (Hadoop, SSIS, Microsoft Azure Data Factory).\nUnderstanding of relational and dimensional data models.,Experience in Analytics/Data Product solution architecture (Hadoop, Microsoft Azure Analytics),Experience in Data Extract, Load and transformation technics and tools (Hadoop, SSIS, Microsoft Azure Data Factory).,Understanding of relational and dimensional data models.,Data Visualization and Analytics\nExperience with analytics solutions (Microsoft Azure Analytics Paas/Saas).\nExperience in Data Visualization (Spotfire, PowerBI).,Experience with analytics solutions (Microsoft Azure Analytics Paas/Saas).,Experience in Data Visualization (Spotfire, PowerBI).,Software Engineering\nExperience in software engineering, Agile and DevOps methodologies and tools (Jira, Jenkins, VSTS).\nExperience with programming languages / scripting tools (Java, Python, R, C).\nExperience with information security management.,Experience in software engineering, Agile and DevOps methodologies and tools (Jira, Jenkins, VSTS).,Experience with programming languages / scripting tools (Java, Python, R, C).,Experience with information security management.,Learning Agility\nWork productively in uncertain and fast changing environments. Finds opportunities in ambiguity; Embraces and adapts to change; adept of learning by doing.,Work productively in uncertain and fast changing environments. Finds opportunities in ambiguity; Embraces and adapts to change; adept of learning by doing.,Analytical thinking and problem solving:\nKnowledge of techniques and tools that promote effective analysis, determine the root cause of problems and create alternative solutions to solve them in the best interest of the business.,Knowledge of techniques and tools that promote effective analysis, determine the root cause of problems and create alternative solutions to solve them in the best interest of the business.", "10+ years of relevant experience,BS/MS/PhD in Computer Science or related field,Experience with Java, Scala & Python,Expert knowledge of machine learning algorithms and operationalization of data science pipelines,Demonstrable experience with ETL/ELT tools,Expert Knowledge of distributed data processing such as Hadoop ecosystem and spark,Strong knowledge of SQL (eg: MySQL) & Linux,Comfortable with Data Security Concepts & SDLC,Familiarity with leading cloud vendors such as GCP, Azure, AWS and related tools", "Bachelors Degree,At least 2 years of experience using database management tools (SQL),At least 1 year of experience creating data visualizations using Tableau,At least 1 year of experience using relational database systems (Snowflake, PostgreSQL, or MySQL),1+ years of coding experience,2+ years of experience creating Tableau visualizations,2+ years of experience with relational database systems including Snowflake, PostgreSQL, or MySQL,3+ years of experience building data pipelines and using ETL tools,3+ years of experience creating automated solutions", "Experience and demonstrated proficiency in database development and management;,Experience extracting, cleaning, and transforming data for analysis required;,Experience preferred in a higher education data analysis and reporting setting (institutional research, enrollment analysis, etc.) and/or a higher education IT setting;,Advanced knowledge of SQL required;,Advanced knowledge of relational databases required;,Experience preferred with SQL server and Oracle;,Advanced practical experience with SQL-based data query tools such as BRIO/Hyperion, Toad, and Toad Intelligence Central strongly preferred;,Practical experience with data management tools such as Alteryx preferred;,Familiarity with dimensional data modeling concepts (e.g. Star Schema) preferred;,Familiarity with Python and/or other open source data engineering tools a plus;,Strong orientation to detail;,Strong organization skills;,Excellent interpersonal and communication skills, including a strong customer service orientation;,Ability to manage concurrent projects and activities with a high degree of quality and data accuracy;,Ability to meet deadlines and deliver projects in a timely manner;,Bachelor\u2019s degree required, master\u2019s degree preferred.", "Bachelor's degree in Computer Science, Engineering or a related field, or equivalent training, fellowship, or work experience", "BS or MS in computational sciences (computational biology, bioinformatics, or related fields), computer science, applied math/statistics,Experience in scientific programming,Experience in data analysis (especially as pertains to protein sequence and structure, and/or next-generation DNA sequence analysis),Excellent problem-solving skills,Knowledge of biochemistry, bioinformatics, protein structures, molecular biology, and NGS data analysis,Ability to execute and automate standard statistical analyses using tools/languages including UNIX, Python, R, SQL, etc.,Knowledge of cloud computing, AWS, Docker, etc.,Experience in scientific software administration and tool development,Strong communication skills", "B.S. Degree in Engineering or related discipline with 8 to 12 years of experience in pharmaceutical/biotech (6 to 10 years of experience with Master\u2019s Degree)", "Bachelor\u2019s degree or equivalent experience,None,Database development knowledge and experience (i.e. SQL),Programming skills (i.e. Python, R, Java),Computer Proficiency in Oracle, UNIX/Linux,Intermediate Analytic, Data Sourcing, and Data Management skills,Ability to extract data from various data sources.,Solid experience in time and task management,Ability to learn new technologies,Strong attention to detail,Intermediate written and verbal communication skills, including the ability to effectively collaborate with multi-disciplinary groups and all organizational levels", "3 or more years working in Agile and/or devOps teams (SCRUM),Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Lambda,Experience with data pipeline and workflow management tools: MuleSoft, Informatica Cloud, etc.,Familiarity with various application technology stacks (BI stacks) and technology domains (Data Analytics),Working experience with data analytic tools (e.g., Tableau, Splunk, Spotfire, Hadoop),Clear understanding of Relational and Dimensional database modeling,Skilled programmer with sufficient experience in high level programming languages such as C++, C#, Java, Python, Visual Basic,Experience programming in compiled (C, C++) and interpreted languages (Python, Ruby etc.),DevOps experience building and deploying infrastructure with cloud deployment, build and test automation technologies like ansible, chef, puppet, docker, jenkins, etc.,Experience with Big Data, Advanced Analytic techniques and Real Time data,Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc.),Demonstrated ability to adopt new cloud technologies and other paradigms such as emerging Big Data technologies (Hadoop, hive, etc.),Exceptional teaming skills encompassing cross-functional teams, peer relationships, informing, understanding and appreciating differences,Proven experience as a member and leader of a high performing team,Strong ability to convey and influence complex technical issues in a manner that is easily understood and actionable,Experience applying change management methodologies in large / global corporate environments involving multiple businesses,Certified Information Security Manager (CISM),CompTIA Security+,Certified Information Systems Security Professional (CISSP)", "Demonstrated knowledge of SQL and relational databases such as PostgreSQL. Professional experience with complex data systems strongly preferred; experience with public sector data and data systems preferred.,Programming experience in either Python or R (experience in both is preferred).,Familiarity with version control systems such as git is preferred.,Ability to balance many competing demands, as well as to demonstrate good decision-making and initiative.,Excellent written and verbal communication skills, including in discussing technical concepts with non-technical users.,Ability to work independently with a high degree of initiative required.,Ability to manage multiple, concurrent tasks required.,Demonstrated judgment and discretion in the handling of sensitive information required.,Must be able to remain in a stationary position for extended periods of time.,Must be able to operate a computer extensively for four (4) or more hours per day.", "Bachelor\u2019s, Master\u2019s, or Ph.D. degree in computer science/electrical engineering or relevant field,Proven and significant experience building and managing data pipelines.,Ability to build and manage data pipelines with Python, Scala or Java.,Knowledge of software engineering best practices such code reviews, testing frameworks, maintainability and readability.,Commercial client-facing project experience is helpful, including working in close-knit teams,Ability to work across structured, semi-structured, and unstructured data, extract information and identify linkages across disparate data sets.,Meaningful experience in multiple database technologies (such as Hadoop, MS SQL Server, Oracle, MySQL, Teradata).,Confirmed ability in clearly communicating complex solutions,Deep understanding of Information Security principles to ensure compliant handling and management of process data,Experience and interest in Cloud platforms such as: AWS, Azure, or Google Platform,Familiarity with data warehousing and deploying ETL processes with Python. Extraordinary attention to detail.,Strong organizational and interpersonal skills: can get things done in a way that optimizes results, strengthens internal and external relationships, and with consideration of resources.,Knowledge of cGMP\u2019s, Health Authority regulations, and Quality Systems.,Work Environment/Physical Demands/Safety Considerations,Ability to work in international/global environment. 10-30% travel anticipated.,May work in the clean room environment that requires gowning in the form of hospital scrubs,,Ability to sit, stand and move within work space for extended periods", "Be wary of Google Hangout or Skype interviews as these are not publicly-listed numbers that can be used to verify the legitimacy of the interviewer.", "You understand our desire to be the best place to work and that trust is the foundation of that.", "Proven attention to detail and proactive communication,Proficient in one of the following coding languages: Python, Java, Scala,Demonstrable experience writing SQL using any RDBMS (Redshift, Postgres, MySQL, Teradata, Oracle, etc.),Experience with Schema Design & Dimensional data modeling,Experience with AWS Services like EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, and API gateway,Experience with software DevOps CI/CD tools, such as Git, Jenkins, Linux, and Shell Script,Experience with Spark, Hive, Kafka, Kinesis, Spark Streaming, and Airflow,Hands-on experience using Databricks/Jupyter or similar notebook environment,Experience building data warehouses and ETL pipelines a plus\nOur culture is what makes Amgen a special place to work. We have a powerful shared purpose around our mission - to serve patients. We respect one another, recognize contributions, and have embedded collaboration, trust, empowerment and inclusion in all that we do.\nWe equip all our staff members to live well-rounded, healthy lives. Most recently, Amgen added benefits for transgender employees and continues to pride itself on industry-leading, family-friendly offerings for families of all compositions.\nAmgen focuses on areas of high unmet medical need and uses its expertise to strive for solutions that improve health outcomes and dramatically improve people's lives. A biotechnology pioneer since 1980, Amgen has grown to be one of the world's leading independent biotechnology companies, has reached millions of patients around the world and is developing a pipeline of medicines with breakaway potential.\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.", "Embed with our billing team to create billing pipelines that enable more granular bills and help our better users understand their costs.", "BS/MS degree in Computer Science, Engineering or related field,3 or more years of experience architecting and building processes that extract, process and add value to data sets from multiple source systems.,Experiencing with data modeling and tuning of relational as well as NoSQL datastores (Oracle, Red-shift, Impala, HDFS/Hive, Athena, etc.),Experience working with distributed computing tools (Spark, Hive, etc.),Experience with AWS cloud services: EC2, EMR, RDS, Redshift, S3, Lambda,Experience with data pipeline and workflow management tools: Airflow, etc.,3 or more years of experience with one or more general purpose programming languages, including but not limited to: Java, Scala, C, C++, C#, Swift/Objective C, Python, or JavaScript.,3 or more years experience working with and leading agile development methodologies such as Sprint and Scrum,Experience with Software engineering best-practices, including but not limited to version control (Git, TFS, Subversion, etc.), CI/CD (Jenkins, Maven, Gradle, etc.), automated unit testing, Dev Ops.,Experience with Semantic technologies and approaches is a plus.,Biotech / Pharma experience is a plus,Full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc) is a plus.", "An existing affinity with: Data Science, Machine Learning, Cloud Computing, Networking, Security, Encryption, RESTful interfaces and DevOps", "Computer Science or Computer Engineering or Mathematics degree,Strong analytical and problem-solving skills,Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc),Experience in data modeling, ETL development, and data warehousing,Data Warehousing Experience (for example Oracle, Redshift, Teradata, etc.),Experience building data products incrementally and integrating and managing datasets from multiple sources,Knowledge of Mathematical Statistics,Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.) a plus,Good English,Free Glovo credits,The opportunity to change the world and see how everyone uses the product you build,Work in an international, dynamic and passionate environment with a great company culture,We consider only variants of relocation to Barcelona, Spain. We cover visa and relocation costs.", "Various experience levels considered. Junior candidates must have a strong background of coursework or academic projects around data engineering or machine learning at scale or have appropriate industry experience contributing to such projects.,Senior candidates must demonstrate a track record of successful technical leadership in the execution of large-scale data projects.,Software Engineering \u2013 Level-appropriate experience in software engineering and SDLC (our stack may include Python, Golang and Scala). Must consider code readability, reuse, and extensibility a priority when developing solutions.,Big Data Engineering - Experience in building scalable data pipelines involving machine learning, optimization or prediction,Big Data Devops - Experience in performing operations and automation of various big-data ecosystems in production environments on AWS or a related cloud service,Ability to thrive in a fast paced, cross regional, diverse, and dynamic work environment,Experience with AWS data stack \u2013 Redshift, Athena, EMR, Kinesis, DocumentDB, DynamoDB,Experience with establishing well-organized data lakes,Experience setting up and optimizing data warehouses,Background in data modeling and performance tuning in relational and no-SQL databases,Experience with data practices (security, data management and governance),Experience in operations research, machine learning or optimization", "Graduate degree in Computer Science, Information Systems or equivalent quantitative field and 5+ years of experience in a similar Data Engineer role.,Experience working with and extracting value from large, disconnected and/or unstructured datasets,Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management,Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.,Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets.,Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.,Experience developing IT frameworks that support business applications.", "Connections to recruiters and industry experts through online and live Devex events", "10+ years of relevant experience,BS/MS/PhD in Computer Science or related field,Experience with Java, Scala & Python,Expert knowledge of machine learning algorithms and operationalization of data science pipelines,Demonstrable experience with ETL/ELT tools,Expert Knowledge of distributed data processing such as Hadoop ecosystem and spark,Strong knowledge of SQL (eg: MySQL) & Linux,Comfortable with Data Security Concepts & SDLC,Familiarity with leading cloud vendors such as GCP, Azure, AWS and related tools", "5 years of relevant experience with data tools, techniques, and manipulation required.,Education:,College Degree in STEM related field,Technical Knowledge:,Advanced knowledge of data tools, techniques, and manipulation preferred. Examples (but not limited to):,Data Science/Engineering, Big data and Cloud platforms,Programming languages - SAS, SQL, Spark, Python, R, H2O, KNIME, Hive, AWS,Development and Visualization platforms: Python Notebook, IDEs, GitHub, QlikView, Tableau, MicroStrategy and Qlik Sense,Experience:,Communication Skills,Ability to communicate thoughts/designs/ideas in an unambiguous manner and adjusts communication based on audience.,Exhibits active and effective communication skills with team members - including active listening and effective written and verbal communication skills.,Effectively contributes and communicates with the immediate team.,Able to present complex technical concepts to audiences of varying size and level.,Business Knowledge & Partnership,Able to develop business partnerships and influence business priorities through solution identification aligned with business objectives and goals. Able to communicate in business terms and describe data capabilities and concepts in ways that the business can understand.,Problem Solving & Decision Making,Able to proficiently diagnose root causes and solve complex issues. Able to evaluate alternative solutions and assess risk before taking action. Has the ability to reach sound decisions quickly and escalates appropriately. Demonstrates ability to optimize the use of all available resources.,Team Orientation,Able to maintain and enhance partnerships across the organization to achieve objectives. Practices objectivity and openness to others' views. Able to recognize and support team priorities.,Leadership,Accountable to set technical goals and priorities for self and other team members. Exhibits team leadership and collaborates with partners.,Planning and Project Management,Demonstrates ability to identify critical project tasks and establish clear priorities while keeping the bigger picture in mind. Able to effectively collaborate with Project Manager and utilize sound project management practices. Able to manage time and competing priorities.,Financial Awareness", "Experience with the Hadoop eco-system (HDFS, Spark)", "15 years of work experience, including 10 years of experience in data engineering,Processing and extracting value from large disconnected datasets,Continuous integration systems (Jenkins, Travis, Drone CI)", "15 years of work experience, including 10 years of experience in data engineering,Processing and extracting value from large disconnected datasets,Continuous integration systems (Jenkins, Travis, Drone CI)", "A competitive salary", "Work with cool people and impact millions of daily players!", "At least five years of proficiency on SQL Microsoft stack,Banking or financial sector experience beneficial,Fove years of .Net Language experience,Visualisation and Storyboarding experience,Stats experience,Assist in driving data services strategy across multiple platforms,Building in operation frameworks/processes to take data services to the next level of excellence,Building out of Big Data Platform and consolidating Group Data for provisioning via required channels,Operational/analytical data provisioning and insights into data landscape,Understanding and optimising Data Flow Patterns in order to rationalise data inputs,Shaping of unstructured data acquisition and Realtime Data Integration Patterns,Engagement and shaping Data Initiatives to ensure Strategic Alignment to Group Architecture", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "Experience in designing, implementing and supporting highly scalable data systems and services in Java and/or Scala,Experience with Hadoop-ecosystem technologies in particular MapReduce, Spark / Spark-SQL / Spark Streaming, Hive, YARN/MR2,Experience building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components,Experience in data-modeling and data-architecture optimized for big data patterns, ie. warehousing concepts; efficient storage and query on HDFS; data security and privacy techniques),Knowledgable about distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues", "Minimum 5 years of experience in an analytics and data engineering role,BS or MS degree in a quantitative field or equivalent practical experience,Mastery of relational databases (SQL or MySQL),Ability to translate business processes and data into analytics solutions.,A strong foundation in data modeling, including at least 3 years of relevant business or people analytics experience, and a commitment to data governance,Demonstrated ability to manage technical and non-technical stakeholders and drive collaboration,Experience with Python or any scripting language a plus,Experience with Tableau a plus,Desired Attributes,Outstanding written, verbal, and visual communication capabilities,Ability to pivot between widely divergent tasks and subject matter on short notice, and rapidly adapt to varied audiences.,A record of adding value to work outcomes through innovation and an orientation toward continuous refinement and improvement,Highly thorough and detail-oriented", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "5 years of experience as a Software Engineer,Excellent programming skills - e.g. Python, Go, Java,Excellent problem-solving and analytic skills,Solid computer science and systems foundations; ability to quickly learn new domains,Proven system development skills in UNIX-type OS (e.g. Linux, Mac OS),Experience working with large data sets and pipelines, ideally using the Apache software stack (e.g. Spark, HBase),Experience with continuous integration and continuous development solutions (e.g. Jenkins, etc.),Experience with cloud-native deployment (e.g. Kubernetes),Good communication skills and teamwork,Passion for building great products,Curiosity and desire to learn,The following experience is nice to have, but not required: Data modeling Experience working with search engines Machine learning Natural-language processing", "Work with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches;,Plan and execute secure, good practice data integration strategies and approaches;,Acquire, ingest, and process data from multiple sources and systems into Big Data platforms;,Working across different environments and technologies, learning the nuances of each \u2013Amazon Web Services, SQL, NoSQL, and Big Data / Hadoop platforms,Designing, setting up, and running ETL transformations using tools including Informatica PowerCenter, Data Quality, and Informatica Cloud,Create and manage data environments in the Cloud;,Focus on working with our financial services clients;,Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models;,Keep up to date with Information Security principles to ensure compliant handling and management of client data;,Involved in end-to-end data management for cutting edge Advanced Analytics and Data Science.,A graduate in a relevant subject;,Management consulting experience leading on client-facing projects, including working in close-knit teams,Experience working on projects within the cloud, preferably with AWS,In-house experience within a large financial institution preferred (on topics such as AML),A proven ability in clearly communicating complex solutions,Strong development background with experience in at least one scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R,Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models,Distributed Systems experience,Good experience in at least one Database technology: Traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Oracle Exadata, Teradata, IBM Netezza), Distributed Processing (Spark, Hadoop, EMR), NoSQL (MongoDB, DynamoDB, Cassandra, Vertica, Neo4J, Titan),Experience in at least one ETL tool (e.g. Informatica, SAP BODS),The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets,Excellent interpersonal skills when interacting with clients, both verbally, email, written, and in a clear, timely, and professional manner.,A deep personal motivation to always produce outstanding work for your clients and colleagues,Excel in team collaboration and working with others from diverse skill-sets and backgrounds", "Completed BSc (Computer Science) degree or similar (Software Engineering, Data Science, etc. with experience in software),Two to five years\u2019 experience in Data Engineering (Hadoop and Spark),Data Processing products - Big Query, Redshift, Spectrum, S3, Athena, Kafka, Spark, Storm, Flink, Beam, Presto, Hive,ETL processes and transformations,Cloud experience ideally with Google Cloud Platform,DevOps Stack development experience,Apache-Airflow or other data pipeline tools,Exposure to Scala or Java in context of data processing,Experience and proficiency with Python,Experience in the design and implementation of data flows,Support sophisticated predictive data products by maintaining data science production environments (cloud-based, python), ensuring that the outputs from Data Science models are available and integrated into the system and integrate, coordinate and maintain data flows between various sources of data,Manage and maintain cloud service integrations that perform key data functions by working towards replacing third-party elements of the data pipeline by using open-source tools,Make decisions around the infrastructure, layout and processes of the data warehouse, including working with the engineering team on how to best track and record data following up on data inconsistencies to ensure that it is corrected", "401K", "5+ years of hands-on experience in business intelligence or IT management role in corporate or consulting setting,Strong development background with experience in at least two scripting, object oriented or functional programming languages: SQL, Python, Java, Scala, C#, R,Client stakeholder engagement and management,Experience leading a work stream and managing small teams on Agile projects,Data Warehousing experience, building operational ETL / ELT pipelines comprised of several sources, and architecting Data Models/Layers for Analytics,Ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets,Experience in multiple Database technologies: Traditional RDBMS (MS SQL Server, Oracle, MySQL, PostgreSQL), MPP (AWS Redshift, Oracle Exadata, Teradata, IBM Netezza), Distributed Processing (Spark, Hadoop, EMR), NoSQL (MongoDB, DynamoDB, Cassandra, Vertica, Neo4J, Titan),Experience developing solutions in Cloud platforms: Amazon Web Services, Microsoft Azure, Google Cloud Platform,Experience generating Insights in the form of reports, KPIs, dashboards or ad-hoc queries; experience with Tableau is a bonus,Ability to thrive in a lively project and consulting setting, often working on different and multiple projects at the same time,Excellent interpersonal skills when interacting with clients, both verbally, email, written, and in a clear, timely, and professional manner,Problem solving and brainstorming solutions to data integration and Analytics challenges,Passion for developing your knowledge and skills in both the technical aspects of the Data Technology industry, and your personal and professional development in work and life", "50+ career categories", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Design and implement data monitoring solutions and procedures and continuously monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Design and implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Work together with business owners, analysts and IT to manage changes to data in the organisation.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics and liaise with IT infrastructure and IT Operations regarding system and infrastructure management.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Complex solution and service design and implementation.,Cross functional data and team knowledge gathering and sharing.,Responsible for team activities, team dynamics and performance.,Manage project and task delivery of team.,Multiple stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology/engineering/mathematics/statistics/actuarial or related discipline,At least 8 years\u2019 experience working in a data, business intelligence or analytics environment,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Planning and organizing,Presenting and Communicating information,Analysing,Leadership", "We\u2019re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.,Expert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.,A strong coding background in either Java, Python or Scala,The desire to learn and code in Scala,Experience in working in an Agile environment,Experience of building data processing pipelines for use in production \"handsoff\" batch systems, including either traditional ETL pipelines and/or analytics pipelines.,Competitive Salary,Company Bonus,Private Healthcare, Life Insurance and Income protection,Pension Scheme with a company contribution of 6% (if you contribute 3%),25 days annual leave (with the option to buy/sell up to 5 days),Amazing working environment,Employee referral scheme,ETL,Scala", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Virtual company-sponsored social events", "Team player, excellent communication skills", "Development experience with at least two different database platforms, such as Teradata, Oracle, or MS SQL.,Minimum of 5 years experience designing, developing, and testing ETL interfaces aligned with defined requirements.,Exposure to Business Intelligence tools such as Business Objects, Informatica, SSRS, Cognos, MicroStrategy, Tableau, QlikView, SpotFire, etc.,Experience tuning ETL processes to ensure performance and reliability.,MDM experience.,Competitive salary with performance based bonus opportunities,Single and Family Health Insurance plans, including Dental coverage,Short-Term and Long-Term disability,Matching 401(k),Competitive Paid Time Off,Training and Certification opportunities eligible for expense reimbursement,Team building and social activities", "Experience with Spark, Google Big Query, Redis, Amazon Aurora, Dynamo DB, Kinesis or Riak.", "Completed BSc (Computer Science) degree or similar (Software Engineering, Data Science, etc. with experience in software),Two to five years\u2019 experience in Data Engineering (Hadoop and Spark),Data Processing products - Big Query, Redshift, Spectrum, S3, Athena, Kafka, Spark, Storm, Flink, Beam, Presto, Hive,ETL processes and transformations,Cloud experience ideally with Google Cloud Platform,DevOps Stack development experience,Apache-Airflow or other data pipeline tools,Exposure to Scala or Java in context of data processing,Experience and proficiency with Python,Experience in the design and implementation of data flows,Support sophisticated predictive data products by maintaining data science production environments (cloud-based, python), ensuring that the outputs from Data Science models are available and integrated into the system and integrate, coordinate and maintain data flows between various sources of data,Manage and maintain cloud service integrations that perform key data functions by working towards replacing third-party elements of the data pipeline by using open-source tools,Make decisions around the infrastructure, layout and processes of the data warehouse, including working with the engineering team on how to best track and record data,following up on data inconsistencies to ensure that it is corrected", "Machine learning and statistical analysis", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "5+ years experience building traditional data warehouse solutions, are knowledgeable about data modeling, data access, and data storage techniques.,You understand standard methodologies for ETL, and are proficient in debugging and optimizing pipelines.,You can easily transition from one ETL tool set e.g., Informatica to Kettle to more programmatic approaches such as Python/SQL or Spark/Scala,Extensive Experience with SQL and understanding of NoSQL solutions,You have significant coding experience in Java, Python and want to apply those skills to processing big data.,5+ years experience with object-oriented design, coding and testing patterns as well as experience in engineering open source software platforms and large-scale data infrastructures.,You have an understanding of distributed systems, NoSQL solutions such as Redshift or BigQuery.,Familiar with Google Cloud or other cloud provider products and servers,You are able to work in teams and collaborate with others to clarify requirements,You have a Bachelor\u2019s degree or Master\u2019s in information Technology or relevant discipline.,Experience with automation, build tools, release engineering,Bonus: Experience with Spark/Scala, distributed data systems and MPP databases", "You have excellent written and verbal communication skills.,You are tenacious, relentless, & determined.,You are curious: always learning new technologies, rapidly synthesizing new information, and understanding \u201cthe why\u201d before \u201cthe what.\u201d,You are self-directed and capable of operating amid ambiguity.,You are poised and display excellent judgment in prioritizing across difficult tradeoffs.,You are pragmatic: not letting \u201cthe perfect\u201d be the enemy of \u201cthe good.\u201d", "BA/BS Degree in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Experience in areas such as data-driven statistical modeling, discriminative methods, feature extraction and analysis, supervised learning.", "Possess a bachelor's degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field.", "Work with cool people and impact millions of daily players!", "Proficient at designing, accessing, and maintaining data stores, data feeds, and data processing tools: RDBMS, NoSQL, APIs, Kafka, Apache Spark, ELK stack,Experience with AWS services: EC2, S3, Lambda, Glue, Athena, Redshift, etc.,Proven experience deploying machine learning algorithms to production as a Data Engineer, Machine Learning Engineer, or similar role,Proficiency using GitHub, Docker, Luigi/Airflow, Jenkins, Terraform,Proficient in Python, shell scripting, Java (a plus), familiar with BI tools (e.g., Power BI, AWS QuickSight, Tableu, etc.),Proficient at writing high-quality and scalable code and integrating with version control systems,Knowledge of statistical and data mining techniques including model evaluation/validation,Enthusiasm for big data and translating data into actionable insights and demonstrable experience with big data technologies (with structured and unstructured data),Proficient at building robust data pipelines and delivering reliable data services to stakeholders,Ability to work in a complex, fast-paced environment while maintaining a high degree of accuracy and professionalism,Excellent interpersonal, verbal and written communication skills,Agile project development experience is a plus,Experience leading successful data engineering projects and operationalizing machine learning algorithms,BS (required, 5+ years of experience) or Master's degree (preferred, 3+ years of experience) in computer science, applied mathematics, engineering, operations research, or a related field", "Advanced degree in systems or electrical engineering or equivalent experience in systems engineering", "Advanced SQL knowledge,7+ years of data extraction experience,Should be proficient in writing advanced SQL and tuning SQL code,Semantic layer (ESL) development experience with relational databases such as Oracle/Teradata/Vertica/Hadoop desired,Experience with data induction and validation against source systems,Experience working in Capital Projects a plus, including writing requirements and executing UAT,Expert at normalizing data for reporting,Strong analytical skills; should have the ability to evaluate, analyze and present data to answer business questions,Experience with data visualization tools (e.g.: Tableau) a plus,Familiarity with Finance, Operations, Retail Contact Center data desired,Desire for end-to-end ownership of work,Flexibility to balance directional changes and ability to support multiple deadline-specific projects while maintaining day-to-day business support,Ability to deal with ambiguity,Proactive, driven individual who is comfortable working in a global, matrixed, fast-paced environment", "You have excellent written and verbal communication skills.,You are tenacious, relentless, & determined.,You are curious: always learning new technologies, rapidly synthesizing new information, and understanding \u201cthe why\u201d before \u201cthe what.\u201d,You are self-directed and capable of operating amid ambiguity.,You are poised and display excellent judgment in prioritizing across difficult tradeoffs.,You are pragmatic: not letting \u201cthe perfect\u201d be the enemy of \u201cthe good.\u201d", "Bachelor's or Master's degree in Computer Science or related technical field or equivalent professional experience,Greater than 3 years of experience,Safety & Quality First,Valuing Ethics, Integrity & Diversity,Passion for Serving Our Customers Globally,Dedication to Each Other Through Servant Leadership,Creating Value for Shareholders, Customers and Employees,Consistently Delivering Our Commitments.,Competitive Salary,Comprehensive Health, Wellness and Income Protection Benefits,401(k) Savings Plan with Company Match,Paid Vacations and Holidays,Opportunities for Flexible Work Arrangements,Educational Reimbursement Program,Employee Referral Program", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Python", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills.,You are tenacious, relentless, & determined,You are curious: always learning new technologies, rapidly synthesizing new information, and understanding \u201cthe why\u201d before \u201cthe what.\u201d,You are self-directed and capable of operating amid ambiguity.,You are poised and display excellent judgment in prioritizing across difficult tradeoffs.,You are pragmatic: not letting \u201cthe perfect\u201d be the enemy of \u201cthe good.\u201d", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "BSc or MSc in Computer Science or related field (or equivalent experience),Strong analytical, learning and problem solving skills with personal interest in subjects such as math/statistics, machine learning, AI and analytics,Solid knowledge of data structures, algorithms and Unix/Linux,Proficient in Scala, Java and SQL,Strong experience with Apache Spark 2.0,Experience working in an Agile environment using TDD and Continuous Integration,Experience refactoring code with scale and production in mind,Familiar with Git, Python, JavaScript, HTML and CSS,Proficient understanding of distributed computing principles,Proficiency with Hadoop v2, MapReduce, HDFS,Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala,Experience with integration of data from multiple data sources,Experience with NoSQL databases, such as HBase, Cassandra, MongoDB,Knowledge of various ETL techniques and frameworks,Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O,Experience with Cloudera/MapR/Hortonworks,Management of Hadoop cluster, with all included services,Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming,Good understanding of Lambda Architecture, along with its advantages and drawbacks,Experience with various messaging systems, such as Kafka or RabbitMQ", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by designing and implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology / mathematics / engineering / actuarial science or related discipline.,At least 5+ years in a technical data role, preferably in a formal data, data warehouse or business intelligence environment.,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Financial services knowledge, specifically personal or unsecured loans,Business process monitoring and optimising,Microsoft business intelligence visualisation technologies (SSRS, Power BI),IT infrastructure, e.g. storage, networking, servers, security,Unstructured data experience,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Analysing", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Knowledge of best practices and IT operations in an always-up, always-available service,Experience with or knowledge of Agile Software Development methodologies,Excellent problem solving and troubleshooting skills,Process oriented with great documentation skills,Excellent oral and written communication skills with a keen sense of customer service,BS or MS degree in Computer Science or a related technical field,4+ years of Python or Java development experience,4+ years of SQL experience (No-SQL experience is a plus),4+ years of experience with schema design and dimensional data modeling,Ability in managing and communicating data warehouse plans to internal clients,Experience designing, building, and maintaining data processing systems,Experience working with either a Map Reduce or an MPP system on any size/scale", "Bachelor's or Master's degree in Computer Science or related technical field or equivalent professional experience,Greater than 3 years of experience,Safety & Quality First,Valuing Ethics, Integrity & Diversity,Passion for Serving Our Customers Globally,Dedication to Each Other Through Servant Leadership,Creating Value for Shareholders, Customers and Employees,Consistently Delivering Our Commitments.,Competitive Salary,Comprehensive Health, Wellness and Income Protection Benefits,401(k) Savings Plan with Company Match,Paid Vacations and Holidays,Opportunities for Flexible Work Arrangements,Educational Reimbursement Program,Employee Referral Program", "Architect and build data pipelines,Architect and implement data warehouse structure and table schemas,Develop data models to enable end users to effectively analyze data,Optimize and tune data warehouse for query performance and analytical workloads,Identify, troubleshoot and resolve data quality issues,Write complex SQL queries for data analysis,Design and maintain robust data reporting and visualization tools based on requirements,Develop integrations from BI tools to third party productivity applications,5+ years of engineering experience,Expert in SQL, preferably across a number of dialects (we commonly write Snowflake, Redshift/PostgreSQL, MySQL, SQL Server),Experience developing software code in one or more programming languages (Python, Java, Scala, Ruby),Experience managing database or data warehouse technologies (bonus for Redshift and/or Snowflake),Experience implementing ETL tools (Bonus for Stitch, Fivetran or Matillion),Understanding of data analytics ecosystem. Experience with one or more relevant tools (Spark, Kafka, AWS Glue, Amazon Kinesis, Sqoop, Flume, Flink),Experience with implementing Business Intelligence tools (Bonus for Looker),Experience developing data pipelines from scratch", "Workflow scheduling/orchestration such as Airflow or Oozie,Big data warehousing (RDB or MPP DB) such as Oracle, Teradata, Postgress, Hive,Strong Python programming skills with an understanding of data analytics, linear algebra, and ML libraries such as Numpy, Scipy,Experience with query APIs using JSON, ProtocolBuffers, or XML", "Strong software development skills, with a proficiency in both Object Oriented and Functional Python required,Demonstrated skill and experience using PySpark and SparkSQL,Familiarity with cloud-based, distributed systems such as blob storage, elastic compute and virtual instances,Familiarity with Software Development Life Cycles and the tools and methodologies that support them such as git, continuous integration, issue tracking, code reviews, quality assurance processes and scheduling.,Supporting data collection, curation and data provenance when working with machine learning models.,Exposure to Signal Processing and low level data collection,Experience with statistical inference methods for scientific experimentation,Experience with Scala,____________________________________________________________________,We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.,Apple\u2019s most important resource, our soul, is our people. Apple benefits help further the well-being of our employees and their families in meaningful ways. No matter where you work at Apple, you can take advantage of our health and wellness resources and time-away programmes. We\u2019re proud to provide stock grants to employees at all levels of the company, and we also give employees the option to buy Apple stock at a discount \u2014 both offer everyone at Apple the chance to share in the company\u2019s success. You\u2019ll discover many more benefits of working at Apple, such as programmes that match your charitable contributions, reimburse you for continuing your education and give you special employee pricing on Apple products.,Apple benefits programmes vary by country and are subject to eligibility requirements. Apple is committed to working with and providing reasonable accommodation to applicants with physical and mental disabilities. Apple is a drug-free workplace.", "Bachelor's degree in computer science, information systems, related field,3+ years in Data Engineering in an AWS environment developing ETL tools,3+ years practical hands-on work experience with data systems,3-5 years practical, demonstrable, hands-on work experience with SQL reading/writing/database management skills,Experience in one or more of the following: Python, Java, Scala, R,Sharp attention to detail with the ability to effectively prioritize and execute multiple tasks,Excellent problem solving skills,Ability to communicate effectively with stakeholders and peers,Experience working with relational, NoSQL, and columnar data stores,Familiarity with AWS, AWS Lambda, S3, Redshift, MongoDB, Graph Databases, Azure, and other cloud based technologies,Contribute to the development of a newly established data warehouse ecosystem,Communicate effectively and define requirements from a wide range of different teams,Translate business requests into database design and execution,Work closely with domain experts to understand the source systems of the data,Build robust and scalable interfaces, ETL programs, and data pipelines,Develop and maintain data dictionary for published data sources,Develop and improve data release and testing processes,Create and manage data sources and integrate with numerous APIs,Ability to identify and resolve performance issues,Operate in an open source and cloud based environment that includes: AWS, Redshift, Python, R, Hadoop, Spark, and other technologies,Work with structured and unstructured data,Competitive wages including performance bonuses,Medical/Dental/Life/Disability insurance,Paid time off,401k with employer match,Employer funded retirement plan,Health Savings Account/Medical and Dependent Care Flexible Spending Accounts,Wellness Program,Membership to the TPC Sawgrass", "Proficient in SQL and programming (Python preferred),Experience with MPP databases preferred,6+ years of experience in data engineering and ETL pipeline development.,3+ years of Spark development.,6+ years of experience in Big Data Technologies (Hadoop, MapReduce, Hive etc\u2026). Spark experience preferred.", "Data Engineer,Five to seven years\u2019 experience with big data tools: Hadoop, Spark, Kafka,Five to seven years\u2019 experience SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases,Five to seven years' strong analytic skills related to working with unstructured datasets,Experience with AWS cloud services: EC2, EMR, RDS, Redshift,Full SQL Stack (SSIS, SSAS, SSRS),Degree: Information Technology", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Seeks a challenge and gets things done.,Math/Stats/Machine Learning background.,Data modeling / SQL / NoSQL database experience.", "This position requires a Computer Science, Bioinformatics, or related degree; 5+ years' experience in data movement, data wrangling and delivery of data or analytics pipelines,Experience implementing and maintaining, data or analytic pipelines.,Experience with Big Data technologies, Cloud-based offerings (Microsoft Azure, GCP, AWS, etc), and corresponding tools.,Experience with data movement and management in the Pharmaceutical industry or related scientific fields.,Experience with the core components of the Hadoop stack including HDFS and Apache Spark, ideally a Cloudera based stack,Background and experience in LIMS systems, Next Generation Sequencing (NGS) workflows, Cloud computing and HPC systems.,Understanding of diverse \u2018omic data types including RNA-Seq, DNA-Seq, Chip-Seq, WES, WGS, ATAC-seq, microbiome, proteomic, metabolomic data etc. from different sources.,Familiarity with data mining, machine learning and artificial intelligence techniques,Proven ability to contribute to development projects.,Operating at pace and agile decision-making - using evidence and applying judgement to balance pace, rigour and risk.,Committed to delivering high quality results, overcoming challenges, focusing on what matters, execution.,Continuously looking for opportunities to learn, build skills and share learning.,Sustaining energy and well-being.,Building strong relationships and collaboration, honest and open conversations.", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Design and implement data monitoring solutions and procedures and continuously monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Design and implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Work together with business owners, analysts and IT to manage changes to data in the organisation.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics and liaise with IT infrastructure and IT Operations regarding system and infrastructure management.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Complex solution and service design and implementation.,Cross functional data and team knowledge gathering and sharing.,Responsible for team activities, team dynamics and performance.,Manage project and task delivery of team.,Multiple stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology/engineering/mathematics/statistics/actuarial or related discipline,At least 8 years\u2019 experience working in a data, business intelligence or analytics environment,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Planning and organizing,Presenting and Communicating information,Analysing,Leadership", "Agile Engineering (Kanban, Lean, Hybrid agile experience is a big plus)", "Degree educated in Data Science or similar,Strong experience with data preparation techniques including exploration and visualisation,Experience with statistical models, times series analysis and multiple machine learning techniques such as clustering, regression and classification,Strong skills using Python, SQL & Elastic and visualise the data in a surfacing tool,Experience developing machine learning systems,Experience in Amazon Quicksite will be an advantage", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Splunk, Hadoop and similar", "Expertise in Python programming (functional and object-oriented),Strong foundation in statistical analysis,Able to develop optimized pipelines for data acquisition, pruning and preprocessing; insightful data and performance visualizations and iterating over algorithm variants", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Subject to and expected to comply with all applicable University policies and procedures, including but not limited to the personnel policies and other policies found in the University\u2019s Administrative Guide, https://adminguide.stanford.edu.", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "do NOT contact us with unsolicited services or offers", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "Excellent programming skills in C, C++, Python or Java,Prior experience developing production software,2 years minimum experience with Linux system administration and command line tools,Strong analytical thinking,Self-motivated and able to work independently", "5+ years of experience in developing ETL jobs for analyzing and processing high-volume data in Apache Hadoop ecosystem, especially with Spark,Expert knowledge of one or more object-oriented programming languages (Scala preferred),Proficient at schema design and data modeling,Experience with data tools (Jupyter Notebooks, Zeppelin...),Excellent problem-solving and analytic skills,Ability to program in several scripting languages such as Python, Perl, and Bash,Experience with workflow management tools: Oozie, Airflow, Azkaban, etc.,Experience with batch and streaming data processing,Ability to learn and research new technologies rapidly,Passion for customer privacy,Strong interpersonal skills and experience working on cross-functional projects", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by designing and implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology / mathematics / engineering / actuarial science or related discipline.,At least 5+ years in a technical data role, preferably in a formal data, data warehouse or business intelligence environment.,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Financial services knowledge, specifically personal or unsecured loans,Business process monitoring and optimising,Microsoft business intelligence visualisation technologies (SSRS, Power BI),IT infrastructure, e.g. storage, networking, servers, security,Unstructured data experience,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Analysing", "Possess a bachelor's degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field.", "NoSQL and other databases like MongoDB, Cassandra, Neo4J", "You are precise in thought and code.,You have serious interest in SQL and Data.,Your T-SQL Dev skills are strong \u2013 Stored Procs etc.,You do have some knowledge of SSIS \u2013 standard ETL but that\u2019s not what drives you.,Some SSAS would be useful.", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Expert in designing, implementing and supporting highly scalable data systems and services in Java and/or Scala,Extensive experience with Hadoop-ecosystem technologies in particular MapReduce, Spark / Spark-SQL / Spark Streaming, Hive, YARN/MR2,Expertise building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components,Experience in data-modeling and data-architecture optimized for big data patterns, ie. warehousing concepts; efficient storage and query on HDFS; data security and privacy techniques),Knowledgable about distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues", "10+ years of software engineering experience,A track record of making a difference in past projects,Naturally accountable, responsible, self motivated and self sufficient,Experience designing distributed systems/services for scale,Experience working with: Cassandra, SOLR, Spark, Hadoop, Kafka and similar technologies in production contexts at scale,Preference for experience with Scala, Python,Apple\u2019s most important resource, our soul, is our people. Apple benefits help further the well-being of our employees and their families in meaningful ways. No matter where you work at Apple, you can take advantage of our health and wellness resources and time-away programmes. We\u2019re proud to provide stock grants to employees at all levels of the company, and we also give employees the option to buy Apple stock at a discount \u2014 both offer everyone at Apple the chance to share in the company\u2019s success. You\u2019ll discover many more benefits of working at Apple, such as programmes that match your charitable contributions, reimburse you for continuing your education and give you special employee pricing on Apple products.,Apple benefits programmes vary by country and are subject to eligibility requirements.", "High-level understanding as well as hands-on experience in implementing data pipelines,Proficient in scripting and/or functional programming languages such as Python, Scala, Bash, Groovy, Ruby,Experience with database, message queueing, and data streaming solutions (for instance: PostgreSQL, AWS RDS, Apache Kafka, AWS Kinesis, RabbitMQ, Redis, Apache Spark, or similar tools),Experience in (regression) testing data pipelines,Experience with DevOps and application monitoring tools is a plus (for instance: Docker, Kubernetes, Terraform, CloudFormation, Ansible, Chef, Puppet, Salt, Splunk, Elastic/ELK Stack, Sentry, Datadog, or similar tools),Knowledge of Machine Learning and Computer Vision is a plus", "Bachelor\u2019s degree or higher in a quantitative/technical field (Computer Science, Statistics, Engineering),Minimum two years work experience in related field required,Working knowledge of data design, architecture and warehousing,Understanding of data management fundamentals and data storage principles,Knowledge of distributed systems as it pertains to data storage and cloud computing,Understanding and administration of AWS, Docker and Linux-based systems,Experience in custom ETL design, implementation and maintenance,Experience in large scale data processing using traditional and distributed systems like Hadoop, Spark, Dataflow, and Airflow.,Knowledge and practical experience in machine learning and AI fundamentals,Experience implementing machine learning solutions at scale,Experience working with both Batch and Real Time data processing systems,Ability to work and communicate effectively with stakeholders.,Opportunity to work with one of the fastest-growing financial startups in the country,Competitive Salary + Equity,401k with Company Match,Gym + Public Transportation Subsidy,Student Loan Assistance,Relocation Assistance,Unlimited PTO", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Bachelor\u2019s degree or higher in a quantitative/technical field (Computer Science, Statistics, Engineering),Working knowledge of data design, architecture and warehousing,Understanding of data management fundamentals and data storage principles,Knowledge of distributed systems as it pertains to data storage and cloud computing,Understanding and administration of AWS, Docker and Linux-based systems,Experience in custom ETL design, implementation and maintenance,Experience in large scale data processing using traditional and distributed systems like Hadoop, Spark, Dataflow, and Airflow.,Knowledge and practical experience in machine learning and AI fundamentals,Experience implementing machine learning solutions at scale,Experience working with both Batch and Real Time data processing systems,Ability to work and communicate effectively with stakeholders.,Opportunity to work with one of the fastest growing financial startups,Competitive salary + equity,Health/dental/vision insurance + 401K,Gym + public transportation subsidy,Relocation assistance", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Experience with SQL databases, Graph Databases- largest growth - No /SQL- Neo4J currently,Data transformation experience with scripting (Python, Java, Perl, PL/SQL) or platforms such as Spark, or Knime,,Exposure to AWS Data Services,Experience with Architect, design, and implement updates and enhancements to data repositories and indices to support data ingest, enrichment, analysis, visualization, and dissemination", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Relevant degree or equivalent.,Applied knowledge of database architecture concepts, within traditional RDBMS environments and also in-memory, noSQL, virtualised or federated approaches, columnar and streaming databases.,Strong knowledge of concepts such as OLTP, OLAP, star and snowflake schemas, dimensional modelling and normalisation.,Extensive track record of performing in the data engineering domain, including:,Extracting data from enterprise systems such as SAP,Security management,Data modelling,ETL development using tools such as Data Integrator/Services,Experience of experience of scrum/agile project management.,Annual Bonus Plan,Discretionary Cash Award,Group Personal Pension Plan with enhanced company contribution,Medical, Travel, Health & Life Insurances,Holiday, 25 days annual leave with option to buy an additional 5 days per year,Sabbatical, 20 paid days every four-year of service,Volunteering, One (1) paid working day each year (TeamARM),Varies by location: cycle to work, free car parking, gym on site, team and social events", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Experience in areas such as data-driven statistical modeling, discriminative methods, feature extraction and analysis, supervised learning.", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Experience building data pipelines and analytics systems on distributed data systems on AWS, using spark.,Experience Scala and Python,Experience building batch pipelines with data from event data streams, NoSQL and APIs.,Competitive health and insurance benefits,Competitive salary,Annual target bonus or commission,Paid vacation and sick time,Vacation rental on a yearly basis (taxable benefit),Employee Stock Purchase Program,Free snacks and beverages,Frequent company update talks with our leadership team,Free listing on HomeAway. com,Electronic, adjustable stand-up desk,Discounted Metro & Rail pass", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by designing and implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology / mathematics / engineering / actuarial science or related discipline.,At least 5+ years in a technical data role, preferably in a formal data, data warehouse or business intelligence environment.,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Financial services knowledge, specifically personal or unsecured loans,Business process monitoring and optimising,Microsoft business intelligence visualisation technologies (SSRS, Power BI),IT infrastructure, e.g. storage, networking, servers, security,Unstructured data experience,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Analysing", "BS, MS or equivalent in Computer Science or related technical field,10+ years of software engineering with OOP languages,7+ years of architectural experience developing scalable data pipelines and data processing frameworks in a cloud-native or distributed computing environment.,3+ years serving as a lead data or pipeline architect in a large-scale, cloud-based, high-volume data processing organization or business division.,Demonstrated experience writing architectural requirements and systems design documents.,Experience designing and governing scalable cloud-native backend compute capabilities (REST APIs, microservices, distributed computing frameworks, geospatial processing and indexing, messaging frameworks and paradigms, data quality management).,Excellent written and verbal communication including the presentation of complex engineering designs, concepts, and solutions in a clear and concise manner to technical and non-technical audiences alike.,Expertise in processing and aggregating high-volume, geospatially-oriented IoT data.,Expertise in imagery processing and feature extraction from satellite and aerial sources.,Depth of knowledge in the Data Operations and Data Quality management space.,Experience with layered geospatial data structures and data representations.,Expertise designing and implementing highly scalable data-intensive distributed computing solutions using modern cloud-native processing frameworks.,Experience with Amazon Web Services (EC2, S3, RDS, SQS, etc.) is strongly preferred,Experience with a compiled JVM language, including Scala, is a plus,Superb medical, dental, vision, life, disability benefits, and a 401k matching program,A stocked kitchen with a large assortment of snacks & drinks to get you through the day,Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used,Inspire one another,Innovate in all we do,Leave a mark on the world,Find the possible in the impossible", "Extensive experience in relational database development, particularly Oracle and MS SQL server,Experience of writing and optimising stored procedures and views to transform and deliver data,Experience using performance, monitoring and alerting tools,Experience in scripting/automation language (Shell, Python),Familiar with ETL processes,Interest in NoSQL database and BigData concepts and systems,Knowledge of SCRUM or other Agile methodologies,Any experience of Data Architecture and Dimensional Modelling would be a plus,Any experience with Markit EDM would be a plus,Knowledge of investment management, including investment risk, would be a definite plus,Good communicator and collaborator keen to work closely across teams and the business,Take ownership and show a willingness to tackle difficult issues,Embrace, and effectively manage, change,Get hands-on, providing technical direction and/or problem solving to support the team,Ensure all third-party development conforms to organisation best-practice and quality standards,Demonstrate exceptional problem solving skills and intellectual curiosity,Show a passion for innovation and continuous improvement and initiate efforts to implement alternative solutions", "7+ years of data extraction and front end report building (PHP or similar),Proficient with data access/preparation methods using Teradata SQL and relational databases,Proficient with scripting/glue languages like PHP or Python and web-technologies like HTML, CSS, and Javascript,Strong interpersonal skills, both verbal and written,Ability to work on multiple assignments with excellent attention to detail,Flexibility to handle directional changes and ability to support multiple deadline-specific projects while maintaining day-to-day business support,Driven, Self-motivated individual who is experienced working in a global, matrixed, fast-paced environment,Ability to comprehensively understand data elements, sources and relationships,Ability to establish and manage relationships in a cross-functional team environment,Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants.", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Minimum of five years data analytics, programming, database administration, or data management experience.", "Degree educated in Data Science or similar,Strong experience with data preparation techniques including exploration and visualisation,Experience with statistical models, times series analysis and multiple machine learning techniques such as clustering, regression and classification,Strong skills using Python, SQL & Elastic and visualise the data in a surfacing tool,Experience developing machine learning systems,Experience in Amazon Quicksite will be an advantage", "The stated experience level is a guide and does not preclude applications from candidates with more or less experience, provided the requisite skills can be demonstrated.", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Bachelors degree in Computer Science, Computer Engineering, Applied Math, Statistics or related field,Certified Data Management Professional (CDMP) a plus,8+ years experience designing, building, and maintaining data management systems,Broad experience in planning, architecting, and delivering mission-critical enterprise-grade systems and solutions,Experience with cloud architecture and data repositories,Data design experience,Experience in data cleansing and optimization for data consumption,Experience with source control tools such as Git and TFS a plus,Experience in .Net framework and Web Services (SOAP, REST, XML) a plus,Experience with Azure Service Fabric and microservices a plus,Working familiarity of front-end web framework using C#, JavaScript and/or Angular a plus,Experience using CSV/JSON/XML data formats,Experience in developing LOB (Line of Business) Applications, as well as Consumer Websites,Experience in consumer-facing, ecommerce and mobile systems a plus,FREE 7-day resort stays and cruises on your anniversary,Medical Health Insurance,Onsite Wellness Clinic,Long Term Disability,Life Insurance,Dental & Vision Coverage,401K Plan,Pet Care Insurance,Legal Insurance,Flexible Spending Accounts (FSA),Employee Assistance Program,Discounted Employee Services (dry-cleaning, nail services, massage, personal training, etc.),Dedicated Employee Enrichment & Recognition Programs", "50+ career categories", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills.,You are tenacious, relentless, & determined,You are curious: always learning new technologies, rapidly synthesizing new information, and understanding \u201cthe why\u201d before \u201cthe what.\u201d,You are self-directed and capable of operating amid ambiguity.,You are poised and display excellent judgment in prioritizing across difficult tradeoffs.,You are pragmatic: not letting \u201cthe perfect\u201d be the enemy of \u201cthe good.\u201d", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Work with cool people and impact millions of daily players!", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Experience in high level programming languages such as Java, Scala, or Python.,Proficiency with databases and SQL is required.,Proficiency in data processing using technologies like Spark Streaming, Spark SQL, or Map/Reduce.,Expertise in Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig.,Expertise in developing big data pipelines using technologies like Kafka, Flume, or Storm.,Experience with large scale data warehousing, mining or analytic systems.,Ability to work with analysts to gather requirements and translate them into data engineering tasks", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "4+ years' experience in the data warehouse space,4+ years' experience working with either a MapReduce or an MPP system,7+ years' experience in writing complex SQL and ETL processes,4+ years' experience with object-oriented programming languages,BS/BA in Technical Field, Computer Science or Mathematics,Knowledge in Python or Java,Experience analyzing data to identify deliverables, gaps, and inconsistencies,Actively mentored team members in their careers,Experience effectively collaborating and communicating complex technical concepts to a broad variety of audiences", "Four+ years' development experience working with Python,Data skills (SQL, document stores),Large scale ETL (Apache beam or Apache spark),High scale Restful Services,Cloud experience (Google Cloud Platform, Azure, or AWS),BSc or B-Tech in Computer Science or IT preferred", "Experience in Azure Data Factory, Data Bricks, Data Lake", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "2-5+ years of experience in data engineering.,Expertise with various ETL technologies and familiar with ETL tools.,You have engineered metrics and statistical information out of massive and complex datasets (e.g. Hive, Spark MLlib, Druid, Solr, Kafka).,You are proficient in at least one programming language (e.g. Python, Scala) and are comfortable developing code within a team environment (e.g. git, testing, code reviews).,You have built robust data and analytic pipelines and have a keen eye for where to automate (e.g. Oozie, Airflow).,Have solid understanding of both relational and NoSQL database technologies.,Experience with visualization, data mining, or statistical tools", "You have 4 years of experience with a deep understanding of data engineering concepts and database designs.,Advanced SQL knowledge for working with relational data (Postgres) and programming experience (Java/Python) in working with unstructured data and/or APIs.,Experience with ETL and/or other integration tooling (DMS/Stitch).,Experience with data analysis and visualization tools (Mode).,Working knowledge of message queuing (SNS/SQS/RabbitMQ) and stream processing (Kinesis).,Strong communication skills, a positive attitude, and empathy.,Self-awareness and a desire to continually improve.,Desire to write and maintain a clean and well-tested code base, avoiding tech debt.,Experience with business operations tools such as Salesforce, Zuora, Hubspot, etc.,Experience with Apache Spark.,General understanding of the data science and machine learning technology landscape.,Experience with AWS services including Lambda, DynamoDB, etc.,Competitive salary,Employee Stock Option Plan,Generous health and commuter benefits,Dog Friendly Office", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Commercial client-facing project experience, including working in close-knit teams,A proven ability in clearly communicating complex solutions,Data security & governance expertise,Strong experience in traditional data warehousing / ETL tools (Informatica, Talend, Pentaho, DataStage),Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs) and Cloud (AWS and Azure),Strong SQL experience, and optional Python / Scala / Java / R,Exceptional attention to detail", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "Assist in gathering new data sources at scale (including API calls and web scraping),Process unstructured and structured data into a use for analysis,Proficient in one or more common scripting languages (Python preferred),Experience designing data architecture from ground-up,Experience processing large amounts of structured and unstructured data,Extensive AWS Experience", "2+ years experience working with data systems,2+ years experience with data warehousing, processing, pipelines, infrastructure, and query patterns,2+ years experience with Spark or Hadoop,2+ years experience with open source ETL frameworks such as Airflow, Luigi, or similar,4+ years experience with Python and SQL,Experience with systems for data processing (Spark, Flink, Hadoop, Airflow) and storage (S3, Kafka, ElasticSearch, Dynamo, MySQL, or Postgres),Experience with ELK,Experience reading and optimizing data schema queries for content and performance", "8+ years of engineering experience with 4+ years focused on architecting data platforms,Working experience on a large scale data warehouse platform,Hands-on experience working with either a MapReduce/Spark/Presto or an MPP system,Skilled in dimensional data modeling and schema design for data warehousing,Bachelor's Degree in computer science, engineering, mathematics or related fields, or equivalent experience,Expert SQL knowledge and experience optimizing for large datasets,Masters Degree in computer science, engineering, mathematics or related fields, or equivalent experience", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "A curious mind,An obsession for quality,Background in Data science, Data mining, Multivariate statistics, Computer vision, Machine learning,Experience working with large scale data sets,Solid programming skills including:,Python,C/C++,Experience with data visualization and presentation, familiar with data analysis tools such as Tableau", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by designing and implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology / mathematics / engineering / actuarial science or related discipline.,At least 5+ years in a technical data role, preferably in a formal data, data warehouse or business intelligence environment.,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Financial services knowledge, specifically personal or unsecured loans,Business process monitoring and optimising,Microsoft business intelligence visualisation technologies (SSRS, Power BI),IT infrastructure, e.g. storage, networking, servers, security,Unstructured data experience,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Analysing", "Architect and build data pipelines,Architect and implement data warehouse structure and table schemas,Develop data models to enable end users to effectively analyze data,Optimize and tune data warehouse for query performance and analytical workloads,Identify, troubleshoot and resolve data quality issues,Write complex SQL queries for data analysis,Design and maintain robust data reporting and visualization tools based on requirements,Develop integrations from BI tools to third party productivity applications,5+ years of engineering experience,Expert in SQL, preferably across a number of dialects (we commonly write Snowflake, Redshift/PostgreSQL, MySQL, SQL Server),Experience developing software code in one or more programming languages (Python, Java, Scala, Ruby),Experience managing database or data warehouse technologies (bonus for Redshift and/or Snowflake),Experience implementing ETL tools (Bonus for Stitch, Fivetran or Matillion),Understanding of data analytics ecosystem. Experience with one or more relevant tools (Spark, Kafka, AWS Glue, Amazon Kinesis, Sqoop, Flume, Flink),Experience with implementing Business Intelligence tools (Bonus for Looker),Experience developing data pipelines from scratch", "50+ career categories", "50+ career categories", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by designing and implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology / mathematics / engineering / actuarial science or related discipline.,At least 5+ years in a technical data role, preferably in a formal data, data warehouse or business intelligence environment.,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Financial services knowledge, specifically personal or unsecured loans,Business process monitoring and optimising,Microsoft business intelligence visualisation technologies (SSRS, Power BI),IT infrastructure, e.g. storage, networking, servers, security,Unstructured data experience,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Analysing", "Architect and build data pipelines,Architect and implement data warehouse structure and table schemas,Develop data models to enable end users to effectively analyze data,Optimize and tune data warehouse for query performance and analytical workloads,Identify, troubleshoot and resolve data quality issues,Write complex SQL queries for data analysis,Design and maintain robust data reporting and visualization tools based on requirements,Develop integrations from BI tools to third party productivity applications,5+ years of engineering experience,Expert in SQL, preferably across a number of dialects (we commonly write Snowflake, Redshift/PostgreSQL, MySQL, SQL Server),Experience developing software code in one or more programming languages (Python, Java, Scala, Ruby),Experience managing database or data warehouse technologies (bonus for Redshift and/or Snowflake),Experience implementing ETL tools (Bonus for Stitch, Fivetran or Matillion),Understanding of data analytics ecosystem. Experience with one or more relevant tools (Spark, Kafka, AWS Glue, Amazon Kinesis, Sqoop, Flume, Flink),Experience with implementing Business Intelligence tools (Bonus for Looker),Experience developing data pipelines from scratch", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You love to hack at things, and to come up with creative ways to approach problems.,Passionate about social, consumer applications and travel,Detail oriented, communicates clearly in writing and in conversation,2-5 years experience integrating with a variety of consumer APIs.,Comfortable working with Packer/Terraform (to update dependencies.,Experience with NLP, Named Entity Recognition & Machine Learning,PHP, Rust, React, Swift,Experience building browser plugins from scratch,Experience with Mechanical Turk and/or other similar marketplaces,Competitive Salary: 120,000 to 160,000,Equity: .25% to 1%,Healthcare,Flexible hours,Unlimited Vacation Time", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,4+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, Flink, Kafka, etc.) building efficient extraction and transformation pipelines at scale,Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth attitude,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "You have excellent written and verbal communication skills,You are curious and have excellent analytical and problem solving skills,You are excited about digging into massive petabyte-scale semi-structured datasets,1+ years of industry experience working with distributed data technologies (e.g. Hadoop, MapReduce, Spark, etc.),Proficiency in at least one high-level programming language (Python, Go, Java, Scala, or equivalent),Experience with large, complex, highly dimensional data sets; hands-on experience with SQL,You are pragmatic, not letting \u201cthe perfect\u201d be the enemy of \u201cthe good\u201d,You are self-directed and capable of operating amidst ambiguity,You are humble, continually growing in self-awareness and possessing a growth mindset,Extras we\u2019d be excited about...,Experience building stream-processing applications using Apache Flink, Spark-Streaming, Apache Storm, Kafka Streams or others", "50+ career categories", "Agile Engineering (Kanban, Lean, Hybrid agile experience is a big plus)", "You are highly expert and battle tested, a lead or core contributor on data processing projects,Consistent record of designing and implementing scalable, performant data pipelines, data services, and data products.,This is a hands-on position, expect to write more code,Programming experience in building high quality software. Skills with Java, Python or Scala preferred,Proficiency in Hadoop, Kafka, Spark and MPP/No SQL databases like Vertica, Redshift, Snowflake in a large scale environment,Strong aptitude for learning new technologies related to Data Management and Data Science.,Demonstrated ability to work well independently and within a fast-paced, team-oriented environment,Work with noisy, dirty and unstructured data. Data cleansing, scraping unstructured data and converting into structured data,Evaluate, benchmark and improve the scalability, robustness, efficiency and performance of big data platform and applications,Experience building reports using Tableau, Microstrategy,Knowledge in engineering machine learning, feature engineering systems is a plus.", "Design, build and launch extremely efficient and reliable data pipelines to move data across a number of platforms including Data Warehouse, online caches and real-time systems.,BA/BS Degree in Computer Science, any Engineering discipline, Statistics, Information Systems or another quantitative field.,Action Oriented", "Strong Java experience (mainly Core Java),Strong knowledge of OOP,SQL,Hands-on Google Cloud Platform: Dataflow, BigQuery, and Pub/Sub or Hadoop echosystem experience: Hbase, Hive, Spark.,Commercial experience with Linux.,Messagedriven architecture with experience using ActiveMQ/Kafka or similar,Track record of developing technology to enable large scale data transformation.,ETL data load process development,Experience dealing with large and/or complex data sets.,Development and Testing best practices,Maven, Git,Familiar with NoSQL technologies * Embrace agile software engineering practices and tools; TDD, Continuous Integration etc.,Understanding of software development lifecycle, Agile software design principles and build processes,Excellent organisation, communication and interpersonal skills,Ability to work both collaboratively and with limited supervision,Can deliver results within set deadlines,Bachelor's,Java: 5 years", "Travel", "Identify data sources that can add value to decision making.,Work with source system owners and analysts to understand source data, e.g. data profiling, definition and mapping.,Design and implement efficient data loads, using traditional structured data ETL techniques.,Design and implement real time and near real time data load solutions, using technologies like data streaming.,Design and implement unstructured data loads, e.g. text speech, images and video.,Design and implement load monitoring tools and procedures and perform continuous monitoring and optimising of loads.,Work with analysts and architect to design and implement effective and efficient data models using appropriate modelling techniques.,Design and implement data warehouse data models.,Design and implement data pipelines for ad hoc, unstructured and other data models.,Design and implement appropriate aggregation data structures that enhance usability of data, e.g. multi-dimensional OLAP structures, summary tables etc.,Design, implement and maintain appropriate indexing on tables to enhance speed of access.,Design and implement data models that support automated decision making and/or further analytics.,Continuously search for data elements from other sources to enhance existing data objects to supplement / enhance context.,Design and implement interfaces for data access, e.g. batch exports, real-time decision API\u2019s etc.,Design and implement interface monitoring and management solutions to ensure availability and accuracy.,Design and implement data monitoring solutions and procedures and continuously monitor and maintain integrity of existing environment, troubleshoot technical and data issues and make appropriate changes where required.,Design and implement meta-data solutions that assist with understanding and managing data.,Work together with business owners, analysts and IT to maintain good data governance.,Work together with business owners, analysts and IT to manage changes to data in the organisation.,Provide technical and data related support to source system teams and external parties with whom we exchange data.,Manage data growth and usage by implementing effective strategies, e.g. archiving and indexing.,Manage systems, technology and tools that enable data management and analytics and liaise with IT infrastructure and IT Operations regarding system and infrastructure management.,Take ownership of own work by delivering high quality work on time.,Show initiative and be pre-active in finding opportunities to improve data and/or processes.,Take ownership of own career development by continuously improving skills, knowledge and the application thereof in designing and implementing solutions.,Positive engagement in team activities and actively contribute ideas to improve team dynamics and performance.,Complex solution and service design and implementation.,Cross functional data and team knowledge gathering and sharing.,Responsible for team activities, team dynamics and performance.,Manage project and task delivery of team.,Multiple stakeholder management (internal and external).,Assist in development of others, e.g. mentoring and knowledge share.,Quality control of other\u2019s work.,Degree in information technology/engineering/mathematics/statistics/actuarial or related discipline,At least 8 years\u2019 experience working in a data, business intelligence or analytics environment,SQL,Data analysis,Data visualisation,Data modelling,Microsoft business intelligence data technologies (SSIS, SSAS, SQL Server),Data warehouse concepts and best practices,Information gathering and problem analysis,Applying professional / specialist / technical expertise,Creating and innovating,Quality & Detail orientation,Planning and organizing,Presenting and Communicating information,Analysing,Leadership", "An open office environment where ideas flow among marketers and developers, product managers and support reps, who sit shoulder-to-shoulder collaborating and challenging and encouraging each other.", "Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.,Several years of experience building and optimizing data pipelines, architectures and data sets.,Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.,Strong analytic skills related to working with unstructured datasets.,Build processes supporting data transformation, data structures, metadata, dependency and workload management.,Working knowledge of message queuing, stream processing, and highly scalable data stores.,Experience leading cross-functional teams in a dynamic environment.,Experience using the following software/tools:,5+ years Experience with big data tools: Hadoop, Spark, Kafka, etc.,5+ years Experience with relational SQL and NoSQL databases, including Postgres.,Experience with data pipeline and workflow management tools.,5+ years Experience with AWS cloud services: EC2, EMR, RDS, Redshift.,Experience with stream-processing systems.,A comprehensive medical/dental/vision package provided through Empire Blue Cross Blue Shield. Life and disability insurance are also included.,Enrollment in our 401K plans. Once you are enrolled in the plan, you can change your contributions rate or opt out at any time.,Stock option program.,Eight company paid holidays for the 2019 calendar year. Paid time off (PTO) is uncapped, should it be in accordance with the Company\u2019s policy, which will be outlined during New Hire Orientation.", "Robust Perks \u2013 generous PTO, 401k contributions, tuition assistance, entertainment discounts and more!", "Vibrancy 360 Wellness Program: Yoga and fitness classes, onsite massage, volunteer opportunities, company happy hours, product demos, outings, and more.", "3+ years of experience in a Data Engineer role.,Preferred: Bachelor or Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field - or commensurate experience,Experience using the following software/tools:,Intermediate development experience with one or more of the following: Java, Python and/or Scala,Experience with relational SQL and NoSQL databases; strong knowledge of database systems in general (Foreign keys, indexes, basic DBA tasks),Experience with big data tools: Intermediate to Advanced knowledge of Python; EMR/Spark, Lambda is a plus.,Working knowledge of a scripting language: Bash, Python, and/or PowerShell,Familiarity with a Linux/Unix environment,Experience with data pipeline and workflow management tools is a plus,Experience with stream-processing systems is a plus,\u200b\u200b\u200b\u200b\u200b\u200b\u200bWillingness to roll up sleeves and do analyst work for the purposes of data exploration and assessment.,Demonstrated experience in measuring and handling large data sets and relational databases.,Communication, collaboration and project management skills with the ability to distill complex subjects to a wider audience.,Creativity, flexibility, and an entrepreneurial mindset to the solution of business issues; sound business judgment.,The ability to meet and exceed deadlines through a demonstrated sense of urgency.,Clear and concise ability to communicate complex concepts in English.,Ownership\u2014Performance Unit Grants,Peer Recognition Awards,Employee Referral Bonus Program,401k with Company Match,Free Access to Office Health Club (Oakbrook & Chicago employees); Discounted gym memberships through BlueCross BlueShield of Illinois,Medical & Prescription Drug Insurance (monthly premiums 100% paid for employee only coverage!),Dental Insurance (monthly premiums 100% paid for employee only coverage!),Vision Insurance (monthly premiums 100% paid for employee only coverage!),Provided at No Cost to Employees: Group Term Life Insurance, Long-Term Disability, and Accidental Death & Personal Loss Insurance,Flexible Spending Accounts for Health Care, Dependent Care, and Commuter Benefits,Identity Protection Insurance,Voluntary Term Life Insurance & Group Universal Life Insurance,Accident & Critical Illness Insurance,Paid Time Off for Vacation, Illness, & Maternity/Paternity Leave,Corporate-sponsored Activities & Events Year Round", "Work with a phenomenal team of individuals that are constantly pushing the market boundaries to offer healthy food at the utmost convenience.", "Previously healthcare experience is highly desirable", "Distributed computing principles,Legacy and modern database architectures,Hadoop-based technologies (e.g. MapReduce, Hive and Pig),SQL-based technologies (e.g. PostgreSQL and MySQL),NoSQL technologies (e.g. Cassandra and MongoDB),Stream-processing systems (e.g. Storm or Spark-Streaming),ETL tools and APIs,Optimizing data storage and retrieval for specific use cases,Testing and validating the accuracy of data transformations,Cloud computing architectures, preferably with specific expertise in Microsoft Azure and AWS,Programming in multiple programming languages, including Python and Java,Creative problem-solving that is sensitive to available time and resource constraints,Effective listening and communication,Project managing", "SQL Server, T-SQL, SSIS, stored procedures, user-defined functions and table functions,Managing design risk,Software development lifecycles, Unit test techniques, debugging/analytical techniques,Help career growth by joining industry leader and continuing to advance Echo web based technologies,Working with an organization with defined market goals, products, customers, revenue, and development teams,Experienced mentors to learn and adopt new practices,Ability to introduce your own views and takes on our product offerings,Work in wide variety of data management,Ability to constantly enhance and improve applications,Have a clearly defined career growth track with enough flexibility to pave your own way", "The opportunity to work with smart people on challenging problems!", "Bachelor\u2019s Degree in Computer Science or related field,1+ years of experience in Data Platform Administration/Engineering", "Strong knowledge of SQL, Python/Bash,Expert in database design,Experience with Git,General knowledege of Linux server administration,BS/MS in Computer Science or Engineering,Experience with Google Cloud Platform,Experience with Spark or Hadoop stack,Interest in data science/machine learning,Career in rapidly growing IT;,Official employment according to the Labor Code of the Russian Federation with \u201cwhite salary\u201d;,Flexible working day start (between 9.00 and 12.00 am) with 5 working days (Mn-Fr);,Comfortable and cosy office;,Friendly employees;,Corporate education (trainings, seminars, conferences);,Fitness compensation (up to 50%);,Medical insurance.", "Extensive experience with SQL, Ruby, and web services. Big Data skills appreciated.,Experience testing multi-tier, consumer-facing web applications at more than just the UI level,Experience testing batch / streaming ETL processes using Spark, Beam, etc.,Parsing and analysis of free-form and fixed form data sets,Good problem solving and debugging skills,Comfortable working in Agile development environment,Experience executing API tests,Broad experience designing and maintaining automated tests for whitebox and blackbox testing,Experience with unit testing frameworks: RSpec (preferred), Minitest/Test::Unit or similar,Experience with libraries used to implement browser automation: Watir (preferred), Selenium, Capybara, etc.,Knowledge of best practices for the Software Development Life Cycle (SLDC),Working knowledge of JIRA or other issues and project tracking software.,Experience with Git or other distributed revision control and source code management systems.,Background in payments desirable but not required,Experience in platforms non-functional requirements and operating systems,Experience in performance testing and/or security testing", "Excellent understanding of manipulation and analysis of large, complex data sets.", "modern tech tools and hi-tech equipment.", "Employee referral bonuses so you\u2019ll get the opportunity to work with friends (and get some extra cash in your pocket!).", "A commitment to an open, inclusive, and diverse work culture", "Upper intermediate knowledge of English;,Proficiency in R/Java/Python (numpy, scipy, matplotlib, pandas, sklearn).,Hands on experience with Kafka/ActiveMQ/RabbitMQ/Kestrel (or other message brokers) and Cassandra.,Understanding of Hadoop and/or MapReduce (or MPP systems), Apache Storm and Spark, Amazon AWS.,Knowledge of one or several NewSQL solutions - Drawntoscale, VoltDB, SpliceMachine, SQLFire, Impala, Redshift, Clustrix, NuoDB, Hadapt or other.,Skills in analysis of multi-dimensional datasets on production performance,Experience in predictive analytics, statistical analysis solutions development and data mining.,Knowledge of machine learning (k-NN, Naive Bayes, SVM, GBM, etc.) and artificial intelligence (AI).,Experience with common data science toolkits, such as R, Weka, Scikit-learn, Pandas.,Proficiency in NoSQL is strongly desired.,Proficiency in data lakes, enterprise data models.,Hands-on experience in digital twins of production processes.,Proficient in Linux, relational database design and methods for efficiently retrieving data,A degree in applied mathematics, computer science, physics or comparable fields.,Solid experience in custom ETL design, implementation and maintenance.,Great business sense (understanding of business metrics) and communication skills.,Experience in relevant positions is a strong plus.,Knowledge of scrum/agile/waterfall methodologies.,Ability to handle situations with numerous unknowns and set questions and tasks with little or no guidance.,Willingness to dive deep into the unstructured material to find an answer to a yet unknown question.,Ability to clearly communicate findings orally, in written and visually and come up with suggestions and advices.,Be able to work in fast-paced environment.,\u0413\u0440\u0430\u0444\u0438\u043a \u0440\u0430\u0431\u043e\u0442\u044b 5/2;,\u041e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0435 \u0442\u0440\u0443\u0434\u043e\u0443\u0441\u0442\u0440\u043e\u0439\u0441\u0442\u0432\u043e, C\u043e\u0446\u043f\u0430\u043a\u0435\u0442;,\u0423\u0441\u043b\u043e\u0432\u0438\u044f \u043f\u043e \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u043d\u043e\u0439 \u043f\u043b\u0430\u0442\u0435 \u0431\u0443\u0434\u0443\u0442 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u044b \u043f\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u043c \u0441\u043e\u0431\u0435\u0441\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f.", "Bachelors degree in Computer Engineering, Computer Science or related discipline, Masters Degree preferred,10+ years of ETL design, development, and performance tuning on Microsoft SSIS in SQL Server 2012 and above (2016 preferable) in a multi-dimensional Data Warehousing environment,10+ years of SSAS design, development, maintenance and performance tuning on Microsoft SQL Server 2012 and above (2016 preferable), with expert MDX and DAX skills,10+ years of advanced SQL Programming: PL/SQL, T-SQL, U-SQL,7+ years of Enterprise Data & Analytics solution architecture,2+ years of Power BI experience including mobile solutions,2+ years of strong and extensive hands on experience in Azure, preferably data heavy / analytics applications leveraging relational and NoSQL databases, Data Warehouse and Big Data,Experience with Azure Data Lake, Azure SQL Data Warehouse, Data Catalog, Azure Analysis Services, Data Bricks, Storage Account Gen2, Azure SQL Database, Azure DNS, Virtual Network, DocumentDB, Azure App Service, Data Factory,Experience with Big Data Technologies such as: Hadoop, Sqoop, Hive, Kafka, Spark, Pyspark, Python, Scala or Pig,Experience with Big Data Management (BDM) for relational and non-relational data (formats like json, xml, avro, parquet, copybook, etc.),Experience with setting up and operating data pipelines using Python or SQL,Strong analytical abilities and a strong intellectual curiosity", "Ability to work with large amounts of data,Experience with map-reduce frameworks - Hive, Hadoop and Spark (Elastic Map Reduce would be a plus),Strong grasp of statistics, data modelling and designing algorithms,Cloud service credits (AWS, Google Cloud, Azure, Digital Ocean),Public Transit allowance,Mobile data allowance,Flex days,Tea/Coffee Bar,In-office Snacks", "Hackathons", "Analysing problem domains to identify entities and data flows,Designing and implementing efficient & scalable data models,Automating the provisioning and management of data platforms,Familiarity with RDBMS, NoSQL and Distributed data technologies,End-to-end involvement in software delivery,Working with production systems,Ownership of a product or set of features within a product,A range of software delivery tools (source control, agile tools, CI etc.),Implementing and following best practices,Effective prioritisation of tasks and personal time management,Producing estimates for self and others,Demonstrating initiative,Coaching and mentoring more junior team members,Interacting with clients and/or product owners,Degree in computer science related discipline,Delivering in a highly collaborative agile environment,Understanding of blockchain technologies", "Bachelor\u2019s degree in a field such as Computer Science, Mathematics, Data Architecture, or equivalent experience or skills,Strong development knowledge and experience in key languages and environments such as Python, Scala, and SQL and noSQL databases,Advanced data sourcing and content management skills,Experience in time and task management,Strong attention to detail,Good written and verbal communication skills including the ability to collaborate across teams,Experience mentoring junior team members,Master\u2019s degree in Computer Science or other technical field,Strong development knowledge and experience in Hadoop big data stack (e.g. HDFS, Hive, Spark, etc.),Demonstrated capability to learn new technologies and follow industry trends", "Work with data science teams to deliver metrics to consumers", "PhD in a related discipline with at least 6 years of experience; Master\u2019s degree with 12 years of experience; or Bachelor\u2019s degree with experience in biomedical data management, data engineering, quality assurance, assay development, specimen data management or related discipline,Demonstrated proficiency with molecular biology concepts; and ability to support, develop and deploy laboratory and other research data management processes and procedures as they apply to complex, high dimensional data sets,Demonstrated ability to understand and translate high-level scientific datasets and results into data curation and management strategies and the underlying structures, curation processes and infrastructure required.,Strong understanding of LIMS systems and systematic, relational approaches to data integration and data processing workflows,Familiarity with Amazon Web Services (AWS).,Excellent skills in R programming and experience in additional computer languages such as Perl, Python, PHP, S-PLUS or Java (or C/C++),Extensive practical experience in working with diverse but highly-connected scientific knowledge collections and their query interfaces to enable research hypotheses around compound targets, mechanisms of action, and patient response,Proven ability to work in a team environment with clinical personnel, study monitors, computational biologists, biostatisticians, programmers, and medical writers,Knowledge of FDA/ICH guidelines and industry standard practices regarding data management are helpful but not required,Detailed knowledge and experience in case report form design, central laboratories, programming databases, query resolution, data validation,Computer skills: detailed knowledge of at least one data management system (Oracle Clinical or Clintrial preferred), experience with SAS data sets and conversion procedures required; knowledge of MS Office program suite required,Knowledge of distributed database design and implementation, LAMP/ MySQL, etc. with capability to perform/direct/assess implementation of such databases,Working knowledge of both Windows and Linux operating systems is required,Along with programming proficiency must have creativity, and show a strong capacity for independent thinking and the ability to grasp underlying biological questions,Must thrive in a complex, dynamic environment while adapting to dynamically changing priorities,Must have excellent time management and organizational skills", "7+ years of experience in software engineering or a related field,Extensive experience working with our stack (Python, PostgreSQL, AWS) or something similar,BA/BS degree in computer science or other engineering degree,Ability to quickly learn, understand and work with new emerging technologies, methodologies, and solutions,Experience building and managing production-quality ML models a plus,Growth-company DNA \u2013 thrives in a fast-paced environment and shares an excitement to lead change!", "PhD in a related discipline with at least 6 years of experience; Master\u2019s degree with 12 years of experience; or Bachelor\u2019s degree with experience in biomedical data management, data engineering, quality assurance, assay development, specimen data management or related discipline,Demonstrated proficiency with molecular biology concepts; and ability to support, develop and deploy laboratory and other research data management processes and procedures as they apply to complex, high dimensional data sets,Demonstrated ability to understand and translate high-level scientific datasets and results into data curation and management strategies and the underlying structures, curation processes and infrastructure required.,Strong understanding of LIMS systems and systematic, relational approaches to data integration and data processing workflows,Familiarity with Amazon Web Services (AWS).,Excellent skills in R programming and experience in additional computer languages such as Perl, Python, PHP, S-PLUS or Java (or C/C++),Extensive practical experience in working with diverse but highly-connected scientific knowledge collections and their query interfaces to enable research hypotheses around compound targets, mechanisms of action, and patient response,Proven ability to work in a team environment with clinical personnel, study monitors, computational biologists, biostatisticians, programmers, and medical writers,Knowledge of FDA/ICH guidelines and industry standard practices regarding data management are helpful but not required,Detailed knowledge and experience in case report form design, central laboratories, programming databases, query resolution, data validation,Computer skills: detailed knowledge of at least one data management system (Oracle Clinical or Clintrial preferred), experience with SAS data sets and conversion procedures required; knowledge of MS Office program suite required,Knowledge of distributed database design and implementation, LAMP/ MySQL, etc. with capability to perform/direct/assess implementation of such databases,Working knowledge of both Windows and Linux operating systems is required,Along with programming proficiency must have creativity, and show a strong capacity for independent thinking and the ability to grasp underlying biological questions,Must thrive in a complex, dynamic environment while adapting to dynamically changing priorities,Must have excellent time management and organizational skills", "Experience with integration of data from multiple data sources", "Healthy - Wellness programs, competitive medical benefit offerings,Happy \u2013 Recognition programs, a confidential employee assistance program, Perkspot/employee discount program and potentially flexible work arrangements such as staggered start times,Enriched \u2013 Tuition reimbursement, training and learning programs, and leadership development opportunities", "BS/BA (MS is a plus) in Computer Science, Information Systems, Engineering or related field and 10 years experience or equivalent training.,Experience working with enterprise and cloud database systems like MySQL, MongoDB, Oracle, MSSQL, Hadoop.,In-depth knowledge of Mac OS X system,Strong proficiency in PHP, preferable 5 years of hands on experience.,Fluency in bash and python scripting languages, with significant automation experience.,Proficient in scripting languages, such as PHP, Python, Ruby on Rails, NodeJS, Java etc.,Experience designing, developing and programming API solutions for integration from various client end points, including native applications,Desire to build and lead a rapid development team from project definition, scoping, estimating, planning, development, testing and launch. Strong preference in someone who has done this before,Up-to-date on current industry trends, third party integration, and open source tools with regard to database design, integration, data storage, analysis, security, and implementation,Strong analytical skills,Excellent work ethic and meticulous attention to detail", "Bachelors degree in Computer Engineering, Computer Science or related discipline, Masters Degree preferred,7+ years of ETL design, development, and performance tuning on Microsoft SSIS in SQL Server 2012 and above (2016 preferable) in a multi-dimensional Data Warehousing environment,7+ years of SSAS design, development, maintenance and performance tuning on Microsoft SQL Server 2012 and above (2016 preferable), with expert MDX and DAX skills,7+ years of advanced SQL Programming: PL/SQL, T-SQL, U-SQL,5+ years of Enterprise Data & Analytics solution architecture,2+ years of Power BI experience including mobile solutions,2+ years of strong and extensive hands on experience in Azure, preferably data heavy / analytics applications leveraging relational and NoSQL databases, Data Warehouse and Big Data,Experience with Azure Data Lake, Azure SQL Data Warehouse, Data Catalog, Azure Analysis Services, Data Bricks, Storage Account Gen2, Azure SQL Database, Azure DNS, Virtual Network, DocumentDB, Azure App Service, Data Factory,Experience with Big Data Technologies such as: Hadoop, Sqoop, Hive, Kafka, Spark, Pyspark, Python, Scala or Pig,Experience with Big Data Management (BDM) for relational and non-relational data (formats like json, xml, avro, parquet, copybook, etc.),Experience with setting up and operating data pipelines using Python or SQL,Strong analytical abilities and a strong intellectual curiosity", "PhD in a related discipline with at least 6 years of experience; Master\u2019s degree with 12 years of experience; or Bachelor\u2019s degree with experience in biomedical data management, data engineering, quality assurance, assay development, specimen data management or related discipline,Demonstrated proficiency with molecular biology concepts; and ability to support, develop and deploy laboratory and other research data management processes and procedures as they apply to complex, high dimensional data sets,Demonstrated ability to understand and translate high-level scientific datasets and results into data curation and management strategies and the underlying structures, curation processes and infrastructure required.,Strong understanding of LIMS systems and systematic, relational approaches to data integration and data processing workflows,Familiarity with Amazon Web Services (AWS).,Excellent skills in R programming and experience in additional computer languages such as Perl, Python, PHP, S-PLUS or Java (or C/C++),Extensive practical experience in working with diverse but highly-connected scientific knowledge collections and their query interfaces to enable research hypotheses around compound targets, mechanisms of action, and patient response,Proven ability to work in a team environment with clinical personnel, study monitors, computational biologists, biostatisticians, programmers, and medical writers,Knowledge of FDA/ICH guidelines and industry standard practices regarding data management are helpful but not required,Detailed knowledge and experience in case report form design, central laboratories, programming databases, query resolution, data validation,Computer skills: detailed knowledge of at least one data management system (Oracle Clinical or Clintrial preferred), experience with SAS data sets and conversion procedures required; knowledge of MS Office program suite required,Knowledge of distributed database design and implementation, LAMP/ MySQL, etc. with capability to perform/direct/assess implementation of such databases,Working knowledge of both Windows and Linux operating systems is required,Along with programming proficiency must have creativity, and show a strong capacity for independent thinking and the ability to grasp underlying biological questions,Must thrive in a complex, dynamic environment while adapting to dynamically changing priorities,Must have excellent time management and organizational skills", "Live the GoHealth Culture and ensure it is represented within the team.,Design, develop and deploy optimal extraction, transformation, and loading of data from various GoHealth and external data sources.,Monitor, execute and report on all data pipeline tasks while working with appropriate teams to take corrective action quickly, in case of,issues.,Perform unit testing, system integration testing and assist with user acceptance testing.,Adapt data components to accommodate changes in source data and new business requirements.,Create and maintain documentation of the technical detail design, operational support and maintenance procedures for all data pipeline,tasks.,Ensure data quality and compliance with development, architecture, reporting, and regulatory standards throughout entire data pipeline.,Collaborate with the rest of the Data Engineering Team, subject matter experts and department leaders to understand, analyze, build and,deliver new data-related processes and/or reports.,Ability to work with the rest of the Data Engineering Team to cross-train and provide support for various data engineering tasks.,Bachelor\u2019s Degree in computer science or equivalent experience required.,2+ years of experience in the design and development of data pipelines and tasks.,Strong analytical and problem solving ability with strong attention to detail and accuracy.,Good understanding of data warehousing concepts and dimensional data modeling.,Hands-on experience with troubleshooting performance issues and fine tuning SQL queries.,Experience in Python including in modules/libraries such as pandas, numpy, Flask, scikit-learn, and sci-py.,Proven experience extracting data from structured data sources (SQL, Excel, CSV files, Couchbase) and unstructured data sources,(Splunk, log files) both on-premise and in the cloud.,Experience consuming data from web services, REST and SOAP, HTML, XML and JSON.,Knowledge of version control systems using Git, Bitbucket, SVN, or Team Foundation.,Ability to handle multiple tasks and adapt to evolving business and technical environments.,Self-starter with the ability to work independently, take initiative, and learn new skills.,Excellent written and oral communication skills, with the ability to articulate complex processes to individuals of varying technical abilities,Experience in software engineering practices is required.,Experience in Microsoft SQL Server, SSIS, SSRS, Power BI, or Azure is preferred but not required.,Familiar with other data warehouse platforms like AWS Redshift or AWS Data Pipeline.,Join the team's daily meeting the includes other data engineers, data scientists, and data analysts.,Review open git pull requests, JIRA tickets, and Airflow DAG runs.,Work with the Data Engineering team and various teams on the following: Analyzing, designing, and implementing Airflow DAGs and Operators, RESTful services, integration with external and internal,RESTful services (e.g. Hubspot, Five9, etc.),Troubleshooting issues and resolving them \u2013 Airflow, SSIS, Tableau, SQL Server, SSRS, AWS, MySQL, Couchbase, etc.,Design, implement, and evolve GoHealth's data pipelines, Tableau data sources and data extracts, build and test automation (Jenkins, python, bash, gradle), etc.,Participate or lead Demo Wednesdays and Release Thursdays.,Join the weekly Data Engineering team meeting to review the roadmap and progress on projects.,Open vacation policy,401k match program,Medical, life, dental, and vision benefits,Flexible spending accounts,Subsidized gym memberships,Commuter and transit benefits,Professional growth opportunities,Casual dress code,Generous employee referral bonuses,Happy hours, ping-pong tournaments, and more company-sponsored events,GoHealth is an Equal Opportunity Employer", "Track record of successfully executing projects with multiple partners", "Knowledge-sharing activities", "2+ years of data engineering or backend development experience in Python,Experience with technologies like Hadoop, Spark, Hive, Presto, NiFi, and Luigi,AWS experience preferred,Data modeling experience preferred,Scala or Java experience preferred,You\u2019re a pragmatic engineer who will not only help execute, but also will provide a strong voice for technological direction of systems at Avant,You have a passion for data and empowering business with it,You are excited about evaluating and automating new technologies,You\u2019re entrepreneurial, self-driven, and take pride in improving our user\u2019s experiences,You have strong knowledge of Software Engineering & Data fundamentals as well as DevOps best practices,You thrive in a collaborative environment involving different stakeholders and subject matter experts and enjoys working with a diverse group of people with different expertise", "Bachelor\u2019s degree in a relevant discipline with at least 14 years \u2019 experience, Master\u2019s degree with at least 12 years\u2019 experience or PhD with at least 6 years\u2019 experience in biomedical data management, assay development, specimen data management or related discipline,Demonstrated proficiency with molecular biology assay concepts and ability to support, develop and deploy laboratory and other research data management processes and procedures as they apply to complex, high dimensional data sets,Extensive practical experience in curating and working with diverse but highly-connected scientific knowledge collections and their query interfaces to enable research hypotheses around compound targets, mechanisms of action, and patient response,Demonstrated ability to understand and translate high-level scientific datasets and results into data curation and management strategies,Proven ability to work in a team environment with clinical personnel, operational personnel, study monitors, computational biologists, biostatisticians, programmers, and medical writers,Demonstrated proficiency with current software engineering methodologies, such as Agile, source control, project management and issue tracking,Working knowledge of cloud computing. Preference will be given to candidates with AWS experience,Working knowledge of Rest APIs and container strategies strongly preferred.,Knowledge of distributed database design and implementation, LAMP/ MySQL, etc. with capability to perform/direct/assess implementation of such databases,Strong understanding of LIMS systems and systematic, relational approaches to data integration and data processing workflows,Excellent skills in R programming and experience in additional computer languages such as Perl, Python, PHP, S-PLUS or Java (or C/C++),Experience producing visualization of data sets (eg., R/shiny, Spotfire, etc),Working knowledge of both Windows and Linux operating systems is required,Along with programming proficiency must have creativity, and show a strong capacity for independent thinking and the ability to grasp underlying biological questions,Must thrive in a complex, dynamic environment while adapting to dynamically changing priorities,Must have excellent written and verbal communication and presentation skills,Must have excellent time management and organizational skills", "Ingestion pipelines and ETL using technologies like Kafka, Apache Spark,Experience with big data technologies such as Hadoop, Kafka, Akka, Mesos or similar highly desirable,Microservices design and implementation with REST / JSON a must,Experience with ElasticSearch highly desirable,Experience with Semantic Web, RDF, OWL, SPARQL & Linked Data highly desirable,Experience with large-scale production databases,Experience with a graph database highly desirable - Neo4J or Neptune,Any experience of other commercial search engines may be advantageous,Pair programming experience,Scrum agile experience,Kanban agile experience,JIRA experience,Release of search applications into a cloud environment, ideally blue-green deployments,Experience of working across a matrixed and distributed international organization,AWS system administration,Release processes and dev-ops", "Proficient understanding of distributed computing principles,Management of Hadoop cluster, with all included services {{unless you are going to have specific Big Data DevOps roles for this}},Ability to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data DevOps roles for this}},Proficiency with Hadoop v2, MapReduce, HDFS,Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}},Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala,Experience with Spark {{if you are including or planning to include it}},Experience with integration of data from multiple data sources,Experience with NoSQL databases, such as HBase, Cassandra, MongoDB,Knowledge of various ETL techniques and frameworks, such as Flume,Experience with various messaging systems, such as Kafka or RabbitMQ,Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O {{if you are going to integrate Machine Learning in your Big Data infrastructure}},Good understanding of Lambda Architecture, along with its advantages and drawbacks,Experience with Cloudera/MapR/Hortonworks {{you can specify the distribution you are currently using or planning to use here}},{{List any other technologies you are using or planning to use. Most Big Data Engineers will know some of the ones listed here: The Hadoop Ecosystem Table}},{{List education level or certification you require}}", "Hadoop/Spark/Google Cloud Dataflow,Kafka/Kinesis (AWS)/Google Pub/Sub (GCP),Elasticsearch/BigQuery,Other distributed SQL/NoSSQL databases,Python, Scala, Golang and R,3+ year(s) experience with provisioned and on-demand cloud computing platforms GCP/AWS/Azure,3+ year(s) experience with standard development tooling (e.g. Git, Jira, etc.),Knowledge of distributed computing fundamentals and the ability to design for scalability on Linux platforms,Some experience with machine learning algorithms and libraries (e.g. scikit-learn),A bachelor\u2019s degree in relevant technical field of Computer Science, Mathematics/Statistics, or similarly relevant engineering or computational discipline.,Data analytics/modeling experience,Strong mathematical/statistical analysis skills", "2 years of experience in data engineering constructing and maintaining databases and data pipelines.,1 year of experience creating and supporting production databases,1 year of experience with working with a data lake environment,Experience collecting, transforming and storing large amounts of data.,Bachelor\u2019s degree and experience working in data engineering OR Bachelor\u2019s degree or Master\u2019s degree from a computer science field such as Computer Science, Information Sciences, or Informatics.,Experience creating data pipelines for machine learning,Data management certifications.,Strong knowledge of SQL and NoSQL database technologies.,Strong knowledge of at least one scripting language such as Java, Python.,Knowledge of Hadoop, Spark or other big data processing frameworks,Submit the Staff Vacancy Application.,Submit the Voluntary Self-Identification of Disability forms.,Upload your cover letter, resume (months and years of employment must be included), and academic credentials (unofficial transcripts or diploma may be acceptable) and names/contact information for three references.", "Relevant degree or work experience", "This is a full time exempt position", "Bonus points if you bring real world experience with AWS (EMR, E2, Kinesis, S3)", "Distributed computing principles,Legacy and modern database architectures,Hadoop-based technologies (e.g. MapReduce, Hive and Pig),SQL-based technologies (e.g. PostgreSQL and MySQL),NoSQL technologies (e.g. Cassandra and MongoDB),Stream-processing systems (e.g. Storm or Spark-Streaming),ETL tools and APIs,Optimizing data storage and retrieval for specific use cases,Testing and validating the accuracy of data transformations,Cloud computing architectures, preferably with specific expertise in Microsoft Azure and AWS,Creative problem-solving that is sensitive to available time and resource constraints,Effective listening and communication", "Demonstrate strong understanding of development processes and agile methodologies", "You have an undergraduate and / or graduate degree in computer science or a similar technical field, with a sound understanding of statistics,You have 1-2 years of industry experience as a data engineer,You have hands-on experience doing ETL and have written data pipelines in either Spark or MapReduce,You have a sound understanding of SQL or CQL,You have worked with data lakes such as S3 or HDFS,You have worked with various databases, such as Postgres, Cassandra, or Redshift before, and understand their pros and cons,You have a working knowledge of the following technologies, or are not afraid of picking them up on the fly: Mesos, Chronos/cron, Marathon, Jenkins,You are fluent in at least one scripting language (preferably NodeJS or python) and one compiled language (such as Scala, Java, or C),You have great communication skills and ability to work with others", "Data serialization such as JSON, avro, parquet", "Building Cube/Cube-like products", "Cluster managers (eg Docker, Apache Mesos, Kubernetes)", "401k retirement savings plan", "Daily catered lunches from LA\u2019s best restaurants and fully stocked kitchen", "Indemnity according to the profile", "Ability to work successfully with 3rd party vendors to support application enhancements or troubleshoot problems as required to meet business processes and priorities.", "Excellent experience of Amazon Web Services (AWS),Strong coding skills in Python and Java,Demonstrable experience of messaging queue technology such as Apache Kafka (or similar),Hands-on experience of real-time systems at production stage,5+ years of post-academic experience in a data engineering capacity,Experience of Docker,Apache Spark,ElasticSearch", "Google Analytics", "Agile/Scrum working practices", "Bachelors and 2+ years of experience or Masters degree and 1+ years of experience,Experience and proficiency with Scala,Experience and proficiency with Spark,A deep understanding of machine learning and interest in applying it at scale.,Experience with data cleaning, preparation, and feature building and selection techniques.,Experience working with large data sets to solve problems.,Effective communication, interpersonal and teamwork skills.,Ability to handle multiple concurrent projects while working independently and in teams.,Ability to work in a fast-paced and deadline driven environment.,Experience in hierarchical models, random effect models, and online learning", "Performs other duties as assigned.", "Bachelor Degree and 4 years Information Technology experience OR Technical Certification and/or College Courses and 6 years Information Technology experience OR 8 years Information Technology experience.,Proficiency in domain driven design and domain modeling.,Experience with NoSql solutions, such as Gemfire, Cassandra, HBase,Distributed Caching,Experience with any relational database system, such as (but not limited to) MySQL, SQL Server, Oracle, or PostgreSQL,Communicate in written and verbal form effectively,CI/CD tools, such as Jenkins, Concourse and Ansible,CA", "All persons employed by the Baltimore County Public Schools, regular and temporary, are required to be fingerprinted and have a criminal background investigation (State of Maryland, Senate Bill 315, effective October 1, 1986) completed. The fee charged for fingerprinting is $81.00. An identification card will be issued which must be shown prior to employment.,Anyone offered employment is required to provide proper identification and documentation of eligibility for employment in the US.,If you have military experience you will be asked to provide a copy of DD214.,Official transcripts for all higher education must be received prior to contract signing.,Some positions will require employees to undergo a physical examination and/or drug testing.,All newly hired personnel must attend a Badges and Benefits session.,Additional job verification will be required for salary credit.", "Fluent in English,3+ years experience working with Java, Scala or any other Object Oriented language,Strong Object-Oriented experience, and a true passion for writing high-quality code,Experience with quick prototyping and ability to work in a dynamic environment,Experience with NoSQL databases (MongoDB, etc\u2026),Experience with API-based architecture,Experience with distributed framework for parallel processing such as Hadoop, Pig, Spark and Hive,Strong experience with Java or Scala is a plus,Experience with large scale data/distributed systems,Ability to work independently while being a strong part of the team,Experience with real-time applications is a plus,Very attractive compensation package including equity,Have a real impact,Awesome work environment at a company with a huge vision,Be right there from the start of a success story,Relaxed, fun work environment and flexible working hours,Extras like catered lunch, gym membership,private health insurance\u2026", "A good understanding of and adherence to data security standards", "3+ years of experience with at least one of the following: Google Cloud Dataflow/Google PubSub, Elasticsearch, Spark. Any of the following are a plus: Hadoop Kafka/Kinesis, BigQuery, Other distributed SQL/NoSSQL databases,3+ years of experience with at least one of the following: Python, Golang, Clojure and R,3+ year(s) experience with provisioned and on-demand cloud computing platforms such as GCP or AWS/Azure,3+ year(s) experience with standard development tooling (e.g. Git, Jira, etc.),Knowledge of distributed computing fundamentals and the ability to design for scalability on Linux platforms", "Google Analytics", "Their current tech stack includes: Python, Go, AWS, Spark, GraphDB, Docker, Kubernetes, Jenkins, JSON and Git.,Extensive experience with developing data solutions on AWS.,Experience in building a data vision and/or strategy together with upper management.,Previous experience within FinTech or a related area is a plus.", "A workplace recognized as the Best Consumer Web Company by Built in Chicago, Top Company Culture by Entrepreneur, a Top Workplace by Chicago Tribune, and one of Chicago\u2019s Best Places to Work for Women Under 35 by Crain\u2019s Chicago Business.", "Power BI skills", "Excellent experience of Amazon Web Services (AWS),Strong coding skills in Python and Java,Demonstrable experience of messaging queue technology such as Apache Kafka (or similar),Hands-on experience of real-time systems at production stage,5+ years of post-academic experience in a data engineering capacity,Experience of Docker,Apache Spark,ElasticSearch", "Experience with HTTP, REST, SSL, identity authentication", "Comfortable building and maintaining data infrastructure in the cloud", "8 years and above", "Spontaneous nerf-gun wars to wake you up and Thursday happy hours to wind you down", "Excellent verbal and written communication skills", "Experience with Big Data and analytics systems (Hadoop, Storm, Spark, etc\u2026),2+ Years doing back-end web services programming (Java, PHP),2+ Years doing database work (mysql, Postgres / RedShift),Experience using Amazon Web Services a plus,Experience with scalable systems in a load balanced environment a plus,B.S. in Computer Science,Object Oriented Programming (Python / Java),Database Technologies (Redshift / Postgres / Spark / Presto),Amazon Web Services (S3, SQS, Kinesis, ECS, ECR, EMR)", "Bachelor's degree in MIS, Computer Science, or a related field from an accredited college/university (or equivalent),1 year developing ETL processes using enterprise tools such as SAP Data Services, Talend, or Informatica,1 year working with relational databases and developing complex SQL,Experience with cloud-native databases and development tools,Familiarity with in-memory databases such as SAP HANA,Proven ability to collaborate on cross-functional teams,System availability,Data Availability,Data Quality", "Advanced degree in computer science, information systems, or closely related field,1-5 years of hands-on experience in software engineering or IT infrastructure role,Advanced knowledge of Hadoop stack, with prior experience in Hive, Pig, HBase, Impala and Sqoop,Advanced knowledge of object oriented programming, distributed systems and software design principles,Advanced knowledge of database maintenance and administration using MS SQL Server,Strong programming experience in Java, Python and R,Hands-on experience in Microsoft Azure or Amazon EC2 cloud platform,Demonstrated ability to design and implement ETL workflows across both Windows and Linux environments,Should be a highly motivated individual with the ability to work effectively with people across all levels in an organization,Position requires ability to travel about 20%,Nice to have: programming experience in C++, SAS, and JavaScript,Experienced in RDBMS systems like Oracle Database, IBM DB2, or MySQL,Experienced in NoSQL systems like MongoDB, Redis, or Cassandra,Experienced in Apache Spark", "Share success: Given how hard it is to promote change in our industry, celebrate each success along the way.", "Google Analytics", "Experienced in data modeling, ETL development, and data warehousing,Experience with various AWS services including RDS, redshift, and S3,Experience consuming and cleaning data from third-party APIs and other sources,Experience with Big Data technologies such as Hive, Hadoop, or Spark,Strong organizational and analytical skills with an ability to extract meaning from data,Strong communication skills to interact with stakeholders and customers,Strong experience in Python or similar languages is a plus", "Inspire offers competitive compensation and equity packages, plus benefits such as health, vision, life, and dental insurance - Not to mention, unlimited vacation, 401(k) plan, and LOTS of cupcakes!", "A collaborative nature and entrepreneurial spirit. Prior startup experience a huge plus", "Are excited to hear from you", "MSSQL server,SQL query,Microsoft Excel,Embrace our VBA project,Develop internal tools/utilities,Set up a unit test strategy and develop them,Improve code and performance,Participate to the innovation process,VBA language and Microsoft Excel,Unit test strategy,Object oriented programming,Software versioning and revision control (SVN),Familiarity with Windows and Mac OS (cross platform development),Continuous integration environment (Jenkins),Embrace our C++ project under Visual Studio and XCode,Develop internal tools/utilities,Set up a unit test strategy and develop them,Improve our C++ architecture and performance,Lead our statistical developers for improving C++ code,Participate to the innovation process,OOP and C++ ( C++ 14 / 17),Visual studio 2017 and XCode 9.4 or higher,Software versioning and revision control (SVN),C++ unit test strategy,HPC and multi-threading,COM object,Familiarity with design pattern,Continuous integration environment (Jenkins),+ 400 m2 of office space;,8 doctors and 6 data scientists and 30 developers specializing in mathematics and statistics;,55 people in total spread over the 4 floors of the place;,XLSTAT: www.xlstat.com,About Addinsoft: www.addinsoft.com", "Bachelor\u2019s degree from an accredited college or university in Computer Science, Computer Engineering, Engineering or a related field with seven years of experience; a Master\u2019s degree with five years of experience, or a PhD with two years of experience,Fluency in several programming languages such as Python, Scala, or Java, with the ability to pick up new languages and technologies quickly,Understanding of cloud, and distributed systems principles, including load balancing, networks, scaling, in-memory vs. disk,Experience with Large Scale/ Big Data methods, such as MapReduce, Hadoop, Spark, Hive, Impala, or Storm,Strong written and verbal communication skills, ability to work in dynamic team environments, and multi-task effectively.", "Google Analytics", "Are excited to hear from you", "At least three years working as a Data Engineer, ideally having been involved in setting up data infrastructure/storage in the cloud - even better if this was on GCP,Experience with building production level Python applications, and data modelling using SQL.,Comfort in delivering automated reports and data visualisation solutions using tools like Tableau, this is where the \u00b4full stack-ness\u00b4 comes into play.,An autonomous mindset - you will be given the mandate to design and implement modern data solutions of your choosing. You should be up to that challenge.", "LI-PA1", "Working for a truly global company with offices in 15+ countries", "Monthly team outings (Ball games, happy hours, hikes, etc.)", "At least 5 years experience developing business intelligence solutions using Tableau.,Experience with tools, such as SSRS, Cognos, Microstrategy, QlikView, Spotfire, etc. is a plus.,Familiarity with at least two different database platforms, such as Teradata, Oracle, MS SQL server or other platforms; Teradata and MS SQL strongly preferred - Extremely strong SQL skills.,Dimensional data modeling skills.,Hands-on experience with ETL development a plus.,Excellent interpersonal, team management, facilitation and communication skills; must be able to communicate effectively at all levels of the client's organization.,Competitive salary with performance based bonus opportunities,Single and Family Health Insurance plans, including Dental coverage,Short-Term and Long-Term disability,Matching 401(k),Competitive Paid Time Off,Training and Certification opportunities eligible for expense reimbursement,Team building and social activities,Mentor program to help you develop your career"], "meta": ["www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.usajobs.gov", "www.usajobs.gov", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.yelp.com", "www.indeed.com", "www.indeed.com", "www.airbnb.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.llnl.gov", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.mckinsey.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.excite.com", "www.indeed.com", "www.mediabistro.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.llnl.gov", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.meetup.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "japan.careers.vmware.com", "www.indeed.com", "www.indeed.com", "indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.mediabistro.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.sitepoint.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.monster.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.accenture.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "www.sophos.com", "stackoverflow.com", "www.sitepoint.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.glassdoor.com", "stackoverflow.com", "www.accenture.com", "www.accenture.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "www.accenture.com", "www.mediabistro.com", "japan.careers.vmware.com", "stackoverflow.com", "stackoverflow.com", "www.sitepoint.com", "www.sitepoint.com", "www.mediabistro.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "www.sitepoint.com", "www.accenture.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.glassdoor.com", "dfwishiring.dallasnews.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "www.sophos.com", "stackoverflow.com", "www.biospace.com", "www.geekwire.com", "www.snagajob.com", "zapier.com", "www.governmentjobs.com", "www.kdnuggets.com", "www.standardmedia.co.ke", "www.themuse.com", "www.insurancejournal.com", "www.timesofmalta.com", "www.snagajob.com", "www.biospace.com", "www.insurancejournal.com", "www.biospace.com", "www.biospace.com", "www.careercast.com", "www.careercast.com", "www.devex.com", "usa.visa.com", "www.internships.com", "www.devex.com", "www.biospace.com", "www.geekwire.com", "feedproxy.google.com", "www.kdnuggets.com", "www.rigzone.com", "www.careercast.com", "www.careercast.com", "www.airweb.org", "slack.com", "www.biospace.com", "www.gene.com", "www.insurancejournal.com", "www.biospace.com", "feedproxy.google.com", "www.gene.com", "www.internships.com", "www.geekwire.com", "www.biospace.com", "stripe.com", "www.biospace.com", "tweakers.net", "hh.ru", "www.insurancejournal.com", "www.devex.com", "www.devex.com", "www.careercast.com", "www.insurancejournal.com", "www.themuse.com", "www.gene.com", "www.gene.com", "feedproxy.google.com", "www.zynga.com", "www.bizcommunity.com", "jobs.apple.com", "www.flexjobs.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.latpro.com", "www.indeed.co.uk", "www.bizcommunity.com", "www.shakeshack.com", "www.indeed.co.uk", "www.flexjobs.com", "www.flexjobs.com", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.bizcommunity.com", "www.reed.co.uk", "www.flexjobs.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "technical.ly", "www.indeed.co.uk", "www.smartrecruiters.com", "www.indeed.co.uk", "www.bizcommunity.com", "www.computerworld.co.nz", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.nytco.com", "jobs.apple.com", "www.careerbliss.com", "www.flexjobs.com", "jobs.apple.com", "www.flexjobs.com", "jobs.apple.com", "www.cs.mcgill.ca", "www.computerworld.co.nz", "www.zynga.com", "www.ziprecruiter.com", "optics.org", "jobs.apple.com", "jobs.apple.com", "www.gettinghired.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.indeed.co.uk", "www.flexjobs.com", "jobs.apple.com", "www.bizcommunity.com", "jobs.apple.com", "hiring.monster.com", "www.gettinghired.com", "www.ziprecruiter.com", "jobs.apple.com", "jobs.apple.com", "www.sportsbusinessdaily.com", "jobs.apple.com", "www.bizcommunity.com", "jobs.apple.com", "jobs.apple.com", "www.ziprecruiter.com", "www.gettinghired.com", "www.bizcommunity.com", "www.techworld.com.au", "www.techworld.com.au", "jobs.apple.com", "www.computerworld.co.nz", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.diglib.org", "jobs.apple.com", "newyork.craigslist.org", "jobs.apple.com", "www.flexjobs.com", "jobs.apple.com", "jobs.apple.com", "www.bizcommunity.com", "www.techworld.com.au", "www.indeed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "technical.ly", "jobs.apple.com", "technical.ly", "jobs.apple.com", "www.gettinghired.com", "jobs.apple.com", "careers.peopleclick.com", "jobs.apple.com", "www.cs.mcgill.ca", "jobs.apple.com", "www.flexjobs.com", "jobs.apple.com", "www.ziprecruiter.com", "www.bizcommunity.com", "www.ziprecruiter.com", "www.efinancialcareers.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "technical.ly", "www.computerworld.co.nz", "www.indeed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.flexjobs.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.zynga.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.gettinghired.com", "www.bizcommunity.com", "www.henkel.com", "jobs.apple.com", "jobs.apple.com", "technical.ly", "jobs.apple.com", "www.indeed.co.uk", "jobs.apple.com", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.gettinghired.com", "www.flexjobs.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "jobs.apple.com", "www.flexjobs.com", "jobs.apple.com", "www.bizcommunity.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.flexjobs.com", "www.bizcommunity.com", "www.ziprecruiter.com", "jobs.apple.com", "www.ziprecruiter.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.computerworld.co.nz", "jobs.apple.com", "www.careerbliss.com", "www.indeed.co.uk", "www.net-temps.com", "www.bizcommunity.com", "www.builtincolorado.com", "jobs.lever.co", "www.builtincolorado.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "careers.insidehighered.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "spb.hh.ru", "www.builtincolorado.com", "jobs.theguardian.com", "www.pracuj.pl", "boards.greenhouse.io", "boards.greenhouse.io", "spb.hh.ru", "jobs.newscientist.com", "ca.indeed.com", "www.builtincolorado.com", "jobs.theguardian.com", "www.builtinchicago.org", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "www.builtinchicago.org", "careers.insidehighered.com", "jobs.newscientist.com", "jobs.newscientist.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.level39.co", "www.builtinchicago.org", "jobs.newscientist.com", "jobs.theguardian.com", "www.toptal.com", "www.builtincolorado.com", "careers.insidehighered.com", "jobs.theguardian.com", "www.builtinchicago.org", "www.builtinchicago.org", "careers.insidehighered.com", "www.builtinchicago.org", "jobs.lever.co", "www.builtinchicago.org", "www.builtinchicago.org", "www.jobserve.com", "boards.greenhouse.io", "www.builtinla.com", "www.parking-net.com", "jobs.seattletimes.com", "jobs.telegraph.co.uk", "www.careerjet.co.uk", "www.careerjet.co.uk", "careers.walmart.com", "electricenergyonline.com", "illinoisjoblink.illinois.gov", "www.topschooljobs.org", "www.indeed.es", "illinoisjoblink.illinois.gov", "www.builtincolorado.com", "www.careerjet.co.uk", "www.iamexpat.nl", "www.parking-net.com", "www.randstad.co.uk", "jobs.telegraph.co.uk", "www.connecticum.de", "www.builtinla.com", "ejob.bz", "www.parking-net.com", "www.builtinla.com", "jobs.gamasutra.com", "electricenergyonline.com", "www.indeed.es", "marketingevolution.theresumator.com", "www.careerjet.co.uk", "www.builtinla.com", "www.builtinla.com", "www.parking-net.com", "careers.walmart.com", "www.xlstat.com", "www.classifiedads.com", "www.careerjet.co.uk", "careers.walmart.com", "www.iamexpat.nl", "www.indeed.es", "join.irdeto.com", "www.builtinla.com", "justjobs.com"]}}; }
plotInterface = buildViz(1000,
600,
null,
null,
false,
false,
false,
false,
false,
true,
false,
false,
true,
0.1,
false,
undefined,
undefined,
getDataAndInfo(),
true,
false,
null,
null,
null,
null,
true,
false,
true,
false,
null,
null,
10,
null,
null,
null,
false,
true,
true,
undefined,
null,
false,
false,
".3f",
".3f",
false,
-1,
true,
false,
true,
false,
false,
false,
true,
null,
null,
null,
false,
null,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
14,
0);


autocomplete(
    document.getElementById('searchInput'),
    plotInterface.data.map(x => x.term).sort(),
    plotInterface
);

</script>
