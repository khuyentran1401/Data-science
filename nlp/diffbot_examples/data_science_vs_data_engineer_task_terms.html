<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}

svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.grid line {
  stroke: lightgrey;
  stroke-opacity: 0.4;
  shape-rendering: crispEdges;
}

.grid path {
  stroke-width: 0;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 50px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}
/*
input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}*/

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}
/*
input:focus {
  background-color: lightyellow;
  outline: none;
}*/

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}

.topic_preview {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;
  font-weight: normal;
  text-decoration: none;
}


#d3-div-1-categoryinfo {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;    

}


#d3-div-1-title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}

.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}

.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  color: darkblue;
}

.not_match {
    background-color: #F0F8FF;
}
    
.contexts {
  width: 45%;
  float: left;
}

.neut_display {
  display: none;
  float: left
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.obscured {
  /*font-size: 14px;
  font-weight: normal;
  color: dimgrey;
  font-family: Helvetica;*/
  text-align: center;
}

.small_label {
  font-size: 10px;
}

#d3-div-1-corpus-stats {
  text-align: center;
}

#d3-div-1-cat {
}

#d3-div-1-notcat {
}

#d3-div-1-neut {
}

#d3-div-1-neutcol {
  display: none;
}
/* Adapted from https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_autocomplete */

.autocomplete {
  position: relative;
  display: inline-block;
}

input {
  border: 1px solid transparent;
  background-color: #f1f1f1;
  padding: 10px;
  font-size: 16px;
}

input[type=text] {
  background-color: #f1f1f1;
  width: 100%;
}

input[type=submit] {
  background-color: DodgerBlue;
  color: #fff;
  cursor: pointer;
}

.autocomplete-items {
  position: absolute;
  border: 2px solid #d4d4d4;
  border-bottom: none;
  border-top: none;
  z-index: 99;
  /*position the autocomplete items to be the same width as the container:*/
  top: 100%;
  left: 0;
  right: 0;
}

.autocomplete-items div {
  padding: 10px;
  cursor: pointer;
  background-color: #fff;
  border-bottom: 2px solid #d4d4d4;
}

/*when hovering an item:*/
.autocomplete-items div:hover {
  background-color: #e9e9e9;
}

/*when navigating through the items using the arrow keys:*/
.autocomplete-active {
  background-color: DodgerBlue !important;
  color: #ffffff;
}
</style>

<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>

<!-- INSERT SEMIOTIC SQUARE -->
<!--<a onclick="maxFreq = Math.log(data.map(d => d.cat + d.ncat).reduce((a,b) => Math.max(a,b))); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, false); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, true)">View Score Plot</a>-->
<span id="d3-div-1-title-div"></span>
<div class="scattertext" id="d3-div-1" style="float: left"></div>
<div style="floag: left;">
    <div autocomplete="off">
        <div class="autocomplete">
            <input id="searchInput" type="text" placeholder="Search the chart">
        </div>
    </div>
</div>
<br/>
<div id="d3-div-1-corpus-stats"></div>
<div id="d3-div-1-overlapped-terms"></div>
<a name="d3-div-1-snippets"></a>
<a name="d3-div-1-snippetsalt"></a>
<div id="d3-div-1-termstats"></div>
<div id="d3-div-1-overlapped-terms-clicked"></div>
<div id="d3-div-1-categoryinfo" style="display: hidden"></div>
<div id="d3-div-2">
  <div class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-cathead"></div>
    <div class="snippet" id="d3-div-1-cat"></div>
  </div>
  <div id="d3-div-1-notcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-notcathead"></div>
    <div class="snippet" id="d3-div-1-notcat"></div>
  </div>
  <div id="d3-div-1-neutcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-neuthead"></div>
    <div class="snippet" id="d3-div-1-neut"></div>
  </div>
</div>
<script charset="utf-8">
    // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 1000,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.1,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null,
                     fullData = null,
                     showTopTerms = true,
                     showNeutral = false,
                     getTooltipContent = null,
                     xAxisValues = null,
                     yAxisValues = null,
                     colorFunc = null,
                     showAxes = true,
                     showExtra = false,
                     doCensorPoints = true,
                     centerLabelsOverPoints = false,
                     xAxisLabels = null,
                     yAxisLabels = null,
                     topic_model_preview_size = 10,
                     verticalLines = null,
                     horizontal_line_y_position = null,
                     vertical_line_x_position = null,
                     unifiedContexts = false,
                     showCategoryHeadings = true,
                     showCrossAxes = true,
                     divName = 'd3-div-1',
                     alternativeTermFunc = null,
                     includeAllContexts = false,
                     showAxesAndCrossHairs = false,
                     x_axis_values_format = '.3f',
                     y_axis_values_format = '.3f',
                     matchFullLine = false,
                     maxOverlapping = -1,
                     showCorpusStats = true,
                     sortDocLabelsByName = false,
                     alwaysJump = true,
                     highlightSelectedCategory = false,
                     showDiagonal = false,
                     useGlobalScale = false,
                     enableTermCategoryDescription = true,
                     getCustomTermHtml = null,
                     headerNames = null,
                     headerSortingAlgos = null,
                     ignoreCategories = false,
                     backgroundLabels = null,
                     labelPriorityColumn = null,
                     textColorColumn = undefined,
                     suppressTextColumn = undefined,
                     backgroundColor = undefined,
                     censorPointColumn = undefined,
                     rightOrderColumn = undefined,
                     subwordEncoding = null,
                     topTermsLength = 14,
                     topTermsLeftBuffer = 0
    ) {
        function formatTermForDisplay(term) {
            if (subwordEncoding === 'RoBERTa' && (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289))
                term = '_' + term.substr(1, term.length - 1);
            return term;
        }

        //var divName = 'd3-div-1';
        // Set the dimensions of the canvas / graph
        var padding = {top: 30, right: 20, bottom: 30, left: 50};
        if (!showAxes) {
            padding = {top: 30, right: 20, bottom: 30, left: 50};
        }
        var margin = padding,
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;
        fullData.data.forEach(function (x, i) {
            x.i = i
        });

        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        if (unifiedContexts) {
            document.querySelectorAll('#' + divName + '-' + 'notcol')
                .forEach(function (x) {
                    x.style.display = 'none'
                });
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '90%'
                });
        } else if (showNeutral) {
            if (showExtra) {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '25%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol', 'extracol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '25%'
                        });
                })

            } else {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '33%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '33%'
                        });
                })


            }
        } else {
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '45%'
                    //x.style.display = 'inline'
                    x.style.float = 'left'
                });

            ['notcol'].forEach(function (columnName) {
                document.querySelectorAll('#' + divName + '-' + columnName)
                    .forEach(function (x) {
                        //x.style.display = 'inline'
                        x.style.float = 'left'
                        x.style.width = '45%'
                    });
            })
        }

        var yAxis = null;
        var xAxis = null;

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }


        function bs(ar, x) {
            function bsa(s, e) {
                var mid = Math.floor((s + e) / 2);
                var midval = ar[mid];
                if (s == e) {
                    return s;
                }
                if (midval == x) {
                    return mid;
                } else if (midval < x) {
                    return bsa(mid + 1, e);
                } else {
                    return bsa(s, mid);
                }
            }

            return bsa(0, ar.length);
        }


        console.log("fullData");
        console.log(fullData);


        var sortedX = fullData.data.map(x => x).sort(function (a, b) {
            return a.x < b.x ? -1 : (a.x == b.x ? 0 : 1);
        }).map(function (x) {
            return x.x
        });

        var sortedOx = fullData.data.map(x => x).sort(function (a, b) {
            return a.ox < b.ox ? -1 : (a.ox == b.ox ? 0 : 1);
        }).map(function (x) {
            return x.ox
        });

        var sortedY = fullData.data.map(x => x).sort(function (a, b) {
            return a.y < b.y ? -1 : (a.y == b.y ? 0 : 1);
        }).map(function (x) {
            return x.y
        });

        var sortedOy = fullData.data.map(x => x).sort(function (a, b) {
            return a.oy < b.oy ? -1 : (a.oy == b.oy ? 0 : 1);
        }).map(function (x) {
            return x.oy
        });
        console.log(fullData.data[0])

        function labelWithZScore(axis, axisName, tickPoints, axis_values_format) {
            var myVals = axisName === 'x' ? sortedOx : sortedOy;
            var myPlotedVals = axisName === 'x' ? sortedX : sortedY;
            var ticks = tickPoints.map(function (x) {
                return myPlotedVals[bs(myVals, x)]
            });
            return axis.tickValues(ticks).tickFormat(
                function (d, i) {
                    return d3.format(axis_values_format)(tickPoints[i]);
                })
        }

        if (xAxisValues) {
            xAxis = labelWithZScore(d3.axisBottom(x), 'x', xAxisValues, x_axis_values_format);
        } else if (xAxisLabels) {
            xAxis = d3.axisBottom(x)
                .ticks(xAxisLabels.length)
                .tickFormat(function (d, i) {
                    return xAxisLabels[i];
                });
        } else {
            xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        }
        if (yAxisValues) {
            yAxis = labelWithZScore(d3.axisLeft(y), 'y', yAxisValues, y_axis_values_format);
        } else if (yAxisLabels) {
            yAxis = d3.axisLeft(y)
                .ticks(yAxisLabels.length)
                .tickFormat(function (d, i) {
                    return yAxisLabels[i];
                });
        } else {
            yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));
        }

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");


        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230),
            d3.rgb(130, 130, 130));
        // setup fill color
        if (color == null) {
            color = d3.interpolateRdYlBu;
        }
        if ((headerNames !== undefined && headerNames !== null)
            && (headerSortingAlgos !== undefined && headerSortingAlgos !== null)) {
            showTopTerms = true;
        }

        var pixelsToAddToWidth = 200;
        if (!showTopTerms && !showCharacteristic) {
            pixelsToAddToWidth = 0;
        }

        if (backgroundColor !== undefined) {
            document.body.style.backgroundColor = backgroundColor;
        }

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + pixelsToAddToWidth)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");


        origSVGLeft = svg.node().getBoundingClientRect().left;
        origSVGTop = svg.node().getBoundingClientRect().top;
        var lastCircleSelected = null;

        function getCorpusWordCounts() {
            var binaryLabels = fullData.docs.labels.map(function (label) {
                return 1 * (fullData.docs.categories[label] != fullData.info.category_internal_name);
            });
            var wordCounts = {}; // word -> [cat counts, not-cat-counts]
            var wordCountSums = [0, 0];
            fullData.docs.texts.forEach(function (text, i) {
                text.toLowerCase().trim().split(/\W+/).forEach(function (word) {
                    if (word.trim() !== '') {
                        if (!(word in wordCounts))
                            wordCounts[word] = [0, 0];
                        wordCounts[word][binaryLabels[i]]++;
                        wordCountSums[binaryLabels[i]]++;
                    }
                })
            });
            return {
                avgDocLen: (wordCountSums[0] + wordCountSums[1]) / fullData.docs.texts.length,
                counts: wordCounts,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)]
                })
            };
        }

        function getContextWordCounts(query) {
            var wordCounts = {};
            var wordCountSums = [0, 0];
            var priorCountSums = [0, 0];
            gatherTermContexts(termDict[query])
                .contexts
                .forEach(function (contextSet, categoryIdx) {
                    contextSet.forEach(function (context) {
                        context.snippets.forEach(function (snippet) {
                            var tokens = snippet.toLowerCase().trim().replace('<b>', '').replace('</b>', '').split(/\W+/);
                            var matchIndices = [];
                            tokens.forEach(function (word, i) {
                                if (word === query) matchIndices.push(i)
                            });
                            tokens.forEach(function (word, i) {
                                if (word.trim() !== '') {
                                    var isValid = false;
                                    for (var matchI in matchIndices) {
                                        if (Math.abs(i - matchI) < 3) {
                                            isValid = true;
                                            break
                                        }
                                    }
                                    if (isValid) {
                                        //console.log([word, i, matchI, isValid]);
                                        if (!(word in wordCounts)) {
                                            var priorCounts = corpusWordCounts.counts[word]
                                            wordCounts[word] = [0, 0].concat(priorCounts);
                                            priorCountSums[0] += priorCounts[0];
                                            priorCountSums[1] += priorCounts[1];
                                        }
                                        wordCounts[word][categoryIdx]++;
                                        wordCountSums[categoryIdx]++;
                                    }
                                }
                            })
                        })
                    })
                });
            return {
                counts: wordCounts,
                priorSums: priorCountSums,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)];
                })
            }

        }

        function denseRank(ar) {
            var markedAr = ar.map((x, i) => [x, i]).sort((a, b) => a[0] - b[0]);
            var curRank = 1
            var rankedAr = markedAr.map(
                function (x, i) {
                    if (i > 0 && x[0] != markedAr[i - 1][0]) {
                        curRank++;
                    }
                    return [curRank, x[0], x[1]];
                }
            )
            return rankedAr.map(x => x).sort((a, b) => (a[2] - b[2])).map(x => x[0]);
        }


        function getDenseRanks(fullData, categoryNum) {
            console.log("GETTING DENSE RANKS")
            console.log("CAT NUM " + categoryNum)
            console.log(fullData)

            var fgFreqs = Array(fullData.data.length).fill(0);
            var bgFreqs = Array(fullData.data.length).fill(0);
            var categoryTermCounts = fullData.termCounts[categoryNum];

            Object.keys(categoryTermCounts).forEach(
                key => fgFreqs[key] = categoryTermCounts[key][0]
            )
            fullData.termCounts.forEach(
                function (categoryTermCounts, otherCategoryNum) {
                    if (otherCategoryNum != categoryNum) {
                        Object.keys(categoryTermCounts).forEach(
                            key => bgFreqs[key] += categoryTermCounts[key][0]
                        )
                    }
                }
            )
            var fgDenseRanks = denseRank(fgFreqs);
            var bgDenseRanks = denseRank(bgFreqs);

            var maxfgDenseRanks = Math.max(...fgDenseRanks);
            var minfgDenseRanks = Math.min(...fgDenseRanks);
            var scalefgDenseRanks = fgDenseRanks.map(
                x => (x - minfgDenseRanks) / (maxfgDenseRanks - minfgDenseRanks)
            )

            var maxbgDenseRanks = Math.max(...bgDenseRanks);
            var minbgDenseRanks = Math.min(...bgDenseRanks);
            var scalebgDenseRanks = bgDenseRanks.map(
                x => (x - minbgDenseRanks) / (maxbgDenseRanks - minbgDenseRanks)
            )

            return {'fg': scalefgDenseRanks,
                'bg': scalebgDenseRanks,
                'bgFreqs': bgFreqs,
                'fgFreqs': fgFreqs,
                'term': fullData.data.map((x)=>x.term)}
        }

        function getCategoryDenseRankScores(fullData, categoryNum) {
            var denseRanks = getDenseRanks(fullData, categoryNum)
            return denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
        }

        function getTermCounts(fullData) {
            var counts = Array(fullData.data.length).fill(0);
            fullData.termCounts.forEach(
                function (categoryTermCounts) {
                    Object.keys(categoryTermCounts).forEach(
                        key => counts[key] = categoryTermCounts[key][0]
                    )
                }
            )
            return counts;
        }

        function getContextWordLORIPs(query) {
            var contextWordCounts = getContextWordCounts(query);
            var ni_k = contextWordCounts.sums[0];
            var nj_k = contextWordCounts.sums[1];
            var n = ni_k + nj_k;
            //var ai_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            //var aj_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            var a0 = 0.00001 //corpusWordCounts.avgDocLen;
            var a_k0 = Object.keys(contextWordCounts.counts)
                .map(function (x) {
                    var counts = contextWordCounts.counts[x];
                    return a0 * (counts[2] + counts[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                })
                .reduce(function (a, b) {
                    return a + b
                });
            var ai_k0 = a_k0 / ni_k;
            var aj_k0 = a_k0 / nj_k;
            var scores = Object.keys(contextWordCounts.counts).map(
                function (word) {
                    var countData = contextWordCounts.counts[word];
                    var yi = countData[0];
                    var yj = countData[1];
                    //var ai = countData[2];
                    //var aj = countData[3];
                    //var ai = countData[2] + countData[3];
                    //var aj = ai;
                    //var ai = (countData[2] + countData[3]) * a0/ni_k;
                    //var aj = (countData[2] + countData[3]) * a0/nj_k;
                    var ai = a0 * (countData[2] + countData[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                    var aj = ai;
                    var deltahat_i_j =
                        +Math.log((yi + ai) * 1. / (ni_k + ai_k0 - yi - ai))
                        - Math.log((yj + aj) * 1. / (nj_k + aj_k0 - yj - aj));
                    var var_deltahat_i_j = 1. / (yi + ai) + 1. / (ni_k + ai_k0 - yi - ai)
                        + 1. / (yj + aj) + 1. / (nj_k + aj_k0 - yj - aj);
                    var zeta_ij = deltahat_i_j / Math.sqrt(var_deltahat_i_j);
                    return [word, yi, yj, ai, aj, ai_k0, zeta_ij];
                }
            ).sort(function (a, b) {
                return b[5] - a[5];
            });
            return scores;
        }

        function getContextWordSFS(query) {
            // from https://stackoverflow.com/questions/14846767/std-normal-cdf-normal-cdf-or-error-function
            function cdf(x, mean, variance) {
                return 0.5 * (1 + erf((x - mean) / (Math.sqrt(2 * variance))));
            }

            function erf(x) {
                // save the sign of x
                var sign = (x >= 0) ? 1 : -1;
                x = Math.abs(x);

                // constants
                var a1 = 0.254829592;
                var a2 = -0.284496736;
                var a3 = 1.421413741;
                var a4 = -1.453152027;
                var a5 = 1.061405429;
                var p = 0.3275911;

                // A&S formula 7.1.26
                var t = 1.0 / (1.0 + p * x);
                var y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);
                return sign * y; // erf(-x) = -erf(x);
            }

            function scale(a) {
                return Math.log(a + 0.0000001);
            }

            var contextWordCounts = getContextWordCounts(query);
            var wordList = Object.keys(contextWordCounts.counts).map(function (word) {
                return contextWordCounts.counts[word].concat([word]);
            });
            var cat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - cat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - cat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            var ncat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - ncat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - ncat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            function scaledFScore(cnt, other, freq_xbar, freq_var, prec_xbar, prec_var) {
                var beta = 1.5;
                var normFreq = cdf(scale(cnt), freq_xbar, freq_var);
                var normPrec = cdf(scale(cnt / (cnt + other)), prec_xbar, prec_var);
                return (1 + Math.pow(beta, 2)) * normFreq * normPrec / (Math.pow(beta, 2) * normFreq + normPrec);
            }

            var sfs = wordList.map(function (x) {
                cat_sfs = scaledFScore(x[0], x[1], cat_freq_xbar,
                    cat_freq_var, cat_prec_xbar, cat_prec_var);
                ncat_sfs = scaledFScore(x[1], x[0], ncat_freq_xbar,
                    ncat_freq_var, ncat_prec_xbar, ncat_prec_var);
                return [cat_sfs > ncat_sfs ? cat_sfs : -ncat_sfs].concat(x);

            }).sort(function (a, b) {
                return b[0] - a[0];
            });
            return sfs;
        }

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
                + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d, includeAll = true) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], [], [], []];
            console.log("searching")

            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d, includeAll);
            } else {
                return searchInExtraFeatures(d, includeAll);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], [], [], []];
            var term = d.term;
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });

            var pattern = null;
            if ('metalists' in fullData && term in fullData.metalists) {
                // from https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(str) {
                    return str.replace(/[\\?\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|\']/g, "\\$&");
                }

                console.log('term');
                console.log(term);
                pattern = new RegExp(
                    '\\W(' + fullData.metalists[term].map(escapeRegExp).join('|') + ')\\W',
                    'gim'
                );
            }

            for (var i in fullData.docs.extra) {
                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }
                    var text = fullData.docs.texts[i];

                    if (fullData.offsets !== undefined) {

                        if (fullData.offsets[term] !== undefined && fullData.offsets[term][i] !== undefined) {
                            var curMatch = {
                                'id': i,
                                'snippets': [],
                                'strength': strength,
                                'docLabel': docLabel,
                                'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                            }
                            for (const offset_i in fullData.offsets[term][i]) {
                                var offset = fullData.offsets[term][i][offset_i];
                                var spanStart = Math.max(offset[0] - 50, 0);
                                var spanEnd = Math.min(50, text.length-offset[1]);
                                var leftContext = text.substr(spanStart, offset[0] - spanStart);
                                var matchStr = text.substr(offset[0], offset[1] - offset[0]);
                                var rightContext = text.substr(offset[1], spanEnd);
                                var snippet = leftContext + '<b style="background-color: lightgoldenrodyellow">' + matchStr + '</b>' + rightContext;
                                if(spanStart > 0)
                                    snippet = '...' + snippet;
                                if(text.length - offset[1] > 50)
                                    snippet = snippet + '...'
                                curMatch.snippets.push(snippet)
                            }
                            matches[numericLabel].push(curMatch);
                        }
                    } else {

                        if (!useFullDoc)
                            text = text.slice(0, 300);
                        if (pattern !== null) {
                            text = text.replace(pattern, '<b>$&</b>');
                        }
                        var curMatch = {
                            'id': i,
                            'snippets': [text],
                            'strength': strength,
                            'docLabel': docLabel,
                            'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                        }

                        matches[numericLabel].push(curMatch);
                    }
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://mathiasbynens.be/notes/es-unicode-property-escapes#emoji
        var emojiRE = (/(?:[\u261D\u26F9\u270A-\u270D]|\uD83C[\uDF85\uDFC2-\uDFC4\uDFC7\uDFCA-\uDFCC]|\uD83D[\uDC42\uDC43\uDC46-\uDC50\uDC66-\uDC69\uDC6E\uDC70-\uDC78\uDC7C\uDC81-\uDC83\uDC85-\uDC87\uDCAA\uDD74\uDD75\uDD7A\uDD90\uDD95\uDD96\uDE45-\uDE47\uDE4B-\uDE4F\uDEA3\uDEB4-\uDEB6\uDEC0\uDECC]|\uD83E[\uDD18-\uDD1C\uDD1E\uDD1F\uDD26\uDD30-\uDD39\uDD3D\uDD3E\uDDD1-\uDDDD])(?:\uD83C[\uDFFB-\uDFFF])?|(?:[\u231A\u231B\u23E9-\u23EC\u23F0\u23F3\u25FD\u25FE\u2614\u2615\u2648-\u2653\u267F\u2693\u26A1\u26AA\u26AB\u26BD\u26BE\u26C4\u26C5\u26CE\u26D4\u26EA\u26F2\u26F3\u26F5\u26FA\u26FD\u2705\u270A\u270B\u2728\u274C\u274E\u2753-\u2755\u2757\u2795-\u2797\u27B0\u27BF\u2B1B\u2B1C\u2B50\u2B55]|\uD83C[\uDC04\uDCCF\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE1A\uDE2F\uDE32-\uDE36\uDE38-\uDE3A\uDE50\uDE51\uDF00-\uDF20\uDF2D-\uDF35\uDF37-\uDF7C\uDF7E-\uDF93\uDFA0-\uDFCA\uDFCF-\uDFD3\uDFE0-\uDFF0\uDFF4\uDFF8-\uDFFF]|\uD83D[\uDC00-\uDC3E\uDC40\uDC42-\uDCFC\uDCFF-\uDD3D\uDD4B-\uDD4E\uDD50-\uDD67\uDD7A\uDD95\uDD96\uDDA4\uDDFB-\uDE4F\uDE80-\uDEC5\uDECC\uDED0-\uDED2\uDEEB\uDEEC\uDEF4-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])|(?:[#\*0-9\xA9\xAE\u203C\u2049\u2122\u2139\u2194-\u2199\u21A9\u21AA\u231A\u231B\u2328\u23CF\u23E9-\u23F3\u23F8-\u23FA\u24C2\u25AA\u25AB\u25B6\u25C0\u25FB-\u25FE\u2600-\u2604\u260E\u2611\u2614\u2615\u2618\u261D\u2620\u2622\u2623\u2626\u262A\u262E\u262F\u2638-\u263A\u2640\u2642\u2648-\u2653\u2660\u2663\u2665\u2666\u2668\u267B\u267F\u2692-\u2697\u2699\u269B\u269C\u26A0\u26A1\u26AA\u26AB\u26B0\u26B1\u26BD\u26BE\u26C4\u26C5\u26C8\u26CE\u26CF\u26D1\u26D3\u26D4\u26E9\u26EA\u26F0-\u26F5\u26F7-\u26FA\u26FD\u2702\u2705\u2708-\u270D\u270F\u2712\u2714\u2716\u271D\u2721\u2728\u2733\u2734\u2744\u2747\u274C\u274E\u2753-\u2755\u2757\u2763\u2764\u2795-\u2797\u27A1\u27B0\u27BF\u2934\u2935\u2B05-\u2B07\u2B1B\u2B1C\u2B50\u2B55\u3030\u303D\u3297\u3299]|\uD83C[\uDC04\uDCCF\uDD70\uDD71\uDD7E\uDD7F\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE02\uDE1A\uDE2F\uDE32-\uDE3A\uDE50\uDE51\uDF00-\uDF21\uDF24-\uDF93\uDF96\uDF97\uDF99-\uDF9B\uDF9E-\uDFF0\uDFF3-\uDFF5\uDFF7-\uDFFF]|\uD83D[\uDC00-\uDCFD\uDCFF-\uDD3D\uDD49-\uDD4E\uDD50-\uDD67\uDD6F\uDD70\uDD73-\uDD7A\uDD87\uDD8A-\uDD8D\uDD90\uDD95\uDD96\uDDA4\uDDA5\uDDA8\uDDB1\uDDB2\uDDBC\uDDC2-\uDDC4\uDDD1-\uDDD3\uDDDC-\uDDDE\uDDE1\uDDE3\uDDE8\uDDEF\uDDF3\uDDFA-\uDE4F\uDE80-\uDEC5\uDECB-\uDED2\uDEE0-\uDEE5\uDEE9\uDEEB\uDEEC\uDEF0\uDEF3-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])\uFE0F/g);

        function isEmoji(str) {
            if (str.match(emojiRE)) return true;
            return false;
        }

        function displayObscuredTerms(obscuredTerms, data, term, termInfo, div = '#' + divName + '-' + 'overlapped-terms') {
            d3.select('#' + divName + '-' + 'overlapped-terms')
                .selectAll('div')
                .remove();
            d3.select(div)
                .selectAll('div')
                .remove();
            if (obscuredTerms.length > 1 && maxOverlapping !== 0) {
                var obscuredDiv = d3.select(div)
                    .append('div')
                    .attr("class", "obscured")
                    .style('align', 'center')
                    .style('text-align', 'center')
                    .html("<b>\"" + term + "\" obstructs</b>: ");
                obscuredTerms.map(
                    function (term, i) {
                        if (maxOverlapping === -1 || i < maxOverlapping) {
                            makeWordInteractive(
                                data,
                                svg,
                                obscuredDiv.append("text").text(term),
                                term,
                                data.filter(t => t.term === term)[0],//termInfo
                                false
                            );
                            if (i < obscuredTerms.length - 1
                                && (maxOverlapping === -1 || i < maxOverlapping - 1)) {
                                obscuredDiv.append("text").text(", ");
                            }
                        } else if (i === maxOverlapping && i !== obscuredTerms.length - 1) {
                            obscuredDiv.append("text").text("...");
                        }
                    }
                )
            }
        }

        function displayTermContexts(data, termInfo, jump = alwaysJump, includeAll = false) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            var notmatches = termInfo.notmatches;
            if (contexts[0].length + contexts[1].length + contexts[2].length + contexts[3].length == 0) {
                //return null;
            }
            //!!! Future feature: context words
            //var contextWords = getContextWordSFS(info.term);
            //var contextWords = getContextWordLORIPs(info.term);
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;


            function addSnippets(contexts, divId, isMatch = true) {
                var meta = contexts.meta ? contexts.meta : '&nbsp;';
                var headClass = 'snippet_meta docLabel' + contexts.docLabel;
                var snippetClass = 'snippet docLabel' + contexts.docLabel;
                if (!isMatch) {
                    headClass = 'snippet_meta not_match docLabel' + contexts.docLabel;
                    snippetClass = 'snippet not_match docLabel' + contexts.docLabel;
                }
                d3.select(divId)
                    .append("div")
                    .attr('class', headClass)
                    .html(meta);
                contexts.snippets.forEach(function (snippet) {
                    d3.select(divId)
                        .append("div")
                        .attr('class', snippetClass)
                        .html(snippet);
                })
            }


            if (ignoreCategories) {
                divId = '#' + divName + '-' + 'cat';

                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                var numDocs = fullData.docs.texts.length.toLocaleString('en');
                var numMatches = allContexts.length;
                d3.select(divId)
                    .append("div")
                    .attr('class', 'topic_preview')
                    .attr('text-align', "center")
                    .html(
                        "Matched " + numMatches + " out of " + numDocs + ' documents: '
                        + (100 * numMatches / numDocs).toFixed(2) + '%'
                    );

                if (allContexts.length > 0) {
                    var headerClassName = 'text_header';
                    allContexts.forEach(function (singleDoc) {
                        addSnippets(singleDoc, divId);
                    });
                    if (includeAll) {
                        allNotMatches.forEach(function (singleDoc) {
                            addSnippets(singleDoc, divId, false);
                        });
                    }
                }

            } else if (unifiedContexts) {
                divId = '#' + divName + '-' + 'cat';
                var docLabelCounts = fullData.docs.labels.reduce(
                    function (map, label) {
                        map[label] = (map[label] || 0) + 1;
                        return map;
                    },
                    Object.create(null)
                );
                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                allContexts.forEach(function (singleDoc) {
                    numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel] || 0) + 1;
                });
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);

                /*contexts.forEach(function(context) {
                     context.forEach(function (singleDoc) {
                         numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel]||0) + 1;
                         addSnippets(singleDoc, divId);
                     });
                 });*/
                console.log("ORDERING !!!!!");
                console.log(fullData.info.category_name);
                console.log(sortDocLabelsByName);
                var docLabelCountsSorted = Object.keys(docLabelCounts).map(key => (
                    {
                        "label": fullData.docs.categories[key],
                        "labelNum": key,
                        "matches": numMatches[key] || 0,
                        "overall": docLabelCounts[key],
                        'percent': (numMatches[key] || 0) * 100. / docLabelCounts[key]
                    }))
                    .sort(function (a, b) {
                        if (highlightSelectedCategory) {
                            if (a['label'] === fullData.info.category_name) {
                                return -1;
                            }
                            if (b['label'] === fullData.info.category_name) {
                                return 1;
                            }
                        }
                        if (sortDocLabelsByName) {
                            return a['label'] < b['label'] ? 1 : a['label'] > b['label'] ? -1 : 0;
                        } else {
                            return b.percent - a.percent;
                        }
                    });
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted);
                console.log(numMatches)
                console.log('#' + divName + '-' + 'categoryinfo')
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                if (showCategoryHeadings) {
                    d3.select('#' + divName + '-' + 'categoryinfo').attr('display', 'inline');
                }

                function getCategoryStatsHTML(counts) {
                    return counts.matches + " document"
                        + (counts.matches == 1 ? "" : "s") + " out of " + counts.overall + ': '
                        + counts['percent'].toFixed(2) + '%';
                }

                function getCategoryInlineHeadingHTML(counts) {
                    return '<a name="' + divName + '-category'
                        + counts.labelNum + '"></a>'
                        + (ignoreCategories ? "" : counts.label + ": ") + "<span class=topic_preview>"
                        + getCategoryStatsHTML(counts)
                        + "</span>";
                }


                docLabelCountsSorted.forEach(function (counts) {

                    var htmlToAdd = "";
                    if (!ignoreCategories) {
                        htmlToAdd += "<b>" + counts.label + "</b>: " + getCategoryStatsHTML(counts);
                        ;
                    }

                    if (counts.matches > 0) {
                        var headerClassName = 'text_header';
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId)
                                .append('div')
                                .attr('class', 'separator')
                                .html("<b>Selected category</b>");
                        }
                        d3.select(divId)
                            .append("div")
                            .attr('class', headerClassName)
                            .html(getCategoryInlineHeadingHTML(counts));

                        allContexts
                            .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                            .forEach(function (singleDoc) {
                                addSnippets(singleDoc, divId);
                            });
                        if (includeAll) {
                            allNotMatches
                                .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                                .forEach(function (singleDoc) {
                                    addSnippets(singleDoc, divId, false);
                                });
                        }
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId).append('div').attr('class', 'separator').html("<b>End selected category</b>");
                            d3.select(divId).append('div').html("<br />");
                        }
                    }


                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'categoryinfo')
                            .attr('display', 'inline')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }

                })


            } else {
                var contextColumns = [
                    fullData.info.category_internal_name,
                    fullData.info.not_category_name
                ];
                if (showNeutral) {
                    if ('neutral_category_name' in fullData.info) {
                        contextColumns.push(fullData.info.neutral_category_name)
                    } else {
                        contextColumns.push("Neutral")
                    }
                    if (showExtra) {
                        if ('extra_category_name' in fullData.info) {
                            contextColumns.push(fullData.info.extra_category_name)
                        } else {
                            contextColumns.push("Extra")
                        }
                    }

                }
                contextColumns.map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        //var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var divId = null
                        if (fullData.info.category_internal_name == catName) {
                            divId = '#' + divName + '-' + 'cat'
                        } else if (fullData.info.not_category_name == catName) {
                            divId = '#' + divName + '-' + 'notcat'
                        } else if (fullData.info.neutral_category_name == catName) {
                            divId = '#' + divName + '-' + 'neut';
                        } else if (fullData.info.extra_category_name == catName) {
                            divId = '#' + divName + '-' + 'extra'
                        } else {
                            return;
                        }

                        var temp = d3.select(divId).selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            addSnippets(context, divId);
                        });
                        if (includeAll) {
                            notmatches[catIndex].forEach(function (context) {
                                addSnippets(context, divId, false);
                            });
                        }
                    }
                );
            }

            var obscuredTerms = getObscuredTerms(data, termInfo.info);
            displayObscuredTerms(obscuredTerms, data, info.term, info, '#' + divName + '-' + 'overlapped-terms-clicked');

            d3.select('#' + divName + '-' + 'termstats')
                .selectAll("div")
                .remove();
            var termHtml = 'Term: <b>' + formatTermForDisplay(info.term) + '</b>';
            if ('metalists' in fullData && info.term in fullData.metalists) {
                termHtml = 'Topic: <b>' + formatTermForDisplay(info.term) + '</b>';
            }
            if (getCustomTermHtml !== null) {
                termHtml = getCustomTermHtml(info);
            }
            d3.select('#' + divName + '-' + 'termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html(termHtml);
            if ('metalists' in fullData && info.term in fullData.metalists && topic_model_preview_size > 0) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Topic preview</b>: "
                        + fullData.metalists[info.term]
                            .slice(0, topic_model_preview_size)
                            .reduce(function (x, y) {
                                return x + ', ' + y
                            }));
            }
            if ('metadescriptions' in fullData && info.term in fullData.metadescriptions) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Description</b>: " + fullData.metadescriptions[info.term]);
            }
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;


            var numCatDocs = fullData.docs.labels
                .map(function (x) {
                    return (x == fullData.docs.categories.indexOf(
                        fullData.info.category_internal_name)) + 0
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });


            var numNCatDocs = fullData.docs.labels
                .map(function (x) {
                    return notCategoryNumList.indexOf(x) > -1
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            function getFrequencyDescription(name, count25k, count, ndocs) {
                var desc = name;
                if (!enableTermCategoryDescription) {
                    return desc + ':';
                }
                desc += ' frequency: <div class=text_subhead>' + count25k + ' per 25,000 terms</div>';
                if (!isNaN(Math.round(ndocs))) {
                    desc += '<div class=text_subhead>' + Math.round(ndocs) + ' per 1,000 docs</div>';
                }
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    if (!isNaN(Math.round(ndocs))) {
                        desc += '<u>Some of the ' + count + ' mentions:</u>';
                    } else {
                        desc += count + ' mentions';
                    }
                }
                /*
                desc += '<br><b>Discriminative:</b> ';

                desc += contextWords
                    .slice(cat_name === name ? 0 : contextWords.length - 3,
                        cat_name === name ? 3 : contextWords.length)
                    .filter(function (x) {
                        //return Math.abs(x[5]) > 1.96;
                        return true;
                    })
                    .map(function (x) {return x.join(', ')}).join('<br>');
                */
                return desc;
            }

            if (!unifiedContexts && !ignoreCategories) {
                console.log("NOT UNIFIED CONTEXTS")
                d3.select('#' + divName + '-' + 'cathead')
                    .style('fill', color(1))
                    .html(
                        getFrequencyDescription(cat_name,
                            info.cat25k,
                            info.cat,
                            termInfo.contexts[0].length * 1000 / numCatDocs
                        )
                    );
                d3.select('#' + divName + '-' + 'notcathead')
                    .style('fill', color(0))
                    .html(
                        getFrequencyDescription(ncat_name,
                            info.ncat25k,
                            info.ncat,
                            termInfo.contexts[1].length * 1000 / numNCatDocs)
                    );
                if (showNeutral) {
                    var numList = fullData.docs.categories.map(function (x, i) {
                        if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                            return i;
                        } else {
                            return -1;
                        }
                    }).filter(function (x) {
                        return x > -1
                    });

                    var numDocs = fullData.docs.labels
                        .map(function (x) {
                            return numList.indexOf(x) > -1
                        })
                        .reduce(function (a, b) {
                            return a + b;
                        }, 0);

                    d3.select("#" + divName + "-neuthead")
                        .style('fill', color(0))
                        .html(
                            getFrequencyDescription(fullData.info.neutral_category_name,
                                info.neut25k,
                                info.neut,
                                termInfo.contexts[2].length * 1000 / numDocs)
                        );

                    if (showExtra) {
                        var numList = fullData.docs.categories.map(function (x, i) {
                            if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                                return i;
                            } else {
                                return -1;
                            }
                        }).filter(function (x) {
                            return x > -1
                        });

                        var numDocs = fullData.docs.labels
                            .map(function (x) {
                                return numList.indexOf(x) > -1
                            })
                            .reduce(function (a, b) {
                                return a + b;
                            }, 0);

                        d3.select("#" + divName + "-extrahead")
                            .style('fill', color(0))
                            .html(
                                getFrequencyDescription(fullData.info.extra_category_name,
                                    info.extra25k,
                                    info.extra,
                                    termInfo.contexts[3].length * 1000 / numDocs)
                            );

                    }
                }
            } else if (unifiedContexts && !ignoreCategories) {
                // extra unified context code goes here
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted)

                docLabelCountsSorted.forEach(function (counts) {
                    var htmlToAdd = (ignoreCategories ? "" : "<b>" + counts.label + "</b>: ") + getCategoryStatsHTML(counts);
                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'contexts')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }
                })
            }
            if (jump) {
                if (window.location.hash === '#' + divName + '-' + 'snippets') {
                    window.location.hash = '#' + divName + '-' + 'snippetsalt';
                } else {
                    window.location.hash = '#' + divName + '-' + 'snippets';
                }
            }
        }

        function searchInText(d, includeAll = true) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function removeUnderScoreJoin(term) {
                /*
                '_ _asjdklf_jaksdlf_jaksdfl skld_Jjskld asdfjkl_sjkdlf'
                  ->
                "_ _asjdklf jaksdlf jaksdfl skld Jjskld asdfjkl_sjkdlf"
                 */
                return term.replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3");
            }

            function buildMatcher(term) {


                var boundary = '(?:\\W|^|$)';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                if (matchFullLine) {
                    boundary = '($|^)';
                }
                var termToRegex = term;


                // https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(string) {
                    return string.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\,\\\^\$\|\'#?]/g, "\\$&");
                    //return string.replace(/[\?#.*+^${}()|[\]\\]'\%/g, '\\$&'); // $& means the whole matched string
                }

                /*
                ['[', ']', '(', ')', '{', '}', '^', '$', '|', '?', '"',
                    '*', '+', '-', '=', '~', '`', '{'].forEach(function (a) {
                    termToRegex = termToRegex.replace(a, '\\\\' + a)
                });
                ['.', '#'].forEach(function(a) {termToRegex = termToRegex.replace(a, '\\' + a)})
                */
                termToRegex = escapeRegExp(termToRegex);
                console.log("termToRegex")
                console.log(termToRegex)

                var regexp = new RegExp(boundary + '('
                    + removeUnderScoreJoin(
                        termToRegex.replace(' ', wordSep, 'gim')
                    ) + ')' + boundary, 'gim');
                console.log(regexp);

                if (subwordEncoding === 'RoBERTa') {
                    if (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289) {
                        // Starts with character  indicating it's a word start
                        console.log("START")
                        regexp = new RegExp(boundary + escapeRegExp(term.substr(1, term.length)), 'gim');
                    } else {
                        regexp = new RegExp("\w" + escapeRegExp(term), 'gim');
                    }
                    console.log("SP")
                    console.log(regexp)
                }


                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], [], [], []];
            var notmatches = [[], [], [], []];
            var pattern = buildMatcher(d.term);
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            console.log('extraCategoryNumList')
            console.log(extraCategoryNumList);
            console.log("categoryNum");
            console.log(categoryNum);
            console.log("categoryNum");
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    //var numericLabel = 1 * (fullData.docs.categories[fullData.docs.labels[i]] != fullData.info.category_internal_name);

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }

                    var text = removeUnderScoreJoin(fullData.docs.texts[i]);
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': [], 'notsnippets': [], 'docLabel': docLabel};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }

                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[numericLabel].push(curMatch);
                    } else {
                        if (includeAll) {
                            curMatch.snippets = [
                                text.replace(/\n$/g, '\n\n')
                            ];
                            notmatches[numericLabel].push(curMatch);
                        }

                    }
                }
            }
            var toRet = {
                'contexts': matches,
                'notmatches': notmatches,
                'info': d,
                'docLabel': docLabel
            };
            return toRet;
        }

        function getDefaultTooltipContent(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            message += '<br/>score: ' + d.os.toFixed(5);
            return message;
        }

        function getDefaultTooltipContentWithoutScore(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            return message;
        }

        function getObscuredTerms(data, d) {
            //data = fullData['data']
            var matches = (data.filter(function (term) {
                    return term.x === d.x && term.y === d.y && (term.display === undefined || term.display === true);
                }).map(function (term) {
                    return formatTermForDisplay(term.term)
                }).sort()
            );
            return matches;
        }

        function showTooltip(data, d, pageX, pageY, showObscured = true) {
            deselectLastCircle();

            var obscuredTerms = getObscuredTerms(data, d);
            var message = '';
            console.log("!!!!! " + obscuredTerms.length)
            console.log(showObscured)
            if (obscuredTerms.length > 1 && showObscured)
                displayObscuredTerms(obscuredTerms, data, d.term, d);
            if (getTooltipContent !== null) {
                message += getTooltipContent(d);
            } else {
                if (sortByDist) {
                    message += getDefaultTooltipContentWithoutScore(d);
                } else {
                    message += getDefaultTooltipContent(d);
                }
            }
            pageX -= (svg.node().getBoundingClientRect().left) - origSVGLeft;
            pageY -= (svg.node().getBoundingClientRect().top) - origSVGTop;
            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX - 40) + "px")
                .style("top", (pageY - 85 > 0 ? pageY - 85 : 0) + "px");
            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            }).on('mouseout', function () {
                tooltip.transition().style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            var searchTerm = document
                .getElementById(this.divName + "-searchTerm")
                .value;
            handleSearchTerm(searchTerm);
            return false;
        };

        function highlightTerm(searchTerm, showObscured) {
            deselectLastCircle();
            var cleanedTerm = searchTerm.toLowerCase()
                .replace("'", " '")
                .trim();
            if (this.termDict[cleanedTerm] === undefined) {
                cleanedTerm = searchTerm.replace("'", " '").trim();
            }
            if (this.termDict[cleanedTerm] !== undefined) {
                showToolTipForTerm(this.data, this.svg, cleanedTerm, this.termDict[cleanedTerm], showObscured);
            }
            return cleanedTerm;
        }

        function handleSearchTerm(searchTerm, jump = false) {
            console.log("Handle search term.");
            console.log(searchTerm);
            console.log("this");
            console.log(this)
            highlighted = highlightTerm.call(this, searchTerm, true);
            console.log("found searchTerm");
            console.log(searchTerm);
            if (this.termDict[searchTerm] != null) {
                var runDisplayTermContexts = true;
                if (alternativeTermFunc != null) {
                    runDisplayTermContexts = this.alternativeTermFunc(this.termDict[searchTerm]);
                }
                if (runDisplayTermContexts) {
                    displayTermContexts(
                        this.data,
                        this.gatherTermContexts(this.termDict[searchTerm], this.includeAllContexts),
                        alwaysJump,
                        this.includeAllContexts
                    );
                }
            }
        }

        function getCircleForSearchTerm(mysvg, searchTermInfo) {
            var circle = mysvg;
            if (circle.tagName !== "circle") { // need to clean this thing up
                circle = mysvg._groups[0][searchTermInfo.ci];
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0].children !== undefined) {
                        circle = mysvg._groups[0].children[searchTermInfo.ci];
                    }
                }
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0][0].children !== undefined) {
                        circle = Array.prototype.filter.call(
                            mysvg._groups[0][0].children,
                            x => (x.tagName == "circle" && x.__data__['term'] == searchTermInfo.term)
                        )[0];
                    }
                }
                if ((circle === undefined || circle.tagName != 'circle') && mysvg._groups[0][0].children !== undefined) {
                    circle = mysvg._groups[0][0].children[searchTermInfo.ci];
                }
            }
            return circle;
        }

        function showToolTipForTerm(data, mysvg, searchTerm, searchTermInfo, showObscured = true) {
            //var searchTermInfo = termDict[searchTerm];
            console.log("showing tool tip")
            console.log(searchTerm)
            console.log(searchTermInfo)
            if (searchTermInfo === undefined) {
                console.log("can't show")
                d3.select("#" + divName + "-alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#" + divName + "-alertMessage").text("");
                var circle = getCircleForSearchTerm(mysvg, searchTermInfo);
                if (circle) {
                    var mySVGMatrix = circle.getScreenCTM().translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;
                    circle.style["stroke"] = "black";
                    //var circlePos = circle.position();
                    //var el = circle.node()
                    //showTooltip(searchTermInfo, pageX, pageY, circle.cx.baseVal.value, circle.cx.baseVal.value);
                    showTooltip(
                        data,
                        searchTermInfo,
                        pageX,
                        pageY,
                        showObscured
                    );

                    lastCircleSelected = circle;
                }

            }
        };


        function makeWordInteractive(data, svg, domObj, term, termInfo, showObscured = true) {
            return domObj
                .on("mouseover", function (d) {
                    showToolTipForTerm(data, svg, term, termInfo, showObscured);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    if (showObscured) {
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    }
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(termInfo);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(termInfo, includeAllContexts), alwaysJump, includeAllContexts);
                    }
                });
        }



        function processData(fullData) {

            modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            var data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                //!!!
                //termDict[x.term].i = i;
            });

            var padding = 0.1;
            if (showAxes || showAxesAndCrossHairs) {
                padding = 0.1;
            }

            // Scale the range of the data.  Add some space on either end.
            if (useGlobalScale) {
                var axisMax = Math.max(
                    d3.max(data, function (d) {
                        return d.x;
                    }),
                    d3.max(data, function (d) {
                        return d.y;
                    }),
                )
                var axisMin = Math.min(
                    d3.min(data, function (d) {
                        return d.x;
                    }),
                    d3.min(data, function (d) {
                        return d.y;
                    }),
                )
                axisMin = axisMin - (axisMax - axisMin) * padding;
                axisMax = axisMax + (axisMax - axisMin) * padding;
                x.domain([axisMin, axisMax]);
                y.domain([axisMin, axisMax]);
            } else {
                var xMax = d3.max(data, function (d) {
                    return d.x;
                });
                var yMax = d3.max(data, function (d) {
                    return d.y;
                })
                x.domain([-1 * padding, xMax + padding]);
                y.domain([-1 * padding, yMax + padding]);
            }

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            var axisRectHolder = new RectangleHolder();
            // Add the scatterplot
            data.forEach(function (d, i) {
                d.ci = i
            });

            //console.log('XXXXX'); console.log(data)


            function getFilter(data) {
                return data.filter(d => d.display === undefined || d.display === true);
            }


            var mysvg = svg
                .selectAll("dot")
                .data(getFilter(data))
                //.filter(function (d) {return d.display === undefined || d.display === true})
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (colorFunc) {
                        return colorFunc(d);
                    } else if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else if (d.p <= minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        if (d.term === "the") {
                            console.log("COLS " + d.s + " " + color(d.s) + " " + d.term)
                            console.log(d)
                            console.log(color)
                        }
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    /*var mySVGMatrix = circle.getScreenCTM()n
                        .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;*/

                    /*showTooltip(
                        d,
                        d3.event.pageX,
                        d3.event.pageY
                    );*/
                    console.log("point MOUSOEVER")
                    console.log(d)
                    showToolTipForTerm(data, this, d.term, d, true);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(d);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                    }
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    d3.select('#' + divName + '-' + 'overlapped-terms')
                        .selectAll('div')
                        .remove();
                })


            coords = Object();

            var pointStore = [];
            var pointRects = [];

            function censorPoints(datum, getX, getY) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(getX(datum)))
                    .attr("y", y(getY(datum)) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            function censorCircle(xCoord, yCoord) {
                var curLabel = svg.append("text")
                    .attr("x", x(xCoord))
                    .attr("y", y(yCoord) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            var configs = [
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': 10, 'alignment-baseline': 'ideographic'},

                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
            ];
            if (centerLabelsOverPoints) {
                configs = [{'anchor': 'middle', 'xoff': 0, 'yoff': 0, 'alignment-baseline': 'middle'}];
            }

            function labelPointsIfPossible(datum, myX, myY) {
                if (suppressTextColumn !== undefined
                    && datum.etc !== undefined
                    && datum.etc[suppressTextColumn] === true) {
                    return false;
                }

                var term = datum.term;
                if (datum.x > datum.y) {
                    configs.sort((a, b) => a.anchor == 'end' && b.anchor == 'end'
                        ? a.group - b.group : (a.anchor == 'end') - (b.anchor == 'end'));
                } else {
                    configs.sort((a, b) => a.anchor == 'start' && b.anchor == 'start'
                        ? a.group - b.group : (a.anchor == 'start') - (b.anchor == 'start'));
                }
                var matchedElement = null;

                var termColor = 'rgb(0,0,0)';
                if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                    termColor = datum.etc[textColorColumn];
                }
                term = formatTermForDisplay(term);

                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = svg.append("text")
                        //.attr("x", x(data[i].x) + config['xoff'])
                        //.attr("y", y(data[i].y) + config['yoff'])
                        .attr("x", x(myX) + config['xoff'])
                        .attr("y", y(myY) + config['yoff'])
                        .attr('class', 'label')
                        .attr('class', 'pointlabel')
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("text-anchor", config['anchor'])
                        .attr("alignment-baseline", config['alignment'])
                        .attr("fill", termColor)
                        .text(term);
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = doCensorPoints ? 0.5 : .25;

                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        curLabel = makeWordInteractive(data, svg, curLabel, term, datum);
                        break;
                    }
                }

                if (!matchedElement) {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    var labelRect = new Rectangle(x1, y1, x2, y2)
                    rectHolder.add(labelRect);
                    pointStore.push([x1, y1]);
                    pointStore.push([x2, y1]);
                    pointStore.push([x1, y2]);
                    pointStore.push([x2, y2]);
                    return {label: curLabel, rect: labelRect};
                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                if (b.bg === a.bg)
                    return (b.cat + b.ncat) - (a.cat + a.ncat);
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return -1;
                    if (!aGood && bGood) return 1;
                }
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return 1;
                    if (!aGood && bGood) return -1;
                }
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            var sortedData = data.map(x => x).sort(sortByDist ? euclideanDistanceSort : scoreSort);
            if (doCensorPoints) {
                for (var i in data) {
                    var d = sortedData[i];

                    if (!(censorPointColumn !== undefined
                        && d.etc !== undefined
                        && d.etc[censorPointColumn] === false)) {

                        censorPoints(
                            d,
                            function (d) {
                                return d.x
                            },
                            function (d) {
                                return d.y
                            }
                        );
                    }

                }
            }


            function registerFigureBBox(curLabel, axis = false) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var rect = new Rectangle(x1, y1, x2, y2)
                if (axis) {
                    axisRectHolder.add(rect)
                } else {
                    rectHolder.add(rect);
                }
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }

            function drawXLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "x label")
                    .attr("text-anchor", "end")
                    .attr("x", width)
                    .attr("y", height - 6)
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            function drawYLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "y label")
                    .attr("text-anchor", "end")
                    .attr("y", 6)
                    .attr("dy", ".75em")
                    .attr("transform", "rotate(-90)")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            d3.selection.prototype.moveToBack = function () {
                return this.each(function () {
                    var firstChild = this.parentNode.firstChild;
                    if (firstChild) {
                        this.parentNode.insertBefore(this, firstChild);
                    }
                });
            };

            if (verticalLines) {
                if (typeof (verticalLines) === "number") {
                    verticalLines = [verticalLines]; // r likes to make single element vectors doubles; this is a hackish workaround
                }
                for (i in verticalLines) {
                    svg.append("g")
                        .attr("transform", "translate(" + x(verticalLines[i]) + ", 1)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#dddddd")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (fullData['line'] !== undefined) {
                var valueline = d3.line()
                    .x(function (d) {
                        return x(d.x);
                    })
                    .y(function (d) {
                        return y(d.y);
                    });
                fullData.line = fullData.line.sort((a, b) => b.x - a.x);
                svg.append("path")
                    .attr("class", "line")
                    .style("stroke-width", "1px")
                    .attr("d", valueline(fullData['line'])).moveToBack();
            }
            if (showAxes || showAxesAndCrossHairs) {

                var myXAxis = svg.append("g")
                    .attr("class", "x axis")
                    .attr("transform", "translate(0," + height + ")")
                    .call(xAxis);

                //rangeTree = registerFigureBBox(myXAxis);


                var xLabel = drawXLabel(svg, getLabelText('x'));

                //console.log('xLabel');
                //console.log(xLabel);

                //rangeTree = registerFigureBBox(xLabel);
                // Add the Y Axis

                if (!yAxisValues) {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr("dx", "30px")
                        .attr("dy", "-13px")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("transform", "rotate(-90)");
                } else {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px');
                }
                registerFigureBBox(myYAxis, true);
                registerFigureBBox(myXAxis, true);

                function getLabelText(axis) {
                    if (axis == 'y') {
                        if (yLabelText == null)
                            return modelInfo['category_name'] + " Frequency";
                        else
                            return yLabelText;
                    } else {
                        if (xLabelText == null)
                            return modelInfo['not_category_name'] + " Frequency";
                        else
                            return xLabelText;
                    }
                }

                var yLabel = drawYLabel(svg, getLabelText('y'))

            }

            if (!showAxes || showAxesAndCrossHairs) {
                horizontal_line_y_position_translated = 0.5;
                if (horizontal_line_y_position !== null) {
                    var loOy = null, hiOy = null, loY = null, hiY = null;
                    for (i in fullData.data) {
                        var curOy = fullData.data[i].oy;
                        if (curOy < horizontal_line_y_position && (curOy > loOy || loOy === null)) {
                            loOy = curOy;
                            loY = fullData.data[i].y
                        }
                        if (curOy > horizontal_line_y_position && (curOy < hiOy || hiOy === null)) {
                            hiOy = curOy;
                            hiY = fullData.data[i].y
                        }
                    }
                    horizontal_line_y_position_translated = loY + (hiY - loY) / 2.
                    if (loY === null) {
                        horizontal_line_y_position_translated = 0;
                    }
                }
                if (vertical_line_x_position === null) {
                    vertical_line_x_position_translated = 0.5;
                } else {
                    if (vertical_line_x_position !== null) {
                        var loOx = null, hiOx = null, loX = null, hiX = null;
                        for (i in fullData.data) {
                            var curOx = fullData.data[i].ox;
                            if (curOx < vertical_line_x_position && (curOx > loOx || loOx === null)) {
                                loOx = curOx;
                                loX = fullData.data[i].x;
                            }
                            if (curOx > vertical_line_x_position && (curOx < hiOx || hiOx === null)) {
                                hiOx = curOx;
                                hiX = fullData.data[i].x
                            }
                        }
                        vertical_line_x_position_translated = loX + (hiX - loX) / 2.
                        if (loX === null) {
                            vertical_line_x_position_translated = 0;
                        }
                    }
                }
                if (showCrossAxes) {
                    var x_line = svg.append("g")
                        .attr("transform", "translate(0, " + y(horizontal_line_y_position_translated) + ")")
                        .append("line")
                        .attr("x2", width)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                    var y_line = svg.append("g")
                        .attr("transform", "translate(" + x(vertical_line_x_position_translated) + ", 0)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (showDiagonal) {
                var diagonal = svg.append("g")
                    .append("line")
                    .attr("x1", 0)
                    .attr("y1", height)
                    .attr("x2", width)
                    .attr("y2", 0)
                    .style("stroke-dasharray", "5,5")
                    .style("stroke", "#cccccc")
                    .style("stroke-width", "1px")
                    .moveToBack();
            }

            function showWordList(word, termDataList, xOffset=null) {
                var maxWidth = word.node().getBBox().width;
                var wordObjList = [];
                for (var i in termDataList) {
                    var datum = termDataList[i];
                    var curTerm = datum.term;
                    word = (function (word, curTerm) {
                        var termColor = 'rgb(0,0,0)';
                        if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                            termColor = datum.etc[textColorColumn];
                        }
                        console.log("Show WORD "); console.log(word.node().getBBox().x)
                        var curWordPrinted = svg.append("text")
                            .attr("text-anchor", "start")
                            .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                            .attr('font-size', '12px')
                            .attr("fill", termColor)
                            .attr("x", xOffset == null ? word.node().getBBox().x : xOffset)
                            .attr("y", word.node().getBBox().y
                                + 2 * word.node().getBBox().height)
                            .text(formatTermForDisplay(curTerm));
                        wordObjList.push(curWordPrinted)
                        return makeWordInteractive(
                            termDataList, //data,
                            svg,
                            curWordPrinted,
                            curTerm,
                            termDataList[i]);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth,
                    'wordObjList': wordObjList
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(isTopPane) {
                console.log("PICK SCORE ALGO")
                console.log(isTopPane)
                if (isTopPane === true) {
                    if (headerSortingAlgos !== null && headerSortingAlgos['upper'] !== undefined)
                        return headerSortingAlgos['upper'];
                    return scoreSortForCategory;
                } else {
                    if (headerSortingAlgos !== null && headerSortingAlgos['lower'] !== undefined)
                        return headerSortingAlgos['lower'];
                    return scoreSortForNotCategory;
                }

            }

            function pickTermSortingAlgorithm(isUpperPane) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(isUpperPane);
                return pickScoreSortAlgo(isUpperPane);
            }

            function showAssociatedWordList(data, word, header, isUpperPane, xOffset, length = topTermsLength) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isUpperPane);
                console.log("showAssociatedWordList");
                console.log(header);
                console.log("WORD");
                console.log(word)
                sortedData = data.filter(term => (term.display === undefined || term.display === true)).sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isUpperPane)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(word, sortedData.slice(0, length), xOffset);

            }

            var characteristicXOffset = width;

            function showCatHeader(startingOffset, catName, registerFigureBBox) {
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset //width
                    )
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(catName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                return catHeader;
            }

            function showNotCatHeader(startingOffset, word, notCatName) {
                console.log("showNotCatHeader")
                return svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("y", word.node().getBBox().y + 3 * word.node().getBBox().height)
                    .text(notCatName);
            }

            function showTopTermsPane(data,
                                      registerFigureBBox,
                                      showAssociatedWordList,
                                      upperHeaderName,
                                      lowerHeaderName,
                                      startingOffset) {
                data = data.filter(term => (term.display === undefined || term.display === true));
                //var catHeader = showCatHeader(startingOffset, catName, registerFigureBBox);
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(upperHeaderName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                var word = catHeader;
                var wordListData = showAssociatedWordList(data, word, catHeader, true, startingOffset);
                word = wordListData.word;
                var maxWidth = wordListData.maxWidth;

                var notCatHeader = showNotCatHeader(startingOffset, word, lowerHeaderName);
                word = notCatHeader;
                characteristicXOffset = Math.max(
                    catHeader.node().getBBox().x + maxWidth + 10,
                    notCatHeader.node().getBBox().x + maxWidth + 10
                )
                console.log("characteristicXOffset", characteristicXOffset)
                console.log(catHeader.node().getBBox().x + maxWidth + 10)
                console.log(notCatHeader.node().getBBox().x + maxWidth + 10)

                var notWordListData = showAssociatedWordList(data, word, notCatHeader, false, startingOffset);
                word = wordListData.word;
                if (wordListData.maxWidth > maxWidth) {
                    maxWidth = wordListData.maxWidth;
                }
                return {
                    wordListData, notWordListData,
                    word, maxWidth, characteristicXOffset, startingOffset,
                    catHeader, notCatHeader, registerFigureBBox
                };
            }

            var payload = Object();
            if (showTopTerms) {
                var upperHeaderName = "Top " + fullData['info']['category_name'];
                var lowerHeaderName = "Top " + fullData['info']['not_category_name'];
                if (headerNames !== null) {
                    if (headerNames.upper !== undefined)
                        upperHeaderName = headerNames.upper;
                    if (headerNames.lower !== undefined)
                        lowerHeaderName = headerNames.lower;
                }
                payload.topTermsPane = showTopTermsPane(
                    data,
                    registerFigureBBox,
                    showAssociatedWordList,
                    upperHeaderName,
                    lowerHeaderName,
                    width + topTermsLeftBuffer
                );
                payload.showTopTermsPane = showTopTermsPane;
                payload.showAssociatedWordList = showAssociatedWordList;
                payload.showWordList = showWordList;

                /*var wordListData = topTermsPane.wordListData;
                var word = topTermsPane.word;
                var maxWidth = topTermsPane.maxWidth;
                var catHeader = topTermsPane.catHeader;
                var notCatHeader = topTermsPane.notCatHeader;
                var startingOffset = topTermsPane.startingOffset;*/
                characteristicXOffset = payload.topTermsPane.characteristicXOffset;
            }


            if ((!nonTextFeaturesMode && !asianMode && showCharacteristic)
                || (headerNames !== null && headerNames.right !== undefined)) {
                var sortMethod = backgroundScoreSort;
                var title = 'Characteristic';
                if (headerNames !== null && headerNames.right !== undefined) {
                    title = headerNames.right;
                }
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                    sortMethod = scoreSortReverse;
                } else if (data.reduce(function (a, b) {
                    return a + b.bg
                }, 0) === 0) {
                    title = 'Most frequent';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", characteristicXOffset)
                    .attr("dy", "6px")
                    .text(title);

                var rightSortMethod = sortMethod;
                if (rightOrderColumn !== undefined && rightOrderColumn !== null) {
                    rightSortMethod = ((a, b) => b.etc[rightOrderColumn] - a.etc[rightOrderColumn]);
                }

                var wordListData = showWordList(
                    word,
                    data.filter(term => (term.display === undefined || term.display === true))
                        .sort(rightSortMethod).slice(0, topTermsLength * 2 + 2),
                    characteristicXOffset
                );

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            function performPartialLabeling(
                data,
                existingLabels,
                getX,
                getY,
                labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            ) {
                for (i in existingLabels) {
                    rectHolder.remove(existingLabels[i].rect);
                    existingLabels[i].label.remove();
                }

                var labeledPoints = [];

                //var filteredData = data.filter(d=>d.display === undefined || d.display === true);
                //for (var i = 0; i < filteredData.length; i++) {
                data.sort(labelPriorityFunction).forEach(function (datum, i) {
                    //console.log(datum.i, datum.ci, i)
                    //var label = labelPointsIfPossible(i, getX(filteredData[i]), getY(filteredData[i]));
                    if (datum.display === undefined || datum.display === true) {
                        var label = labelPointsIfPossible(datum, getX(datum), getY(datum));
                        if (label !== false) {
                            //console.log("labeled")
                            labeledPoints.push(label)
                        }
                    }
                    //if (labelPointsIfPossible(i), true) numPointsLabeled++;
                })
                return labeledPoints;
            }

            //var labeledPoints = performPartialLabeling();
            var labeledPoints = [];
            var labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            if (labelPriorityColumn !== undefined && labelPriorityColumn !== null) {
                labelPriorityFunction = (a, b) => b.etc[labelPriorityColumn] - a.etc[labelPriorityColumn];
            }

            labeledPoints = performPartialLabeling(
                data,
                labeledPoints,
                function (d) {
                    return d.x
                },
                function (d) {
                    return d.y
                },
                labelPriorityFunction
            );

            if (backgroundLabels !== null) {
                backgroundLabels.map(
                    function (label) {
                        svg.append("text")
                            .attr("x", x(label.X))
                            .attr("y", y(label.Y))
                            .attr("text-anchor", "middle")
                            .style("font-size", "30")
                            .style("fill", "rgb(200,200,200)")
                            .text(label.Text)
                            .lower()
                            .on('mouseover', function (d) {
                                d3.select(this).style('stroke', 'black').style('stroke-width', '1px').raise()
                            })
                            .on('mouseout', function (d) {
                                d3.select(this).style('stroke-width', '0px').style('fill', 'rgb(200,200,200)').lower()
                            })
                    }
                )
            }


            /*
            // pointset has to be sorted by X
            function convex(pointset) {
                function _cross(o, a, b) {
                    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0]);
                }

                function _upperTangent(pointset) {
                    var lower = [];
                    for (var l = 0; l < pointset.length; l++) {
                        while (lower.length >= 2 && (_cross(lower[lower.length - 2], lower[lower.length - 1], pointset[l]) <= 0)) {
                            lower.pop();
                        }
                        lower.push(pointset[l]);
                    }
                    lower.pop();
                    return lower;
                }

                function _lowerTangent(pointset) {
                    var reversed = pointset.reverse(),
                        upper = [];
                    for (var u = 0; u < reversed.length; u++) {
                        while (upper.length >= 2 && (_cross(upper[upper.length - 2], upper[upper.length - 1], reversed[u]) <= 0)) {
                            upper.pop();
                        }
                        upper.push(reversed[u]);
                    }
                    upper.pop();
                    return upper;
                }

                var convex,
                    upper = _upperTangent(pointset),
                    lower = _lowerTangent(pointset);
                convex = lower.concat(upper);
                convex.push(pointset[0]);
                return convex;
            }

            console.log("POINTSTORE")
            console.log(pointStore);
            pointStore.sort();
            var convexHull = convex(pointStore);
            var minX = convexHull.sort(function (a,b) {
                return a[0] < b[0] ? -1 : 1;
            })[0][0];
            var minY = convexHull.sort(function (a,b) {
                return a[1] < b[1] ? -1 : 1;
            })[0][0];
            //svg.append("text").text("BLAH BLAH").attr("text-anchor", "middle").attr("cx", x(0)).attr("y", minY);
            console.log("POINTSTORE")
            console.log(pointStore);
            console.log(convexHull);
            for (i in convexHull) {
                var i = parseInt(i);
                if (i + 1 == convexHull.length) {
                    var nextI = 0;
                } else {
                    var nextI = i + 1;
                }
                console.log(i, ',', nextI);
                svg.append("line")
                    .attr("x2", width)
                    .style("stroke", "#cc0000")
                    .style("stroke-width", "1px")
                    .attr("x1", convexHull[i][0])     // x position of the first end of the line
                    .attr("y1", convexHull[i][1])      // y position of the first end of the line
                    .attr("x2", convexHull[nextI][0])     // x position of the second end of the line
                    .attr("y2", convexHull[nextI][1]);    // y position of the second end of the line
            }*/

            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?\-!]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    var name = null;
                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt;
                    } else {
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt
                        }
                    }
                    //!!!

                });
                fullData.docs.labels.forEach(function (x) {

                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                    } else {
                        var name = null;
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                        }
                    }
                });
                console.log("docCounts");
                console.log(docCounts)
                var messages = [];
                if (ignoreCategories) {
                    var wordCount = getCorpusWordCounts();
                    console.log("wordCount")
                    console.log(wordCount)
                    messages.push(
                        '<b>Document count: </b>' + fullData.docs.texts.length.toLocaleString('en') +
                        '; <b>word count: </b>'
                        + wordCount['sums'].reduce((a, b) => a + b, 0).toLocaleString('en')
                    )
                } else if (unifiedContexts) {
                    fullData.docs.categories.forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            var message = '<b>' + x + '</b>: ';
                            message += 'document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en')
                            messages.push(message);
                        }
                    });
                } else {
                    [fullData.info.category_name,
                        fullData.info.not_category_name,
                        fullData.info.neutral_category_name,
                        fullData.info.extra_category_name].forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            messages.push('<b>' + x + '</b> document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en'));
                        }
                    });
                }

                if (showCorpusStats) {
                    d3.select('#' + divName + '-' + 'corpus-stats')
                        .style('width', width + margin.left + margin.right + 200)
                        .append('div')
                        .html(messages.join('<br />'));
                }
            }


            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById(divName);

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }

            function rerender(xCoords, yCoords, color) {
                labeledPoints.forEach(function (p) {
                    p.label.remove();
                    rectHolder.remove(p.rect);
                });
                pointRects.forEach(function (rect) {
                    rectHolder.remove(rect);
                });
                pointRects = []
                /*
                var circles = d3.select('#' + divName).selectAll('circle')
                    .attr("cy", function (d) {return y(yCoords[d.i])})
                    .transition(0)
                    .attr("cx", function (d) {return x(xCoords[d.i])})
                    .transition(0);
                */
                d3.select('#' + divName).selectAll("dot").remove();
                d3.select('#' + divName).selectAll("circle").remove();
                console.log(this.fullData)
                console.log(this)
                console.log("X/Y coords")
                console.log(this.fullData.data.filter(d => d.display === undefined || d.display === true).map(d => [d.x, d.y]))
                var circles = this.svg//.select('#' + divName)
                    .selectAll("dot")
                    .data(this.fullData.data.filter(d => d.display === undefined || d.display === true))
                    //.filter(function (d) {return d.display === undefined || d.display === true})
                    .enter()
                    .append("circle")
                    .attr("cy", d => d.y)
                    .attr("cx", d => d.x)
                    .attr("r", d => 2)
                    .on("mouseover", function (d) {
                        /*var mySVGMatrix = circle.getScreenCTM()n
                            .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                        var pageX = mySVGMatrix.e;
                        var pageY = mySVGMatrix.f;*/

                        /*showTooltip(
                            d,
                            d3.event.pageX,
                            d3.event.pageY
                        );*/
                        console.log("point MOUSOEVER")
                        console.log(d)
                        showToolTipForTerm(data, this, d.term, d, true);
                        d3.select(this).style("stroke", "black");
                    })
                    .on("click", function (d) {
                        var runDisplayTermContexts = true;
                        if (alternativeTermFunc != null) {
                            runDisplayTermContexts = alternativeTermFunc(d);
                        }
                        if (runDisplayTermContexts) {
                            displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                        }
                    })
                    .on("mouseout", function (d) {
                        tooltip.transition()
                            .duration(0)
                            .style("opacity", 0);
                        d3.select(this).style("stroke", null);
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    });

                if (color !== null) {
                    console.log("COLOR")
                    console.log(color)
                    circles.style("fill", d => color(d));
                }
                xCoords.forEach((xCoord, i) => censorCircle(xCoord, yCoords[i]))
                labeledPoints = [];
                labeledPoints = performPartialLabeling(
                    this.fullData.data,
                    labeledPoints,
                    (d => d.ox), //function (d) {return xCoords[d.ci]},
                    (d => d.oy) //function (d) {return yCoords[d.ci]}

                );
            }

            //return [performPartialLabeling, labeledPoints];
            return {
                ...payload,
                ...{
                    'rerender': rerender,
                    'performPartialLabeling': performPartialLabeling,
                    'showToolTipForTerm': showToolTipForTerm,
                    'svg': svg,
                    'data': data,
                    'xLabel': xLabel,
                    'yLabel': yLabel,
                    'drawXLabel': drawXLabel,
                    'drawYLabel': drawYLabel,
                    'populateCorpusStats': populateCorpusStats
                }
            };
        }


        //fullData = getDataAndInfo();
        if (fullData.docs) {
            var corpusWordCounts = getCorpusWordCounts();
        }
        var payload = processData(fullData);

        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            //.attr("class", getTooltipContent == null && sortByDist ? "tooltip" : "tooltipscore")
            .attr("class", "tooltipscore")
            .style("opacity", 0);

        plotInterface = {}
        if (payload.topTermsPane) {
            plotInterface.topTermsPane = payload.topTermsPane;
            plotInterface.showTopTermsPane = payload.showTopTermsPane;
            plotInterface.showAssociatedWordList = payload.showAssociatedWordList;
        }
        plotInterface.includeAllContexts = includeAllContexts;
        plotInterface.divName = divName;
        plotInterface.displayTermContexts = displayTermContexts;
        plotInterface.gatherTermContexts = gatherTermContexts;
        plotInterface.xLabel = payload.xLabel;
        plotInterface.yLabel = payload.yLabel;
        plotInterface.drawXLabel = payload.drawXLabel;
        plotInterface.drawYLabel = payload.drawYLabel;
        plotInterface.svg = payload.svg;
        plotInterface.termDict = termDict;
        plotInterface.showToolTipForTerm = payload.showToolTipForTerm;
        plotInterface.fullData = fullData;
        plotInterface.data = payload.data;
        plotInterface.rerender = payload.rerender;
        plotInterface.populateCorpusStats = payload.populateCorpusStats;
        plotInterface.handleSearch = handleSearch;
        plotInterface.handleSearchTerm = handleSearchTerm;
        plotInterface.highlightTerm = highlightTerm;
        plotInterface.y = y;
        plotInterface.x = x;
        plotInterface.tooltip = tooltip;
        plotInterface.alternativeTermFunc = alternativeTermFunc;

        plotInterface.showTooltipSimple = function (term) {
            plotInterface.showToolTipForTerm(
                plotInterface.data,
                plotInterface.svg,
                term.replace("'", "\\'"),
                plotInterface.termDict[term.replace("'", "\\'")]
            )
        };

        plotInterface.drawCategoryAssociation = function (category, otherCategory = null) {
            console.log("+++++++ Entering drawCategoryAssociation")
            console.log("Category: " + category)
            console.log("Other Category: " + otherCategory)
            var categoryNum = this.fullData.info.categories.indexOf(category);

            var otherCategoryNum = null;
            if(otherCategory !== null)
                otherCategoryNum = this.fullData.info.categories.indexOf(otherCategory);

            console.log("cat/other: " + category + "/" + otherCategory + " ::: " + categoryNum + "/" + otherCategoryNum)

            console.log("Full Data")
            console.log(this.fullData)
            /*
            var rawLogTermCounts = getTermCounts(this.fullData).map(Math.log);
            var maxRawLogTermCounts = Math.max(...rawLogTermCounts);
            var minRawLogTermCounts = Math.min(...rawLogTermCounts);
            var logTermCounts = rawLogTermCounts.map(
                x => (x - minRawLogTermCounts) / maxRawLogTermCounts
            )
            */

            //var rawScores = getCategoryDenseRankScores(this.fullData, categoryNum);
            //console.log("RAW SCORES")
            //console.log(rawScores);
            /*
            function logOddsRatioUninformativeDirichletPrior(fgFreqs, bgFreqs, alpha) {
                var fgVocabSize = fgFreqs.reduce((x,y) => x+y);
                var fgL = fgFreqs.map(x => (x + alpha)/((1+alpha)*fgVocabSize - x - alpha))
                var bgVocabSize = bgFreqs.reduce((x,y) => x+y);
                var bgL = bgFreqs.map(x => (x + alpha)/((1+alpha)*bgVocabSize - x - alpha))
                var pooledVar = fgFreqs.map(function(x, i) {
                    return (
                        1/(x + alpha)
                        + 1/((1+alpha)*fgVocabSize - x - alpha)
                        + 1/(bgFreqs[i] + alpha)
                        + 1/((1+alpha)*bgVocabSize - bgFreqs[i] - alpha))
                })
                return pooledVar.map(function(x, i) {
                    return (Math.log(fgL[i]) - Math.log(bgL[i]))/x;
                })
            }
            var rawScores = logOddsRatioUninformativeDirichletPrior(
                denseRanks.fgFreqs, denseRanks.bgFreqs, 0.01);
            */


            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            if (otherCategoryNum !== null) {
                var otherDenseRanks = getDenseRanks(this.fullData, otherCategoryNum);
                denseRanks.bg = otherDenseRanks.fg;
                denseRanks.bgFreqs = otherDenseRanks.fgFreqs;
            }

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            //!!! OLD and good
            var ox = denseRanks.bg;
            var oy = denseRanks.fg;

            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            var ox = ox.map(x => (x - oxmin) / (oxmax - oxmin))
            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            var oy = oy.map(x => (x - oymin) / (oymax - oymin))
            //var ox = logTermCounts
            //var oy = scores;
            var xf = this.x;
            var yf = this.y;

            this.fullData.data = this.fullData.data.map(function (term, i) {
                //term.ci = i;
                term.s = scores[term.i];
                term.os = rawScores[term.i];
                term.cat = denseRanks.fgFreqs[term.i];
                term.ncat = denseRanks.bgFreqs[term.i];
                term.cat25k = parseInt(denseRanks.fgFreqs[term.i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[term.i] * 25000 / bgFreqSum);
                term.x = xf(ox[term.i]) // logTermCounts[term.i];
                term.y = yf(oy[term.i]) // scores[term.i];
                term.ox = ox[term.i];
                term.oy = oy[term.i];
                term.display = true;
                return term;
            })

            // Feature selection
            var targetTermsToShow = 1500;

            var sortedBg = denseRanks.bg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedFg = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedScores = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]);
            var myFullData = this.fullData

            sortedBg.concat(sortedFg)//.concat(sortedScores.slice(0, parseInt(targetTermsToShow/2))).concat(sortedScores.slice(-parseInt(targetTermsToShow/4)))
                .forEach(function (i) {
                    myFullData.data[i].display = true;
                })

            console.log('newly filtered')
            console.log(myFullData)

            // begin rescaling to ignore hidden terms
            /*
            function scaleDenseRanks(ranks) {
                var max = Math.max(...ranks);
                return ranks.map(x=>x/max)
            }
            var filteredData = myFullData.data.filter(d=>d.display);
            var catRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.cat)))
            var ncatRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.ncat)))
            var rawScores = catRanks.map((x,i) => x - ncatRanks[i]);
            function stretch_0_1(scores) {
                var max = 1.*Math.max(...rawScores);
                var min = -1.*Math.min(...rawScores);
                return scores.map(function(x, i) {
                    if(x == 0) return 0.5;
                    if(x > 0) return (x/max + 1)/2;
                    return (x/min + 1)/2;
                })
            }
            var scores = stretch_0_1(rawScores);
            console.log(scores)
            filteredData.forEach(function(d, i) {
                d.x = xf(catRanks[i]);
                d.y = yf(ncatRanks[i]);
                d.ox = catRanks[i];
                d.oy = ncatRanks[i];
                d.s = scores[i];
                d.os = rawScores[i];
            });
            console.log("rescaled");
            */
            // end rescaling


            this.rerender(//denseRanks.bg,
                fullData.data.map(x => x.ox), //ox
                //denseRanks.fg,
                fullData.data.map(x => x.oy), //oy,
                d => d3.interpolateRdYlBu(d.s));
            if (this.yLabel !== undefined) {
                this.yLabel.remove()
            }
            if (this.xLabel !== undefined) {
                this.xLabel.remove()
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];
            if (otherCategoryNum !== null) {
                bottomName = this.fullData.info.categories[otherCategoryNum];
            }


            this.yLabel = this.drawYLabel(this.svg, leftName + ' Frequncy Rank')
            this.xLabel = this.drawXLabel(this.svg, bottomName + ' Frequency Rank')
            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (
                data,
                word,
                header,
                isUpperPane,
                xOffset=this.topTermsPane.startingOffset,
                length = 14
            ) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            if (otherCategoryNum === null) {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x !== this.fullData.info.categories[categoryNum]);
            } else {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x === this.fullData.info.categories[otherCategoryNum]);

                fullData.info.neutral_category_internal_names = this.fullData.info.categories
                    .filter(x => (x !== this.fullData.info.categories[categoryNum]
                        && x !== this.fullData.info.categories[otherCategoryNum]));
                fullData.info.neutral_category_name = "All Others";

            }
            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();

            console.log(fullData)
        };

        plotInterface.yAxisLogCounts = function (categoryName) {
            var categoryNum = this.fullData.docs.categories.indexOf(categoryName);
            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            console.log("denseRanks")
            console.log(denseRanks);

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            var oy = denseRanks.fgFreqs.map(count => Math.log(count + 1) / Math.log(2))

            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            oy = oy.map(y => (y - oymin) / (oymax - oymin))
            var xf = this.x;
            var yf = this.y;
            var ox = this.fullData.data.map(term => term.ox);
            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            ox = ox.map(y => (y - oxmin) / (oxmax - oxmin))


            this.fullData.data = this.fullData.data.map(function (term, i) {
                term.s = 1;//scores[i];
                term.os = rawScores[i];
                term.cat = denseRanks.fgFreqs[i];
                term.ncat = denseRanks.bgFreqs[i];
                term.cat25k = parseInt(denseRanks.fgFreqs[i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[i] * 25000 / bgFreqSum);
                //term.x = xf(term.ox) // scores[term.i];
                //term.ox = term.ox;
                term.y = yf(oy[i]) // scores[term.i];
                term.oy = oy[i];
                term.x = xf(ox[i]) // scores[term.i];
                term.ox = ox[i];
                term.display = true;
                return term;
            })


            this.rerender(//denseRanks.bg,
                this.fullData.data.map(point => point.ox), //ox
                this.fullData.data.map(point => point.oy), //oy,
                d => d3.interpolateRdYlBu(d.s)
            );

            if (this.yLabel !== undefined) {
                this.yLabel.remove()
                this.yLabel = this.drawYLabel(this.svg, this.fullData.info.categories[categoryNum] + ' log freq.')
            }

            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (data, word, header, isUpperPane, xOffset=this.topTermsPane.startingOffset, length = 14) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];

            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            fullData.info.not_category_internal_names = this.fullData.info.categories
                .filter(x => x !== this.fullData.info.categories[categoryNum]);

            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();
        };

        return plotInterface
    };
}(d3);

; 
 
 // Adapted from https://www.w3schools.com/howto/howto_js_autocomplete.asp
function autocomplete(inputField, autocompleteValues, myPlotInterface) {
    var currentFocus; // current position in autocomplete list.

    inputField.addEventListener("input", function (e) {
        var matchedCandidateListDiv, matchedCandidateDiv, i, userInput = this.value;

        closeAllLists();
        if (!userInput) {
            return false;
        }
        currentFocus = -1;

        matchedCandidateListDiv = document.createElement("div");
        matchedCandidateListDiv.setAttribute("id", this.id + "autocomplete-list");
        matchedCandidateListDiv.setAttribute("class", "autocomplete-items");

        this.parentNode.appendChild(matchedCandidateListDiv);
        autocompleteValues.map(function (candidate) {
            var candidatePrefix = candidate.substr(0, userInput.length);
            if (candidatePrefix.toLowerCase() === userInput.toLowerCase()) {
                matchedCandidateDiv = document.createElement("div");
                matchedCandidateDiv.innerHTML = "<strong>" + candidatePrefix + "</strong>";
                matchedCandidateDiv.innerHTML += candidate.substr(userInput.length);
                matchedCandidateDiv.innerHTML += '<input type=hidden value="' + encodeURIComponent(candidate) + '">';
                matchedCandidateDiv.addEventListener("click", function (e) {
                    console.log("CLICK")
                    console.log(this.getElementsByTagName("input")[0].value)
                    inputField.value = decodeURIComponent(this.getElementsByTagName("input")[0].value);
                    console.log(inputField.value)
                    closeAllLists();
                    myPlotInterface.handleSearchTerm(inputField.value);
                });
                matchedCandidateListDiv.appendChild(matchedCandidateDiv);
            }
        });
    });

    inputField.addEventListener("keydown", function (keyboardEvent) {

        var candidateDivList = document.getElementById(this.id + "autocomplete-list");

        if (!candidateDivList)
            return true;

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList.children,
            x => x.className !== ""
        );

        if (keyboardEvent.keyCode === 40 || keyboardEvent.keyCode === 9) { // down or tab
            keyboardEvent.preventDefault();
            currentFocus++;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 38) { //up
            currentFocus--;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 13) { // enter
            keyboardEvent.preventDefault();
            var selectedTerm = inputField.value;
            console.log("selected term");console.log(selectedTerm);
            console.log(myPlotInterface);
            //if (selectedCandidate)
            //    selectedTerm = selectedCandidate.children[1].value;
            myPlotInterface.handleSearchTerm(selectedTerm);
            closeAllLists(null);
        } else if (keyboardEvent.keyCode === 27) { // esc
            closeAllLists(null);
        }
    });

    function addActive(candidateDivList) {
        if (!candidateDivList) return false;

        removeActive(candidateDivList);

        if (currentFocus >= candidateDivList.length)
            currentFocus = 0;
        if (currentFocus < 0)
            currentFocus = (candidateDivList.length - 1);

        candidateDivList[currentFocus].classList.add("autocomplete-active");

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList,
            x => x.className !== ""
        );

        if (selectedCandidate) {
            var candidateValue = decodeURIComponent(selectedCandidate.children[1].value);

            myPlotInterface.highlightTerm(candidateValue);
            inputField.value = candidateValue;
        }

    }

    function removeActive(candidateDivList) {
        Array.prototype.find.call(
            candidateDivList,
            x => x.classList.remove("autocomplete-active")
        );
    }

    function closeAllLists(elmnt) {
        /*close all autocomplete lists in the document,
        except the one passed as an argument:*/
        var x = document.getElementsByClassName("autocomplete-items");
        for (var i = 0; i < x.length; i++) {
            if (elmnt != x[i] && elmnt != inputField) {
                x[i].parentNode.removeChild(x[i]);
            }
        }
    }

    /*execute a function when someone clicks in the document:*/
    document.addEventListener("click", function (e) {
        closeAllLists(e.target);
    });
}

function getDataAndInfo() { return{"info": {"category_name": "Data scientist", "not_category_name": "Data Engineer", "category_terms": ["statistical", "ml", "research", "statistics", "ai", "algorithms", "machine", "accommodation", "requested", "employment"], "not_category_terms": ["etl", "pipelines", "cloud", "services", "infrastructure", "aws", "technologies", "integration", "code", "kafka"], "category_internal_name": "data scientist", "not_category_internal_names": ["data engineer"], "categories": ["data scientist", "data engineer"], "neutral_category_internal_names": [], "extra_category_internal_names": [], "neutral_category_name": "Neutral", "extra_category_name": "Extra"}, "data": [{"x": 0.27380952380952384, "y": 0.39506172839506176, "ox": 0.27380952380952384, "oy": 0.39506172839506176, "term": "lead", "cat25k": 33, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 46, "s": 0.9085948158253752, "os": 0.12066649725922968, "bg": 3.244088270180531e-06}, {"x": 0.9880952380952381, "y": 0.9938271604938271, "ox": 0.9880952380952381, "oy": 0.9938271604938271, "term": "to", "cat25k": 1004, "ncat25k": 835, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2003, "ncat": 1862, "s": 0.602319236016371, "os": 0.0056993502014738295, "bg": 6.36890650448819e-07}, {"x": 0.10714285714285715, "y": 0.08024691358024692, "ox": 0.10714285714285715, "oy": 0.08024691358024692, "term": "determine", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 18, "s": 0.4965893587994543, "os": -0.026536464950811342, "bg": 1.4481203864519463e-06}, {"x": 0.8511904761904762, "y": 0.8395061728395062, "ox": 0.8511904761904762, "oy": 0.8395061728395062, "term": "from", "cat25k": 109, "ncat25k": 95, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 217, "ncat": 211, "s": 0.568212824010914, "os": -0.011580208371147482, "bg": 3.761476010639219e-07}, {"x": 1.0, "y": 1.0, "ox": 1.0, "oy": 1.0, "term": "and", "cat25k": 1899, "ncat25k": 1891, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3788, "ncat": 4218, "s": 0.5879945429740792, "os": 0.0, "bg": 1.231905276444117e-06}, {"x": 0.8273809523809523, "y": 0.5740740740740741, "ox": 0.8273809523809523, "oy": 0.5740740740740741, "term": "other", "cat25k": 50, "ncat25k": 85, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 100, "ncat": 189, "s": 0.025920873124147342, "os": -0.25171525029948816, "bg": 5.906467944962853e-07}, {"x": 0.49404761904761907, "y": 0.4444444444444445, "ox": 0.49404761904761907, "oy": 0.4444444444444445, "term": "sources", "cat25k": 38, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 90, "s": 0.2967257844474761, "os": -0.04918865938214684, "bg": 5.2661322936051945e-06}, {"x": 0.9821428571428571, "y": 0.9753086419753086, "ox": 0.9821428571428571, "oy": 0.9753086419753086, "term": "of", "cat25k": 620, "ncat25k": 635, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1237, "ncat": 1417, "s": 0.5763983628922237, "os": -0.006788398010672658, "bg": 4.0358721056004343e-07}, {"x": 0.40476190476190477, "y": 0.3271604938271605, "ox": 0.40476190476190477, "oy": 0.3271604938271605, "term": "information", "cat25k": 27, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 70, "s": 0.18690313778990453, "os": -0.07699568011035685, "bg": 2.6375008251571436e-07}, {"x": 0.11904761904761905, "y": 0.10493827160493828, "ox": 0.11904761904761905, "oy": 0.10493827160493828, "term": "automate", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 20, "s": 0.5654843110504775, "os": -0.013830907176825058, "bg": 3.079591629503487e-05}, {"x": 0.3154761904761905, "y": 0.10493827160493828, "ox": 0.3154761904761905, "oy": 0.10493827160493828, "term": "procedures", "cat25k": 9, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 54, "s": 0.036152796725784454, "os": -0.2090971793661742, "bg": 2.947900529117005e-06}, {"x": 0.9464285714285714, "y": 0.9506172839506173, "ox": 0.9464285714285714, "oy": 0.9506172839506173, "term": "for", "cat25k": 256, "ncat25k": 266, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 510, "ncat": 593, "s": 0.5995907230559345, "os": 0.0041746832685954915, "bg": 3.7179173111972393e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "reading", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 1.2579133889942538e-07}, {"x": 0.6428571428571428, "y": 0.5185185185185185, "ox": 0.6428571428571428, "oy": 0.5185185185185185, "term": "understanding", "cat25k": 46, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 91, "ncat": 129, "s": 0.09822646657571624, "os": -0.1234980215631466, "bg": 8.889211055447142e-06}, {"x": 0.4107142857142857, "y": 0.40740740740740744, "ox": 0.4107142857142857, "oy": 0.40740740740740744, "term": "improve", "cat25k": 34, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 71, "s": 0.5804911323328786, "os": -0.003158238646676581, "bg": 4.494208430835402e-06}, {"x": 0.01785714285714286, "y": 0.0617283950617284, "ox": 0.01785714285714286, "oy": 0.0617283950617284, "term": "pricing", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7694406548431105, "os": 0.04381602352343268, "bg": 5.891240984475311e-07}, {"x": 0.17857142857142858, "y": 0.1358024691358025, "ox": 0.17857142857142858, "oy": 0.1358024691358025, "term": "when", "cat25k": 11, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 30, "s": 0.33492496589358794, "os": -0.04232765818419429, "bg": 1.5982103731997947e-07}, {"x": 0.7142857142857142, "y": 0.6604938271604938, "ox": 0.7142857142857142, "oy": 0.6604938271604938, "term": "are", "cat25k": 60, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 120, "ncat": 155, "s": 0.26671214188267395, "os": -0.05339964424438237, "bg": 2.2976772323411612e-07}, {"x": 0.30952380952380953, "y": 0.20370370370370372, "ox": 0.30952380952380953, "oy": 0.20370370370370372, "term": "not", "cat25k": 17, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 53, "s": 0.12687585266030013, "os": -0.10502051040040658, "bg": 6.530999462803651e-08}, {"x": 0.13690476190476192, "y": 0.05555555555555556, "ox": 0.13690476190476192, "oy": 0.05555555555555556, "term": "available", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 23, "s": 0.17871759890859484, "os": -0.08066214106799288, "bg": 1.6853144560699335e-07}, {"x": 0.726190476190476, "y": 0.6481481481481481, "ox": 0.726190476190476, "oy": 0.6481481481481481, "term": "support", "cat25k": 59, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 118, "ncat": 157, "s": 0.1862210095497954, "os": -0.07750390242131633, "bg": 1.4720860474228399e-06}, {"x": 0.05952380952380953, "y": 0.14197530864197533, "ox": 0.05952380952380953, "oy": 0.14197530864197533, "term": "initiatives", "cat25k": 12, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 10, "s": 0.8697135061391542, "os": 0.08215050640723129, "bg": 4.060468247046025e-06}, {"x": 0.994047619047619, "y": 0.9814814814814814, "ox": 0.994047619047619, "oy": 0.9814814814814814, "term": "data", "cat25k": 746, "ncat25k": 1127, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1489, "ncat": 2514, "s": 0.567530695770805, "os": -0.012487748212146488, "bg": 1.966984579054649e-05}, {"x": 0.5059523809523809, "y": 0.36419753086419754, "ox": 0.5059523809523809, "oy": 0.36419753086419754, "term": "process", "cat25k": 30, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 92, "s": 0.08321964529331513, "os": -0.14077758013576802, "bg": 1.7181358662167633e-06}, {"x": 0.726190476190476, "y": 0.45679012345679015, "ox": 0.726190476190476, "oy": 0.45679012345679015, "term": "quality", "cat25k": 39, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 157, "s": 0.022510231923601638, "os": -0.26768795150107094, "bg": 2.4681411106098125e-06}, {"x": 0.04761904761904762, "y": 0.02469135802469136, "ox": 0.04761904761904762, "oy": 0.02469135802469136, "term": "interface", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5368349249658936, "os": -0.022579591244055616, "bg": 4.03941553965851e-07}, {"x": 0.9702380952380952, "y": 0.962962962962963, "ox": 0.9702380952380952, "oy": 0.962962962962963, "term": "with", "cat25k": 464, "ncat25k": 538, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 926, "ncat": 1201, "s": 0.5750341064120055, "os": -0.0072240171343521675, "bg": 1.3363828992119036e-06}, {"x": 0.15476190476190477, "y": 0.1234567901234568, "ox": 0.15476190476190477, "oy": 0.1234567901234568, "term": "others", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 26, "s": 0.44611186903137795, "os": -0.030928957781246602, "bg": 8.25079696420894e-07}, {"x": 0.9642857142857143, "y": 0.9691358024691358, "ox": 0.9642857142857143, "oy": 0.9691358024691358, "term": "in", "cat25k": 528, "ncat25k": 495, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1054, "ncat": 1104, "s": 0.6002728512960437, "os": 0.0048281119541147, "bg": 5.095924191054761e-07}, {"x": 0.8988095238095237, "y": 0.845679012345679, "ox": 0.8988095238095237, "oy": 0.845679012345679, "term": "our", "cat25k": 109, "ncat25k": 125, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 218, "ncat": 279, "s": 0.2789904502046385, "os": -0.05278251715250304, "bg": 9.951293733209113e-07}, {"x": 0.6845238095238095, "y": 0.882716049382716, "ox": 0.6845238095238095, "oy": 0.882716049382716, "term": "science", "cat25k": 142, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 283, "ncat": 141, "s": 0.9556616643929059, "os": 0.19704505027770725, "bg": 4.864062837084072e-06}, {"x": 0.7916666666666666, "y": 0.7592592592592592, "ox": 0.7916666666666666, "oy": 0.7592592592592592, "term": "analytics", "cat25k": 78, "ncat25k": 81, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 156, "ncat": 181, "s": 0.4433833560709413, "os": -0.03216321196500527, "bg": 0.00017007991484901117}, {"x": 0.6547619047619047, "y": 0.691358024691358, "ox": 0.6547619047619047, "oy": 0.691358024691358, "term": "teams", "cat25k": 65, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 133, "s": 0.727830832196453, "os": 0.03644680001452061, "bg": 1.5192734625546929e-05}, {"x": 0.029761904761904767, "y": 0.08024691358024692, "ox": 0.029761904761904767, "oy": 0.08024691358024692, "term": "explain", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.7974079126875854, "os": 0.050386611972265585, "bg": 1.4348029714610118e-06}, {"x": 0.27976190476190477, "y": 0.6296296296296295, "ox": 0.27976190476190477, "oy": 0.6296296296296295, "term": "insights", "cat25k": 56, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 112, "ncat": 47, "s": 0.9843110504774898, "os": 0.3478781718517443, "bg": 4.1371402135362804e-05}, {"x": 0.8869047619047619, "y": 0.8086419753086419, "ox": 0.8869047619047619, "oy": 0.8086419753086419, "term": "team", "cat25k": 95, "ncat25k": 120, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 189, "ncat": 267, "s": 0.18553888130968624, "os": -0.07775801357679601, "bg": 5.308655813946422e-06}, {"x": 0.3035714285714286, "y": 0.17283950617283952, "ox": 0.3035714285714286, "oy": 0.17283950617283952, "term": "members", "cat25k": 14, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 51, "s": 0.09208731241473397, "os": -0.12977819726285986, "bg": 6.967631541342583e-07}, {"x": 0.07142857142857144, "y": 0.030864197530864203, "ox": 0.07142857142857144, "oy": 0.030864197530864203, "term": "employees", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.3587994542974079, "os": -0.04011326097215668, "bg": 6.111744805174203e-07}, {"x": 0.5833333333333333, "y": 0.40740740740740744, "ox": 0.5833333333333333, "oy": 0.40740740740740744, "term": "provide", "cat25k": 34, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 106, "s": 0.053888130968622105, "os": -0.17475587178277124, "bg": 1.910042654840713e-06}, {"x": 0.875, "y": 0.691358024691358, "ox": 0.875, "oy": 0.691358024691358, "term": "technical", "cat25k": 65, "ncat25k": 102, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 228, "s": 0.04911323328785812, "os": -0.18248811122808295, "bg": 6.938786434241033e-06}, {"x": 0.27976190476190477, "y": 0.24074074074074076, "ox": 0.27976190476190477, "oy": 0.24074074074074076, "term": "expertise", "cat25k": 20, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 47, "s": 0.3663028649386084, "os": -0.038624895632918294, "bg": 9.61551362495919e-06}, {"x": 0.9285714285714286, "y": 0.888888888888889, "ox": 0.9285714285714286, "oy": 0.888888888888889, "term": "on", "cat25k": 148, "ncat25k": 171, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 296, "ncat": 382, "s": 0.3635743519781719, "os": -0.03942353069299742, "bg": 3.615488531781428e-07}, {"x": 0.07738095238095238, "y": 0.03703703703703704, "ox": 0.07738095238095238, "oy": 0.03703703703703704, "term": "aspects", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.3601637107776262, "os": -0.03989545141031692, "bg": 1.3340659930148305e-06}, {"x": 0.9226190476190476, "y": 0.9012345679012346, "ox": 0.9226190476190476, "oy": 0.9012345679012346, "term": "as", "cat25k": 157, "ncat25k": 152, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 314, "ncat": 340, "s": 0.5518417462482947, "os": -0.021236432279377104, "bg": 5.81969875429268e-07}, {"x": 0.2142857142857143, "y": 0.2654320987654321, "ox": 0.2142857142857143, "oy": 0.2654320987654321, "term": "needed", "cat25k": 22, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 36, "s": 0.8008185538881311, "os": 0.051003739064144904, "bg": 2.06172711820213e-06}, {"x": 0.22023809523809523, "y": 0.25925925925925924, "ox": 0.22023809523809523, "oy": 0.25925925925925924, "term": "bachelor", "cat25k": 21, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 37, "s": 0.752387448840382, "os": 0.0389516099756779, "bg": 1.2698867887890536e-05}, {"x": 0.7202380952380952, "y": 0.7839506172839505, "ox": 0.7202380952380952, "oy": 0.7839506172839505, "term": "s", "cat25k": 85, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 169, "ncat": 156, "s": 0.8335607094133698, "os": 0.06338258249537154, "bg": 2.0355643659513567e-07}, {"x": 0.4285714285714286, "y": 0.611111111111111, "ox": 0.4285714285714286, "oy": 0.611111111111111, "term": "degree", "cat25k": 54, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 75, "s": 0.9502046384720327, "os": 0.18154426979344396, "bg": 5.451580883236238e-06}, {"x": 0.9583333333333333, "y": 0.95679012345679, "ox": 0.9583333333333333, "oy": 0.95679012345679, "term": "a", "cat25k": 359, "ncat25k": 413, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 716, "ncat": 922, "s": 0.5852660300136426, "os": -0.001524666932878338, "bg": 3.6074198060699654e-07}, {"x": 0.08928571428571429, "y": 0.3888888888888889, "ox": 0.08928571428571429, "oy": 0.3888888888888889, "term": "quantitative", "cat25k": 32, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 15, "s": 0.9809004092769441, "os": 0.2979634805967982, "bg": 2.2691349109852846e-05}, {"x": 0.2916666666666667, "y": 0.5308641975308642, "ox": 0.2916666666666667, "oy": 0.5308641975308642, "term": "field", "cat25k": 47, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 49, "s": 0.9679399727148704, "os": 0.2378843431226631, "bg": 2.0975524059805004e-06}, {"x": 0.8095238095238095, "y": 0.6851851851851851, "ox": 0.8095238095238095, "oy": 0.6851851851851851, "term": "strong", "cat25k": 63, "ncat25k": 82, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 184, "s": 0.09754433833560709, "os": -0.12353432315678659, "bg": 1.0118518303122515e-05}, {"x": 0.03571428571428572, "y": 0.16049382716049385, "ox": 0.03571428571428572, "oy": 0.16049382716049385, "term": "natural", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 6, "s": 0.9120054570259208, "os": 0.12422405343594582, "bg": 6.083163132290273e-07}, {"x": 0.375, "y": 0.29629629629629634, "ox": 0.375, "oy": 0.29629629629629634, "term": "language", "cat25k": 24, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 64, "s": 0.1848567530695771, "os": -0.07808472791955567, "bg": 1.61587337046745e-06}, {"x": 0.7857142857142857, "y": 0.4320987654320988, "ox": 0.7857142857142857, "oy": 0.4320987654320988, "term": "processing", "cat25k": 36, "ncat25k": 81, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 180, "s": 0.01159618008185539, "os": -0.3513994264348205, "bg": 9.065768915708854e-06}, {"x": 0.9523809523809524, "y": 0.9382716049382717, "ox": 0.9523809523809524, "oy": 0.9382716049382717, "term": "experience", "cat25k": 238, "ncat25k": 380, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 475, "ncat": 847, "s": 0.5648021828103683, "os": -0.014012415145024826, "bg": 1.926515471828267e-05}, {"x": 0.8392857142857142, "y": 0.7530864197530863, "ox": 0.8392857142857142, "oy": 0.7530864197530863, "term": "python", "cat25k": 76, "ncat25k": 86, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 152, "ncat": 191, "s": 0.16780354706684858, "os": -0.0856354593966675, "bg": 3.8718691267597455e-05}, {"x": 0.9404761904761905, "y": 0.9320987654320987, "ox": 0.9404761904761905, "oy": 0.9320987654320987, "term": "or", "cat25k": 238, "ncat25k": 251, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 474, "ncat": 560, "s": 0.572987721691678, "os": -0.008313064943551107, "bg": 7.981943944250451e-07}, {"x": 0.28571428571428575, "y": 0.6728395061728395, "ox": 0.28571428571428575, "oy": 0.6728395061728395, "term": "r", "cat25k": 62, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 123, "ncat": 48, "s": 0.9890859481582538, "os": 0.3849057973645043, "bg": 1.0567262974020459e-06}, {"x": 0.15476190476190477, "y": 0.22839506172839508, "ox": 0.15476190476190477, "oy": 0.22839506172839508, "term": "proficiency", "cat25k": 19, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 26, "s": 0.8526603001364257, "os": 0.07336552074636074, "bg": 3.519249316190306e-05}, {"x": 0.053571428571428575, "y": 0.08024691358024692, "ox": 0.053571428571428575, "oy": 0.08024691358024692, "term": "basic", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.689631650750341, "os": 0.02671797291901115, "bg": 5.179708332507326e-07}, {"x": 0.8452380952380952, "y": 0.5864197530864197, "ox": 0.8452380952380952, "oy": 0.5864197530864197, "term": "sql", "cat25k": 51, "ncat25k": 92, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 102, "ncat": 205, "s": 0.02455661664392906, "os": -0.2571967909391222, "bg": 2.3476223880741396e-05}, {"x": 0.20238095238095238, "y": 0.06790123456790124, "ox": 0.20238095238095238, "oy": 0.06790123456790124, "term": "queries", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 34, "s": 0.09004092769440655, "os": -0.13348095981413582, "bg": 6.081621441364553e-06}, {"x": 0.39880952380952384, "y": 0.537037037037037, "ox": 0.39880952380952384, "oy": 0.537037037037037, "term": "analytical", "cat25k": 47, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 94, "ncat": 69, "s": 0.926330150068213, "os": 0.13751043670817148, "bg": 3.89354568365228e-05}, {"x": 0.7440476190476191, "y": 0.7098765432098765, "ox": 0.7440476190476191, "oy": 0.7098765432098765, "term": "skills", "cat25k": 69, "ncat25k": 75, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 138, "ncat": 168, "s": 0.4345156889495225, "os": -0.033905688459723415, "bg": 8.560575105030704e-06}, {"x": 0.25, "y": 0.17283950617283952, "ox": 0.25, "oy": 0.17283950617283952, "term": "self", "cat25k": 14, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 42, "s": 0.1903137789904502, "os": -0.07652375939303738, "bg": 1.2388408534247216e-06}, {"x": 0.029761904761904767, "y": 0.04938271604938272, "ox": 0.029761904761904767, "oy": 0.04938271604938272, "term": "starter", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.6330150068212824, "os": 0.01971176534649871, "bg": 2.6430145329203726e-06}, {"x": 0.3392857142857143, "y": 0.23456790123456792, "ox": 0.3392857142857143, "oy": 0.23456790123456792, "term": "learn", "cat25k": 19, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 58, "s": 0.1323328785811733, "os": -0.10393146259120772, "bg": 1.069429347264552e-06}, {"x": 0.40476190476190477, "y": 0.33333333333333337, "ox": 0.40476190476190477, "oy": 0.33333333333333337, "term": "excellent", "cat25k": 27, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 70, "s": 0.20736698499317874, "os": -0.07086071078520345, "bg": 4.104392509278451e-06}, {"x": 0.4523809523809524, "y": 0.3827160493827161, "ox": 0.4523809523809524, "oy": 0.3827160493827161, "term": "communication", "cat25k": 32, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 80, "s": 0.2148703956343793, "os": -0.06911823429048536, "bg": 4.435118152555565e-06}, {"x": 0.2976190476190476, "y": 0.36419753086419754, "ox": 0.2976190476190476, "oy": 0.36419753086419754, "term": "both", "cat25k": 30, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 50, "s": 0.8383356070941338, "os": 0.06632301158020837, "bg": 9.617264731792784e-07}, {"x": 0.2142857142857143, "y": 0.20987654320987656, "ox": 0.2142857142857143, "oy": 0.20987654320987656, "term": "written", "cat25k": 17, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 36, "s": 0.5798090040927695, "os": -0.004210984862235451, "bg": 1.4261914352314513e-06}, {"x": 0.17261904761904762, "y": 0.1234567901234568, "ox": 0.17261904761904762, "oy": 0.1234567901234568, "term": "verbal", "cat25k": 10, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 29, "s": 0.29740791268758526, "os": -0.04868043707118741, "bg": 1.8262491637455995e-05}, {"x": 0.11904761904761905, "y": 0.14814814814814817, "ox": 0.11904761904761905, "oy": 0.14814814814814817, "term": "demonstrated", "cat25k": 12, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 20, "s": 0.6998635743519781, "os": 0.029113878099248564, "bg": 7.4401654286964134e-06}, {"x": 0.7559523809523809, "y": 0.6049382716049382, "ox": 0.7559523809523809, "oy": 0.6049382716049382, "term": "ability", "cat25k": 53, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 171, "s": 0.0709413369713506, "os": -0.1500344865139579, "bg": 1.0592735513498685e-05}, {"x": 0.863095238095238, "y": 0.7469135802469136, "ox": 0.863095238095238, "oy": 0.7469135802469136, "term": "build", "cat25k": 75, "ncat25k": 100, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 150, "ncat": 223, "s": 0.1043656207366985, "os": -0.11543906777507529, "bg": 9.649454712677266e-06}, {"x": 0.04761904761904762, "y": 0.1358024691358025, "ox": 0.04761904761904762, "oy": 0.1358024691358025, "term": "relationships", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 8, "s": 0.8758526603001364, "os": 0.08784985660870512, "bg": 2.0028726534844712e-06}, {"x": 0.9166666666666666, "y": 0.9074074074074073, "ox": 0.9166666666666666, "oy": 0.9074074074074073, "term": "work", "cat25k": 169, "ncat25k": 152, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 338, "ncat": 339, "s": 0.5702592087312415, "os": -0.009184303190910126, "bg": 3.2269500761389815e-06}, {"x": 0.1904761904761905, "y": 0.16049382716049385, "ox": 0.1904761904761905, "oy": 0.16049382716049385, "term": "part", "cat25k": 13, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 32, "s": 0.4474761255115962, "os": -0.02962210041020802, "bg": 3.830456328767617e-07}, {"x": 0.5952380952380951, "y": 0.8950617283950617, "ox": 0.5952380952380951, "oy": 0.8950617283950617, "term": "analysis", "cat25k": 152, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 303, "ncat": 110, "s": 0.9815825375170532, "os": 0.2980723853777181, "bg": 6.605010677847378e-06}, {"x": 0.2380952380952381, "y": 0.46296296296296297, "ox": 0.2380952380952381, "oy": 0.46296296296296297, "term": "deep", "cat25k": 39, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 40, "s": 0.9645293315143247, "os": 0.22365411841579844, "bg": 4.406754823174481e-06}, {"x": 0.6369047619047619, "y": 0.4382716049382716, "ox": 0.6369047619047619, "oy": 0.4382716049382716, "term": "into", "cat25k": 37, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 126, "s": 0.03956343792633015, "os": -0.19733546302682692, "bg": 8.935344438097082e-07}, {"x": 0.20238095238095238, "y": 0.5, "ox": 0.20238095238095238, "oy": 0.5, "term": "opportunities", "cat25k": 43, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 34, "s": 0.980218281036835, "os": 0.29596689294660034, "bg": 3.421710264031568e-06}, {"x": 0.2380952380952381, "y": 0.25308641975308643, "ox": 0.2380952380952381, "oy": 0.25308641975308643, "term": "partner", "cat25k": 21, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 40, "s": 0.6248294679399727, "os": 0.015065161360583751, "bg": 3.0516226617341646e-06}, {"x": 0.4642857142857143, "y": 0.2654320987654321, "ox": 0.4642857142857143, "oy": 0.2654320987654321, "term": "scientists", "cat25k": 22, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 84, "s": 0.03888130968622101, "os": -0.19751697099502669, "bg": 1.4886968639252529e-05}, {"x": 0.25, "y": 0.06790123456790124, "ox": 0.25, "oy": 0.06790123456790124, "term": "analysts", "cat25k": 6, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 42, "s": 0.05115961800818554, "os": -0.18081823792064472, "bg": 1.4119582742353312e-05}, {"x": 0.4226190476190476, "y": 0.5987654320987654, "ox": 0.4226190476190476, "oy": 0.5987654320987654, "term": "help", "cat25k": 52, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 104, "ncat": 74, "s": 0.9474761255115962, "os": 0.17519149090645075, "bg": 5.824980616558883e-07}, {"x": 0.9761904761904762, "y": 0.9876543209876544, "ox": 0.9761904761904762, "oy": 0.9876543209876544, "term": "the", "cat25k": 794, "ncat25k": 573, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1584, "ncat": 1279, "s": 0.6125511596180082, "os": 0.011398700402947659, "bg": 2.474935053642334e-07}, {"x": 0.6904761904761904, "y": 0.7469135802469136, "ox": 0.6904761904761904, "oy": 0.7469135802469136, "term": "product", "cat25k": 75, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 150, "ncat": 143, "s": 0.8130968622100955, "os": 0.056158565361019375, "bg": 1.4678503157786884e-06}, {"x": 0.13690476190476192, "y": 0.14197530864197533, "ox": 0.13690476190476192, "oy": 0.14197530864197533, "term": "strategy", "cat25k": 12, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 23, "s": 0.6009549795361527, "os": 0.005227429484154361, "bg": 1.6062343962839695e-06}, {"x": 0.2261904761904762, "y": 0.20370370370370372, "ox": 0.2261904761904762, "oy": 0.20370370370370372, "term": "leadership", "cat25k": 17, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 38, "s": 0.5443383356070941, "os": -0.02218027371401607, "bg": 3.920994825971201e-06}, {"x": 0.10714285714285715, "y": 0.308641975308642, "ox": 0.10714285714285715, "oy": 0.308641975308642, "term": "areas", "cat25k": 25, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 18, "s": 0.9583901773533425, "os": 0.2004574000798635, "bg": 1.113904837225946e-06}, {"x": 0.06547619047619048, "y": 0.07407407407407408, "ox": 0.06547619047619048, "oy": 0.07407407407407408, "term": "opportunity", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.6043656207366985, "os": 0.008748684067230547, "bg": 7.648832775647514e-07}, {"x": 0.2916666666666667, "y": 0.5185185185185185, "ox": 0.2916666666666667, "oy": 0.5185185185185185, "term": "drive", "cat25k": 46, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 91, "ncat": 49, "s": 0.9652114597544339, "os": 0.2256144044723563, "bg": 2.5948479238867347e-06}, {"x": 0.13690476190476192, "y": 0.19753086419753088, "ox": 0.13690476190476192, "oy": 0.19753086419753088, "term": "growth", "cat25k": 16, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 23, "s": 0.8212824010914052, "os": 0.060442153410534716, "bg": 1.3731950209696242e-06}, {"x": 0.13095238095238096, "y": 0.3827160493827161, "ox": 0.13095238095238096, "oy": 0.3827160493827161, "term": "evaluate", "cat25k": 32, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 22, "s": 0.9720327421555253, "os": 0.2504083929284495, "bg": 1.0582629795954448e-05}, {"x": 0.19642857142857142, "y": 0.5185185185185185, "ox": 0.19642857142857142, "oy": 0.5185185185185185, "term": "define", "cat25k": 46, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 91, "ncat": 33, "s": 0.9829467939972715, "os": 0.32028896068537405, "bg": 6.852011742358832e-06}, {"x": 0.9285714285714286, "y": 0.9444444444444444, "ox": 0.9285714285714286, "oy": 0.9444444444444444, "term": "business", "cat25k": 243, "ncat25k": 171, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 485, "ncat": 382, "s": 0.6282401091405184, "os": 0.01579119323338296, "bg": 2.721102680442439e-06}, {"x": 0.23214285714285715, "y": 0.308641975308642, "ox": 0.23214285714285715, "oy": 0.308641975308642, "term": "metrics", "cat25k": 25, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 39, "s": 0.8615279672578445, "os": 0.07619704505027775, "bg": 4.4612635752115156e-05}, {"x": 0.07738095238095238, "y": 0.05555555555555556, "ox": 0.07738095238095238, "oy": 0.05555555555555556, "term": "execution", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 13, "s": 0.5504774897680764, "os": -0.021490543434856797, "bg": 2.0076383338098428e-06}, {"x": 0.2142857142857143, "y": 0.5061728395061729, "ox": 0.2142857142857143, "oy": 0.5061728395061729, "term": "communicate", "cat25k": 44, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 36, "s": 0.9795361527967258, "os": 0.29026754274512645, "bg": 1.877924667280987e-05}, {"x": 0.07142857142857144, "y": 0.11111111111111112, "ox": 0.07142857142857144, "oy": 0.11111111111111112, "term": "state", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 12, "s": 0.7551159618008185, "os": 0.039641340254837176, "bg": 1.3238873003601948e-07}, {"x": 0.39285714285714285, "y": 0.4197530864197531, "ox": 0.39285714285714285, "oy": 0.4197530864197531, "term": "stakeholders", "cat25k": 35, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 68, "s": 0.6916780354706685, "os": 0.02686317929357096, "bg": 4.1002310391055074e-05}, {"x": 0.02380952380952381, "y": 0.09876543209876544, "ox": 0.02380952380952381, "oy": 0.09876543209876544, "term": "interpret", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.8553888130968622, "os": 0.07470867971103931, "bg": 7.779647702543614e-06}, {"x": 0.02380952380952381, "y": 0.29012345679012347, "ox": 0.02380952380952381, "oy": 0.29012345679012347, "term": "experiments", "cat25k": 24, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 4, "s": 0.975443383356071, "os": 0.26489272879079395, "bg": 7.3928011221982195e-06}, {"x": 0.10714285714285715, "y": 0.36419753086419754, "ox": 0.10714285714285715, "oy": 0.36419753086419754, "term": "decision", "cat25k": 30, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 18, "s": 0.9733969986357436, "os": 0.25567212400624384, "bg": 2.1936865167687862e-06}, {"x": 0.8571428571428572, "y": 0.7654320987654321, "ox": 0.8571428571428572, "oy": 0.7654320987654321, "term": "tools", "cat25k": 79, "ncat25k": 95, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 158, "ncat": 212, "s": 0.15484311050477492, "os": -0.09111700003630163, "bg": 3.985718975012688e-06}, {"x": 0.5238095238095238, "y": 0.5308641975308642, "ox": 0.5238095238095238, "oy": 0.5308641975308642, "term": "by", "cat25k": 47, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 95, "s": 0.6036834924965894, "os": 0.007115112353432318, "bg": 1.1223359032444534e-07}, {"x": 0.6607142857142857, "y": 0.40740740740740744, "ox": 0.6607142857142857, "oy": 0.40740740740740744, "term": "building", "cat25k": 34, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 134, "s": 0.02660300136425648, "os": -0.2516789487058482, "bg": 3.131861025209565e-06}, {"x": 0.07738095238095238, "y": 0.12962962962962962, "ox": 0.07738095238095238, "oy": 0.12962962962962962, "term": "dashboards", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.8015006821282402, "os": 0.05212908846698369, "bg": 0.0001639526080520019}, {"x": 0.17261904761904762, "y": 0.2654320987654321, "ox": 0.17261904761904762, "oy": 0.2654320987654321, "term": "reports", "cat25k": 22, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 29, "s": 0.8826739427012278, "os": 0.09242385740734019, "bg": 1.2562961922770396e-06}, {"x": 0.2976190476190476, "y": 0.5308641975308642, "ox": 0.2976190476190476, "oy": 0.5308641975308642, "term": "key", "cat25k": 47, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 50, "s": 0.9665757162346522, "os": 0.23196718335934952, "bg": 2.0880546061031735e-06}, {"x": 0.33333333333333337, "y": 0.5432098765432098, "ox": 0.33333333333333337, "oy": 0.5432098765432098, "term": "sets", "cat25k": 48, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 95, "ncat": 57, "s": 0.9618008185538882, "os": 0.20873416342977452, "bg": 5.930203995896299e-06}, {"x": 0.8035714285714285, "y": 0.17901234567901236, "ox": 0.8035714285714285, "oy": 0.17901234567901236, "term": "pipelines", "cat25k": 15, "ncat25k": 82, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 183, "s": 0.0006821282401091405, "os": -0.6206846480560497, "bg": 0.00019975398224168257}, {"x": 0.22023809523809523, "y": 0.14814814814814817, "ox": 0.22023809523809523, "oy": 0.14814814814814817, "term": "operational", "cat25k": 12, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 37, "s": 0.20600272851296045, "os": -0.07147783787708281, "bg": 7.011223071659354e-06}, {"x": 0.029761904761904767, "y": 0.0925925925925926, "ox": 0.029761904761904767, "oy": 0.0925925925925926, "term": "exploratory", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.8321964529331515, "os": 0.06265655062257233, "bg": 3.148882304226115e-05}, {"x": 0.48214285714285715, "y": 0.7345679012345678, "ox": 0.48214285714285715, "oy": 0.7345679012345678, "term": "projects", "cat25k": 72, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 144, "ncat": 87, "s": 0.9727148703956344, "os": 0.2509892184266889, "bg": 4.983263622832438e-06}, {"x": 0.3273809523809524, "y": 0.5679012345679012, "ox": 0.3273809523809524, "oy": 0.5679012345679012, "term": "identify", "cat25k": 50, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 99, "ncat": 56, "s": 0.9686221009549796, "os": 0.23919120049370168, "bg": 8.62047534190891e-06}, {"x": 0.32142857142857145, "y": 0.6049382716049382, "ox": 0.32142857142857145, "oy": 0.6049382716049382, "term": "through", "cat25k": 53, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 55, "s": 0.9774897680763984, "os": 0.2819181762079356, "bg": 9.343609041346793e-07}, {"x": 0.5119047619047619, "y": 0.35185185185185186, "ox": 0.5119047619047619, "oy": 0.35185185185185186, "term": "high", "cat25k": 29, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 93, "s": 0.06275579809004092, "os": -0.15896467854938828, "bg": 8.740450999399374e-07}, {"x": 0.380952380952381, "y": 0.28395061728395066, "ox": 0.380952380952381, "oy": 0.28395061728395066, "term": "real", "cat25k": 23, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 66, "s": 0.14461118690313782, "os": -0.09627182633317605, "bg": 7.522300977840358e-07}, {"x": 0.14285714285714288, "y": 0.17283950617283952, "ox": 0.14285714285714288, "oy": 0.17283950617283952, "term": "world", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 24, "s": 0.7005457025920873, "os": 0.02998511634660761, "bg": 2.4071793884542706e-07}, {"x": 0.1130952380952381, "y": 0.16049382716049385, "ox": 0.1130952380952381, "oy": 0.16049382716049385, "term": "clinical", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 19, "s": 0.7851296043656207, "os": 0.04730097651286891, "bg": 2.090199736235373e-06}, {"x": 0.03571428571428572, "y": 0.12962962962962962, "ox": 0.03571428571428572, "oy": 0.12962962962962962, "term": "explore", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.8854024556616644, "os": 0.09354920681017896, "bg": 1.6956152676589932e-06}, {"x": 0.2619047619047619, "y": 0.17283950617283952, "ox": 0.2619047619047619, "oy": 0.17283950617283952, "term": "re", "cat25k": 14, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 44, "s": 0.15757162346521147, "os": -0.0883580789196646, "bg": 3.341421925764668e-07}, {"x": 0.08333333333333334, "y": 0.09876543209876544, "ox": 0.08333333333333334, "oy": 0.09876543209876544, "term": "position", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 14, "s": 0.6268758526603001, "os": 0.01553708207790322, "bg": 6.877118962134549e-07}, {"x": 0.3273809523809524, "y": 0.23456790123456792, "ox": 0.3273809523809524, "oy": 0.23456790123456792, "term": "existing", "cat25k": 19, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 56, "s": 0.1514324693042292, "os": -0.09209714306458056, "bg": 3.1913200982159317e-06}, {"x": 0.02380952380952381, "y": 0.10493827160493828, "ox": 0.02380952380952381, "oy": 0.10493827160493828, "term": "novel", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.8676671214188268, "os": 0.08084364903619269, "bg": 1.9153742071148857e-06}, {"x": 0.43452380952380953, "y": 0.3765432098765432, "ox": 0.43452380952380953, "oy": 0.3765432098765432, "term": "collaborate", "cat25k": 31, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 76, "s": 0.25784447476125516, "os": -0.05750172432569789, "bg": 0.00010332663082929728}, {"x": 0.10714285714285715, "y": 0.06790123456790124, "ox": 0.10714285714285715, "oy": 0.06790123456790124, "term": "computational", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 18, "s": 0.36493860845839016, "os": -0.03880640360111809, "bg": 8.811880481122598e-06}, {"x": 0.09523809523809525, "y": 0.19753086419753088, "ox": 0.09523809523809525, "oy": 0.19753086419753088, "term": "scientific", "cat25k": 16, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 16, "s": 0.8929058663028651, "os": 0.10186227175372999, "bg": 2.257648542867772e-06}, {"x": 0.02380952380952381, "y": 0.09876543209876544, "ox": 0.02380952380952381, "oy": 0.09876543209876544, "term": "champion", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.8553888130968622, "os": 0.07470867971103931, "bg": 2.759464134140863e-06}, {"x": 0.053571428571428575, "y": 0.06790123456790124, "ox": 0.053571428571428575, "oy": 0.06790123456790124, "term": "discovery", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6193724420190996, "os": 0.014448034268704404, "bg": 1.753120258220589e-06}, {"x": 0.8928571428571429, "y": 0.7777777777777777, "ox": 0.8928571428571429, "oy": 0.7777777777777777, "term": "development", "cat25k": 83, "ncat25k": 124, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 165, "ncat": 277, "s": 0.10572987721691678, "os": -0.11435001996587646, "bg": 3.086608635654283e-06}, {"x": 0.7738095238095238, "y": 0.8024691358024691, "ox": 0.7738095238095238, "oy": 0.8024691358024691, "term": "develop", "cat25k": 93, "ncat25k": 79, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 186, "ncat": 177, "s": 0.699181446111869, "os": 0.028533052601009246, "bg": 1.3029896127498719e-05}, {"x": 0.14285714285714288, "y": 0.16049382716049385, "ox": 0.14285714285714288, "oy": 0.16049382716049385, "term": "innovative", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 24, "s": 0.6316507503410641, "os": 0.01771517769630085, "bg": 5.619238384163278e-06}, {"x": 0.08333333333333334, "y": 0.09876543209876544, "ox": 0.08333333333333334, "oy": 0.09876543209876544, "term": "creative", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 14, "s": 0.6268758526603001, "os": 0.01553708207790322, "bg": 1.2540724959244736e-06}, {"x": 0.08928571428571429, "y": 0.2777777777777778, "ox": 0.08928571428571429, "oy": 0.2777777777777778, "term": "approaches", "cat25k": 23, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 15, "s": 0.9536152796725785, "os": 0.18753403274403746, "bg": 6.986563557409117e-06}, {"x": 0.40476190476190477, "y": 0.6666666666666666, "ox": 0.40476190476190477, "oy": 0.6666666666666666, "term": "understand", "cat25k": 61, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 122, "ncat": 70, "s": 0.9740791268758526, "os": 0.2604276327730788, "bg": 6.210900742188081e-06}, {"x": 0.6369047619047619, "y": 0.6049382716049382, "ox": 0.6369047619047619, "oy": 0.6049382716049382, "term": "complex", "cat25k": 53, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 126, "s": 0.4447476125511596, "os": -0.03169129124768577, "bg": 9.835472974483333e-06}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "conditions", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 7.097919202398302e-08}, {"x": 0.7499999999999999, "y": 0.6419753086419753, "ox": 0.7499999999999999, "oy": 0.6419753086419753, "term": "using", "cat25k": 59, "ncat25k": 76, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 117, "ncat": 170, "s": 0.11869031377899045, "os": -0.10730751079972412, "bg": 2.129429334047278e-06}, {"x": 0.16666666666666669, "y": 0.05555555555555556, "ox": 0.16666666666666669, "oy": 0.05555555555555556, "term": "multi", "cat25k": 5, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 28, "s": 0.11391541609822646, "os": -0.11024793988456091, "bg": 1.3396545718789853e-06}, {"x": 0.08928571428571429, "y": 0.03703703703703704, "ox": 0.08928571428571429, "oy": 0.03703703703703704, "term": "dimensional", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.28717598908594816, "os": -0.05172977093694413, "bg": 3.9515651020350556e-06}, {"x": 0.2678571428571429, "y": 0.28395061728395066, "ox": 0.2678571428571429, "oy": 0.28395061728395066, "term": "within", "cat25k": 23, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 45, "s": 0.6289222373806276, "os": 0.016154209169782552, "bg": 6.959910407444282e-07}, {"x": 0.07738095238095238, "y": 0.030864197530864203, "ox": 0.07738095238095238, "oy": 0.030864197530864203, "term": "outside", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.3226466575716235, "os": -0.04603042073547029, "bg": 5.331557411306136e-07}, {"x": 0.20833333333333334, "y": 0.7901234567901235, "ox": 0.20833333333333334, "oy": 0.7901234567901235, "term": "research", "cat25k": 89, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 178, "ncat": 35, "s": 0.9986357435197818, "os": 0.5783932914654953, "bg": 1.3669390046627222e-06}, {"x": 0.053571428571428575, "y": 0.0925925925925926, "ox": 0.053571428571428575, "oy": 0.0925925925925926, "term": "area", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.7530695770804912, "os": 0.0389879115693179, "bg": 1.8463086449313495e-07}, {"x": 0.07142857142857144, "y": 0.08024691358024692, "ox": 0.07142857142857144, "oy": 0.08024691358024692, "term": "prepare", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.607094133697135, "os": 0.008966493629070316, "bg": 2.3590394878204446e-06}, {"x": 0.005952380952380953, "y": 0.11728395061728396, "ox": 0.005952380952380953, "oy": 0.11728395061728396, "term": "publications", "cat25k": 10, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 1, "s": 0.9010914051841746, "os": 0.11086506697644026, "bg": 6.492565809255722e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "regulatory", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 7.234013790290913e-07}, {"x": 0.25595238095238093, "y": 0.14197530864197533, "ox": 0.25595238095238093, "oy": 0.14197530864197533, "term": "documentation", "cat25k": 12, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 43, "s": 0.10641200545702592, "os": -0.11311576578211782, "bg": 3.001451815886307e-06}, {"x": 0.2619047619047619, "y": 0.09876543209876544, "ox": 0.2619047619047619, "oy": 0.09876543209876544, "term": "standards", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 44, "s": 0.061391541609822645, "os": -0.16197771082150508, "bg": 1.4583236972569584e-06}, {"x": 0.029761904761904767, "y": 0.2160493827160494, "ox": 0.029761904761904767, "oy": 0.2160493827160494, "term": "10", "cat25k": 18, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.9529331514324693, "os": 0.1853559371256398, "bg": 0.0}, {"x": 0.06547619047619048, "y": 0.1358024691358025, "ox": 0.06547619047619048, "oy": 0.1358024691358025, "term": "network", "cat25k": 11, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 11, "s": 0.8472032742155525, "os": 0.07009837731876428, "bg": 2.929094036007726e-07}, {"x": 0.30952380952380953, "y": 0.03703703703703704, "ox": 0.30952380952380953, "oy": 0.03703703703703704, "term": "pipeline", "cat25k": 3, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 53, "s": 0.020463847203274217, "os": -0.27066468217954764, "bg": 7.386075882415424e-06}, {"x": 0.10119047619047619, "y": 0.20987654320987656, "ox": 0.10119047619047619, "oy": 0.20987654320987656, "term": "strategies", "cat25k": 17, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 17, "s": 0.8983628922237381, "os": 0.10821505064072313, "bg": 2.7591776741625413e-06}, {"x": 0.13690476190476192, "y": 0.154320987654321, "ox": 0.13690476190476192, "oy": 0.154320987654321, "term": "translate", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 23, "s": 0.6302864938608458, "os": 0.017497368134461094, "bg": 8.837554968901472e-06}, {"x": 0.5238095238095238, "y": 0.8148148148148149, "ox": 0.5238095238095238, "oy": 0.8148148148148149, "term": "problems", "cat25k": 97, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 194, "ncat": 95, "s": 0.9788540245566166, "os": 0.28932370131048746, "bg": 4.76919218028398e-06}, {"x": 0.33333333333333337, "y": 0.5802469135802468, "ox": 0.33333333333333337, "oy": 0.5802469135802468, "term": "project", "cat25k": 51, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 101, "ncat": 57, "s": 0.970668485675307, "os": 0.24554397938069478, "bg": 1.3425706043010558e-06}, {"x": 0.04761904761904762, "y": 0.10493827160493828, "ox": 0.04761904761904762, "oy": 0.10493827160493828, "term": "clearly", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.8178717598908595, "os": 0.05717500998293825, "bg": 1.5410542555581434e-06}, {"x": 0.06547619047619048, "y": 0.0925925925925926, "ox": 0.06547619047619048, "oy": 0.0925925925925926, "term": "identifying", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 11, "s": 0.6930422919508867, "os": 0.027153592042690675, "bg": 4.397480852353566e-06}, {"x": 0.08333333333333334, "y": 0.02469135802469136, "ox": 0.08333333333333334, "oy": 0.02469135802469136, "term": "risks", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.252387448840382, "os": -0.05808254982393726, "bg": 1.8640866464755016e-06}, {"x": 0.02380952380952381, "y": 0.08641975308641976, "ox": 0.02380952380952381, "oy": 0.08641975308641976, "term": "scope", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.8308321964529333, "os": 0.062438741060732564, "bg": 1.534784745160504e-06}, {"x": 0.16666666666666669, "y": 0.19753086419753088, "ox": 0.16666666666666669, "oy": 0.19753086419753088, "term": "cross", "cat25k": 16, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 28, "s": 0.7012278308321964, "os": 0.030856354593966684, "bg": 1.6142558593922879e-06}, {"x": 0.6309523809523809, "y": 0.3888888888888889, "ox": 0.6309523809523809, "oy": 0.3888888888888889, "term": "database", "cat25k": 32, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 124, "s": 0.0286493860845839, "os": -0.24049805786474027, "bg": 3.6210093659911142e-06}, {"x": 0.10714285714285715, "y": 0.29012345679012347, "ox": 0.10714285714285715, "oy": 0.29012345679012347, "term": "subject", "cat25k": 24, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 18, "s": 0.9508867667121419, "os": 0.18205249210440339, "bg": 5.07169577372041e-07}, {"x": 0.08333333333333334, "y": 0.2654320987654321, "ox": 0.08333333333333334, "oy": 0.2654320987654321, "term": "matter", "cat25k": 22, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 14, "s": 0.9488403819918144, "os": 0.1811812538570443, "bg": 1.7081343244468095e-06}, {"x": 0.08928571428571429, "y": 0.25308641975308643, "ox": 0.08928571428571429, "oy": 0.25308641975308643, "term": "experts", "cat25k": 21, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 15, "s": 0.9427012278308322, "os": 0.162994155443424, "bg": 3.496667129870994e-06}, {"x": 0.4583333333333333, "y": 0.8580246913580246, "ox": 0.4583333333333333, "oy": 0.8580246913580246, "term": "complete", "cat25k": 127, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 253, "ncat": 81, "s": 0.9904502046384721, "os": 0.39735724398301087, "bg": 4.470643934887855e-06}, {"x": 0.06547619047619048, "y": 0.09876543209876544, "ox": 0.06547619047619048, "oy": 0.09876543209876544, "term": "deliverables", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 11, "s": 0.7244201909959073, "os": 0.03328856136784404, "bg": 4.237128545064215e-05}, {"x": 0.47023809523809523, "y": 0.4320987654320988, "ox": 0.47023809523809523, "oy": 0.4320987654320988, "term": "time", "cat25k": 36, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 85, "s": 0.3683492496589359, "os": -0.03778995897919918, "bg": 3.455058693211174e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "budget", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 2.702107098048608e-07}, {"x": 0.9107142857142857, "y": 0.8209876543209876, "ox": 0.9107142857142857, "oy": 0.8209876543209876, "term": "design", "cat25k": 101, "ncat25k": 133, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 202, "ncat": 297, "s": 0.15620736698499318, "os": -0.08915671397974367, "bg": 3.7723659437787904e-06}, {"x": 0.053571428571428575, "y": 0.0925925925925926, "ox": 0.053571428571428575, "oy": 0.0925925925925926, "term": "propose", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 9, "s": 0.7530695770804912, "os": 0.0389879115693179, "bg": 7.501153302320232e-06}, {"x": 0.19642857142857142, "y": 0.728395061728395, "ox": 0.19642857142857142, "oy": 0.728395061728395, "term": "algorithms", "cat25k": 71, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 142, "ncat": 33, "s": 0.9965893587994543, "os": 0.5288779177405888, "bg": 3.427144393622613e-05}, {"x": 0.16666666666666669, "y": 0.4938271604938272, "ox": 0.16666666666666669, "oy": 0.4938271604938272, "term": "analyze", "cat25k": 43, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 85, "ncat": 28, "s": 0.9836289222373806, "os": 0.32533488220132867, "bg": 2.5532534097230596e-05}, {"x": 0.08928571428571429, "y": 0.14197530864197533, "ox": 0.08928571428571429, "oy": 0.14197530864197533, "term": "leverage", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 15, "s": 0.8028649386084584, "os": 0.05256470759066324, "bg": 1.831327066339341e-05}, {"x": 0.34523809523809523, "y": 0.29012345679012347, "ox": 0.34523809523809523, "oy": 0.29012345679012347, "term": "well", "cat25k": 24, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 59, "s": 0.2660300136425648, "os": -0.05463389842814098, "bg": 5.853289814344443e-07}, {"x": 0.8333333333333334, "y": 0.845679012345679, "ox": 0.8333333333333334, "oy": 0.845679012345679, "term": "new", "cat25k": 109, "ncat25k": 85, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 218, "ncat": 190, "s": 0.6132332878581174, "os": 0.012306240243946664, "bg": 5.259881762589293e-07}, {"x": 0.6964285714285714, "y": 0.2716049382716049, "ox": 0.6964285714285714, "oy": 0.2716049382716049, "term": "code", "cat25k": 22, "ncat25k": 66, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 147, "s": 0.005457025920873124, "os": -0.4221512324391041, "bg": 1.5258520153940732e-06}, {"x": 0.4880952380952381, "y": 0.9197530864197531, "ox": 0.4880952380952381, "oy": 0.9197530864197531, "term": "models", "cat25k": 175, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 349, "ncat": 88, "s": 0.9918144611186904, "os": 0.42912113841797656, "bg": 1.0083035877045243e-05}, {"x": 0.05952380952380953, "y": 0.14814814814814817, "ox": 0.05952380952380953, "oy": 0.14814814814814817, "term": "applying", "cat25k": 12, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 10, "s": 0.8772169167803547, "os": 0.08828547573238466, "bg": 4.674454169829657e-06}, {"x": 0.5833333333333333, "y": 0.691358024691358, "ox": 0.5833333333333333, "oy": 0.691358024691358, "term": "large", "cat25k": 65, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 106, "s": 0.897680763983629, "os": 0.10745271717428395, "bg": 2.831826534501426e-06}, {"x": 0.15476190476190477, "y": 0.17901234567901236, "ox": 0.15476190476190477, "oy": 0.17901234567901236, "term": "structured", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 26, "s": 0.6514324693042293, "os": 0.024285766145133753, "bg": 1.2707767669197862e-05}, {"x": 0.13095238095238096, "y": 0.19135802469135804, "ox": 0.13095238095238096, "oy": 0.19135802469135804, "term": "unstructured", "cat25k": 16, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 22, "s": 0.820600272851296, "os": 0.06022434384869496, "bg": 0.00012165394273542333}, {"x": 0.32142857142857145, "y": 0.42592592592592593, "ox": 0.32142857142857145, "oy": 0.42592592592592593, "term": "more", "cat25k": 36, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 55, "s": 0.8942701227830833, "os": 0.1040040657784877, "bg": 1.631196325230044e-07}, {"x": 0.125, "y": 0.33333333333333337, "ox": 0.125, "oy": 0.33333333333333337, "term": "visualization", "cat25k": 27, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 21, "s": 0.9611186903137791, "os": 0.20724579809053617, "bg": 3.553516310995219e-05}, {"x": 0.4226190476190476, "y": 0.4506172839506173, "ox": 0.4226190476190476, "oy": 0.4506172839506173, "term": "products", "cat25k": 38, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 74, "s": 0.6978171896316507, "os": 0.02795222710276979, "bg": 7.237908074523877e-07}, {"x": 0.07738095238095238, "y": 0.14197530864197533, "ox": 0.07738095238095238, "oy": 0.14197530864197533, "term": "share", "cat25k": 12, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 13, "s": 0.8356070941336972, "os": 0.06439902711729045, "bg": 6.030105452222858e-07}, {"x": 0.4107142857142857, "y": 0.45679012345679015, "ox": 0.4107142857142857, "oy": 0.45679012345679015, "term": "across", "cat25k": 39, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 71, "s": 0.7810368349249659, "os": 0.04592151595455041, "bg": 3.8589944797996565e-06}, {"x": 0.02380952380952381, "y": 0.07407407407407408, "ox": 0.02380952380952381, "oy": 0.07407407407407408, "term": "group", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 4, "s": 0.7967257844474761, "os": 0.05016880241042582, "bg": 9.939443320355526e-08}, {"x": 0.17857142857142858, "y": 0.14814814814814817, "ox": 0.17857142857142858, "oy": 0.14814814814814817, "term": "users", "cat25k": 12, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 30, "s": 0.44679399727148705, "os": -0.030057719533887528, "bg": 7.227613918103693e-07}, {"x": 0.07142857142857144, "y": 0.04320987654320988, "ox": 0.07142857142857144, "oy": 0.04320987654320988, "term": "continuously", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.4877216916780355, "os": -0.027843322321849932, "bg": 6.646800894729365e-06}, {"x": 0.1488095238095238, "y": 0.14197530864197533, "ox": 0.1488095238095238, "oy": 0.14197530864197533, "term": "out", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 25, "s": 0.5770804911323328, "os": -0.006606890042472863, "bg": 1.2943089827254515e-07}, {"x": 0.3392857142857143, "y": 0.30246913580246915, "ox": 0.3392857142857143, "oy": 0.30246913580246915, "term": "industry", "cat25k": 25, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 58, "s": 0.3703956343792633, "os": -0.03644680001452061, "bg": 1.329858714940124e-06}, {"x": 0.5119047619047619, "y": 0.36419753086419754, "ox": 0.5119047619047619, "oy": 0.36419753086419754, "term": "best", "cat25k": 30, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 93, "s": 0.07571623465211459, "os": -0.14669473989908155, "bg": 8.226701636170781e-07}, {"x": 0.5178571428571429, "y": 0.2716049382716049, "ox": 0.5178571428571429, "oy": 0.2716049382716049, "term": "practices", "cat25k": 22, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 94, "s": 0.027967257844474763, "os": -0.24463643953969577, "bg": 7.240770575397278e-06}, {"x": 0.4523809523809524, "y": 0.5246913580246914, "ox": 0.4523809523809524, "oy": 0.5246913580246914, "term": "create", "cat25k": 46, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 80, "s": 0.849931787175989, "os": 0.07198606018804221, "bg": 2.5076923097110224e-06}, {"x": 0.16666666666666669, "y": 0.17283950617283952, "ox": 0.16666666666666669, "oy": 0.17283950617283952, "term": "capabilities", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 28, "s": 0.6030013642564802, "os": 0.00631647729335319, "bg": 6.194481955474064e-06}, {"x": 0.5892857142857143, "y": 0.5432098765432098, "ox": 0.5892857142857143, "oy": 0.5432098765432098, "term": "at", "cat25k": 48, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 95, "ncat": 107, "s": 0.32605729877216916, "os": -0.04570370639271071, "bg": 1.7778719764370494e-07}, {"x": 0.2380952380952381, "y": 0.22222222222222224, "ox": 0.2380952380952381, "oy": 0.22222222222222224, "term": "decisions", "cat25k": 18, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 40, "s": 0.5600272851296044, "os": -0.015609685265183137, "bg": 4.836471885398016e-06}, {"x": 0.27976190476190477, "y": 0.08641975308641976, "ox": 0.27976190476190477, "oy": 0.08641975308641976, "term": "coding", "cat25k": 7, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 47, "s": 0.04365620736698499, "os": -0.19199912876175265, "bg": 1.0926931608335061e-05}, {"x": 0.4166666666666667, "y": 0.2716049382716049, "ox": 0.4166666666666667, "oy": 0.2716049382716049, "term": "testing", "cat25k": 22, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 72, "s": 0.07844474761255116, "os": -0.1440447235633644, "bg": 4.093823301163469e-06}, {"x": 0.2142857142857143, "y": 0.07407407407407408, "ox": 0.2142857142857143, "oy": 0.07407407407407408, "term": "monitoring", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 36, "s": 0.0859481582537517, "os": -0.1391803100156097, "bg": 2.2991742539616632e-06}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "classifiers", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 2.7232214932201512e-05}, {"x": 0.4107142857142857, "y": 0.45679012345679015, "ox": 0.4107142857142857, "oy": 0.45679012345679015, "term": "production", "cat25k": 39, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 71, "s": 0.7810368349249659, "os": 0.04592151595455041, "bg": 3.2628043271884187e-06}, {"x": 0.17261904761904762, "y": 0.0925925925925926, "ox": 0.17261904761904762, "oy": 0.0925925925925926, "term": "environments", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 29, "s": 0.18212824010914053, "os": -0.07935528369695428, "bg": 6.548480971937453e-06}, {"x": 0.9047619047619048, "y": 0.8703703703703703, "ox": 0.9047619047619048, "oy": 0.8703703703703703, "term": "that", "cat25k": 129, "ncat25k": 130, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 257, "ncat": 291, "s": 0.4304229195088677, "os": -0.0341597996152031, "bg": 3.2233983227913265e-07}, {"x": 0.08333333333333334, "y": 0.01851851851851852, "ox": 0.08333333333333334, "oy": 0.01851851851851852, "term": "saas", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 14, "s": 0.23124147339699866, "os": -0.06421751914909064, "bg": 7.89517070984551e-05}, {"x": 0.3273809523809524, "y": 0.25308641975308643, "ox": 0.3273809523809524, "oy": 0.25308641975308643, "term": "platforms", "cat25k": 21, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 56, "s": 0.20259208731241476, "os": -0.07369223508912043, "bg": 1.4625332160894333e-05}, {"x": 0.5, "y": 0.24074074074074076, "ox": 0.5, "oy": 0.24074074074074076, "term": "maintain", "cat25k": 20, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 91, "s": 0.023874488403819918, "os": -0.2575598068755218, "bg": 7.867149979982643e-06}, {"x": 0.2142857142857143, "y": 0.33333333333333337, "ox": 0.2142857142857143, "oy": 0.33333333333333337, "term": "perform", "cat25k": 27, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 36, "s": 0.9065484311050478, "os": 0.11848840164083202, "bg": 5.904416549502267e-06}, {"x": 0.5476190476190476, "y": 0.4012345679012346, "ox": 0.5476190476190476, "oy": 0.4012345679012346, "term": "scale", "cat25k": 33, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 100, "s": 0.07776261937244201, "os": -0.14538788252804302, "bg": 7.152966093734191e-06}, {"x": 0.2976190476190476, "y": 0.4382716049382716, "ox": 0.2976190476190476, "oy": 0.4382716049382716, "term": "closely", "cat25k": 37, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 50, "s": 0.927012278308322, "os": 0.13994264348204888, "bg": 1.63629956308141e-05}, {"x": 0.03571428571428572, "y": 0.1358024691358025, "ox": 0.03571428571428572, "oy": 0.1358024691358025, "term": "groups", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 6, "s": 0.8908594815825376, "os": 0.09968417613533233, "bg": 4.4472357908136237e-07}, {"x": 0.4523809523809524, "y": 0.16049382716049385, "ox": 0.4523809523809524, "oy": 0.16049382716049385, "term": "like", "cat25k": 13, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 80, "s": 0.018417462482946796, "os": -0.28997712999600683, "bg": 4.0715047419020036e-07}, {"x": 0.8452380952380952, "y": 0.7530864197530863, "ox": 0.8452380952380952, "oy": 0.7530864197530863, "term": "engineering", "cat25k": 76, "ncat25k": 92, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 152, "ncat": 205, "s": 0.15347885402455663, "os": -0.09155261915998114, "bg": 8.295419644541268e-06}, {"x": 0.39285714285714285, "y": 0.4197530864197531, "ox": 0.39285714285714285, "oy": 0.4197530864197531, "term": "customer", "cat25k": 35, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 68, "s": 0.6916780354706685, "os": 0.02686317929357096, "bg": 1.4958243251016784e-06}, {"x": 0.19642857142857142, "y": 0.3580246913580247, "ox": 0.19642857142857142, "oy": 0.3580246913580247, "term": "operations", "cat25k": 30, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 33, "s": 0.9399727148703957, "os": 0.16077975823138638, "bg": 2.95579983670491e-06}, {"x": 0.04761904761904762, "y": 0.02469135802469136, "ox": 0.04761904761904762, "oy": 0.02469135802469136, "term": "acquire", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5368349249658936, "os": -0.022579591244055616, "bg": 2.6935053525562826e-06}, {"x": 0.7023809523809523, "y": 0.34567901234567905, "ox": 0.7023809523809523, "oy": 0.34567901234567905, "term": "requirements", "cat25k": 29, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 150, "s": 0.010231923601637109, "os": -0.35444876030057726, "bg": 4.253113291771102e-06}, {"x": 0.8690476190476191, "y": 0.6358024691358024, "ox": 0.8690476190476191, "oy": 0.6358024691358024, "term": "systems", "cat25k": 58, "ncat25k": 102, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 116, "ncat": 227, "s": 0.03137789904502047, "os": -0.2317856753911497, "bg": 3.0671160370206806e-06}, {"x": 0.22023809523809523, "y": 0.4506172839506173, "ox": 0.22023809523809523, "oy": 0.4506172839506173, "term": "solve", "cat25k": 38, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 37, "s": 0.9658935879945431, "os": 0.2291356590554325, "bg": 1.666803969029012e-05}, {"x": 0.3392857142857143, "y": 0.3395061728395062, "ox": 0.3392857142857143, "oy": 0.3395061728395062, "term": "their", "cat25k": 28, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 58, "s": 0.5886766712141883, "os": 0.0003630159363996466, "bg": 2.9120404949372993e-07}, {"x": 0.44047619047619047, "y": 0.24074074074074076, "ox": 0.44047619047619047, "oy": 0.24074074074074076, "term": "test", "cat25k": 20, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 77, "s": 0.038199181446111875, "os": -0.19838820924238573, "bg": 1.495748177848301e-06}, {"x": 0.10714285714285715, "y": 0.030864197530864203, "ox": 0.10714285714285715, "oy": 0.030864197530864203, "term": "troubleshooting", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 18, "s": 0.19577080491132334, "os": -0.07561621955203834, "bg": 6.942041349514728e-06}, {"x": 0.6785714285714285, "y": 0.4444444444444445, "ox": 0.6785714285714285, "oy": 0.4444444444444445, "term": "management", "cat25k": 38, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 139, "s": 0.030695770804911326, "os": -0.23262061204486878, "bg": 1.4064694327479734e-06}, {"x": 0.11904761904761905, "y": 0.07407407407407408, "ox": 0.11904761904761905, "oy": 0.07407407407407408, "term": "may", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 20, "s": 0.33083219645293316, "os": -0.04450575380259193, "bg": 7.730134398870519e-08}, {"x": 0.25595238095238093, "y": 0.2777777777777778, "ox": 0.25595238095238093, "oy": 0.2777777777777778, "term": "one", "cat25k": 23, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 43, "s": 0.6473396998635744, "os": 0.02185355937125638, "bg": 1.7712592948806552e-07}, {"x": 0.16666666666666669, "y": 0.09876543209876544, "ox": 0.16666666666666669, "oy": 0.09876543209876544, "term": "following", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 28, "s": 0.22100954979536155, "os": -0.0673031546084873, "bg": 3.9852078323698773e-07}, {"x": 0.08928571428571429, "y": 0.47530864197530864, "ox": 0.08928571428571429, "oy": 0.47530864197530864, "term": "predictive", "cat25k": 41, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 82, "ncat": 15, "s": 0.9884038199181447, "os": 0.38385305114894547, "bg": 7.343939395086526e-05}, {"x": 0.053571428571428575, "y": 0.4320987654320988, "ox": 0.053571428571428575, "oy": 0.4320987654320988, "term": "feature", "cat25k": 36, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 9, "s": 0.9877216916780356, "os": 0.37641122445275343, "bg": 2.5156992444283667e-06}, {"x": 0.13095238095238096, "y": 0.25308641975308643, "ox": 0.13095238095238096, "oy": 0.25308641975308643, "term": "extraction", "cat25k": 21, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 22, "s": 0.9099590723055935, "os": 0.12157403710022871, "bg": 1.951710046520403e-05}, {"x": 0.27380952380952384, "y": 0.23456790123456792, "ox": 0.27380952380952384, "oy": 0.23456790123456792, "term": "driven", "cat25k": 19, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 46, "s": 0.364256480218281, "os": -0.03884270519475802, "bg": 9.422105268471112e-06}, {"x": 0.5654761904761905, "y": 0.8765432098765432, "ox": 0.5654761904761905, "oy": 0.8765432098765432, "term": "application", "cat25k": 138, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 276, "ncat": 103, "s": 0.9822646657571623, "os": 0.30925327621882603, "bg": 4.958020544702789e-06}, {"x": 0.39285714285714285, "y": 0.9135802469135803, "ox": 0.39285714285714285, "oy": 0.9135802469135803, "term": "machine", "cat25k": 170, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 339, "ncat": 68, "s": 0.9959072305593452, "os": 0.517660725305841, "bg": 1.166640641789942e-05}, {"x": 0.5595238095238094, "y": 0.9259259259259259, "ox": 0.5595238095238094, "oy": 0.9259259259259259, "term": "learning", "cat25k": 221, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 440, "ncat": 102, "s": 0.9863574351978173, "os": 0.3642501905833666, "bg": 9.203599385080695e-06}, {"x": 0.13095238095238096, "y": 0.22839506172839508, "ox": 0.13095238095238096, "oy": 0.22839506172839508, "term": "making", "cat25k": 19, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 22, "s": 0.8888130968622101, "os": 0.09703415979961519, "bg": 9.492751294887701e-07}, {"x": 0.5297619047619048, "y": 0.4197530864197531, "ox": 0.5297619047619048, "oy": 0.4197530864197531, "term": "related", "cat25k": 35, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 97, "s": 0.11527967257844475, "os": -0.10923149526264209, "bg": 1.4444226450026162e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "utilities", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 4.127609306919707e-07}, {"x": 0.16666666666666669, "y": 0.5555555555555556, "ox": 0.16666666666666669, "oy": 0.5555555555555556, "term": "results", "cat25k": 49, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 28, "s": 0.989768076398363, "os": 0.3866845754528624, "bg": 9.318358775509213e-07}, {"x": 0.029761904761904767, "y": 0.20987654320987656, "ox": 0.029761904761904767, "oy": 0.20987654320987656, "term": "form", "cat25k": 17, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 5, "s": 0.9481582537517054, "os": 0.17922096780048646, "bg": 3.8709321831490274e-07}, {"x": 0.36904761904761907, "y": 0.33333333333333337, "ox": 0.36904761904761907, "oy": 0.33333333333333337, "term": "internal", "cat25k": 27, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 63, "s": 0.3717598908594816, "os": -0.035357752205321835, "bg": 4.4468108403822565e-06}, {"x": 0.4761904761904762, "y": 0.5617283950617284, "ox": 0.4761904761904762, "oy": 0.5617283950617284, "term": "technology", "cat25k": 49, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 86, "s": 0.873806275579809, "os": 0.08512723708570802, "bg": 1.5411605022340543e-06}, {"x": 0.01785714285714286, "y": 0.0617283950617284, "ox": 0.01785714285714286, "oy": 0.0617283950617284, "term": "papers", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7694406548431105, "os": 0.04381602352343268, "bg": 5.084946475560037e-07}, {"x": 0.13690476190476192, "y": 0.8271604938271605, "ox": 0.13690476190476192, "oy": 0.8271604938271605, "term": "statistical", "cat25k": 108, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 215, "ncat": 23, "s": 1.0, "os": 0.6862090245761789, "bg": 2.5850103180469612e-05}, {"x": 0.32142857142857145, "y": 0.16666666666666669, "ox": 0.32142857142857145, "oy": 0.16666666666666669, "term": "system", "cat25k": 14, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 55, "s": 0.06548431105047749, "os": -0.15366464587795403, "bg": 4.1301322055318996e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "resolution", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 3.96139227092754e-07}, {"x": 0.053571428571428575, "y": 0.030864197530864203, "ox": 0.053571428571428575, "oy": 0.030864197530864203, "term": "under", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5409276944065484, "os": -0.022361781682215844, "bg": 8.93418221465646e-08}, {"x": 0.06547619047619048, "y": 0.04938271604938272, "ox": 0.06547619047619048, "oy": 0.04938271604938272, "term": "priorities", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.5586630286493861, "os": -0.015791193233382947, "bg": 3.3348217169206747e-06}, {"x": 0.3511904761904762, "y": 0.7222222222222222, "ox": 0.3511904761904762, "oy": 0.7222222222222222, "term": "modeling", "cat25k": 70, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 140, "ncat": 60, "s": 0.9870395634379264, "os": 0.36889679456928165, "bg": 2.567174286874282e-05}, {"x": 0.1488095238095238, "y": 0.22839506172839508, "ox": 0.1488095238095238, "oy": 0.22839506172839508, "term": "this", "cat25k": 19, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 25, "s": 0.8622100954979537, "os": 0.07928268050967435, "bg": 3.8407019875517875e-08}, {"x": 0.2916666666666667, "y": 0.6481481481481481, "ox": 0.2916666666666667, "oy": 0.6481481481481481, "term": "model", "cat25k": 59, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 118, "ncat": 49, "s": 0.984993178717599, "os": 0.3544487603005772, "bg": 2.0705149575891278e-06}, {"x": 0.20238095238095238, "y": 0.20370370370370372, "ox": 0.20238095238095238, "oy": 0.20370370370370372, "term": "concepts", "cat25k": 17, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 34, "s": 0.5907230559345157, "os": 0.0014883653392383789, "bg": 5.839534388647597e-06}, {"x": 0.17261904761904762, "y": 0.08024691358024692, "ox": 0.17261904761904762, "oy": 0.08024691358024692, "term": "designs", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 29, "s": 0.1527967257844475, "os": -0.09162522234726103, "bg": 2.5406550171584954e-06}, {"x": 0.625, "y": 0.6975308641975309, "ox": 0.625, "oy": 0.6975308641975309, "term": "such", "cat25k": 67, "ncat25k": 53, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 133, "ncat": 118, "s": 0.8506139154160982, "os": 0.07216756815624203, "bg": 1.3181638996747769e-06}, {"x": 0.07738095238095238, "y": 0.23456790123456792, "ox": 0.07738095238095238, "oy": 0.23456790123456792, "term": "generation", "cat25k": 19, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 13, "s": 0.9358799454297408, "os": 0.15642356699459106, "bg": 2.5487956940345686e-06}, {"x": 0.1130952380952381, "y": 0.03703703703703704, "ox": 0.1130952380952381, "oy": 0.03703703703703704, "term": "functions", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 19, "s": 0.19713506139154163, "os": -0.07539840999019856, "bg": 9.6296303572017e-07}, {"x": 0.15476190476190477, "y": 0.154320987654321, "ox": 0.15476190476190477, "oy": 0.154320987654321, "term": "different", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 26, "s": 0.58731241473397, "os": -0.0002541111554797415, "bg": 5.669788357639911e-07}, {"x": 0.07738095238095238, "y": 0.10493827160493828, "ox": 0.07738095238095238, "oy": 0.10493827160493828, "term": "uses", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 13, "s": 0.6964529331514324, "os": 0.027589211166370198, "bg": 1.072405532869015e-06}, {"x": 0.5357142857142857, "y": 0.46296296296296297, "ox": 0.5357142857142857, "oy": 0.46296296296296297, "term": "ensure", "cat25k": 39, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 98, "s": 0.20532060027285132, "os": -0.07220386974988202, "bg": 5.904613452299665e-06}, {"x": 0.07738095238095238, "y": 0.3209876543209877, "ox": 0.07738095238095238, "oy": 0.3209876543209877, "term": "accuracy", "cat25k": 26, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 13, "s": 0.9693042291950887, "os": 0.2423131375467383, "bg": 4.385564677717502e-06}, {"x": 0.005952380952380953, "y": 0.06790123456790124, "ox": 0.005952380952380953, "oy": 0.06790123456790124, "term": "feasibility", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.8267394270122783, "os": 0.06178531237521328, "bg": 5.7574142301209685e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "opinions", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 3.6098932735053686e-07}, {"x": 0.1488095238095238, "y": 0.20987654320987656, "ox": 0.1488095238095238, "oy": 0.20987654320987656, "term": "deploy", "cat25k": 17, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 25, "s": 0.8219645293315143, "os": 0.06087777253421425, "bg": 2.946192046779538e-05}, {"x": 0.125, "y": 0.11111111111111112, "ox": 0.125, "oy": 0.11111111111111112, "term": "ways", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 21, "s": 0.5661664392905866, "os": -0.013613097614985303, "bg": 1.33971992708351e-06}, {"x": 0.05952380952380953, "y": 0.030864197530864203, "ox": 0.05952380952380953, "oy": 0.030864197530864203, "term": "achieve", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.48362892223738063, "os": -0.028278941445529455, "bg": 1.0933182076403699e-06}, {"x": 0.619047619047619, "y": 0.5123456790123456, "ox": 0.619047619047619, "oy": 0.5123456790123456, "term": "performance", "cat25k": 45, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 89, "ncat": 117, "s": 0.12551159618008187, "os": -0.10596435183504549, "bg": 2.9466999342413867e-06}, {"x": 0.1130952380952381, "y": 0.1234567901234568, "ox": 0.1130952380952381, "oy": 0.1234567901234568, "term": "improvement", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 19, "s": 0.6118690313778992, "os": 0.010491160561948681, "bg": 2.094499690752491e-06}, {"x": 0.5238095238095238, "y": 0.3703703703703704, "ox": 0.5238095238095238, "oy": 0.3703703703703704, "term": "environment", "cat25k": 31, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 95, "s": 0.0688949522510232, "os": -0.15239409010055543, "bg": 3.0568434740359846e-06}, {"x": 0.03571428571428572, "y": 0.05555555555555556, "ox": 0.03571428571428572, "oy": 0.05555555555555556, "term": "interact", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.6371077762619373, "os": 0.019929574908338472, "bg": 3.710249316386564e-06}, {"x": 0.3511904761904762, "y": 0.35185185185185186, "ox": 0.3511904761904762, "oy": 0.35185185185185186, "term": "multiple", "cat25k": 29, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 60, "s": 0.5893587994542975, "os": 0.0007986350600791559, "bg": 3.5607540687348656e-06}, {"x": 0.6666666666666666, "y": 0.6481481481481481, "ox": 0.6666666666666666, "oy": 0.6481481481481481, "term": "including", "cat25k": 59, "ncat25k": 61, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 118, "ncat": 135, "s": 0.5572987721691678, "os": -0.018332304788180154, "bg": 2.3611502497429684e-06}, {"x": 0.15476190476190477, "y": 0.2654320987654321, "ox": 0.15476190476190477, "oy": 0.2654320987654321, "term": "external", "cat25k": 22, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 26, "s": 0.9004092769440656, "os": 0.110175336697281, "bg": 2.721280478415306e-06}, {"x": 0.2619047619047619, "y": 0.22839506172839508, "ox": 0.2619047619047619, "oy": 0.22839506172839508, "term": "customers", "cat25k": 19, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 44, "s": 0.43997271487039563, "os": -0.033143354993284246, "bg": 1.918876737884908e-06}, {"x": 0.16071428571428573, "y": 0.10493827160493828, "ox": 0.16071428571428573, "oy": 0.10493827160493828, "term": "monitor", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 27, "s": 0.26398362892223737, "os": -0.05525102552002034, "bg": 2.002053652034794e-06}, {"x": 0.05952380952380953, "y": 0.11728395061728396, "ox": 0.05952380952380953, "oy": 0.11728395061728396, "term": "proactively", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 10, "s": 0.8192360163710778, "os": 0.05761062910661778, "bg": 7.235519915768567e-05}, {"x": 0.04166666666666667, "y": 0.16049382716049385, "ox": 0.04166666666666667, "oy": 0.16049382716049385, "term": "evaluation", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 7, "s": 0.9058663028649386, "os": 0.11830689367263222, "bg": 1.4031735618969836e-06}, {"x": 0.7321428571428571, "y": 0.6790123456790123, "ox": 0.7321428571428571, "oy": 0.6790123456790123, "term": "is", "cat25k": 62, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 161, "s": 0.27967257844474763, "os": -0.05274621555886305, "bg": 1.211258141819837e-07}, {"x": 0.6071428571428571, "y": 0.25925925925925924, "ox": 0.6071428571428571, "oy": 0.25925925925925924, "term": "it", "cat25k": 21, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 115, "s": 0.013642564802182811, "os": -0.3456637746397067, "bg": 1.116138612146635e-07}, {"x": 0.1904761904761905, "y": 0.14814814814814817, "ox": 0.1904761904761905, "oy": 0.14814814814814817, "term": "role", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 32, "s": 0.3356070941336971, "os": -0.04189203906051475, "bg": 1.517704058495133e-06}, {"x": 0.10119047619047619, "y": 0.10493827160493828, "ox": 0.10119047619047619, "oy": 0.10493827160493828, "term": "works", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 17, "s": 0.597544338335607, "os": 0.003920572113115764, "bg": 6.890377498653539e-07}, {"x": 0.2261904761904762, "y": 0.10493827160493828, "ox": 0.2261904761904762, "oy": 0.10493827160493828, "term": "wide", "cat25k": 9, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 38, "s": 0.1009549795361528, "os": -0.12033978291647004, "bg": 1.2533020235700533e-06}, {"x": 0.3154761904761905, "y": 0.1851851851851852, "ox": 0.3154761904761905, "oy": 0.1851851851851852, "term": "variety", "cat25k": 15, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 54, "s": 0.09276944065484312, "os": -0.12934257813918032, "bg": 3.3632761673145814e-06}, {"x": 0.3273809523809524, "y": 0.4876543209876543, "ox": 0.3273809523809524, "oy": 0.4876543209876543, "term": "company", "cat25k": 42, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 56, "s": 0.9386084583901774, "os": 0.15943659926670778, "bg": 8.631877663554676e-07}, {"x": 0.738095238095238, "y": 0.4876543209876543, "ox": 0.738095238095238, "oy": 0.4876543209876543, "term": "software", "cat25k": 42, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 162, "s": 0.027285129604365622, "os": -0.24884742440193125, "bg": 1.3274912780585248e-06}, {"x": 0.04761904761904762, "y": 0.06790123456790124, "ox": 0.04761904761904762, "oy": 0.06790123456790124, "term": "driving", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.6418826739427013, "os": 0.02036519403201801, "bg": 9.466748519163862e-07}, {"x": 0.4642857142857143, "y": 0.3765432098765432, "ox": 0.4642857142857143, "oy": 0.3765432098765432, "term": "based", "cat25k": 31, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 84, "s": 0.16166439290586632, "os": -0.08708752314226592, "bg": 1.1563808242436983e-06}, {"x": 0.11904761904761905, "y": 0.06790123456790124, "ox": 0.11904761904761905, "oy": 0.06790123456790124, "term": "responsible", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 20, "s": 0.29195088676671216, "os": -0.0506407231277453, "bg": 9.476310065476717e-07}, {"x": 0.1488095238095238, "y": 0.1234567901234568, "ox": 0.1488095238095238, "oy": 0.1234567901234568, "term": "planning", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 25, "s": 0.49863574351978174, "os": -0.02501179801793299, "bg": 8.655067350560597e-07}, {"x": 0.06547619047619048, "y": 0.04320987654320988, "ox": 0.06547619047619048, "oy": 0.04320987654320988, "term": "handling", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.5463847203274215, "os": -0.02192616255853632, "bg": 1.1345602657140143e-06}, {"x": 0.03571428571428572, "y": 0.11111111111111112, "ox": 0.03571428571428572, "oy": 0.11111111111111112, "term": "many", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.8587994542974079, "os": 0.07514429883471883, "bg": 1.5043575361425423e-07}, {"x": 0.05952380952380953, "y": 0.0925925925925926, "ox": 0.05952380952380953, "oy": 0.0925925925925926, "term": "social", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 10, "s": 0.7223738062755798, "os": 0.03307075180600429, "bg": 3.5797894253306754e-07}, {"x": 0.6904761904761904, "y": 0.7037037037037036, "ox": 0.6904761904761904, "oy": 0.7037037037037036, "term": "etc", "cat25k": 69, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 137, "ncat": 143, "s": 0.6139154160982265, "os": 0.013213780084945781, "bg": 1.0845836791654286e-05}, {"x": 0.13095238095238096, "y": 0.04320987654320988, "ox": 0.13095238095238096, "oy": 0.04320987654320988, "term": "while", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 22, "s": 0.16234652114597548, "os": -0.08701491995498603, "bg": 2.5629509357510623e-07}, {"x": 0.18452380952380953, "y": 0.10493827160493828, "ox": 0.18452380952380953, "oy": 0.10493827160493828, "term": "open", "cat25k": 9, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 31, "s": 0.18281036834924966, "os": -0.07891966457327476, "bg": 4.0037201399788975e-07}, {"x": 0.19642857142857142, "y": 0.30246913580246915, "ox": 0.19642857142857142, "oy": 0.30246913580246915, "term": "problem", "cat25k": 25, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 33, "s": 0.8956343792633016, "os": 0.105565034305006, "bg": 1.1621420108341964e-06}, {"x": 0.04761904761904762, "y": 0.0925925925925926, "ox": 0.04761904761904762, "oy": 0.0925925925925926, "term": "potential", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 8, "s": 0.7789904502046385, "os": 0.0449050713326315, "bg": 6.975815393962511e-07}, {"x": 0.2142857142857143, "y": 0.28395061728395066, "ox": 0.2142857142857143, "oy": 0.28395061728395066, "term": "impact", "cat25k": 23, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 36, "s": 0.8451568894952252, "os": 0.06940864703960503, "bg": 2.683060383746347e-06}, {"x": 0.7678571428571428, "y": 0.5925925925925926, "ox": 0.7678571428571428, "oy": 0.5925925925925926, "term": "we", "cat25k": 52, "ncat25k": 79, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 176, "s": 0.05525238744884038, "os": -0.17413874469089197, "bg": 4.0121693525668226e-07}, {"x": 0.10119047619047619, "y": 0.11111111111111112, "ox": 0.10119047619047619, "oy": 0.11111111111111112, "term": "execute", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 17, "s": 0.6105047748976808, "os": 0.01005554143826913, "bg": 7.949864068681375e-06}, {"x": 0.3869047619047619, "y": 0.40740740740740744, "ox": 0.3869047619047619, "oy": 0.40740740740740744, "term": "needs", "cat25k": 34, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 67, "s": 0.6439290586630287, "os": 0.020510400406577867, "bg": 2.166487621554709e-06}, {"x": 0.1130952380952381, "y": 0.3580246913580247, "ox": 0.1130952380952381, "oy": 0.3580246913580247, "term": "apply", "cat25k": 30, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 19, "s": 0.9699863574351979, "os": 0.24361999491777692, "bg": 2.0405980647595833e-06}, {"x": 0.7023809523809523, "y": 0.6172839506172839, "ox": 0.7023809523809523, "oy": 0.6172839506172839, "term": "an", "cat25k": 55, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 150, "s": 0.16848567530695774, "os": -0.0845101099938288, "bg": 3.411545173221644e-07}, {"x": 0.09523809523809525, "y": 0.0617283950617284, "ox": 0.09523809523809525, "oy": 0.0617283950617284, "term": "long", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 16, "s": 0.4406548431105048, "os": -0.033107053399644246, "bg": 2.0583768825845696e-07}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "improving", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 1.7814794942246104e-06}, {"x": 0.43452380952380953, "y": 0.46913580246913583, "ox": 0.43452380952380953, "oy": 0.46913580246913583, "term": "engineers", "cat25k": 40, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 76, "s": 0.7271487039563438, "os": 0.03452281555160269, "bg": 1.7581997758465435e-05}, {"x": 0.16071428571428573, "y": 0.10493827160493828, "ox": 0.16071428571428573, "oy": 0.10493827160493828, "term": "core", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 27, "s": 0.26398362892223737, "os": -0.05525102552002034, "bg": 1.7136890945402353e-06}, {"x": 0.8214285714285714, "y": 0.34567901234567905, "ox": 0.8214285714285714, "oy": 0.34567901234567905, "term": "technologies", "cat25k": 29, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 188, "s": 0.004092769440654844, "os": -0.4727919555668494, "bg": 8.400583288500222e-06}, {"x": 0.13690476190476192, "y": 0.08024691358024692, "ox": 0.13690476190476192, "oy": 0.08024691358024692, "term": "focus", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 23, "s": 0.26125511596180084, "os": -0.05612226376737939, "bg": 1.0953337746717086e-06}, {"x": 0.10119047619047619, "y": 0.02469135802469136, "ox": 0.10119047619047619, "oy": 0.02469135802469136, "term": "depth", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 17, "s": 0.1950886766712142, "os": -0.0758340291138781, "bg": 1.3130511942716453e-06}, {"x": 0.15476190476190477, "y": 0.04320987654320988, "ox": 0.15476190476190477, "oy": 0.04320987654320988, "term": "accurate", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 26, "s": 0.11186903137789905, "os": -0.11068355900824048, "bg": 2.613744414606396e-06}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "interpretation", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 8.635716013738685e-07}, {"x": 0.7976190476190477, "y": 0.4012345679012346, "ox": 0.7976190476190477, "oy": 0.4012345679012346, "term": "big", "cat25k": 33, "ncat25k": 82, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 182, "s": 0.007503410641200547, "os": -0.39390859258721456, "bg": 2.2878501678068907e-06}, {"x": 0.613095238095238, "y": 0.22222222222222224, "ox": 0.613095238095238, "oy": 0.22222222222222224, "term": "hadoop", "cat25k": 18, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 116, "s": 0.008185538881309688, "os": -0.38839075035394055, "bg": 0.0028471884014535645}, {"x": 0.6011904761904762, "y": 0.3209876543209877, "ox": 0.6011904761904762, "oy": 0.3209876543209877, "term": "spark", "cat25k": 26, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 114, "s": 0.019781718963165076, "os": -0.2783969216248593, "bg": 6.708142654715128e-05}, {"x": 0.5535714285714286, "y": 0.06790123456790124, "ox": 0.5535714285714286, "oy": 0.06790123456790124, "term": "aws", "cat25k": 6, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 101, "s": 0.003410641200545703, "os": -0.48259338584963873, "bg": 0.0002592664611054011}, {"x": 0.053571428571428575, "y": 0.14814814814814817, "ox": 0.053571428571428575, "oy": 0.14814814814814817, "term": "conduct", "cat25k": 12, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 9, "s": 0.8860845839017736, "os": 0.09420263549569827, "bg": 2.1954485424450395e-06}, {"x": 0.011904761904761908, "y": 0.14814814814814817, "ox": 0.011904761904761908, "oy": 0.14814814814814817, "term": "records", "cat25k": 12, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.9249658935879946, "os": 0.13562275383889352, "bg": 5.104145424955688e-07}, {"x": 0.17261904761904762, "y": 0.0617283950617284, "ox": 0.17261904761904762, "oy": 0.0617283950617284, "term": "writing", "cat25k": 5, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 29, "s": 0.1145975443383356, "os": -0.11003013032272116, "bg": 9.366739256494177e-07}, {"x": 0.125, "y": 0.08024691358024692, "ox": 0.125, "oy": 0.08024691358024692, "term": "clean", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 21, "s": 0.33151432469304226, "os": -0.044287944240752164, "bg": 1.4948864112758753e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "messy", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 4.082122785490911e-06}, {"x": 0.13690476190476192, "y": 0.0617283950617284, "ox": 0.13690476190476192, "oy": 0.0617283950617284, "term": "integrating", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 23, "s": 0.19918144611186903, "os": -0.07452717174283952, "bg": 1.0797867519327366e-05}, {"x": 0.14285714285714288, "y": 0.04938271604938272, "ox": 0.14285714285714288, "oy": 0.04938271604938272, "term": "apis", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 24, "s": 0.15006821282401092, "os": -0.09271427015645987, "bg": 3.1149964201877075e-05}, {"x": 0.029761904761904767, "y": 0.0617283950617284, "ox": 0.029761904761904767, "oy": 0.0617283950617284, "term": "discover", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7189631650750341, "os": 0.03198170399680546, "bg": 1.1556607189688916e-06}, {"x": 0.04761904761904762, "y": 0.1851851851851852, "ox": 0.04761904761904762, "oy": 0.1851851851851852, "term": "interesting", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 8, "s": 0.9256480218281038, "os": 0.13692961120993213, "bg": 1.6139341296569274e-06}, {"x": 0.11904761904761905, "y": 0.22222222222222224, "ox": 0.11904761904761905, "oy": 0.22222222222222224, "term": "trends", "cat25k": 18, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 20, "s": 0.8935879945429741, "os": 0.10273351000108905, "bg": 4.309918546387614e-06}, {"x": 0.08928571428571429, "y": 0.11111111111111112, "ox": 0.08928571428571429, "oy": 0.11111111111111112, "term": "validation", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 15, "s": 0.6480218281036836, "os": 0.021889860964896354, "bg": 7.553619539977704e-06}, {"x": 0.36309523809523814, "y": 0.0925925925925926, "ox": 0.36309523809523814, "oy": 0.0925925925925926, "term": "designing", "cat25k": 8, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 62, "s": 0.021828103683492497, "os": -0.2687043961229898, "bg": 1.6074052533761252e-05}, {"x": 0.04166666666666667, "y": 0.11728395061728396, "ox": 0.04166666666666667, "oy": 0.11728395061728396, "term": "visualizations", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 7, "s": 0.8601637107776263, "os": 0.0753621083965586, "bg": 0.00012380981859480618}, {"x": 0.11904761904761905, "y": 0.20987654320987656, "ox": 0.11904761904761905, "oy": 0.20987654320987656, "term": "ideas", "cat25k": 17, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 20, "s": 0.8792633015006821, "os": 0.09046357135078231, "bg": 1.6019857265741735e-06}, {"x": 0.07142857142857144, "y": 0.154320987654321, "ox": 0.07142857142857144, "oy": 0.154320987654321, "term": "leaders", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 12, "s": 0.8710777626193725, "os": 0.0825861255309108, "bg": 2.123464724243571e-06}, {"x": 0.07738095238095238, "y": 0.04938271604938272, "ox": 0.07738095238095238, "oy": 0.04938271604938272, "term": "curious", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 13, "s": 0.49113233287858116, "os": -0.02762551276001017, "bg": 4.902688062153011e-06}, {"x": 0.9345238095238094, "y": 0.8333333333333333, "ox": 0.9345238095238094, "oy": 0.8333333333333333, "term": "you", "cat25k": 108, "ncat25k": 174, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 216, "ncat": 389, "s": 0.13506139154160984, "os": -0.10055541438269144, "bg": 4.0383297568249646e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "why", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 6.246509177386899e-08}, {"x": 0.5714285714285714, "y": 0.6049382716049382, "ox": 0.5714285714285714, "oy": 0.6049382716049382, "term": "your", "cat25k": 53, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 104, "s": 0.7257844474761255, "os": 0.03339746614876393, "bg": 2.0269876668216667e-07}, {"x": 0.11904761904761905, "y": 0.08024691358024692, "ox": 0.11904761904761905, "oy": 0.08024691358024692, "term": "know", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 20, "s": 0.3676671214188268, "os": -0.03837078447743855, "bg": 2.155401397038438e-07}, {"x": 0.3392857142857143, "y": 0.14197530864197533, "ox": 0.3392857142857143, "oy": 0.14197530864197533, "term": "scala", "cat25k": 12, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 58, "s": 0.040927694406548434, "os": -0.19595600246850833, "bg": 0.0001350059627633554}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "constantly", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 2.4646143001850927e-06}, {"x": 0.2678571428571429, "y": 0.11728395061728396, "ox": 0.2678571428571429, "oy": 0.11728395061728396, "term": "source", "cat25k": 10, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 45, "s": 0.07162346521145975, "os": -0.14948996260935854, "bg": 7.108324253096995e-07}, {"x": 0.20238095238095238, "y": 0.2777777777777778, "ox": 0.20238095238095238, "oy": 0.2777777777777778, "term": "how", "cat25k": 23, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 34, "s": 0.8574351978171896, "os": 0.07510799724107886, "bg": 2.762456155646529e-07}, {"x": 0.15476190476190477, "y": 0.11728395061728396, "ox": 0.15476190476190477, "oy": 0.11728395061728396, "term": "extract", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 26, "s": 0.36971350613915416, "os": -0.03706392710639998, "bg": 8.130984743562293e-06}, {"x": 0.13095238095238096, "y": 0.030864197530864203, "ox": 0.13095238095238096, "oy": 0.030864197530864203, "term": "api", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 22, "s": 0.14051841746248298, "os": -0.09928485860529276, "bg": 3.7675802095151348e-06}, {"x": 0.06547619047619048, "y": 0.16049382716049385, "ox": 0.06547619047619048, "oy": 0.16049382716049385, "term": "bring", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.8867667121418826, "os": 0.09463825461937778, "bg": 1.2821683254488357e-06}, {"x": 0.20238095238095238, "y": 0.17901234567901236, "ox": 0.20238095238095238, "oy": 0.17901234567901236, "term": "transformation", "cat25k": 15, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 34, "s": 0.5184174624829468, "os": -0.023051511961375115, "bg": 1.0952357938355958e-05}, {"x": 0.08928571428571429, "y": 0.04938271604938272, "ox": 0.08928571428571429, "oy": 0.04938271604938272, "term": "two", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 15, "s": 0.36289222373806274, "os": -0.03945983228663738, "bg": 1.0418906118050075e-07}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "map", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 8.392966056579167e-08}, {"x": 0.20238095238095238, "y": 0.1358024691358025, "ox": 0.20238095238095238, "oy": 0.1358024691358025, "term": "tableau", "cat25k": 11, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 34, "s": 0.22237380627557982, "os": -0.06599629723744874, "bg": 0.0001832271869882522}, {"x": 0.2380952380952381, "y": 0.19135802469135804, "ox": 0.2380952380952381, "oy": 0.19135802469135804, "term": "better", "cat25k": 16, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 40, "s": 0.3212824010914052, "os": -0.04628453189095, "bg": 9.033879459126877e-07}, {"x": 0.07738095238095238, "y": 0.08024691358024692, "ox": 0.07738095238095238, "oy": 0.08024691358024692, "term": "serve", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 13, "s": 0.5914051841746248, "os": 0.0030493338657567037, "bg": 1.4128706319677956e-06}, {"x": 0.18452380952380953, "y": 0.20370370370370372, "ox": 0.18452380952380953, "oy": 0.20370370370370372, "term": "manage", "cat25k": 17, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 31, "s": 0.6323328785811733, "os": 0.019239844629179215, "bg": 3.502423882546872e-06}, {"x": 0.36904761904761907, "y": 0.24074074074074076, "ox": 0.36904761904761907, "oy": 0.24074074074074076, "term": "developing", "cat25k": 20, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 63, "s": 0.09481582537517053, "os": -0.12738229208262244, "bg": 4.757907297913777e-06}, {"x": 0.8809523809523809, "y": 0.7962962962962963, "ox": 0.8809523809523809, "oy": 0.7962962962962963, "term": "solutions", "cat25k": 93, "ncat25k": 103, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 185, "ncat": 229, "s": 0.169849931787176, "os": -0.08411079246378916, "bg": 7.79582075088178e-06}, {"x": 0.2261904761904762, "y": 0.06790123456790124, "ox": 0.2261904761904762, "oy": 0.06790123456790124, "term": "batch", "cat25k": 6, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 38, "s": 0.0641200545702592, "os": -0.15714959886739027, "bg": 1.2339897697211398e-05}, {"x": 0.5238095238095238, "y": 0.6543209876543209, "ox": 0.5238095238095238, "oy": 0.6543209876543209, "term": "use", "cat25k": 60, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 119, "ncat": 95, "s": 0.9188267394270123, "os": 0.12981449885649976, "bg": 5.943725493059839e-07}, {"x": 0.10714285714285715, "y": 0.154320987654321, "ox": 0.10714285714285715, "oy": 0.154320987654321, "term": "cases", "cat25k": 13, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 18, "s": 0.7844474761255116, "os": 0.04708316695102914, "bg": 9.958044673548431e-07}, {"x": 0.20238095238095238, "y": 0.16666666666666669, "ox": 0.20238095238095238, "oy": 0.16666666666666669, "term": "do", "cat25k": 14, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 34, "s": 0.37244201909959074, "os": -0.03532145061168185, "bg": 1.2830511842255865e-07}, {"x": 0.20238095238095238, "y": 0.2777777777777778, "ox": 0.20238095238095238, "oy": 0.2777777777777778, "term": "user", "cat25k": 23, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 34, "s": 0.8574351978171896, "os": 0.07510799724107886, "bg": 4.991266421013803e-07}, {"x": 0.1130952380952381, "y": 0.09876543209876544, "ox": 0.1130952380952381, "oy": 0.09876543209876544, "term": "passion", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 19, "s": 0.5641200545702593, "os": -0.014048716738664827, "bg": 5.193947952486358e-06}, {"x": 0.23214285714285715, "y": 0.29629629629629634, "ox": 0.23214285714285715, "oy": 0.29629629629629634, "term": "solving", "cat25k": 24, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 39, "s": 0.834242837653479, "os": 0.06392710639997096, "bg": 1.7326556193606542e-05}, {"x": 0.44642857142857145, "y": 0.5679012345679012, "ox": 0.44642857142857145, "oy": 0.5679012345679012, "term": "computer", "cat25k": 50, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 99, "ncat": 78, "s": 0.9092769440654843, "os": 0.1208480052274295, "bg": 1.5783571107333082e-06}, {"x": 0.08333333333333334, "y": 0.29012345679012347, "ox": 0.08333333333333334, "oy": 0.29012345679012347, "term": "mathematics", "cat25k": 24, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 14, "s": 0.9597544338335607, "os": 0.20572113115765783, "bg": 4.896441860050701e-06}, {"x": 0.10714285714285715, "y": 0.0617283950617284, "ox": 0.10714285714285715, "oy": 0.0617283950617284, "term": "discipline", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 18, "s": 0.32878581173260574, "os": -0.04494137292627147, "bg": 4.729084672994351e-06}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "graduate", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 4.3111814610766414e-07}, {"x": 0.23214285714285715, "y": 0.14814814814814817, "ox": 0.23214285714285715, "oy": 0.14814814814814817, "term": "2", "cat25k": 12, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 39, "s": 0.1712141882673943, "os": -0.08331215740371001, "bg": 0.0}, {"x": 0.7797619047619048, "y": 0.6604938271604938, "ox": 0.7797619047619048, "oy": 0.6604938271604938, "term": "years", "cat25k": 60, "ncat25k": 80, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 120, "ncat": 179, "s": 0.10231923601637108, "os": -0.11848840164083208, "bg": 1.7695018066495084e-06}, {"x": 0.19642857142857142, "y": 0.0617283950617284, "ox": 0.19642857142857142, "oy": 0.0617283950617284, "term": "equivalent", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 33, "s": 0.08935879945429741, "os": -0.1336987693759756, "bg": 3.5599959283581453e-06}, {"x": 0.35714285714285715, "y": 0.24074074074074076, "ox": 0.35714285714285715, "oy": 0.24074074074074076, "term": "implementation", "cat25k": 20, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 61, "s": 0.10368349249658937, "os": -0.11554797255599522, "bg": 4.253385035213349e-06}, {"x": 0.2142857142857143, "y": 0.08024691358024692, "ox": 0.2142857142857143, "oy": 0.08024691358024692, "term": "similar", "cat25k": 7, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 36, "s": 0.09072305593451568, "os": -0.13304534069045632, "bg": 8.217513490096546e-07}, {"x": 0.6071428571428571, "y": 0.5, "ox": 0.6071428571428571, "oy": 0.5, "term": "programming", "cat25k": 43, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 115, "s": 0.11937244201909959, "os": -0.10639997095872511, "bg": 7.923178438611155e-06}, {"x": 0.39880952380952384, "y": 0.2716049382716049, "ox": 0.39880952380952384, "oy": 0.2716049382716049, "term": "languages", "cat25k": 22, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 69, "s": 0.09549795361527967, "os": -0.1262932442734236, "bg": 6.029439887444763e-06}, {"x": 0.36309523809523814, "y": 0.17283950617283952, "ox": 0.36309523809523814, "oy": 0.17283950617283952, "term": "relational", "cat25k": 14, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 62, "s": 0.045020463847203276, "os": -0.18894979489599592, "bg": 5.6884151632922784e-05}, {"x": 0.5773809523809523, "y": 0.22839506172839508, "ox": 0.5773809523809523, "oy": 0.22839506172839508, "term": "databases", "cat25k": 19, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 105, "s": 0.012960436562073671, "os": -0.3467528224489055, "bg": 1.307275887917664e-05}, {"x": 0.01785714285714286, "y": 0.22839506172839508, "ox": 0.01785714285714286, "oy": 0.22839506172839508, "term": "timelines", "cat25k": 19, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 3, "s": 0.9631650750341064, "os": 0.20946019530257376, "bg": 6.352685200625739e-05}, {"x": 0.0, "y": 0.154320987654321, "ox": 0.0, "oy": 0.154320987654321, "term": "mathematical", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 0, "s": 0.9338335607094134, "os": 0.1535920426906741, "bg": 4.51328182114171e-06}, {"x": 0.005952380952380953, "y": 0.05555555555555556, "ox": 0.005952380952380953, "oy": 0.05555555555555556, "term": "algorithmic", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7905866302864939, "os": 0.049515373724906525, "bg": 2.763335511762828e-05}, {"x": 0.02380952380952381, "y": 0.17283950617283952, "ox": 0.02380952380952381, "oy": 0.17283950617283952, "term": "applied", "cat25k": 14, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 4, "s": 0.9324693042291952, "os": 0.1483283116128798, "bg": 1.4201122509977397e-06}, {"x": 0.01785714285714286, "y": 0.5740740740740741, "ox": 0.01785714285714286, "oy": 0.5740740740740741, "term": "ai", "cat25k": 50, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 100, "ncat": 3, "s": 0.9972714870395635, "os": 0.5530184775111627, "bg": 1.6672649126122522e-05}, {"x": 0.27976190476190477, "y": 0.7407407407407407, "ox": 0.27976190476190477, "oy": 0.7407407407407407, "term": "techniques", "cat25k": 73, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 145, "ncat": 47, "s": 0.9931787175989086, "os": 0.45830761970450506, "bg": 9.508466038544992e-06}, {"x": 0.863095238095238, "y": 0.6234567901234568, "ox": 0.863095238095238, "oy": 0.6234567901234568, "term": "working", "cat25k": 56, "ncat25k": 100, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 111, "ncat": 223, "s": 0.029331514324693043, "os": -0.23813845427814273, "bg": 4.5201441373818506e-06}, {"x": 0.7142857142857142, "y": 0.154320987654321, "ox": 0.7142857142857142, "oy": 0.154320987654321, "term": "cloud", "cat25k": 13, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 155, "s": 0.0013642564802182813, "os": -0.556467128906959, "bg": 3.1457477354985395e-05}, {"x": 0.34523809523809523, "y": 0.05555555555555556, "ox": 0.34523809523809523, "oy": 0.05555555555555556, "term": "agile", "cat25k": 5, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 59, "s": 0.019099590723055938, "os": -0.28776273278396924, "bg": 7.099443894295545e-05}, {"x": 0.40476190476190477, "y": 0.20987654320987656, "ox": 0.40476190476190477, "oy": 0.20987654320987656, "term": "issues", "cat25k": 17, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 70, "s": 0.042974079126875855, "os": -0.19356009728827092, "bg": 1.384700410546363e-06}, {"x": 0.10714285714285715, "y": 0.25925925925925924, "ox": 0.10714285714285715, "oy": 0.25925925925925924, "term": "collection", "cat25k": 21, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 18, "s": 0.9331514324693043, "os": 0.15137764547863647, "bg": 1.1191047833196314e-06}, {"x": 0.19642857142857142, "y": 0.07407407407407408, "ox": 0.19642857142857142, "oy": 0.07407407407407408, "term": "automation", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 33, "s": 0.10027285129604366, "os": -0.12142883072566886, "bg": 7.087069531396662e-06}, {"x": 0.02380952380952381, "y": 0.17901234567901236, "ox": 0.02380952380952381, "oy": 0.17901234567901236, "term": "address", "cat25k": 15, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 4, "s": 0.9351978171896317, "os": 0.1544632809380332, "bg": 2.519280906077242e-07}, {"x": 0.053571428571428575, "y": 0.08641975308641976, "ox": 0.053571428571428575, "oy": 0.08641975308641976, "term": "engage", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 9, "s": 0.7216916780354706, "os": 0.03285294224416452, "bg": 4.587798749226432e-06}, {"x": 0.04761904761904762, "y": 0.16666666666666669, "ox": 0.04761904761904762, "oy": 0.16666666666666669, "term": "prototype", "cat25k": 14, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.907230559345157, "os": 0.118524703234472, "bg": 1.1855938826065587e-05}, {"x": 0.04761904761904762, "y": 0.02469135802469136, "ox": 0.04761904761904762, "oy": 0.02469135802469136, "term": "cognitive", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5368349249658936, "os": -0.022579591244055616, "bg": 2.8851091821484834e-06}, {"x": 0.30952380952380953, "y": 0.11111111111111112, "ox": 0.30952380952380953, "oy": 0.11111111111111112, "term": "modern", "cat25k": 9, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 53, "s": 0.040245566166439296, "os": -0.1970450502777072, "bg": 2.485267787604114e-06}, {"x": 0.13690476190476192, "y": 0.08641975308641976, "ox": 0.13690476190476192, "oy": 0.08641975308641976, "term": "oriented", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 23, "s": 0.2946793997271487, "os": -0.04998729444222602, "bg": 4.380652410563506e-06}, {"x": 0.380952380952381, "y": 0.16049382716049385, "ox": 0.380952380952381, "oy": 0.16049382716049385, "term": "functional", "cat25k": 13, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 66, "s": 0.03274215552523875, "os": -0.21897121283624355, "bg": 4.384120534341375e-06}, {"x": 0.6011904761904762, "y": 0.2469135802469136, "ox": 0.6011904761904762, "oy": 0.2469135802469136, "term": "java", "cat25k": 20, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 114, "s": 0.010914051841746248, "os": -0.3520165535266998, "bg": 5.5528589534342826e-06}, {"x": 0.15476190476190477, "y": 0.1358024691358025, "ox": 0.15476190476190477, "oy": 0.1358024691358025, "term": "c", "cat25k": 11, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 26, "s": 0.5566166439290587, "os": -0.01865901913093987, "bg": 1.6087680351506162e-07}, {"x": 0.2976190476190476, "y": 0.16049382716049385, "ox": 0.2976190476190476, "oy": 0.16049382716049385, "term": "frameworks", "cat25k": 13, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 50, "s": 0.08731241473396999, "os": -0.13613097614985298, "bg": 4.7428294347483406e-05}, {"x": 0.17857142857142858, "y": 0.03703703703703704, "ox": 0.17857142857142858, "oy": 0.03703703703703704, "term": "apache", "cat25k": 3, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 30, "s": 0.08458390177353342, "os": -0.14048716738664827, "bg": 4.32172194689252e-06}, {"x": 0.04166666666666667, "y": 0.11111111111111112, "ox": 0.04166666666666667, "oy": 0.11111111111111112, "term": "tensorflow", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.8437926330150068, "os": 0.06922713907140524, "bg": 0.0004688452341881945}, {"x": 0.07142857142857144, "y": 0.09876543209876544, "ox": 0.07142857142857144, "oy": 0.09876543209876544, "term": "scikit", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 12, "s": 0.6957708049113234, "os": 0.02737140160453043, "bg": 0.0005250918910809392}, {"x": 0.11904761904761905, "y": 0.08641975308641976, "ox": 0.11904761904761905, "oy": 0.08641975308641976, "term": "linux", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 20, "s": 0.4427012278308322, "os": -0.032235815152285185, "bg": 7.475740808810292e-07}, {"x": 0.07738095238095238, "y": 0.012345679012345682, "ox": 0.07738095238095238, "oy": 0.012345679012345682, "term": "bash", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.22919508867667124, "os": -0.06443532871093041, "bg": 6.213073632378232e-06}, {"x": 0.17857142857142858, "y": 0.08641975308641976, "ox": 0.17857142857142858, "oy": 0.08641975308641976, "term": "scripting", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 30, "s": 0.1541609822646658, "os": -0.09140741278542128, "bg": 1.1946062442068384e-05}, {"x": 0.1488095238095238, "y": 0.09876543209876544, "ox": 0.1488095238095238, "oy": 0.09876543209876544, "term": "oracle", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 25, "s": 0.296043656207367, "os": -0.0495516753185465, "bg": 4.734187769549207e-06}, {"x": 0.07142857142857144, "y": 0.00617283950617284, "ox": 0.07142857142857144, "oy": 0.00617283950617284, "term": "postgresql", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22578444747612553, "os": -0.06465313827277018, "bg": 5.5023555372425885e-06}, {"x": 0.13095238095238096, "y": 0.03703703703703704, "ox": 0.13095238095238096, "oy": 0.03703703703703704, "term": "mysql", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 22, "s": 0.1493860845839018, "os": -0.0931498892801394, "bg": 2.455044733326246e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "quantify", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 7.413136572215071e-06}, {"x": 0.32142857142857145, "y": 0.2160493827160494, "ox": 0.32142857142857145, "oy": 0.2160493827160494, "term": "them", "cat25k": 18, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 55, "s": 0.130968622100955, "os": -0.10458489127672704, "bg": 4.4653143202598006e-07}, {"x": 0.5178571428571429, "y": 0.4506172839506173, "ox": 0.5178571428571429, "oy": 0.4506172839506173, "term": "will", "cat25k": 38, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 94, "s": 0.22169167803547069, "os": -0.06672232911024795, "bg": 2.5066344143619633e-07}, {"x": 0.08928571428571429, "y": 0.03703703703703704, "ox": 0.08928571428571429, "oy": 0.03703703703703704, "term": "collect", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.28717598908594816, "os": -0.05172977093694413, "bg": 2.6168297668124302e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "cleanse", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 1.2491421906362741e-05}, {"x": 0.06547619047619048, "y": 0.09876543209876544, "ox": 0.06547619047619048, "oy": 0.09876543209876544, "term": "usage", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 11, "s": 0.7244201909959073, "os": 0.03328856136784404, "bg": 2.1137467401817342e-06}, {"x": 0.3392857142857143, "y": 0.30246913580246915, "ox": 0.3392857142857143, "oy": 0.30246913580246915, "term": "various", "cat25k": 25, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 58, "s": 0.3703956343792633, "os": -0.03644680001452061, "bg": 2.344874858521318e-06}, {"x": 0.20238095238095238, "y": 0.34567901234567905, "ox": 0.20238095238095238, "oy": 0.34567901234567905, "term": "datasets", "cat25k": 29, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 34, "s": 0.9297407912687586, "os": 0.14259265981776598, "bg": 9.285695335315642e-05}, {"x": 0.27380952380952384, "y": 0.5432098765432098, "ox": 0.27380952380952384, "oy": 0.5432098765432098, "term": "advanced", "cat25k": 48, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 95, "ncat": 46, "s": 0.9761255115961801, "os": 0.26790576106291064, "bg": 1.6250893323707734e-06}, {"x": 0.17857142857142858, "y": 0.42592592592592593, "ox": 0.17857142857142858, "oy": 0.42592592592592593, "term": "mining", "cat25k": 36, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 30, "s": 0.9713506139154161, "os": 0.24601590009801433, "bg": 1.1811081402148437e-05}, {"x": 0.04761904761904762, "y": 0.1358024691358025, "ox": 0.04761904761904762, "oy": 0.1358024691358025, "term": "strategic", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 8, "s": 0.8758526603001364, "os": 0.08784985660870512, "bg": 1.9331508073820457e-06}, {"x": 0.1488095238095238, "y": 0.3827160493827161, "ox": 0.1488095238095238, "oy": 0.3827160493827161, "term": "analytic", "cat25k": 32, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 25, "s": 0.9672578444747613, "os": 0.2326569136385087, "bg": 6.511616205340857e-05}, {"x": 0.14285714285714288, "y": 0.14197530864197533, "ox": 0.14285714285714288, "oy": 0.14197530864197533, "term": "set", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 24, "s": 0.5859481582537518, "os": -0.0006897302791592508, "bg": 2.9976759273656486e-07}, {"x": 0.1488095238095238, "y": 0.154320987654321, "ox": 0.1488095238095238, "oy": 0.154320987654321, "term": "objectives", "cat25k": 13, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 25, "s": 0.6016371077762619, "os": 0.00566304860783387, "bg": 3.885415219754228e-06}, {"x": 0.08928571428571429, "y": 0.11111111111111112, "ox": 0.08928571428571429, "oy": 0.11111111111111112, "term": "plans", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 15, "s": 0.6480218281036836, "os": 0.021889860964896354, "bg": 8.526880273027606e-07}, {"x": 0.2142857142857143, "y": 0.19135802469135804, "ox": 0.2142857142857143, "oy": 0.19135802469135804, "term": "methodologies", "cat25k": 16, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 36, "s": 0.5361527967257844, "os": -0.022615892837695578, "bg": 4.2074908408576243e-05}, {"x": 0.053571428571428575, "y": 0.11111111111111112, "ox": 0.053571428571428575, "oy": 0.11111111111111112, "term": "validate", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 9, "s": 0.8185538881309686, "os": 0.05739281954477801, "bg": 1.4126440537233766e-05}, {"x": 0.005952380952380953, "y": 0.2160493827160494, "ox": 0.005952380952380953, "oy": 0.2160493827160494, "term": "forecasting", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.9624829467939974, "os": 0.20902457617889425, "bg": 2.0308239588713018e-05}, {"x": 0.18452380952380953, "y": 0.24074074074074076, "ox": 0.18452380952380953, "oy": 0.24074074074074076, "term": "non", "cat25k": 20, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 31, "s": 0.8110504774897681, "os": 0.05604966058009947, "bg": 7.134985470137215e-07}, {"x": 0.1488095238095238, "y": 0.308641975308642, "ox": 0.1488095238095238, "oy": 0.308641975308642, "term": "partners", "cat25k": 25, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 25, "s": 0.9379263301500682, "os": 0.15903728173666826, "bg": 2.5318724766725924e-06}, {"x": 0.01785714285714286, "y": 0.0617283950617284, "ox": 0.01785714285714286, "oy": 0.0617283950617284, "term": "makers", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7694406548431105, "os": 0.04381602352343268, "bg": 1.944474047815365e-06}, {"x": 0.029761904761904767, "y": 0.0617283950617284, "ox": 0.029761904761904767, "oy": 0.0617283950617284, "term": "travel", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7189631650750341, "os": 0.03198170399680546, "bg": 1.0422966445111344e-07}, {"x": 0.4285714285714286, "y": 0.16666666666666669, "ox": 0.4285714285714286, "oy": 0.16666666666666669, "term": "required", "cat25k": 14, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 75, "s": 0.02319236016371078, "os": -0.260173521617599, "bg": 1.2783100720795186e-06}, {"x": 0.04166666666666667, "y": 0.08641975308641976, "ox": 0.04166666666666667, "oy": 0.08641975308641976, "term": "masters", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.7776261937244202, "os": 0.044687261770791735, "bg": 2.7025656614066165e-06}, {"x": 0.125, "y": 0.691358024691358, "ox": 0.125, "oy": 0.691358024691358, "term": "statistics", "cat25k": 65, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 21, "s": 0.9979536152796726, "os": 0.5630740189494319, "bg": 4.3289283697417375e-06}, {"x": 0.01785714285714286, "y": 0.09876543209876544, "ox": 0.01785714285714286, "oy": 0.09876543209876544, "term": "physics", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.8656207366984994, "os": 0.08062583947435292, "bg": 1.2738237729171792e-06}, {"x": 0.08928571428571429, "y": 0.0925925925925926, "ox": 0.08928571428571429, "oy": 0.0925925925925926, "term": "delivering", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 15, "s": 0.5961800818553888, "os": 0.0034849529894362408, "bg": 7.362740117362077e-06}, {"x": 0.07738095238095238, "y": 0.1358024691358025, "ox": 0.07738095238095238, "oy": 0.1358024691358025, "term": "sas", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 13, "s": 0.819918144611187, "os": 0.05826405779213706, "bg": 1.1373059756005626e-05}, {"x": 0.005952380952380953, "y": 0.08024691358024692, "ox": 0.005952380952380953, "oy": 0.08024691358024692, "term": "matlab", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.8533424283765347, "os": 0.07405525102552002, "bg": 1.1893434823977164e-05}, {"x": 0.13690476190476192, "y": 0.06790123456790124, "ox": 0.13690476190476192, "oy": 0.06790123456790124, "term": "most", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 23, "s": 0.21896316507503413, "os": -0.06839220241768613, "bg": 1.6333704696923726e-07}, {"x": 0.08928571428571429, "y": 0.6790123456790123, "ox": 0.08928571428571429, "oy": 0.6790123456790123, "term": "ml", "cat25k": 62, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 15, "s": 0.999317871759891, "os": 0.5863070388790067, "bg": 1.3954724848967712e-05}, {"x": 0.761904761904762, "y": 0.7716049382716049, "ox": 0.761904761904762, "oy": 0.7716049382716049, "term": "knowledge", "cat25k": 82, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 163, "ncat": 174, "s": 0.6091405184174624, "os": 0.009692525501869498, "bg": 7.5008656043422934e-06}, {"x": 0.13690476190476192, "y": 0.12962962962962962, "ox": 0.13690476190476192, "oy": 0.12962962962962962, "term": "influence", "cat25k": 11, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 23, "s": 0.5757162346521145, "os": -0.0070425091661524, "bg": 3.312337754262856e-06}, {"x": 0.1488095238095238, "y": 0.20987654320987656, "ox": 0.1488095238095238, "oy": 0.20987654320987656, "term": "over", "cat25k": 17, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 25, "s": 0.8219645293315143, "os": 0.06087777253421425, "bg": 2.568961742399085e-07}, {"x": 0.08928571428571429, "y": 0.02469135802469136, "ox": 0.08928571428571429, "oy": 0.02469135802469136, "term": "organizations", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.23192360163710782, "os": -0.06399970958725087, "bg": 7.004140534540891e-07}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "productivity", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 1.192137092904555e-06}, {"x": 0.05952380952380953, "y": 0.1358024691358025, "ox": 0.05952380952380953, "oy": 0.1358024691358025, "term": "effectiveness", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 10, "s": 0.8608458390177354, "os": 0.0760155370820779, "bg": 4.282265819108126e-06}, {"x": 0.02380952380952381, "y": 0.05555555555555556, "ox": 0.02380952380952381, "oy": 0.05555555555555556, "term": "prior", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.7175989085948159, "os": 0.031763894434965696, "bg": 4.123339095223949e-07}, {"x": 0.053571428571428575, "y": 0.08024691358024692, "ox": 0.053571428571428575, "oy": 0.08024691358024692, "term": "art", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.689631650750341, "os": 0.02671797291901115, "bg": 2.048328085252979e-07}, {"x": 0.2142857142857143, "y": 0.23456790123456792, "ox": 0.2142857142857143, "oy": 0.23456790123456792, "term": "contribute", "cat25k": 19, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 36, "s": 0.6412005457025921, "os": 0.020328892438378043, "bg": 6.618426387737558e-06}, {"x": 0.011904761904761908, "y": 0.07407407407407408, "ox": 0.011904761904761908, "oy": 0.07407407407407408, "term": "intellectual", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.8281036834924965, "os": 0.062003121937053034, "bg": 1.4214387793171114e-06}, {"x": 0.0, "y": 0.0617283950617284, "ox": 0.0, "oy": 0.0617283950617284, "term": "property", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8233287858117326, "os": 0.061567502813373504, "bg": 1.0422636720409378e-07}, {"x": 0.18452380952380953, "y": 0.1358024691358025, "ox": 0.18452380952380953, "oy": 0.1358024691358025, "term": "assist", "cat25k": 11, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 31, "s": 0.2980900409276944, "os": -0.0482448179475079, "bg": 3.52505474227111e-06}, {"x": 0.07142857142857144, "y": 0.03703703703703704, "ox": 0.07142857142857144, "oy": 0.03703703703703704, "term": "career", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.43110504774897684, "os": -0.033978291647003306, "bg": 5.04516508866282e-07}, {"x": 0.04166666666666667, "y": 0.09876543209876544, "ox": 0.04166666666666667, "oy": 0.09876543209876544, "term": "actively", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.8165075034106413, "os": 0.05695720042109848, "bg": 5.388320440605306e-06}, {"x": 0.13690476190476192, "y": 0.06790123456790124, "ox": 0.13690476190476192, "oy": 0.06790123456790124, "term": "mentor", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 23, "s": 0.21896316507503413, "os": -0.06839220241768613, "bg": 1.2047146863578465e-05}, {"x": 0.10119047619047619, "y": 0.11111111111111112, "ox": 0.10119047619047619, "oy": 0.11111111111111112, "term": "community", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 17, "s": 0.6105047748976808, "os": 0.01005554143826913, "bg": 2.672308831921135e-07}, {"x": 0.13095238095238096, "y": 0.14814814814814817, "ox": 0.13095238095238096, "oy": 0.14814814814814817, "term": "managers", "cat25k": 12, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 22, "s": 0.6296043656207367, "os": 0.01727955857262134, "bg": 3.7002057877492782e-06}, {"x": 0.02380952380952381, "y": 0.08024691358024692, "ox": 0.02380952380952381, "oy": 0.08024691358024692, "term": "guide", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.8137789904502046, "os": 0.0563037717355792, "bg": 1.5926144360455789e-07}, {"x": 0.24404761904761904, "y": 0.05555555555555556, "ox": 0.24404761904761904, "oy": 0.05555555555555556, "term": "hive", "cat25k": 5, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 41, "s": 0.04843110504774898, "os": -0.18717101680763784, "bg": 9.240632539778612e-05}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "impala", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 1.749390993260471e-05}, {"x": 0.33333333333333337, "y": 0.1234567901234568, "ox": 0.33333333333333337, "oy": 0.1234567901234568, "term": "hands", "cat25k": 10, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 57, "s": 0.03683492496589359, "os": -0.20844375068065488, "bg": 3.130436083569146e-06}, {"x": 0.2619047619047619, "y": 0.1234567901234568, "ox": 0.2619047619047619, "oy": 0.1234567901234568, "term": "least", "cat25k": 10, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 44, "s": 0.08663028649386084, "os": -0.1374378335208916, "bg": 1.149668084536747e-06}, {"x": 0.04166666666666667, "y": 0.17283950617283952, "ox": 0.04166666666666667, "oy": 0.17283950617283952, "term": "major", "cat25k": 14, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.9201909959072305, "os": 0.13057683232293898, "bg": 5.705022294941877e-07}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "frame", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 3.03118102304005e-07}, {"x": 0.07142857142857144, "y": 0.03703703703703704, "ox": 0.07142857142857144, "oy": 0.03703703703703704, "term": "terms", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.43110504774897684, "os": -0.033978291647003306, "bg": 1.2958377963546847e-07}, {"x": 0.3035714285714286, "y": 0.46296296296296297, "ox": 0.3035714285714286, "oy": 0.46296296296296297, "term": "can", "cat25k": 39, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 51, "s": 0.9372442019099592, "os": 0.1585653610193487, "bg": 2.076575328195004e-07}, {"x": 0.6726190476190476, "y": 0.7160493827160493, "ox": 0.6726190476190476, "oy": 0.7160493827160493, "term": "be", "cat25k": 70, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 139, "ncat": 138, "s": 0.7639836289222374, "os": 0.04323519802519327, "bg": 2.309458175145297e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "achieving", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 1.3513245683418888e-06}, {"x": 0.13095238095238096, "y": 0.10493827160493828, "ox": 0.13095238095238096, "oy": 0.10493827160493828, "term": "efforts", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 22, "s": 0.4979536152796726, "os": -0.025665226703452282, "bg": 2.035001397889422e-06}, {"x": 0.011904761904761908, "y": 0.12962962962962962, "ox": 0.011904761904761908, "oy": 0.12962962962962962, "term": "risk", "cat25k": 11, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 2, "s": 0.9038199181446113, "os": 0.1172178458634334, "bg": 5.022230027786689e-07}, {"x": 0.1904761904761905, "y": 0.19135802469135804, "ox": 0.1904761904761905, "oy": 0.19135802469135804, "term": "value", "cat25k": 16, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 32, "s": 0.5900409276944065, "os": 0.0010527462155588696, "bg": 6.916694941859415e-07}, {"x": 0.19642857142857142, "y": 0.20987654320987656, "ox": 0.19642857142857142, "oy": 0.20987654320987656, "term": "appropriate", "cat25k": 17, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 33, "s": 0.6145975443383357, "os": 0.013540494427705385, "bg": 1.9203791321399696e-06}, {"x": 0.01785714285714286, "y": 0.04938271604938272, "ox": 0.01785714285714286, "oy": 0.04938271604938272, "term": "visual", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.7135061391541611, "os": 0.031546084873125935, "bg": 4.393341363961952e-07}, {"x": 0.25, "y": 0.6666666666666666, "ox": 0.25, "oy": 0.6666666666666666, "term": "methods", "cat25k": 61, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 122, "ncat": 42, "s": 0.9911323328785813, "os": 0.4142737866192326, "bg": 3.7270833236747338e-06}, {"x": 0.01785714285714286, "y": 0.07407407407407408, "ox": 0.01785714285714286, "oy": 0.07407407407407408, "term": "prescriptive", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.8117326057298773, "os": 0.05608596217373943, "bg": 4.477752287758273e-05}, {"x": 0.05952380952380953, "y": 0.030864197530864203, "ox": 0.05952380952380953, "oy": 0.030864197530864203, "term": "structure", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.48362892223738063, "os": -0.028278941445529455, "bg": 4.4577747097721195e-07}, {"x": 0.07142857142857144, "y": 0.04320987654320988, "ox": 0.07142857142857144, "oy": 0.04320987654320988, "term": "facilitate", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.4877216916780355, "os": -0.027843322321849932, "bg": 3.3223706758471348e-06}, {"x": 0.20238095238095238, "y": 0.1851851851851852, "ox": 0.20238095238095238, "oy": 0.1851851851851852, "term": "effectively", "cat25k": 15, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 34, "s": 0.557980900409277, "os": -0.01691654263622172, "bg": 6.525786159674262e-06}, {"x": 0.13690476190476192, "y": 0.20987654320987656, "ox": 0.13690476190476192, "oy": 0.20987654320987656, "term": "recommendations", "cat25k": 17, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 23, "s": 0.8512960436562074, "os": 0.07271209206084148, "bg": 3.181771863440696e-06}, {"x": 0.125, "y": 0.17901234567901236, "ox": 0.125, "oy": 0.17901234567901236, "term": "effective", "cat25k": 15, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 21, "s": 0.8035470668485676, "os": 0.05387156496170181, "bg": 1.3146220201396939e-06}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "creates", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 1.1874760855510549e-06}, {"x": 0.10714285714285715, "y": 0.23456790123456792, "ox": 0.10714285714285715, "oy": 0.23456790123456792, "term": "training", "cat25k": 19, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 18, "s": 0.91268758526603, "os": 0.126837768178023, "bg": 6.355120862766669e-07}, {"x": 0.05952380952380953, "y": 0.22222222222222224, "ox": 0.05952380952380953, "oy": 0.22222222222222224, "term": "evaluating", "cat25k": 18, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 10, "s": 0.942019099590723, "os": 0.16190510763422514, "bg": 1.1779838908142096e-05}, {"x": 0.08928571428571429, "y": 0.030864197530864203, "ox": 0.08928571428571429, "oy": 0.030864197530864203, "term": "provides", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.2551159618008185, "os": -0.0578647402620975, "bg": 3.658971724115984e-07}, {"x": 0.09523809523809525, "y": 0.08641975308641976, "ox": 0.09523809523809525, "oy": 0.08641975308641976, "term": "those", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 16, "s": 0.5716234652114598, "os": -0.008567176099030752, "bg": 2.2212285021777534e-07}, {"x": 0.10714285714285715, "y": 0.0617283950617284, "ox": 0.10714285714285715, "oy": 0.0617283950617284, "term": "deploying", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 18, "s": 0.32878581173260574, "os": -0.04494137292627147, "bg": 2.1677699067084736e-05}, {"x": 0.053571428571428575, "y": 0.09876543209876544, "ox": 0.053571428571428575, "oy": 0.09876543209876544, "term": "interpersonal", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 9, "s": 0.7796725784447477, "os": 0.045122880894471265, "bg": 1.954845416688984e-05}, {"x": 0.06547619047619048, "y": 0.03703703703703704, "ox": 0.06547619047619048, "oy": 0.03703703703703704, "term": "facing", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 11, "s": 0.48567530695770805, "os": -0.028061131883689694, "bg": 2.520058180731457e-06}, {"x": 0.11904761904761905, "y": 0.04938271604938272, "ox": 0.11904761904761905, "oy": 0.04938271604938272, "term": "range", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 20, "s": 0.21555252387448842, "os": -0.06904563110320543, "bg": 4.360638102140659e-07}, {"x": 0.19642857142857142, "y": 0.1234567901234568, "ox": 0.19642857142857142, "oy": 0.1234567901234568, "term": "people", "cat25k": 10, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 33, "s": 0.20463847203274216, "os": -0.07234907612444186, "bg": 2.2064484333882031e-07}, {"x": 0.17261904761904762, "y": 0.12962962962962962, "ox": 0.17261904761904762, "oy": 0.12962962962962962, "term": "must", "cat25k": 11, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 29, "s": 0.33424283765347884, "os": -0.042545467746034044, "bg": 3.414351501327571e-07}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "willing", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 1.4034885435119506e-06}, {"x": 0.13690476190476192, "y": 0.09876543209876544, "ox": 0.13690476190476192, "oy": 0.09876543209876544, "term": "us", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 23, "s": 0.36903137789904505, "os": -0.037717355791919274, "bg": 6.345491093870989e-08}, {"x": 0.34523809523809523, "y": 0.20370370370370372, "ox": 0.34523809523809523, "oy": 0.20370370370370372, "term": "up", "cat25k": 17, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 59, "s": 0.08390177353342428, "os": -0.14052346898028825, "bg": 2.2166642685330896e-07}, {"x": 0.13095238095238096, "y": 0.11728395061728396, "ox": 0.13095238095238096, "oy": 0.11728395061728396, "term": "4", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 22, "s": 0.5668485675306958, "os": -0.013395288053145535, "bg": 0.0}, {"x": 0.08928571428571429, "y": 0.11111111111111112, "ox": 0.08928571428571429, "oy": 0.11111111111111112, "term": "year", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 15, "s": 0.6480218281036836, "os": 0.021889860964896354, "bg": 1.4627684342976146e-07}, {"x": 0.2142857142857143, "y": 0.12962962962962962, "ox": 0.2142857142857143, "oy": 0.12962962962962962, "term": "5", "cat25k": 11, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 36, "s": 0.17053206002728516, "os": -0.08396558608922933, "bg": 0.0}, {"x": 0.01785714285714286, "y": 0.1851851851851852, "ox": 0.01785714285714286, "oy": 0.1851851851851852, "term": "regression", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 3, "s": 0.9447476125511596, "os": 0.16651541002650017, "bg": 1.0278848015240105e-05}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "trees", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 5.821411325497519e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "bayes", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 1.7745782788661037e-05}, {"x": 0.011904761904761908, "y": 0.14197530864197533, "ox": 0.011904761904761908, "oy": 0.14197530864197533, "term": "neural", "cat25k": 12, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.9167803547066848, "os": 0.12948778451374016, "bg": 9.260428817120977e-06}, {"x": 0.01785714285714286, "y": 0.17901234567901236, "ox": 0.01785714285714286, "oy": 0.17901234567901236, "term": "networks", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 3, "s": 0.9392905866302865, "os": 0.16038044070134677, "bg": 1.0460941785512773e-06}, {"x": 0.15476190476190477, "y": 0.14814814814814817, "ox": 0.15476190476190477, "oy": 0.14814814814814817, "term": "patterns", "cat25k": 12, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 26, "s": 0.577762619372442, "os": -0.006389080480633108, "bg": 3.858547798493978e-06}, {"x": 0.005952380952380953, "y": 0.154320987654321, "ox": 0.005952380952380953, "oy": 0.154320987654321, "term": "series", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 1, "s": 0.931787175989086, "os": 0.1476748829273605, "bg": 3.2173200116568453e-07}, {"x": 0.11904761904761905, "y": 0.06790123456790124, "ox": 0.11904761904761905, "oy": 0.06790123456790124, "term": "common", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 20, "s": 0.29195088676671216, "os": -0.0506407231277453, "bg": 5.743616484423877e-07}, {"x": 0.029761904761904767, "y": 0.14814814814814817, "ox": 0.029761904761904767, "oy": 0.14814814814814817, "term": "excellence", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.9045020463847203, "os": 0.1178712745489527, "bg": 4.016795190261737e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "spotfire", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 9.159186664224217e-05}, {"x": 0.2976190476190476, "y": 0.25308641975308643, "ox": 0.2976190476190476, "oy": 0.25308641975308643, "term": "plus", "cat25k": 21, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 50, "s": 0.3328785811732606, "os": -0.04410643627255234, "bg": 1.919082983699963e-06}, {"x": 0.10714285714285715, "y": 0.08641975308641976, "ox": 0.10714285714285715, "oy": 0.08641975308641976, "term": "query", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 18, "s": 0.5545702592087313, "os": -0.020401495625657975, "bg": 2.038803203405821e-06}, {"x": 0.4166666666666667, "y": 0.0925925925925926, "ox": 0.4166666666666667, "oy": 0.0925925925925926, "term": "nosql", "cat25k": 8, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 72, "s": 0.015006821282401094, "os": -0.3219588339928123, "bg": 0.0016306334167392955}, {"x": 0.2142857142857143, "y": 0.030864197530864203, "ox": 0.2142857142857143, "oy": 0.030864197530864203, "term": "mongodb", "cat25k": 3, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 36, "s": 0.049795361527967257, "os": -0.1821250952916833, "bg": 0.0007687908420134819}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "cassandra", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 2.2437245828074604e-05}, {"x": 0.2261904761904762, "y": 0.04938271604938272, "ox": 0.2261904761904762, "oy": 0.04938271604938272, "term": "hbase", "cat25k": 4, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 38, "s": 0.05320600272851296, "os": -0.1755545068428504, "bg": 0.0008625053906586915}, {"x": 0.2678571428571429, "y": 0.0925925925925926, "ox": 0.2678571428571429, "oy": 0.0925925925925926, "term": "security", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 45, "s": 0.055934515688949527, "os": -0.17402983990997206, "bg": 5.214654766888224e-07}, {"x": 0.10714285714285715, "y": 0.04320987654320988, "ox": 0.10714285714285715, "oy": 0.04320987654320988, "term": "built", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 18, "s": 0.2353342428376535, "os": -0.06334628090173158, "bg": 8.115626380518769e-07}, {"x": 0.08928571428571429, "y": 0.08641975308641976, "ox": 0.08928571428571429, "oy": 0.08641975308641976, "term": "quickly", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 15, "s": 0.5832196452933152, "os": -0.0026500163357171397, "bg": 1.44034225313285e-06}, {"x": 0.053571428571428575, "y": 0.06790123456790124, "ox": 0.053571428571428575, "oy": 0.06790123456790124, "term": "prototypes", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6193724420190996, "os": 0.014448034268704404, "bg": 1.2911639197154276e-05}, {"x": 0.04761904761904762, "y": 0.02469135802469136, "ox": 0.04761904761904762, "oy": 0.02469135802469136, "term": "exciting", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5368349249658936, "os": -0.022579591244055616, "bg": 1.4112021343021081e-06}, {"x": 0.09523809523809525, "y": 0.09876543209876544, "ox": 0.09523809523809525, "oy": 0.09876543209876544, "term": "so", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 16, "s": 0.596862210095498, "os": 0.0037027625512759954, "bg": 9.668897503403192e-08}, {"x": 0.06547619047619048, "y": 0.04320987654320988, "ox": 0.06547619047619048, "oy": 0.04320987654320988, "term": "getting", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.5463847203274215, "os": -0.02192616255853632, "bg": 3.846566571240117e-07}, {"x": 0.07738095238095238, "y": 0.01851851851851852, "ox": 0.07738095238095238, "oy": 0.01851851851851852, "term": "backend", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.24965893587994545, "os": -0.058300359385777045, "bg": 1.3436683826431637e-05}, {"x": 0.1130952380952381, "y": 0.05555555555555556, "ox": 0.1130952380952381, "oy": 0.05555555555555556, "term": "ll", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 19, "s": 0.2605729877216917, "os": -0.05699350201473844, "bg": 1.3337387200417217e-07}, {"x": 0.03571428571428572, "y": 0.16666666666666669, "ox": 0.03571428571428572, "oy": 0.16666666666666669, "term": "alongside", "cat25k": 14, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 6, "s": 0.9195088676671214, "os": 0.13035902276109923, "bg": 1.3607568447100165e-05}, {"x": 0.06547619047619048, "y": 0.07407407407407408, "ox": 0.06547619047619048, "oy": 0.07407407407407408, "term": "performing", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.6043656207366985, "os": 0.008748684067230547, "bg": 2.1890089290625955e-06}, {"x": 0.08928571428571429, "y": 0.030864197530864203, "ox": 0.08928571428571429, "oy": 0.030864197530864203, "term": "colleagues", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.2551159618008185, "os": -0.0578647402620975, "bg": 4.05321917306425e-06}, {"x": 0.06547619047619048, "y": 0.07407407407407408, "ox": 0.06547619047619048, "oy": 0.07407407407407408, "term": "offer", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.6043656207366985, "os": 0.008748684067230547, "bg": 3.6410953044743656e-07}, {"x": 0.19642857142857142, "y": 0.04938271604938272, "ox": 0.19642857142857142, "oy": 0.04938271604938272, "term": "competitive", "cat25k": 4, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 33, "s": 0.07639836289222374, "os": -0.14596870802628237, "bg": 3.2208660586745168e-06}, {"x": 0.09523809523809525, "y": 0.012345679012345682, "ox": 0.09523809523809525, "oy": 0.012345679012345682, "term": "salary", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 16, "s": 0.17462482946793997, "os": -0.08218680800087123, "bg": 1.693089921557733e-06}, {"x": 0.06547619047619048, "y": 0.012345679012345682, "ox": 0.06547619047619048, "oy": 0.012345679012345682, "term": "equity", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28035470668485674, "os": -0.05260100918430319, "bg": 8.315939426569279e-07}, {"x": 0.05952380952380953, "y": 0.03703703703703704, "ox": 0.05952380952380953, "oy": 0.03703703703703704, "term": "options", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5450204638472033, "os": -0.022143972120376082, "bg": 2.322444896384157e-07}, {"x": 0.13095238095238096, "y": 0.05555555555555556, "ox": 0.13095238095238096, "oy": 0.05555555555555556, "term": "reviews", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 22, "s": 0.1984993178717599, "os": -0.07474498130467927, "bg": 2.014355505948694e-07}, {"x": 0.05952380952380953, "y": 0.07407407407407408, "ox": 0.05952380952380953, "oy": 0.07407407407407408, "term": "interest", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 10, "s": 0.6234652114597544, "os": 0.014665843830544159, "bg": 3.655104656860852e-07}, {"x": 0.02380952380952381, "y": 0.04938271604938272, "ox": 0.02380952380952381, "oy": 0.04938271604938272, "term": "topic", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.6780354706684857, "os": 0.025628925109812323, "bg": 1.6095679802847236e-07}, {"x": 0.25, "y": 0.10493827160493828, "ox": 0.25, "oy": 0.10493827160493828, "term": "any", "cat25k": 9, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 42, "s": 0.07912687585266029, "os": -0.1440084219697245, "bg": 1.659989261388791e-07}, {"x": 0.1904761904761905, "y": 0.08641975308641976, "ox": 0.1904761904761905, "oy": 0.08641975308641976, "term": "job", "cat25k": 7, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 32, "s": 0.13301500682128242, "os": -0.1032417323120485, "bg": 5.173957593062555e-07}, {"x": 0.05952380952380953, "y": 0.07407407407407408, "ox": 0.05952380952380953, "oy": 0.07407407407407408, "term": "important", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 10, "s": 0.6234652114597544, "os": 0.014665843830544159, "bg": 3.230303844508678e-07}, {"x": 0.2619047619047619, "y": 0.4197530864197531, "ox": 0.2619047619047619, "oy": 0.4197530864197531, "term": "relevant", "cat25k": 35, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 44, "s": 0.93656207366985, "os": 0.15704069408647037, "bg": 5.027690557024673e-06}, {"x": 0.20833333333333334, "y": 0.05555555555555556, "ox": 0.20833333333333334, "oy": 0.05555555555555556, "term": "access", "cat25k": 5, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 35, "s": 0.06957708049113233, "os": -0.1516680582277562, "bg": 4.0349639160513285e-07}, {"x": 0.25, "y": 0.08641975308641976, "ox": 0.25, "oy": 0.08641975308641976, "term": "modelling", "cat25k": 7, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 42, "s": 0.06070941336971351, "os": -0.16241332994518462, "bg": 1.6861304330284047e-05}, {"x": 0.6488095238095238, "y": 0.4814814814814815, "ox": 0.6488095238095238, "oy": 0.4814814814814815, "term": "e", "cat25k": 42, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 83, "ncat": 130, "s": 0.059345156889495224, "os": -0.1662249972773805, "bg": 7.181473784174086e-07}, {"x": 0.5416666666666667, "y": 0.4320987654320988, "ox": 0.5416666666666667, "oy": 0.4320987654320988, "term": "g", "cat25k": 36, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 99, "s": 0.11664392905866303, "os": -0.10879587613896252, "bg": 1.5008002095573478e-06}, {"x": 0.06547619047619048, "y": 0.154320987654321, "ox": 0.06547619047619048, "oy": 0.154320987654321, "term": "b", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 11, "s": 0.8778990450204639, "os": 0.08850328529422441, "bg": 1.7148068930950082e-07}, {"x": 0.005952380952380953, "y": 0.14197530864197533, "ox": 0.005952380952380953, "oy": 0.14197530864197533, "term": "ph", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.9236016371077763, "os": 0.13540494427705377, "bg": 2.193487836218844e-06}, {"x": 0.07142857142857144, "y": 0.17283950617283952, "ox": 0.07142857142857144, "oy": 0.17283950617283952, "term": "d", "cat25k": 14, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 12, "s": 0.8915416098226467, "os": 0.10099103350637093, "bg": 2.2123868230705418e-07}, {"x": 0.39285714285714285, "y": 0.611111111111111, "ox": 0.39285714285714285, "oy": 0.611111111111111, "term": "all", "cat25k": 54, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 108, "ncat": 68, "s": 0.9638472032742156, "os": 0.21704722837332557, "bg": 1.7403629247234833e-07}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "collaboratively", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 4.112634785179093e-05}, {"x": 0.4107142857142857, "y": 0.19135802469135804, "ox": 0.4107142857142857, "oy": 0.19135802469135804, "term": "deliver", "cat25k": 16, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 71, "s": 0.034106412005457026, "os": -0.2178821650270447, "bg": 8.90293652014237e-06}, {"x": 0.08333333333333334, "y": 0.04320987654320988, "ox": 0.08333333333333334, "oy": 0.04320987654320988, "term": "against", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.3608458390177353, "os": -0.03967764184847714, "bg": 2.8500378098230297e-07}, {"x": 0.0, "y": 0.0925925925925926, "ox": 0.0, "oy": 0.0925925925925926, "term": "classification", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.8799454297407913, "os": 0.09224234943914038, "bg": 1.4972100987020788e-06}, {"x": 0.02380952380952381, "y": 0.06790123456790124, "ox": 0.02380952380952381, "oy": 0.06790123456790124, "term": "recommendation", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.7721691678035472, "os": 0.04403383308527245, "bg": 1.6613551297219313e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "sentiment", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 4.675378715416321e-06}, {"x": 0.06547619047619048, "y": 0.04938271604938272, "ox": 0.06547619047619048, "oy": 0.04938271604938272, "term": "move", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.5586630286493861, "os": -0.015791193233382947, "bg": 5.411580688683169e-07}, {"x": 0.005952380952380953, "y": 0.05555555555555556, "ox": 0.005952380952380953, "oy": 0.05555555555555556, "term": "descriptive", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7905866302864939, "os": 0.049515373724906525, "bg": 4.79594339923519e-06}, {"x": 0.7083333333333333, "y": 0.5493827160493827, "ox": 0.7083333333333333, "oy": 0.5493827160493827, "term": "implement", "cat25k": 48, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 151, "s": 0.06343792633015007, "os": -0.15791193233382939, "bg": 2.3715171761932127e-05}, {"x": 0.01785714285714286, "y": 0.09876543209876544, "ox": 0.01785714285714286, "oy": 0.09876543209876544, "term": "audiences", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.8656207366984994, "os": 0.08062583947435292, "bg": 7.627970241280728e-06}, {"x": 0.10119047619047619, "y": 0.10493827160493828, "ox": 0.10119047619047619, "oy": 0.10493827160493828, "term": "sales", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 17, "s": 0.597544338335607, "os": 0.003920572113115764, "bg": 3.9226799629519023e-07}, {"x": 0.19642857142857142, "y": 0.20987654320987656, "ox": 0.19642857142857142, "oy": 0.20987654320987656, "term": "client", "cat25k": 17, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 33, "s": 0.6145975443383357, "os": 0.013540494427705385, "bg": 1.9337275770938947e-06}, {"x": 0.13095238095238096, "y": 0.2777777777777778, "ox": 0.13095238095238096, "oy": 0.2777777777777778, "term": "present", "cat25k": 23, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 22, "s": 0.9304229195088677, "os": 0.14611391440084218, "bg": 1.3463624917637028e-06}, {"x": 0.16071428571428573, "y": 0.030864197530864203, "ox": 0.16071428571428573, "oy": 0.030864197530864203, "term": "junior", "cat25k": 3, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 27, "s": 0.09345156889495225, "os": -0.12887065742186082, "bg": 2.2096249814123927e-06}, {"x": 0.3154761904761905, "y": 0.4506172839506173, "ox": 0.3154761904761905, "oy": 0.4506172839506173, "term": "level", "cat25k": 38, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 54, "s": 0.9229195088676672, "os": 0.13446110284241475, "bg": 1.2828711556446784e-06}, {"x": 0.02380952380952381, "y": 0.06790123456790124, "ox": 0.02380952380952381, "oy": 0.06790123456790124, "term": "adapt", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.7721691678035472, "os": 0.04403383308527245, "bg": 6.2732607280077245e-06}, {"x": 0.24404761904761904, "y": 0.030864197530864203, "ox": 0.24404761904761904, "oy": 0.030864197530864203, "term": "azure", "cat25k": 3, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 41, "s": 0.03547066848567531, "os": -0.21171089410825134, "bg": 8.00749919707413e-05}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "spss", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 1.6143124945769186e-05}, {"x": 0.01785714285714286, "y": 0.06790123456790124, "ox": 0.01785714285714286, "oy": 0.06790123456790124, "term": "experimentation", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.7953615279672578, "os": 0.04995099284858606, "bg": 1.4208077291940469e-05}, {"x": 0.10714285714285715, "y": 0.11111111111111112, "ox": 0.10714285714285715, "oy": 0.11111111111111112, "term": "tests", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 18, "s": 0.5989085948158254, "os": 0.0041383816749555186, "bg": 1.696401882129615e-06}, {"x": 0.011904761904761908, "y": 0.19753086419753088, "ox": 0.011904761904761908, "oy": 0.19753086419753088, "term": "economics", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 2, "s": 0.9522510231923601, "os": 0.1847025084401205, "bg": 2.195230571039195e-06}, {"x": 0.27380952380952384, "y": 0.17901234567901236, "ox": 0.27380952380952384, "oy": 0.17901234567901236, "term": "preferred", "cat25k": 15, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 46, "s": 0.14733969986357437, "os": -0.0940574291211384, "bg": 7.5961128563602415e-06}, {"x": 0.13690476190476192, "y": 0.0617283950617284, "ox": 0.13690476190476192, "oy": 0.0617283950617284, "term": "google", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 23, "s": 0.19918144611186903, "os": -0.07452717174283952, "bg": 7.794477853361119e-07}, {"x": 0.33333333333333337, "y": 0.14197530864197533, "ox": 0.33333333333333337, "oy": 0.14197530864197533, "term": "highly", "cat25k": 12, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 57, "s": 0.04433833560709414, "os": -0.19003884270519475, "bg": 3.7240503805362693e-06}, {"x": 0.07738095238095238, "y": 0.08024691358024692, "ox": 0.07738095238095238, "oy": 0.08024691358024692, "term": "online", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 13, "s": 0.5914051841746248, "os": 0.0030493338657567037, "bg": 8.646150719982015e-08}, {"x": 0.4761904761904762, "y": 0.1234567901234568, "ox": 0.4761904761904762, "oy": 0.1234567901234568, "term": "platform", "cat25k": 10, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 86, "s": 0.01227830832196453, "os": -0.3504555850001815, "bg": 5.5734913603917045e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "climate", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 4.761382899223506e-07}, {"x": 0.01785714285714286, "y": 0.11111111111111112, "ox": 0.01785714285714286, "oy": 0.11111111111111112, "term": "developments", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.8847203274215553, "os": 0.09289577812465967, "bg": 2.625425483017342e-06}, {"x": 0.125, "y": 0.25925925925925924, "ox": 0.125, "oy": 0.25925925925925924, "term": "questions", "cat25k": 21, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 21, "s": 0.9222373806275581, "os": 0.13362616618869566, "bg": 8.03518159622007e-07}, {"x": 0.05952380952380953, "y": 0.0925925925925926, "ox": 0.05952380952380953, "oy": 0.0925925925925926, "term": "sciences", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 10, "s": 0.7223738062755798, "os": 0.03307075180600429, "bg": 1.1633722485228373e-06}, {"x": 0.0, "y": 0.04938271604938272, "ox": 0.0, "oy": 0.04938271604938272, "term": "pytorch", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.786493860845839, "os": 0.04929756416306676, "bg": 0.0001500543947180853}, {"x": 0.005952380952380953, "y": 0.04938271604938272, "ox": 0.005952380952380953, "oy": 0.04938271604938272, "term": "keras", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.7646657571623465, "os": 0.04338040439975315, "bg": 0.0001688096108938469}, {"x": 0.15476190476190477, "y": 0.09876543209876544, "ox": 0.15476190476190477, "oy": 0.09876543209876544, "term": "control", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 26, "s": 0.26330150068212826, "os": -0.05546883508186011, "bg": 3.8948914395224327e-07}, {"x": 0.04761904761904762, "y": 0.41358024691358025, "ox": 0.04761904761904762, "oy": 0.41358024691358025, "term": "optimization", "cat25k": 34, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 8, "s": 0.9856753069577081, "os": 0.36392347624060695, "bg": 9.397458457360208e-06}, {"x": 0.08333333333333334, "y": 0.17901234567901236, "ox": 0.08333333333333334, "oy": 0.17901234567901236, "term": "critical", "cat25k": 15, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 14, "s": 0.8874488403819918, "os": 0.09529168330489708, "bg": 1.8597519934216676e-06}, {"x": 0.09523809523809525, "y": 0.14197530864197533, "ox": 0.09523809523809525, "oy": 0.14197530864197533, "term": "they", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 16, "s": 0.7837653478854024, "os": 0.04664754782734963, "bg": 8.83021725249545e-08}, {"x": 0.01785714285714286, "y": 0.308641975308642, "ox": 0.01785714285714286, "oy": 0.308641975308642, "term": "analyses", "cat25k": 25, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 3, "s": 0.9781718963165075, "os": 0.28921479652956766, "bg": 1.0513016353988733e-05}, {"x": 0.16071428571428573, "y": 0.06790123456790124, "ox": 0.16071428571428573, "oy": 0.06790123456790124, "term": "full", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 27, "s": 0.15211459754433834, "os": -0.09206084147094058, "bg": 2.451329521625592e-07}, {"x": 0.03571428571428572, "y": 0.05555555555555556, "ox": 0.03571428571428572, "oy": 0.05555555555555556, "term": "lifecycle", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.6371077762619373, "os": 0.019929574908338472, "bg": 8.69389827443507e-06}, {"x": 0.5119047619047619, "y": 0.14814814814814817, "ox": 0.5119047619047619, "oy": 0.14814814814814817, "term": "architecture", "cat25k": 12, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 93, "s": 0.009549795361527969, "os": -0.36141866627944963, "bg": 5.627547953502505e-06}, {"x": 0.08928571428571429, "y": 0.12962962962962962, "ox": 0.08928571428571429, "oy": 0.12962962962962962, "term": "ms", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 15, "s": 0.7564802182810368, "os": 0.04029476894035648, "bg": 1.0516151560003167e-06}, {"x": 0.08333333333333334, "y": 0.05555555555555556, "ox": 0.08333333333333334, "oy": 0.05555555555555556, "term": "exposure", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 14, "s": 0.49317871759890863, "os": -0.027407703198170395, "bg": 1.6842865620258481e-06}, {"x": 0.15476190476190477, "y": 0.07407407407407408, "ox": 0.15476190476190477, "oy": 0.07407407407407408, "term": "very", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 26, "s": 0.18076398362892226, "os": -0.0800087123824736, "bg": 2.2684533952786665e-07}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "stochastic", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 4.4102551663382865e-06}, {"x": 0.005952380952380953, "y": 0.16049382716049385, "ox": 0.005952380952380953, "oy": 0.16049382716049385, "term": "linear", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 1, "s": 0.9345156889495225, "os": 0.15380985225251387, "bg": 2.342584909701377e-06}, {"x": 0.44642857142857145, "y": 0.154320987654321, "ox": 0.44642857142857145, "oy": 0.154320987654321, "term": "good", "cat25k": 13, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 78, "s": 0.017735334242837655, "os": -0.2901949395578466, "bg": 5.629905548851034e-07}, {"x": 0.23214285714285715, "y": 0.16049382716049385, "ox": 0.23214285714285715, "oy": 0.16049382716049385, "term": "structures", "cat25k": 13, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 39, "s": 0.20668485675306958, "os": -0.07104221875340327, "bg": 5.08372981211786e-06}, {"x": 0.4761904761904762, "y": 0.3209876543209877, "ox": 0.4761904761904762, "oy": 0.3209876543209877, "term": "applications", "cat25k": 26, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 86, "s": 0.06480218281036836, "os": -0.15413656659527353, "bg": 2.7757959444012093e-06}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "specifically", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 5.339295037621055e-07}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "celery", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 8.804018153885432e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "django", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 2.2212905142565202e-05}, {"x": 0.011904761904761908, "y": 0.08641975308641976, "ox": 0.011904761904761908, "oy": 0.08641975308641976, "term": "energy", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 2, "s": 0.854706684856753, "os": 0.07427306058735977, "bg": 2.722068310525421e-07}, {"x": 0.22023809523809523, "y": 0.05555555555555556, "ox": 0.22023809523809523, "oy": 0.05555555555555556, "term": "mapreduce", "cat25k": 5, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 37, "s": 0.06002728512960437, "os": -0.16350237775438342, "bg": 0.0008625053906586915}, {"x": 0.08928571428571429, "y": 0.03703703703703704, "ox": 0.08928571428571429, "oy": 0.03703703703703704, "term": "hdfs", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.28717598908594816, "os": -0.05172977093694413, "bg": 0.00027371549229686403}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "particularly", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 4.2586584203897754e-07}, {"x": 0.053571428571428575, "y": 0.1234567901234568, "ox": 0.053571428571428575, "oy": 0.1234567901234568, "term": "packages", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 9, "s": 0.8465211459754434, "os": 0.06966275819508477, "bg": 1.2705877940676868e-06}, {"x": 0.33333333333333337, "y": 0.0617283950617284, "ox": 0.33333333333333337, "oy": 0.0617283950617284, "term": "distributed", "cat25k": 5, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 57, "s": 0.02114597544338336, "os": -0.2697934439321886, "bg": 4.751504808664702e-06}, {"x": 0.13690476190476192, "y": 0.154320987654321, "ox": 0.13690476190476192, "oy": 0.154320987654321, "term": "computing", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 23, "s": 0.6302864938608458, "os": 0.017497368134461094, "bg": 2.6310200719699094e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "interfaces", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 1.2975649814460372e-06}, {"x": 0.20833333333333334, "y": 0.11111111111111112, "ox": 0.20833333333333334, "oy": 0.11111111111111112, "term": "proven", "cat25k": 9, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 35, "s": 0.14392905866302866, "os": -0.09645333430137584, "bg": 8.13653598647861e-06}, {"x": 0.15476190476190477, "y": 0.25308641975308643, "ox": 0.15476190476190477, "oy": 0.25308641975308643, "term": "take", "cat25k": 21, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 26, "s": 0.8894952251023193, "os": 0.09790539804697426, "bg": 5.066996125275625e-07}, {"x": 0.24404761904761904, "y": 0.0617283950617284, "ox": 0.24404761904761904, "oy": 0.0617283950617284, "term": "meet", "cat25k": 5, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 41, "s": 0.0504774897680764, "os": -0.18103604748248447, "bg": 1.0922812414735803e-06}, {"x": 0.029761904761904767, "y": 0.07407407407407408, "ox": 0.029761904761904767, "oy": 0.07407407407407408, "term": "*", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.7742155525238745, "os": 0.044251642647112205, "bg": 0.0}, {"x": 0.16071428571428573, "y": 0.3271604938271605, "ox": 0.16071428571428573, "oy": 0.3271604938271605, "term": "clients", "cat25k": 27, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 27, "s": 0.9433833560709414, "os": 0.1656078701855011, "bg": 3.68786942655751e-06}, {"x": 0.2142857142857143, "y": 0.10493827160493828, "ox": 0.2142857142857143, "oy": 0.10493827160493828, "term": "familiarity", "cat25k": 9, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 36, "s": 0.11732605729877217, "os": -0.10850546338984282, "bg": 5.6663129675710494e-05}, {"x": 0.27380952380952384, "y": 0.16666666666666669, "ox": 0.27380952380952384, "oy": 0.16666666666666669, "term": "intelligence", "cat25k": 14, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 46, "s": 0.12005457025920874, "os": -0.10632736777144514, "bg": 4.6144090738751405e-06}, {"x": 0.25595238095238093, "y": 0.02469135802469136, "ox": 0.25595238095238093, "oy": 0.02469135802469136, "term": "bi", "cat25k": 2, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 43, "s": 0.032060027285129605, "os": -0.22968018296003195, "bg": 7.133801118428232e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "pentaho", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 9.288860334244157e-05}, {"x": 0.6904761904761904, "y": 0.3209876543209877, "ox": 0.6904761904761904, "oy": 0.3209876543209877, "term": "processes", "cat25k": 26, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 143, "s": 0.008867667121418827, "os": -0.36715431807456345, "bg": 9.859126979361941e-06}, {"x": 0.22023809523809523, "y": 0.16666666666666669, "ox": 0.22023809523809523, "oy": 0.16666666666666669, "term": "expert", "cat25k": 14, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 37, "s": 0.26739427012278305, "os": -0.053072929901622684, "bg": 3.400286389121124e-06}, {"x": 0.25595238095238093, "y": 0.11111111111111112, "ox": 0.25595238095238093, "oy": 0.11111111111111112, "term": "deployment", "cat25k": 9, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 43, "s": 0.07980900409276943, "os": -0.1437906124078847, "bg": 1.2346913453804125e-05}, {"x": 0.14285714285714288, "y": 0.09876543209876544, "ox": 0.14285714285714288, "oy": 0.09876543209876544, "term": "integrity", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 24, "s": 0.3335607094133697, "os": -0.043634515555232886, "bg": 5.9908498754427425e-06}, {"x": 0.15476190476190477, "y": 0.05555555555555556, "ox": 0.15476190476190477, "oy": 0.05555555555555556, "term": "back", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 26, "s": 0.14188267394270124, "os": -0.09841362035793372, "bg": 1.4340419650337792e-07}, {"x": 0.08928571428571429, "y": 0.154320987654321, "ox": 0.08928571428571429, "oy": 0.154320987654321, "term": "health", "cat25k": 13, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 15, "s": 0.8376534788540246, "os": 0.06483464624096998, "bg": 1.816022851796434e-07}, {"x": 0.08333333333333334, "y": 0.08641975308641976, "ox": 0.08333333333333334, "oy": 0.08641975308641976, "term": "benefits", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 14, "s": 0.5934515688949523, "os": 0.0032671434275964722, "bg": 7.302354061182695e-07}, {"x": 0.07142857142857144, "y": 0.01851851851851852, "ox": 0.07142857142857144, "oy": 0.01851851851851852, "term": "bonus", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.2830832196452933, "os": -0.05238319962246343, "bg": 1.0742629759240122e-06}, {"x": 0.14285714285714288, "y": 0.19135802469135804, "ox": 0.14285714285714288, "oy": 0.19135802469135804, "term": "great", "cat25k": 16, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 24, "s": 0.7858117326057299, "os": 0.04839002432206774, "bg": 3.647286143076304e-07}, {"x": 0.1488095238095238, "y": 0.0617283950617284, "ox": 0.1488095238095238, "oy": 0.0617283950617284, "term": "life", "cat25k": 5, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 25, "s": 0.16439290586630287, "os": -0.08636149126946674, "bg": 2.282614699921276e-07}, {"x": 0.07738095238095238, "y": 0.0617283950617284, "ox": 0.07738095238095238, "oy": 0.0617283950617284, "term": "culture", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.5607094133697135, "os": -0.015355574109703424, "bg": 6.694504940777499e-07}, {"x": 0.36904761904761907, "y": 0.17283950617283952, "ox": 0.36904761904761907, "oy": 0.17283950617283952, "term": "3", "cat25k": 14, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 63, "s": 0.04160982264665757, "os": -0.19486695465930956, "bg": 0.0}, {"x": 0.08333333333333334, "y": 0.17283950617283952, "ox": 0.08333333333333334, "oy": 0.17283950617283952, "term": "scientist", "cat25k": 14, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 14, "s": 0.8785811732605731, "os": 0.08915671397974372, "bg": 9.647058904597133e-06}, {"x": 0.01785714285714286, "y": 0.04938271604938272, "ox": 0.01785714285714286, "oy": 0.04938271604938272, "term": "valuable", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.7135061391541611, "os": 0.031546084873125935, "bg": 1.2544706912025168e-06}, {"x": 0.04761904761904762, "y": 0.00617283950617284, "ox": 0.04761904761904762, "oy": 0.00617283950617284, "term": "undertake", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3417462482946794, "os": -0.04098449921951574, "bg": 3.1468135366128254e-06}, {"x": 0.10119047619047619, "y": 0.07407407407407408, "ox": 0.10119047619047619, "oy": 0.07407407407407408, "term": "amounts", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 17, "s": 0.49522510231923605, "os": -0.02675427451265111, "bg": 1.9534483162201715e-06}, {"x": 0.17261904761904762, "y": 0.09876543209876544, "ox": 0.17261904761904762, "oy": 0.09876543209876544, "term": "challenges", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 29, "s": 0.2032742155525239, "os": -0.07322031437180092, "bg": 4.041141877621383e-06}, {"x": 0.04166666666666667, "y": 0.17283950617283952, "ox": 0.04166666666666667, "oy": 0.17283950617283952, "term": "phd", "cat25k": 14, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.9201909959072305, "os": 0.13057683232293898, "bg": 5.946337531787422e-06}, {"x": 0.005952380952380953, "y": 0.07407407407407408, "ox": 0.005952380952380953, "oy": 0.07407407407407408, "term": "supervised", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8403819918144612, "os": 0.06792028170036664, "bg": 8.004302620516321e-06}, {"x": 0.125, "y": 0.20987654320987656, "ox": 0.125, "oy": 0.20987654320987656, "term": "senior", "cat25k": 17, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 21, "s": 0.8724420190995907, "os": 0.0845464115874687, "bg": 1.9284178989578447e-06}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "researcher", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 2.2991635478786852e-06}, {"x": 0.04761904761904762, "y": 0.0617283950617284, "ox": 0.04761904761904762, "oy": 0.0617283950617284, "term": "exploration", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.616643929058663, "os": 0.014230224706864629, "bg": 3.301887744254853e-06}, {"x": 0.029761904761904767, "y": 0.12962962962962962, "ox": 0.029761904761904767, "oy": 0.12962962962962962, "term": "selection", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 5, "s": 0.8901773533424283, "os": 0.09946636657349257, "bg": 6.905402086756736e-07}, {"x": 0.06547619047619048, "y": 0.0925925925925926, "ox": 0.06547619047619048, "oy": 0.0925925925925926, "term": "include", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 11, "s": 0.6930422919508867, "os": 0.027153592042690675, "bg": 2.8464152965569797e-07}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "behavioral", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 2.3400564889636435e-06}, {"x": 0.04166666666666667, "y": 0.09876543209876544, "ox": 0.04166666666666667, "oy": 0.09876543209876544, "term": "train", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.8165075034106413, "os": 0.05695720042109848, "bg": 1.4409396129147534e-06}, {"x": 0.25, "y": 0.08024691358024692, "ox": 0.25, "oy": 0.08024691358024692, "term": "developers", "cat25k": 7, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 42, "s": 0.058663028649386086, "os": -0.168548299270338, "bg": 4.551643275633944e-06}, {"x": 0.1904761904761905, "y": 0.11728395061728396, "ox": 0.1904761904761905, "oy": 0.11728395061728396, "term": "integrate", "cat25k": 10, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 32, "s": 0.20395634379263303, "os": -0.07256688568628163, "bg": 1.332660209274688e-05}, {"x": 0.10714285714285715, "y": 0.06790123456790124, "ox": 0.10714285714285715, "oy": 0.06790123456790124, "term": "document", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 18, "s": 0.36493860845839016, "os": -0.03880640360111809, "bg": 5.869826220050211e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "thorough", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 4.152559760525654e-06}, {"x": 0.09523809523809525, "y": 0.10493827160493828, "ox": 0.09523809523809525, "oy": 0.10493827160493828, "term": "way", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 16, "s": 0.6098226466575717, "os": 0.009837731876429376, "bg": 2.1595285843334374e-07}, {"x": 0.1130952380952381, "y": 0.07407407407407408, "ox": 0.1130952380952381, "oy": 0.07407407407407408, "term": "order", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 19, "s": 0.3669849931787176, "os": -0.03858859403927832, "bg": 1.84119492980768e-07}, {"x": 0.07738095238095238, "y": 0.0925925925925926, "ox": 0.07738095238095238, "oy": 0.0925925925925926, "term": "promote", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 13, "s": 0.6255115961800819, "os": 0.01531927251606345, "bg": 1.6857452389211246e-06}, {"x": 0.029761904761904767, "y": 0.07407407407407408, "ox": 0.029761904761904767, "oy": 0.07407407407407408, "term": "smes", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.7742155525238745, "os": 0.044251642647112205, "bg": 1.6903161786127278e-05}, {"x": 0.1488095238095238, "y": 0.00617283950617284, "ox": 0.1488095238095238, "oy": 0.00617283950617284, "term": "ingestion", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 25, "s": 0.08117326057298772, "os": -0.1415762151958471, "bg": 4.015406807041171e-05}, {"x": 0.09523809523809525, "y": 0.030864197530864203, "ox": 0.09523809523809525, "oy": 0.030864197530864203, "term": "curation", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.23328785811732608, "os": -0.06378190002541112, "bg": 0.00016254310294783528}, {"x": 0.011904761904761908, "y": 0.10493827160493828, "ox": 0.011904761904761908, "oy": 0.10493827160493828, "term": "experimental", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 2, "s": 0.8840381991814461, "os": 0.0926779685628199, "bg": 1.9515741525507513e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "theoretical", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 1.530295645467226e-06}, {"x": 0.24404761904761904, "y": 0.1234567901234568, "ox": 0.24404761904761904, "oy": 0.1234567901234568, "term": "own", "cat25k": 10, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 41, "s": 0.10163710777626193, "os": -0.11968635423095073, "bg": 5.255938258131397e-07}, {"x": 0.0, "y": 0.04938271604938272, "ox": 0.0, "oy": 0.04938271604938272, "term": "assessment", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.786493860845839, "os": 0.04929756416306676, "bg": 2.6836754438061176e-07}, {"x": 0.1130952380952381, "y": 0.06790123456790124, "ox": 0.1130952380952381, "oy": 0.06790123456790124, "term": "review", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 19, "s": 0.330150068212824, "os": -0.04472356336443169, "bg": 1.7690012880334245e-07}, {"x": 0.10119047619047619, "y": 0.04938271604938272, "ox": 0.10119047619047619, "oy": 0.04938271604938272, "term": "components", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 17, "s": 0.2892223738062756, "os": -0.051294151813264605, "bg": 9.079125871203412e-07}, {"x": 0.029761904761904767, "y": 0.05555555555555556, "ox": 0.029761904761904767, "oy": 0.05555555555555556, "term": "developed", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.6800818553888132, "os": 0.025846734671652084, "bg": 4.4193987297543204e-07}, {"x": 0.029761904761904767, "y": 0.04938271604938272, "ox": 0.029761904761904767, "oy": 0.04938271604938272, "term": "preparation", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.6330150068212824, "os": 0.01971176534649871, "bg": 1.0246059917066814e-06}, {"x": 0.10714285714285715, "y": 0.14814814814814817, "ox": 0.10714285714285715, "oy": 0.14814814814814817, "term": "presentation", "cat25k": 12, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 18, "s": 0.757162346521146, "os": 0.040948197625875773, "bg": 2.2403663447046857e-06}, {"x": 0.005952380952380953, "y": 0.06790123456790124, "ox": 0.005952380952380953, "oy": 0.06790123456790124, "term": "discussion", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.8267394270122783, "os": 0.06178531237521328, "bg": 2.1413257508036796e-07}, {"x": 0.04166666666666667, "y": 0.0617283950617284, "ox": 0.04166666666666667, "oy": 0.0617283950617284, "term": "conferences", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.6391541609822646, "os": 0.02014738447017824, "bg": 1.8151296824740262e-06}, {"x": 0.10119047619047619, "y": 0.03703703703703704, "ox": 0.10119047619047619, "oy": 0.03703703703703704, "term": "meetings", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 17, "s": 0.23465211459754437, "os": -0.06356409046357135, "bg": 1.1670895167300506e-06}, {"x": 0.07738095238095238, "y": 0.08024691358024692, "ox": 0.07738095238095238, "oy": 0.08024691358024692, "term": "levels", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 13, "s": 0.5914051841746248, "os": 0.0030493338657567037, "bg": 7.707415937771153e-07}, {"x": 0.07738095238095238, "y": 0.012345679012345682, "ox": 0.07738095238095238, "oy": 0.012345679012345682, "term": "ingest", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.22919508867667124, "os": -0.06443532871093041, "bg": 6.643929662931302e-05}, {"x": 0.01785714285714286, "y": 0.07407407407407408, "ox": 0.01785714285714286, "oy": 0.07407407407407408, "term": "segmentation", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.8117326057298773, "os": 0.05608596217373943, "bg": 1.2219590936973794e-05}, {"x": 0.4523809523809524, "y": 0.30246913580246915, "ox": 0.4523809523809524, "oy": 0.30246913580246915, "term": "scalable", "cat25k": 25, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 80, "s": 0.0723055934515689, "os": -0.1488728355174792, "bg": 8.085716112387692e-05}, {"x": 0.16071428571428573, "y": 0.0617283950617284, "ox": 0.16071428571428573, "oy": 0.0617283950617284, "term": "i", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 27, "s": 0.14256480218281037, "os": -0.09819581079609396, "bg": 2.397668221774619e-08}, {"x": 0.2142857142857143, "y": 0.09876543209876544, "ox": 0.2142857142857143, "oy": 0.09876543209876544, "term": "efficient", "cat25k": 8, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 36, "s": 0.10504774897680765, "os": -0.1146404327149962, "bg": 4.5079493134848325e-06}, {"x": 0.19642857142857142, "y": 0.05555555555555556, "ox": 0.19642857142857142, "oy": 0.05555555555555556, "term": "automated", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 33, "s": 0.08526603001364257, "os": -0.13983373870112897, "bg": 6.4997303772559574e-06}, {"x": 0.07738095238095238, "y": 0.10493827160493828, "ox": 0.07738095238095238, "oy": 0.10493827160493828, "term": "ad", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 13, "s": 0.6964529331514324, "os": 0.027589211166370198, "bg": 7.606411895900827e-07}, {"x": 0.07142857142857144, "y": 0.08024691358024692, "ox": 0.07142857142857144, "oy": 0.08024691358024692, "term": "hoc", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.607094133697135, "os": 0.008966493629070316, "bg": 1.1103109470219573e-05}, {"x": 0.03571428571428572, "y": 0.10493827160493828, "ox": 0.03571428571428572, "oy": 0.10493827160493828, "term": "behavior", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.8431105047748977, "os": 0.06900932950956547, "bg": 1.4270263401762763e-06}, {"x": 0.06547619047619048, "y": 0.0925925925925926, "ox": 0.06547619047619048, "oy": 0.0925925925925926, "term": "extensive", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 11, "s": 0.6930422919508867, "os": 0.027153592042690675, "bg": 2.3896259353087813e-06}, {"x": 0.1488095238095238, "y": 0.04320987654320988, "ox": 0.1488095238095238, "oy": 0.04320987654320988, "term": "optimizing", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 25, "s": 0.12892223738062758, "os": -0.10476639924492687, "bg": 2.2637678647540186e-05}, {"x": 0.6309523809523809, "y": 0.1358024691358025, "ox": 0.6309523809523809, "oy": 0.1358024691358025, "term": "services", "cat25k": 11, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 124, "s": 0.002046384720327422, "os": -0.49203180019602866, "bg": 5.19283217724943e-07}, {"x": 0.06547619047619048, "y": 0.14814814814814817, "ox": 0.06547619047619048, "oy": 0.14814814814814817, "term": "media", "cat25k": 12, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 11, "s": 0.8703956343792634, "os": 0.08236831596907104, "bg": 3.2326715585146e-07}, {"x": 0.19642857142857142, "y": 0.08641975308641976, "ox": 0.19642857142857142, "oy": 0.08641975308641976, "term": "solid", "cat25k": 7, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 33, "s": 0.1159618008185539, "os": -0.10915889207536211, "bg": 2.50445533278603e-06}, {"x": 0.03571428571428572, "y": 0.08024691358024692, "ox": 0.03571428571428572, "oy": 0.08024691358024692, "term": "experiment", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.776261937244202, "os": 0.04446945220895197, "bg": 2.552377815323106e-06}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "sample", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 2.606276347824954e-07}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "power", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 1.4115384196673458e-07}, {"x": 0.029761904761904767, "y": 0.0617283950617284, "ox": 0.029761904761904767, "oy": 0.0617283950617284, "term": "market", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7189631650750341, "os": 0.03198170399680546, "bg": 1.8461903723202895e-07}, {"x": 0.0, "y": 0.0617283950617284, "ox": 0.0, "oy": 0.0617283950617284, "term": "prediction", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8233287858117326, "os": 0.061567502813373504, "bg": 2.7593514420370637e-06}, {"x": 0.0, "y": 0.0617283950617284, "ox": 0.0, "oy": 0.0617283950617284, "term": "multivariate", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8233287858117326, "os": 0.061567502813373504, "bg": 1.490385152783108e-05}, {"x": 0.04166666666666667, "y": 0.0925925925925926, "ox": 0.04166666666666667, "oy": 0.0925925925925926, "term": "methodology", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.8001364256480219, "os": 0.050822231095945115, "bg": 3.929608564546143e-06}, {"x": 0.16071428571428573, "y": 0.012345679012345682, "ox": 0.16071428571428573, "oy": 0.012345679012345682, "term": "scalability", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 27, "s": 0.07366984993178717, "os": -0.14727556539732095, "bg": 2.873158949097531e-05}, {"x": 0.16666666666666669, "y": 0.08641975308641976, "ox": 0.16666666666666669, "oy": 0.08641975308641976, "term": "1", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 28, "s": 0.1814461118690314, "os": -0.07957309325879405, "bg": 0.0}, {"x": 0.01785714285714286, "y": 0.2160493827160494, "ox": 0.01785714285714286, "oy": 0.2160493827160494, "term": "detection", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 3, "s": 0.956343792633015, "os": 0.19719025665226703, "bg": 4.3019673122938455e-06}, {"x": 0.125, "y": 0.14814814814814817, "ox": 0.125, "oy": 0.14814814814814817, "term": "digital", "cat25k": 12, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 21, "s": 0.650068212824011, "os": 0.02319671833593495, "bg": 4.4454667481260577e-07}, {"x": 0.6011904761904762, "y": 0.154320987654321, "ox": 0.6011904761904762, "oy": 0.154320987654321, "term": "integration", "cat25k": 13, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 114, "s": 0.004774897680763984, "os": -0.44404109340400044, "bg": 8.535614868390185e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "merge", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 2.6366905710132454e-06}, {"x": 0.053571428571428575, "y": 0.030864197530864203, "ox": 0.053571428571428575, "oy": 0.030864197530864203, "term": "join", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5409276944065484, "os": -0.022361781682215844, "bg": 1.6054637509334912e-07}, {"x": 0.029761904761904767, "y": 0.11728395061728396, "ox": 0.029761904761904767, "oy": 0.11728395061728396, "term": "clustering", "cat25k": 10, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.8751705320600274, "os": 0.08719642792318583, "bg": 1.7251138844712794e-05}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "scipy", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 3.0535866283441545e-05}, {"x": 0.08333333333333334, "y": 0.00617283950617284, "ox": 0.08333333333333334, "oy": 0.00617283950617284, "term": "bigquery", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 14, "s": 0.19099590723055934, "os": -0.07648745779939739, "bg": 0.000281333520889014}, {"x": 0.09523809523809525, "y": 0.0925925925925926, "ox": 0.09523809523809525, "oy": 0.0925925925925926, "term": "comfortable", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 16, "s": 0.5845839017735335, "os": -0.002432206773877371, "bg": 3.158891530695917e-06}, {"x": 0.16071428571428573, "y": 0.04938271604938272, "ox": 0.16071428571428573, "oy": 0.04938271604938272, "term": "lake", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 27, "s": 0.11255115961800818, "os": -0.11046574944640071, "bg": 8.201701876572823e-07}, {"x": 0.8154761904761905, "y": 0.09876543209876544, "ox": 0.8154761904761905, "oy": 0.09876543209876544, "term": "etl", "cat25k": 8, "ncat25k": 83, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 185, "s": 0.0, "os": -0.7122735688096707, "bg": 0.0007020448365650098}, {"x": 0.10119047619047619, "y": 0.05555555555555556, "ox": 0.10119047619047619, "oy": 0.05555555555555556, "term": "some", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 17, "s": 0.3274215552523875, "os": -0.04515918248811123, "bg": 9.472869428700352e-08}, {"x": 0.13690476190476192, "y": 0.07407407407407408, "ox": 0.13690476190476192, "oy": 0.07407407407407408, "term": "managing", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 23, "s": 0.23806275579809005, "os": -0.06225723309253277, "bg": 2.782074979623288e-06}, {"x": 0.08928571428571429, "y": 0.08641975308641976, "ox": 0.08928571428571429, "oy": 0.08641975308641976, "term": "collaborative", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 15, "s": 0.5832196452933152, "os": -0.0026500163357171397, "bg": 5.702634922130029e-06}, {"x": 0.07142857142857144, "y": 0.04320987654320988, "ox": 0.07142857142857144, "oy": 0.04320987654320988, "term": "individual", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.4877216916780355, "os": -0.027843322321849932, "bg": 4.003298675969013e-07}, {"x": 0.01785714285714286, "y": 0.05555555555555556, "ox": 0.01785714285714286, "oy": 0.05555555555555556, "term": "skill", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.746930422919509, "os": 0.0376810541982793, "bg": 1.505484857707844e-06}, {"x": 0.011904761904761908, "y": 0.0617283950617284, "ox": 0.011904761904761908, "oy": 0.0617283950617284, "term": "experiences", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.7939972714870396, "os": 0.04973318328674629, "bg": 9.902983769009603e-07}, {"x": 0.0, "y": 0.08024691358024692, "ox": 0.0, "oy": 0.08024691358024692, "term": "econometrics", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.8628922237380628, "os": 0.07997241078883363, "bg": 3.0012766969333882e-05}, {"x": 0.11904761904761905, "y": 0.05555555555555556, "ox": 0.11904761904761905, "oy": 0.05555555555555556, "term": "focused", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 20, "s": 0.23738062755798092, "os": -0.06291066177805205, "bg": 2.894788512241412e-06}, {"x": 0.44642857142857145, "y": 0.3148148148148148, "ox": 0.44642857142857145, "oy": 0.3148148148148148, "term": "end", "cat25k": 26, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 78, "s": 0.09140518417462483, "os": -0.1306857371038589, "bg": 1.1678484425317421e-06}, {"x": 0.08333333333333334, "y": 0.08641975308641976, "ox": 0.08333333333333334, "oy": 0.08641975308641976, "term": "change", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 14, "s": 0.5934515688949523, "os": 0.0032671434275964722, "bg": 2.657707761605816e-07}, {"x": 0.08928571428571429, "y": 0.29629629629629634, "ox": 0.08928571428571429, "oy": 0.29629629629629634, "term": "goals", "cat25k": 24, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 15, "s": 0.9604365620736699, "os": 0.2059389407194976, "bg": 3.4170922302482305e-06}, {"x": 0.15476190476190477, "y": 0.08641975308641976, "ox": 0.15476190476190477, "oy": 0.08641975308641976, "term": "creating", "cat25k": 7, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 26, "s": 0.2203274215552524, "os": -0.06773877373216686, "bg": 2.0528366276143577e-06}, {"x": 0.08333333333333334, "y": 0.11728395061728396, "ox": 0.08333333333333334, "oy": 0.11728395061728396, "term": "clear", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 14, "s": 0.7264665757162346, "os": 0.03394199005336335, "bg": 7.453057341801828e-07}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "fashion", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 7.274328947701029e-07}, {"x": 0.0, "y": 0.04938271604938272, "ox": 0.0, "oy": 0.04938271604938272, "term": "logistic", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.786493860845839, "os": 0.04929756416306676, "bg": 1.1135221959771227e-05}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "competencies", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 5.412600873120178e-06}, {"x": 0.011904761904761908, "y": 0.2777777777777778, "ox": 0.011904761904761908, "oy": 0.2777777777777778, "term": "findings", "cat25k": 23, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 2, "s": 0.9747612551159618, "os": 0.2644571096671144, "bg": 4.505338514600436e-06}, {"x": 0.16666666666666669, "y": 0.08024691358024692, "ox": 0.16666666666666669, "oy": 0.08024691358024692, "term": "supporting", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 28, "s": 0.1664392905866303, "os": -0.08570806258394742, "bg": 3.334959058090432e-06}, {"x": 0.01785714285714286, "y": 0.05555555555555556, "ox": 0.01785714285714286, "oy": 0.05555555555555556, "term": "bayesian", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.746930422919509, "os": 0.0376810541982793, "bg": 1.5002184693145939e-05}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "shaping", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 3.260943999808691e-06}, {"x": 0.07142857142857144, "y": 0.11728395061728396, "ox": 0.07142857142857144, "oy": 0.11728395061728396, "term": "healthcare", "cat25k": 10, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 12, "s": 0.7803547066848567, "os": 0.04577630957999056, "bg": 2.1650842519838194e-06}, {"x": 0.14285714285714288, "y": 0.03703703703703704, "ox": 0.14285714285714288, "oy": 0.03703703703703704, "term": "ecosystem", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 24, "s": 0.1275579809004093, "os": -0.10498420880676662, "bg": 1.2718509288963259e-05}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "ideal", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 8.415280312987225e-07}, {"x": 0.7678571428571428, "y": 0.5493827160493827, "ox": 0.7678571428571428, "oy": 0.5493827160493827, "term": "have", "cat25k": 48, "ncat25k": 79, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 176, "s": 0.03478854024556617, "os": -0.21708352996696556, "bg": 3.477572376939936e-07}, {"x": 0.011904761904761908, "y": 0.05555555555555556, "ox": 0.011904761904761908, "oy": 0.05555555555555556, "term": "organizational", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7673942701227832, "os": 0.04359821396159291, "bg": 1.9477531162721847e-06}, {"x": 0.07142857142857144, "y": 0.14197530864197533, "ox": 0.07142857142857144, "oy": 0.14197530864197533, "term": "player", "cat25k": 12, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 12, "s": 0.84924965893588, "os": 0.07031618688060406, "bg": 7.541767385174437e-07}, {"x": 0.17857142857142858, "y": 0.17283950617283952, "ox": 0.17857142857142858, "oy": 0.17283950617283952, "term": "track", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 30, "s": 0.5791268758526603, "os": -0.005517842233274034, "bg": 1.0311851639550187e-06}, {"x": 0.1488095238095238, "y": 0.07407407407407408, "ox": 0.1488095238095238, "oy": 0.07407407407407408, "term": "record", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 25, "s": 0.20054570259208734, "os": -0.07409155261915999, "bg": 7.230700381272878e-07}, {"x": 0.05952380952380953, "y": 0.0925925925925926, "ox": 0.05952380952380953, "oy": 0.0925925925925926, "term": "previous", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 10, "s": 0.7223738062755798, "os": 0.03307075180600429, "bg": 2.4777090059910655e-07}, {"x": 0.13095238095238096, "y": 0.04320987654320988, "ox": 0.13095238095238096, "oy": 0.04320987654320988, "term": "english", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 22, "s": 0.16234652114597548, "os": -0.08701491995498603, "bg": 3.362743910904216e-07}, {"x": 0.17261904761904762, "y": 0.11111111111111112, "ox": 0.17261904761904762, "oy": 0.11111111111111112, "term": "fast", "cat25k": 9, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 29, "s": 0.23942701227830834, "os": -0.06095037572149417, "bg": 9.994881451083258e-07}, {"x": 0.01785714285714286, "y": 0.08024691358024692, "ox": 0.01785714285714286, "oy": 0.08024691358024692, "term": "researchers", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.829467939972715, "os": 0.06222093149889281, "bg": 1.7225564460215403e-06}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "compensation", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 8.83663776055178e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "type", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 4.4045832110832086e-08}, {"x": 0.07142857142857144, "y": 0.03703703703703704, "ox": 0.07142857142857144, "oy": 0.03703703703703704, "term": "hours", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.43110504774897684, "os": -0.033978291647003306, "bg": 1.8149777225096898e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "0", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 0.0}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "week", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 8.367766343178582e-08}, {"x": 0.06547619047619048, "y": 0.08641975308641976, "ox": 0.06547619047619048, "oy": 0.08641975308641976, "term": "date", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 11, "s": 0.645293315143247, "os": 0.021018622717537294, "bg": 1.022340137843225e-07}, {"x": 0.03571428571428572, "y": 0.07407407407407408, "ox": 0.03571428571428572, "oy": 0.07407407407407408, "term": "possible", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.7517053206002728, "os": 0.03833448288379859, "bg": 2.9169117919149025e-07}, {"x": 0.1488095238095238, "y": 0.14814814814814817, "ox": 0.1488095238095238, "oy": 0.14814814814814817, "term": "future", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 25, "s": 0.5866302864938608, "os": -0.00047192071731949614, "bg": 8.036011829009413e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "contact", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.8577841177757102e-08}, {"x": 0.07738095238095238, "y": 0.0617283950617284, "ox": 0.07738095238095238, "oy": 0.0617283950617284, "term": "staff", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.5607094133697135, "os": -0.015355574109703424, "bg": 3.0331038959474457e-07}, {"x": 0.06547619047619048, "y": 0.04320987654320988, "ox": 0.06547619047619048, "oy": 0.04320987654320988, "term": "member", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.5463847203274215, "os": -0.02192616255853632, "bg": 1.2677595014163884e-07}, {"x": 0.029761904761904767, "y": 0.05555555555555556, "ox": 0.029761904761904767, "oy": 0.05555555555555556, "term": "university", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.6800818553888132, "os": 0.025846734671652084, "bg": 8.989324759486603e-08}, {"x": 0.06547619047619048, "y": 0.012345679012345682, "ox": 0.06547619047619048, "oy": 0.012345679012345682, "term": "greater", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28035470668485674, "os": -0.05260100918430319, "bg": 4.661127764882389e-07}, {"x": 0.613095238095238, "y": 0.1234567901234568, "ox": 0.613095238095238, "oy": 0.1234567901234568, "term": "infrastructure", "cat25k": 10, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 116, "s": 0.002728512960436562, "os": -0.4865502595563945, "bg": 9.31601604683764e-06}, {"x": 0.0, "y": 0.06790123456790124, "ox": 0.0, "oy": 0.06790123456790124, "term": "inventory", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.8390177353342428, "os": 0.06770247213852688, "bg": 9.70607618899213e-07}, {"x": 0.19642857142857142, "y": 0.3395061728395062, "ox": 0.19642857142857142, "oy": 0.3395061728395062, "term": "optimize", "cat25k": 28, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 33, "s": 0.9290586630286495, "os": 0.14237485025592625, "bg": 3.183925540502663e-05}, {"x": 0.03571428571428572, "y": 0.22839506172839508, "ox": 0.03571428571428572, "oy": 0.22839506172839508, "term": "math", "cat25k": 19, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 6, "s": 0.9549795361527968, "os": 0.19170871601263295, "bg": 2.9924942676336554e-06}, {"x": 0.053571428571428575, "y": 0.06790123456790124, "ox": 0.053571428571428575, "oy": 0.06790123456790124, "term": "love", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6193724420190996, "os": 0.014448034268704404, "bg": 1.9883664061797315e-07}, {"x": 0.2261904761904762, "y": 0.2160493827160494, "ox": 0.2261904761904762, "oy": 0.2160493827160494, "term": "what", "cat25k": 18, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 38, "s": 0.5695770804911323, "os": -0.009910335063709308, "bg": 1.7969180455525495e-07}, {"x": 0.13095238095238096, "y": 0.1234567901234568, "ox": 0.13095238095238096, "oy": 0.1234567901234568, "term": "keep", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 22, "s": 0.5743519781718963, "os": -0.007260318727992154, "bg": 7.017005947814726e-07}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "yourself", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 5.03499837050024e-07}, {"x": 0.3154761904761905, "y": 0.25925925925925924, "ox": 0.3154761904761905, "oy": 0.25925925925925924, "term": "make", "cat25k": 21, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 54, "s": 0.2626193724420191, "os": -0.055722946237339865, "bg": 4.738501851266038e-07}, {"x": 0.16071428571428573, "y": 0.04938271604938272, "ox": 0.16071428571428573, "oy": 0.04938271604938272, "term": "things", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 27, "s": 0.11255115961800818, "os": -0.11046574944640071, "bg": 4.7272847014234826e-07}, {"x": 0.125, "y": 0.08024691358024692, "ox": 0.125, "oy": 0.08024691358024692, "term": "no", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 21, "s": 0.33151432469304226, "os": -0.044287944240752164, "bg": 7.255508252226229e-08}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "offices", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 5.202680499033309e-07}, {"x": 0.08333333333333334, "y": 0.02469135802469136, "ox": 0.08333333333333334, "oy": 0.02469135802469136, "term": "don", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.252387448840382, "os": -0.05808254982393726, "bg": 1.5480369274920833e-07}, {"x": 0.19642857142857142, "y": 0.06790123456790124, "ox": 0.19642857142857142, "oy": 0.06790123456790124, "term": "t", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 33, "s": 0.0941336971350614, "os": -0.12756380005082224, "bg": 2.2648034903874868e-07}, {"x": 0.13095238095238096, "y": 0.08024691358024692, "ox": 0.13095238095238096, "oy": 0.08024691358024692, "term": "diverse", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 22, "s": 0.2939972714870396, "os": -0.050205104004065776, "bg": 5.170254256854982e-06}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "healthy", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 6.564425089455881e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "inclusive", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 1.316876020167392e-06}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "sustainable", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 1.194363883695668e-06}, {"x": 0.053571428571428575, "y": 0.08024691358024692, "ox": 0.053571428571428575, "oy": 0.08024691358024692, "term": "successful", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 9, "s": 0.689631650750341, "os": 0.02671797291901115, "bg": 1.0197380024231755e-06}, {"x": 0.01785714285714286, "y": 0.04938271604938272, "ox": 0.01785714285714286, "oy": 0.04938271604938272, "term": "creatively", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.7135061391541611, "os": 0.031546084873125935, "bg": 1.9829591699693453e-05}, {"x": 0.07738095238095238, "y": 0.0617283950617284, "ox": 0.07738095238095238, "oy": 0.0617283950617284, "term": "progress", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.5607094133697135, "os": -0.015355574109703424, "bg": 9.627172065295248e-07}, {"x": 0.10714285714285715, "y": 0.08641975308641976, "ox": 0.10714285714285715, "oy": 0.08641975308641976, "term": "creation", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 18, "s": 0.5545702592087313, "os": -0.020401495625657975, "bg": 2.1697351977189847e-06}, {"x": 0.03571428571428572, "y": 0.08641975308641976, "ox": 0.03571428571428572, "oy": 0.08641975308641976, "term": "broad", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.7987721691678036, "os": 0.05060442153410534, "bg": 1.8624778050848064e-06}, {"x": 0.10119047619047619, "y": 0.19753086419753088, "ox": 0.10119047619047619, "oy": 0.19753086419753088, "term": "master", "cat25k": 16, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 17, "s": 0.888130968622101, "os": 0.09594511199041637, "bg": 1.5854791850365193e-06}, {"x": 0.125, "y": 0.0925925925925926, "ox": 0.125, "oy": 0.0925925925925926, "term": "professional", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 21, "s": 0.4440654843110505, "os": -0.03201800559044542, "bg": 5.840872107753253e-07}, {"x": 0.15476190476190477, "y": 0.08024691358024692, "ox": 0.15476190476190477, "oy": 0.08024691358024692, "term": "if", "cat25k": 7, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 26, "s": 0.20122783083219647, "os": -0.07387374305732022, "bg": 6.871674161463655e-08}, {"x": 0.053571428571428575, "y": 0.06790123456790124, "ox": 0.053571428571428575, "oy": 0.06790123456790124, "term": "additional", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6193724420190996, "os": 0.014448034268704404, "bg": 3.023560901795785e-07}, {"x": 0.2261904761904762, "y": 0.030864197530864203, "ox": 0.2261904761904762, "oy": 0.030864197530864203, "term": "storage", "cat25k": 3, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 38, "s": 0.04229195088676672, "os": -0.19395941481831053, "bg": 1.056158994862818e-06}, {"x": 0.09523809523809525, "y": 0.02469135802469136, "ox": 0.09523809523809525, "oy": 0.02469135802469136, "term": "together", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 16, "s": 0.2121418826739427, "os": -0.06991686935056449, "bg": 4.2453648973383206e-07}, {"x": 0.08928571428571429, "y": 0.1358024691358025, "ox": 0.08928571428571429, "oy": 0.1358024691358025, "term": "financial", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 15, "s": 0.7830832196452934, "os": 0.04642973826550985, "bg": 4.985282838741851e-07}, {"x": 0.005952380952380953, "y": 0.07407407407407408, "ox": 0.005952380952380953, "oy": 0.07407407407407408, "term": "mine", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8403819918144612, "os": 0.06792028170036664, "bg": 8.995882999545707e-07}, {"x": 0.011904761904761908, "y": 0.2160493827160494, "ox": 0.011904761904761908, "oy": 0.2160493827160494, "term": "recognition", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.9590723055934517, "os": 0.20310741641558064, "bg": 3.333447000872957e-06}, {"x": 0.07738095238095238, "y": 0.14197530864197533, "ox": 0.07738095238095238, "oy": 0.14197530864197533, "term": "utilize", "cat25k": 12, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 13, "s": 0.8356070941336972, "os": 0.06439902711729045, "bg": 1.1730952558235628e-05}, {"x": 0.2380952380952381, "y": 0.08641975308641976, "ox": 0.2380952380952381, "oy": 0.08641975308641976, "term": "participate", "cat25k": 7, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 40, "s": 0.07025920873124147, "os": -0.15057901041855737, "bg": 4.401475537617822e-06}, {"x": 0.13095238095238096, "y": 0.41358024691358025, "ox": 0.13095238095238096, "oy": 0.41358024691358025, "term": "marketing", "cat25k": 34, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 22, "s": 0.9768076398362893, "os": 0.28108323955421644, "bg": 1.640794931234421e-06}, {"x": 0.04166666666666667, "y": 0.06790123456790124, "ox": 0.04166666666666667, "oy": 0.06790123456790124, "term": "supply", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.6889495225102319, "os": 0.02628235379533162, "bg": 4.909505070475468e-07}, {"x": 0.053571428571428575, "y": 0.030864197530864203, "ox": 0.053571428571428575, "oy": 0.030864197530864203, "term": "chain", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5409276944065484, "os": -0.022361781682215844, "bg": 7.770223178292682e-07}, {"x": 0.05952380952380953, "y": 0.02469135802469136, "ox": 0.05952380952380953, "oy": 0.02469135802469136, "term": "policy", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.4195088676671214, "os": -0.03441391077068283, "bg": 7.282023636502061e-08}, {"x": 0.029761904761904767, "y": 0.10493827160493828, "ox": 0.029761904761904767, "oy": 0.10493827160493828, "term": "presentations", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.8567530695770805, "os": 0.07492648927287908, "bg": 2.9090103222925592e-06}, {"x": 0.03571428571428572, "y": 0.08641975308641976, "ox": 0.03571428571428572, "oy": 0.08641975308641976, "term": "fields", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.7987721691678036, "os": 0.05060442153410534, "bg": 9.802822835567964e-07}, {"x": 0.25, "y": 0.07407407407407408, "ox": 0.25, "oy": 0.07407407407407408, "term": "background", "cat25k": 6, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 42, "s": 0.054570259208731244, "os": -0.17468326859549138, "bg": 2.0641883053488013e-06}, {"x": 0.06547619047619048, "y": 0.1358024691358025, "ox": 0.06547619047619048, "oy": 0.1358024691358025, "term": "cleaning", "cat25k": 11, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 11, "s": 0.8472032742155525, "os": 0.07009837731876428, "bg": 2.251859114413413e-06}, {"x": 0.01785714285714286, "y": 0.04938271604938272, "ox": 0.01785714285714286, "oy": 0.04938271604938272, "term": "preparing", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.7135061391541611, "os": 0.031546084873125935, "bg": 1.7372868506969366e-06}, {"x": 0.053571428571428575, "y": 0.01851851851851852, "ox": 0.053571428571428575, "oy": 0.01851851851851852, "term": "command", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4154160982264666, "os": -0.0346317203325226, "bg": 4.001533787900903e-07}, {"x": 0.07738095238095238, "y": 0.02469135802469136, "ox": 0.07738095238095238, "oy": 0.02469135802469136, "term": "line", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 13, "s": 0.2844474761255116, "os": -0.052165390060623665, "bg": 1.2137818474312347e-07}, {"x": 0.08928571428571429, "y": 0.06790123456790124, "ox": 0.08928571428571429, "oy": 0.06790123456790124, "term": "leveraging", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 15, "s": 0.5525238744884039, "os": -0.021054924311177253, "bg": 2.323358826239121e-05}, {"x": 0.13690476190476192, "y": 0.20987654320987656, "ox": 0.13690476190476192, "oy": 0.20987654320987656, "term": "acquisition", "cat25k": 17, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 23, "s": 0.8512960436562074, "os": 0.07271209206084148, "bg": 5.91265238629723e-06}, {"x": 0.40476190476190477, "y": 0.8641975308641976, "ox": 0.40476190476190477, "oy": 0.8641975308641976, "term": "specific", "cat25k": 127, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 254, "ncat": 70, "s": 0.9924965893587995, "os": 0.45674665117798674, "bg": 6.005965759155046e-06}, {"x": 0.2380952380952381, "y": 0.14814814814814817, "ox": 0.2380952380952381, "oy": 0.14814814814814817, "term": "continuous", "cat25k": 12, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 40, "s": 0.15552523874488405, "os": -0.08922931716702362, "bg": 5.823569149060076e-06}, {"x": 0.16666666666666669, "y": 0.01851851851851852, "ox": 0.16666666666666669, "oy": 0.01851851851851852, "term": "stream", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 28, "s": 0.07435197817189632, "os": -0.14705775583548117, "bg": 1.602105559479422e-06}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "sure", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 1.6269661467843977e-07}, {"x": 0.029761904761904767, "y": 0.06790123456790124, "ox": 0.029761904761904767, "oy": 0.06790123456790124, "term": "select", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.7503410641200546, "os": 0.03811667332195884, "bg": 1.8959628203112906e-07}, {"x": 0.08333333333333334, "y": 0.03703703703703704, "ox": 0.08333333333333334, "oy": 0.03703703703703704, "term": "defined", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.3246930422919509, "os": -0.045812611173630516, "bg": 7.821694615895446e-07}, {"x": 0.16071428571428573, "y": 0.01851851851851852, "ox": 0.16071428571428573, "oy": 0.01851851851851852, "term": "optimal", "cat25k": 2, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 27, "s": 0.082537517053206, "os": -0.14114059607216758, "bg": 5.7481263024296185e-06}, {"x": 0.01785714285714286, "y": 0.06790123456790124, "ox": 0.01785714285714286, "oy": 0.06790123456790124, "term": "case", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.7953615279672578, "os": 0.04995099284858606, "bg": 1.1881038521916657e-07}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "qe", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 2.0223101253023357e-05}, {"x": 0.04761904761904762, "y": 0.06790123456790124, "ox": 0.04761904761904762, "oy": 0.06790123456790124, "term": "beyond", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.6418826739427013, "os": 0.02036519403201801, "bg": 8.013178388829022e-07}, {"x": 0.02380952380952381, "y": 0.08024691358024692, "ox": 0.02380952380952381, "oy": 0.08024691358024692, "term": "hypotheses", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 4, "s": 0.8137789904502046, "os": 0.0563037717355792, "bg": 1.679684219366759e-05}, {"x": 0.04166666666666667, "y": 0.08641975308641976, "ox": 0.04166666666666667, "oy": 0.08641975308641976, "term": "stay", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.7776261937244202, "os": 0.044687261770791735, "bg": 5.197973869389322e-07}, {"x": 0.053571428571428575, "y": 0.01851851851851852, "ox": 0.053571428571428575, "oy": 0.01851851851851852, "term": "skilled", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4154160982264666, "os": -0.0346317203325226, "bg": 3.0266912551701248e-06}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "proactive", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 5.1656994970545825e-06}, {"x": 0.07142857142857144, "y": 0.00617283950617284, "ox": 0.07142857142857144, "oy": 0.00617283950617284, "term": "without", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22578444747612553, "os": -0.06465313827277018, "bg": 1.1856081507240659e-07}, {"x": 0.02380952380952381, "y": 0.04938271604938272, "ox": 0.02380952380952381, "oy": 0.04938271604938272, "term": "supervision", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.6780354706684857, "os": 0.025628925109812323, "bg": 2.397250114393779e-06}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "quick", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 2.3339611299779453e-07}, {"x": 0.1488095238095238, "y": 0.04320987654320988, "ox": 0.1488095238095238, "oy": 0.04320987654320988, "term": "detail", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 25, "s": 0.12892223738062758, "os": -0.10476639924492687, "bg": 1.4034784247866919e-06}, {"x": 0.47023809523809523, "y": 0.17283950617283952, "ox": 0.47023809523809523, "oy": 0.17283950617283952, "term": "delivery", "cat25k": 14, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 85, "s": 0.016371077762619375, "os": -0.2954586706356409, "bg": 1.927896154914338e-06}, {"x": 0.1130952380952381, "y": 0.10493827160493828, "ox": 0.1130952380952381, "oy": 0.10493827160493828, "term": "enhance", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 19, "s": 0.5736698499317872, "os": -0.007913747413511446, "bg": 3.211837261915872e-06}, {"x": 0.1488095238095238, "y": 0.06790123456790124, "ox": 0.1488095238095238, "oy": 0.06790123456790124, "term": "efficiency", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 25, "s": 0.1800818553888131, "os": -0.08022652194431336, "bg": 3.1151848691317326e-06}, {"x": 0.06547619047619048, "y": 0.12962962962962962, "ox": 0.06547619047619048, "oy": 0.12962962962962962, "term": "content", "cat25k": 11, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 11, "s": 0.8349249658935881, "os": 0.06396340799361092, "bg": 3.386473684371774e-07}, {"x": 0.14285714285714288, "y": 0.16666666666666669, "ox": 0.14285714285714288, "oy": 0.16666666666666669, "term": "organization", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 24, "s": 0.6507503410641201, "os": 0.023850147021454243, "bg": 1.3472326711636324e-06}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "publish", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 1.2532834285155682e-06}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "top", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 6.607196297636082e-08}, {"x": 0.0, "y": 0.0925925925925926, "ox": 0.0, "oy": 0.0925925925925926, "term": "simulation", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.8799454297407913, "os": 0.09224234943914038, "bg": 1.8437190024742712e-06}, {"x": 0.08333333333333334, "y": 0.08641975308641976, "ox": 0.08333333333333334, "oy": 0.08641975308641976, "term": "directly", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 14, "s": 0.5934515688949523, "os": 0.0032671434275964722, "bg": 9.61357959318593e-07}, {"x": 0.09523809523809525, "y": 0.07407407407407408, "ox": 0.09523809523809525, "oy": 0.07407407407407408, "term": "global", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 16, "s": 0.5538881309686221, "os": -0.0208371147493375, "bg": 5.985640256628768e-07}, {"x": 0.07738095238095238, "y": 0.04938271604938272, "ox": 0.07738095238095238, "oy": 0.04938271604938272, "term": "libraries", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 13, "s": 0.49113233287858116, "os": -0.02762551276001017, "bg": 1.4888096282878366e-06}, {"x": 0.13690476190476192, "y": 0.22222222222222224, "ox": 0.13690476190476192, "oy": 0.22222222222222224, "term": "find", "cat25k": 18, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 23, "s": 0.8731241473396999, "os": 0.08498203071114821, "bg": 2.3498967739137447e-07}, {"x": 0.03571428571428572, "y": 0.09876543209876544, "ox": 0.03571428571428572, "oy": 0.09876543209876544, "term": "inform", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 6, "s": 0.8328785811732606, "os": 0.06287436018441209, "bg": 3.7464909641875486e-06}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "revenue", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 9.939477280404954e-07}, {"x": 0.06547619047619048, "y": 0.10493827160493828, "ox": 0.06547619047619048, "oy": 0.10493827160493828, "term": "audience", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 11, "s": 0.7544338335607095, "os": 0.03942353069299742, "bg": 2.278339347742365e-06}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "personalization", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 9.744424623047373e-06}, {"x": 0.10119047619047619, "y": 0.0925925925925926, "ox": 0.10119047619047619, "oy": 0.0925925925925926, "term": "independently", "cat25k": 8, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 17, "s": 0.5723055934515688, "os": -0.008349366537190983, "bg": 8.453609037225071e-06}, {"x": 0.08333333333333334, "y": 0.10493827160493828, "ox": 0.08333333333333334, "oy": 0.10493827160493828, "term": "meaningful", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 14, "s": 0.6466575716234653, "os": 0.0216720514030566, "bg": 9.017348943043079e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "historical", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 2.81637548669022e-07}, {"x": 0.35714285714285715, "y": 0.1851851851851852, "ox": 0.35714285714285715, "oy": 0.1851851851851852, "term": "reporting", "cat25k": 15, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 61, "s": 0.056616643929058665, "os": -0.17076269648237558, "bg": 4.776628074311316e-06}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "measure", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 9.94084606354456e-07}, {"x": 0.1130952380952381, "y": 0.22222222222222224, "ox": 0.1130952380952381, "oy": 0.22222222222222224, "term": "actionable", "cat25k": 18, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 19, "s": 0.8990450204638473, "os": 0.10865066976440266, "bg": 0.00015242466064727825}, {"x": 0.1130952380952381, "y": 0.04938271604938272, "ox": 0.1130952380952381, "oy": 0.04938271604938272, "term": "unit", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 19, "s": 0.23669849931787176, "os": -0.06312847133989181, "bg": 5.75564057039336e-07}, {"x": 0.11904761904761905, "y": 0.04938271604938272, "ox": 0.11904761904761905, "oy": 0.04938271604938272, "term": "events", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 20, "s": 0.21555252387448842, "os": -0.06904563110320543, "bg": 2.781335818178753e-07}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "varied", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 2.2470562509804537e-06}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "hbo", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 6.460459564791141e-06}, {"x": 0.125, "y": 0.05555555555555556, "ox": 0.125, "oy": 0.05555555555555556, "term": "party", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 21, "s": 0.21759890859481584, "os": -0.06882782154136566, "bg": 4.1432415271573955e-07}, {"x": 0.10119047619047619, "y": 0.04938271604938272, "ox": 0.10119047619047619, "oy": 0.04938271604938272, "term": "availability", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 17, "s": 0.2892223738062756, "os": -0.051294151813264605, "bg": 6.196512325259592e-07}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "compelling", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 3.1886458697868632e-06}, {"x": 0.20238095238095238, "y": 0.1234567901234568, "ox": 0.20238095238095238, "oy": 0.1234567901234568, "term": "but", "cat25k": 10, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 34, "s": 0.18417462482946798, "os": -0.07826623588775547, "bg": 1.0799931658032466e-07}, {"x": 0.029761904761904767, "y": 0.08024691358024692, "ox": 0.029761904761904767, "oy": 0.08024691358024692, "term": "consumer", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.7974079126875854, "os": 0.050386611972265585, "bg": 5.311893194589831e-07}, {"x": 0.01785714285714286, "y": 0.12962962962962962, "ox": 0.01785714285714286, "oy": 0.12962962962962962, "term": "engagement", "cat25k": 11, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 3, "s": 0.9017735334242838, "os": 0.1113006861001198, "bg": 4.11339639798447e-06}, {"x": 0.08333333333333334, "y": 0.03703703703703704, "ox": 0.08333333333333334, "oy": 0.03703703703703704, "term": "study", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.3246930422919509, "os": -0.045812611173630516, "bg": 2.612927561012283e-07}, {"x": 0.053571428571428575, "y": 0.06790123456790124, "ox": 0.053571428571428575, "oy": 0.06790123456790124, "term": "physical", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6193724420190996, "os": 0.014448034268704404, "bg": 6.230850648815362e-07}, {"x": 0.16071428571428573, "y": 0.19753086419753088, "ox": 0.16071428571428573, "oy": 0.19753086419753088, "term": "current", "cat25k": 16, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 27, "s": 0.7285129604365621, "os": 0.03677351435728027, "bg": 5.43543810570818e-07}, {"x": 0.08928571428571429, "y": 0.05555555555555556, "ox": 0.08928571428571429, "oy": 0.05555555555555556, "term": "workflows", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.4379263301500682, "os": -0.03332486296148401, "bg": 7.85974288816077e-05}, {"x": 0.011904761904761908, "y": 0.06790123456790124, "ox": 0.011904761904761908, "oy": 0.06790123456790124, "term": "artificial", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.8096862210095498, "os": 0.05586815261189967, "bg": 2.427622308280396e-06}, {"x": 0.01785714285714286, "y": 0.04938271604938272, "ox": 0.01785714285714286, "oy": 0.04938271604938272, "term": "ip", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.7135061391541611, "os": 0.031546084873125935, "bg": 3.423084695035129e-07}, {"x": 0.11904761904761905, "y": 0.01851851851851852, "ox": 0.11904761904761905, "oy": 0.01851851851851852, "term": "stack", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 20, "s": 0.13847203274215555, "os": -0.0997204777289723, "bg": 4.108161827996265e-06}, {"x": 0.029761904761904767, "y": 0.19753086419753088, "ox": 0.029761904761904767, "oy": 0.19753086419753088, "term": "house", "cat25k": 16, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 5, "s": 0.9454297407912687, "os": 0.16695102915017968, "bg": 3.19768968475909e-07}, {"x": 0.03571428571428572, "y": 0.11728395061728396, "ox": 0.03571428571428572, "oy": 0.11728395061728396, "term": "implemented", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.869031377899045, "os": 0.08127926815987221, "bg": 2.594602852444105e-06}, {"x": 0.2142857142857143, "y": 0.20370370370370372, "ox": 0.2142857142857143, "oy": 0.20370370370370372, "term": "solution", "cat25k": 17, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 36, "s": 0.5688949522510232, "os": -0.010345954187388845, "bg": 1.8621652788272697e-06}, {"x": 0.16071428571428573, "y": 0.10493827160493828, "ox": 0.16071428571428573, "oy": 0.10493827160493828, "term": "collaboration", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 27, "s": 0.26398362892223737, "os": -0.05525102552002034, "bg": 4.979054870146531e-06}, {"x": 0.3392857142857143, "y": 0.8518518518518519, "ox": 0.3392857142857143, "oy": 0.8518518518518519, "term": "accommodation", "cat25k": 122, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 244, "ncat": 58, "s": 0.9952251023192361, "os": 0.5095654699241297, "bg": 9.951113507359746e-06}, {"x": 0.3511904761904762, "y": 0.8518518518518519, "ox": 0.3511904761904762, "oy": 0.8518518518518519, "term": "requested", "cat25k": 122, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 244, "ncat": 60, "s": 0.994542974079127, "os": 0.49773115039750243, "bg": 2.463549994991862e-05}, {"x": 0.375, "y": 0.8518518518518519, "ox": 0.375, "oy": 0.8518518518518519, "term": "employment", "cat25k": 122, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 244, "ncat": 64, "s": 0.9938608458390178, "os": 0.474062511344248, "bg": 7.77659125847516e-06}, {"x": 0.005952380952380953, "y": 0.17283950617283952, "ox": 0.005952380952380953, "oy": 0.17283950617283952, "term": "thoroughly", "cat25k": 14, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 1, "s": 0.9440654843110505, "os": 0.16607979090282063, "bg": 8.719426009205907e-06}, {"x": 0.08333333333333334, "y": 0.2654320987654321, "ox": 0.08333333333333334, "oy": 0.2654320987654321, "term": "domain", "cat25k": 22, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 14, "s": 0.9488403819918144, "os": 0.1811812538570443, "bg": 1.2924573955024386e-06}, {"x": 0.20833333333333334, "y": 0.22222222222222224, "ox": 0.20833333333333334, "oy": 0.22222222222222224, "term": "architectures", "cat25k": 18, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 35, "s": 0.6159618008185539, "os": 0.013976113551384894, "bg": 4.079566485729275e-05}, {"x": 0.029761904761904767, "y": 0.19135802469135804, "ox": 0.029761904761904767, "oy": 0.19135802469135804, "term": "extend", "cat25k": 16, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 5, "s": 0.9406548431105048, "os": 0.16081605982502634, "bg": 4.736007695486281e-06}, {"x": 0.08928571428571429, "y": 0.20370370370370372, "ox": 0.08928571428571429, "oy": 0.20370370370370372, "term": "ownership", "cat25k": 17, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 15, "s": 0.902455661664393, "os": 0.11391440084219696, "bg": 5.156787836856422e-06}, {"x": 0.08928571428571429, "y": 0.22222222222222224, "ox": 0.08928571428571429, "oy": 0.22222222222222224, "term": "text", "cat25k": 18, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 15, "s": 0.9215552523874488, "os": 0.13231930881765708, "bg": 4.883028699125107e-07}, {"x": 0.005952380952380953, "y": 0.19135802469135804, "ox": 0.005952380952380953, "oy": 0.19135802469135804, "term": "image", "cat25k": 16, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 1, "s": 0.951568894952251, "os": 0.18448469887828076, "bg": 3.232634495841734e-07}, {"x": 0.011904761904761908, "y": 0.1851851851851852, "ox": 0.011904761904761908, "oy": 0.1851851851851852, "term": "anomaly", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 2, "s": 0.9461118690313779, "os": 0.17243256978981378, "bg": 3.556229263432684e-05}, {"x": 0.14285714285714288, "y": 0.25308641975308643, "ox": 0.14285714285714288, "oy": 0.25308641975308643, "term": "next", "cat25k": 21, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 24, "s": 0.8997271487039564, "os": 0.10973971757360149, "bg": 3.0515713301324324e-07}, {"x": 0.07738095238095238, "y": 0.1234567901234568, "ox": 0.07738095238095238, "oy": 0.1234567901234568, "term": "generate", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 13, "s": 0.7824010914051842, "os": 0.045994119141830325, "bg": 3.6175989612009775e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "ops", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 2.726298075710661e-06}, {"x": 0.08928571428571429, "y": 0.06790123456790124, "ox": 0.08928571428571429, "oy": 0.06790123456790124, "term": "each", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 15, "s": 0.5525238744884039, "os": -0.021054924311177253, "bg": 1.5249289132392926e-07}, {"x": 0.01785714285714286, "y": 0.08641975308641976, "ox": 0.01785714285714286, "oy": 0.08641975308641976, "term": "first", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.8424283765347885, "os": 0.06835590082404618, "bg": 5.879624917283188e-08}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "class", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 1.673689126272402e-07}, {"x": 0.07738095238095238, "y": 0.04320987654320988, "ox": 0.07738095238095238, "oy": 0.04320987654320988, "term": "start", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 13, "s": 0.4351978171896317, "os": -0.033760482085163544, "bg": 2.4688302771024975e-07}, {"x": 0.02380952380952381, "y": 0.22222222222222224, "ox": 0.02380952380952381, "oy": 0.22222222222222224, "term": "outcomes", "cat25k": 18, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 4, "s": 0.9570259208731242, "os": 0.19740806621410678, "bg": 5.566323686036244e-06}, {"x": 0.07142857142857144, "y": 0.17283950617283952, "ox": 0.07142857142857144, "oy": 0.17283950617283952, "term": "challenging", "cat25k": 14, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 12, "s": 0.8915416098226467, "os": 0.10099103350637093, "bg": 8.838243787929877e-06}, {"x": 0.18452380952380953, "y": 0.10493827160493828, "ox": 0.18452380952380953, "oy": 0.10493827160493828, "term": "which", "cat25k": 9, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 31, "s": 0.18281036834924966, "os": -0.07891966457327476, "bg": 1.1842776001566296e-07}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "requires", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 2.870290074385208e-07}, {"x": 0.06547619047619048, "y": 0.04320987654320988, "ox": 0.06547619047619048, "oy": 0.04320987654320988, "term": "ongoing", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.5463847203274215, "os": -0.02192616255853632, "bg": 2.1064886168865956e-06}, {"x": 0.13690476190476192, "y": 0.03703703703703704, "ox": 0.13690476190476192, "oy": 0.03703703703703704, "term": "search", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 23, "s": 0.1412005457025921, "os": -0.09906704904345301, "bg": 5.66295774211009e-08}, {"x": 0.08333333333333334, "y": 0.04320987654320988, "ox": 0.08333333333333334, "oy": 0.04320987654320988, "term": "cost", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.3608458390177353, "os": -0.03967764184847714, "bg": 2.641610828621931e-07}, {"x": 0.005952380952380953, "y": 0.04938271604938272, "ox": 0.005952380952380953, "oy": 0.04938271604938272, "term": "rigorous", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.7646657571623465, "os": 0.04338040439975315, "bg": 5.755831856802577e-06}, {"x": 0.04761904761904762, "y": 0.11728395061728396, "ox": 0.04761904761904762, "oy": 0.11728395061728396, "term": "thinking", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 8, "s": 0.8458390177353343, "os": 0.06944494863324499, "bg": 1.3030882370755726e-06}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "algebra", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 2.0160097091027593e-06}, {"x": 0.08928571428571429, "y": 0.01851851851851852, "ox": 0.08928571428571429, "oy": 0.01851851851851852, "term": "comfort", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 15, "s": 0.21077762619372445, "os": -0.07013467891240426, "bg": 1.2844663349223132e-06}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "simple", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 3.7328859718355153e-07}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "connect", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 4.86293894986867e-07}, {"x": 0.16666666666666669, "y": 0.08024691358024692, "ox": 0.16666666666666669, "oy": 0.08024691358024692, "term": "between", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 28, "s": 0.1664392905866303, "os": -0.08570806258394742, "bg": 3.208848796575457e-07}, {"x": 0.07142857142857144, "y": 0.030864197530864203, "ox": 0.07142857142857144, "oy": 0.030864197530864203, "term": "ve", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.3587994542974079, "os": -0.04011326097215668, "bg": 8.103137013906068e-08}, {"x": 0.029761904761904767, "y": 0.14814814814814817, "ox": 0.029761904761904767, "oy": 0.14814814814814817, "term": "these", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.9045020463847203, "os": 0.1178712745489527, "bg": 1.0718695341988207e-07}, {"x": 0.08928571428571429, "y": 0.012345679012345682, "ox": 0.08928571428571429, "oy": 0.012345679012345682, "term": "front", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1930422919508868, "os": -0.07626964823755762, "bg": 3.366570626988134e-07}, {"x": 0.10119047619047619, "y": 0.07407407407407408, "ox": 0.10119047619047619, "oy": 0.07407407407407408, "term": "passionate", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 17, "s": 0.49522510231923605, "os": -0.02675427451265111, "bg": 1.5002190837179153e-05}, {"x": 0.25, "y": 0.19753086419753088, "ox": 0.25, "oy": 0.19753086419753088, "term": "about", "cat25k": 16, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 42, "s": 0.28512960436562074, "os": -0.05198388209242388, "bg": 1.2063505881407423e-07}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "cleansing", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 1.0699174324656415e-05}, {"x": 0.005952380952380953, "y": 0.07407407407407408, "ox": 0.005952380952380953, "oy": 0.07407407407407408, "term": "inference", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8403819918144612, "os": 0.06792028170036664, "bg": 9.122525339568462e-06}, {"x": 0.1904761904761905, "y": 0.14197530864197533, "ox": 0.1904761904761905, "oy": 0.14197530864197533, "term": "features", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 32, "s": 0.2987721691678035, "os": -0.04802700838566812, "bg": 6.829423712800929e-07}, {"x": 0.1904761904761905, "y": 0.01851851851851852, "ox": 0.1904761904761905, "oy": 0.01851851851851852, "term": "governance", "cat25k": 2, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 32, "s": 0.0572987721691678, "os": -0.17072639488873562, "bg": 5.083270875668205e-06}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "during", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 8.717926096048182e-08}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "flow", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 5.984071470330173e-07}, {"x": 0.04166666666666667, "y": 0.11111111111111112, "ox": 0.04166666666666667, "oy": 0.11111111111111112, "term": "graph", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.8437926330150068, "os": 0.06922713907140524, "bg": 3.1982824455679514e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "cloudera", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 0.00011254290698328738}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "logical", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 2.888816668472177e-06}, {"x": 0.43452380952380953, "y": 0.02469135802469136, "ox": 0.43452380952380953, "oy": 0.02469135802469136, "term": "warehouse", "cat25k": 2, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 76, "s": 0.0068212824010914054, "os": -0.40719497585944026, "bg": 1.0931273444165515e-05}, {"x": 0.08928571428571429, "y": 0.01851851851851852, "ox": 0.08928571428571429, "oy": 0.01851851851851852, "term": "schemas", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 15, "s": 0.21077762619372445, "os": -0.07013467891240426, "bg": 2.5336197262001617e-05}, {"x": 0.08928571428571429, "y": 0.05555555555555556, "ox": 0.08928571428571429, "oy": 0.05555555555555556, "term": "associated", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.4379263301500682, "os": -0.03332486296148401, "bg": 7.34785643280491e-07}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "oral", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 4.280239451813564e-07}, {"x": 0.1130952380952381, "y": 0.00617283950617284, "ox": 0.1130952380952381, "oy": 0.00617283950617284, "term": "pig", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.12414733969986358, "os": -0.10607325661596544, "bg": 5.605788873938561e-06}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "communicator", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 9.844624969011805e-06}, {"x": 0.08928571428571429, "y": 0.29012345679012347, "ox": 0.08928571428571429, "oy": 0.29012345679012347, "term": "latest", "cat25k": 24, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 15, "s": 0.9577080491132334, "os": 0.19980397139434425, "bg": 9.38975277348876e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "along", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 2.6331100882861464e-07}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "cluster", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 1.3532016717724092e-06}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "according", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 2.2020455819857143e-07}, {"x": 0.03571428571428572, "y": 0.08024691358024692, "ox": 0.03571428571428572, "oy": 0.08024691358024692, "term": "studies", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.776261937244202, "os": 0.04446945220895197, "bg": 3.514239703904219e-07}, {"x": 0.029761904761904767, "y": 0.04938271604938272, "ox": 0.029761904761904767, "oy": 0.04938271604938272, "term": "electrical", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.6330150068212824, "os": 0.01971176534649871, "bg": 7.583904748956564e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "comparable", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 2.0308864832192287e-06}, {"x": 0.08333333333333334, "y": 0.02469135802469136, "ox": 0.08333333333333334, "oy": 0.02469135802469136, "term": "proof", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.252387448840382, "os": -0.05808254982393726, "bg": 1.3630324473281666e-06}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "unsupervised", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 2.7989643831782242e-05}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "several", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 1.8570754957086978e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "identification", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 9.171829642436222e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "iot", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 6.149710304718146e-05}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "welcome", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 1.4018858062724542e-07}, {"x": 0.029761904761904767, "y": 0.11111111111111112, "ox": 0.029761904761904767, "oy": 0.11111111111111112, "term": "practical", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.8683492496589359, "os": 0.08106145859803245, "bg": 1.5728884903782135e-06}, {"x": 0.02380952380952381, "y": 0.0617283950617284, "ox": 0.02380952380952381, "oy": 0.0617283950617284, "term": "measurement", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.7496589358799455, "os": 0.03789886376011907, "bg": 1.3612235727558684e-06}, {"x": 0.005952380952380953, "y": 0.0617283950617284, "ox": 0.005952380952380953, "oy": 0.0617283950617284, "term": "attribution", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.8090040927694407, "os": 0.0556503430500599, "bg": 7.126600164689251e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "churn", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 1.4984191677779942e-05}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "play", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 1.3257767329802734e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "determining", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 9.512923266620524e-07}, {"x": 0.1904761904761905, "y": 0.01851851851851852, "ox": 0.1904761904761905, "oy": 0.01851851851851852, "term": "transform", "cat25k": 2, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 32, "s": 0.0572987721691678, "os": -0.17072639488873562, "bg": 9.215887294437989e-06}, {"x": 0.06547619047619048, "y": 0.03703703703703704, "ox": 0.06547619047619048, "oy": 0.03703703703703704, "term": "raw", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 11, "s": 0.48567530695770805, "os": -0.028061131883689694, "bg": 1.3534595959867378e-06}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "channels", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 5.099265272363232e-07}, {"x": 0.04761904761904762, "y": 0.06790123456790124, "ox": 0.04761904761904762, "oy": 0.06790123456790124, "term": "teach", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.6418826739427013, "os": 0.02036519403201801, "bg": 1.9805672991035013e-06}, {"x": 0.005952380952380953, "y": 0.1358024691358025, "ox": 0.005952380952380953, "oy": 0.1358024691358025, "term": "extracted", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.9154160982264665, "os": 0.12926997495190037, "bg": 9.62068564535141e-06}, {"x": 0.06547619047619048, "y": 0.02469135802469136, "ox": 0.06547619047619048, "oy": 0.02469135802469136, "term": "retention", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3553888130968622, "os": -0.04033107053399644, "bg": 3.2145048746359176e-06}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "channel", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 2.0753254184374875e-07}, {"x": 0.07142857142857144, "y": 0.11111111111111112, "ox": 0.07142857142857144, "oy": 0.11111111111111112, "term": "proficient", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 12, "s": 0.7551159618008185, "os": 0.039641340254837176, "bg": 2.601880986494503e-05}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "ruby", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 1.828579119052752e-06}, {"x": 0.22023809523809523, "y": 0.11111111111111112, "ox": 0.22023809523809523, "oy": 0.11111111111111112, "term": "who", "cat25k": 9, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 37, "s": 0.1180081855388813, "os": -0.10828765382800307, "bg": 1.7431708623133312e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "same", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 5.1079216368712866e-08}, {"x": 0.16071428571428573, "y": 0.05555555555555556, "ox": 0.16071428571428573, "oy": 0.05555555555555556, "term": "providing", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 27, "s": 0.13165075034106413, "os": -0.10433078012124733, "bg": 1.2400044874384618e-06}, {"x": 0.08333333333333334, "y": 0.08024691358024692, "ox": 0.08333333333333334, "oy": 0.08024691358024692, "term": "tech", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 14, "s": 0.581855388813097, "os": -0.0028678258975568943, "bg": 5.774887444235442e-07}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "multidisciplinary", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 9.306221208878135e-06}, {"x": 0.05952380952380953, "y": 0.012345679012345682, "ox": 0.05952380952380953, "oy": 0.012345679012345682, "term": "ux", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185538881309686, "os": -0.046683849420989576, "bg": 8.018651383117131e-06}, {"x": 0.029761904761904767, "y": 0.05555555555555556, "ox": 0.029761904761904767, "oy": 0.05555555555555556, "term": "creativity", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.6800818553888132, "os": 0.025846734671652084, "bg": 3.6212418945613345e-06}, {"x": 0.04761904761904762, "y": 0.0617283950617284, "ox": 0.04761904761904762, "oy": 0.0617283950617284, "term": "insight", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.616643929058663, "os": 0.014230224706864629, "bg": 2.5393788890852206e-06}, {"x": 0.1488095238095238, "y": 0.04320987654320988, "ox": 0.1488095238095238, "oy": 0.04320987654320988, "term": "being", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 25, "s": 0.12892223738062758, "os": -0.10476639924492687, "bg": 2.6349404140956254e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "wellness", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 1.4753002077854963e-06}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "designed", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 2.6294993467905295e-07}, {"x": 0.07738095238095238, "y": 0.07407407407407408, "ox": 0.07738095238095238, "oy": 0.07407407407407408, "term": "innovation", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.5811732605729878, "os": -0.0030856354593966767, "bg": 2.719119275091502e-06}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "worked", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 6.80007738488064e-07}, {"x": 0.0, "y": 0.04938271604938272, "ox": 0.0, "oy": 0.04938271604938272, "term": "automotive", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.786493860845839, "os": 0.04929756416306676, "bg": 3.700672236363416e-07}, {"x": 0.07142857142857144, "y": 0.04320987654320988, "ox": 0.07142857142857144, "oy": 0.04320987654320988, "term": "reduce", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.4877216916780355, "os": -0.027843322321849932, "bg": 9.514893901296047e-07}, {"x": 0.05952380952380953, "y": 0.01851851851851852, "ox": 0.05952380952380953, "oy": 0.01851851851851852, "term": "evolve", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.35402455661664395, "os": -0.04054888009583621, "bg": 8.428384985285985e-06}, {"x": 0.15476190476190477, "y": 0.14814814814814817, "ox": 0.15476190476190477, "oy": 0.14814814814814817, "term": "able", "cat25k": 12, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 26, "s": 0.577762619372442, "os": -0.006389080480633108, "bg": 9.132778062862519e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "largest", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 2.6419221886106123e-07}, {"x": 0.24404761904761904, "y": 0.14197530864197533, "ox": 0.24404761904761904, "oy": 0.14197530864197533, "term": "write", "cat25k": 12, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 41, "s": 0.13437926330150068, "os": -0.1012814462554906, "bg": 1.0098473130586234e-06}, {"x": 0.1488095238095238, "y": 0.11728395061728396, "ox": 0.1488095238095238, "oy": 0.11728395061728396, "term": "where", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 25, "s": 0.4454297407912688, "os": -0.03114676734308637, "bg": 2.4405463292750773e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "capture", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 1.2270289034466574e-06}, {"x": 0.2619047619047619, "y": 0.09876543209876544, "ox": 0.2619047619047619, "oy": 0.09876543209876544, "term": "enterprise", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 44, "s": 0.061391541609822645, "os": -0.16197771082150508, "bg": 2.1230797451398457e-06}, {"x": 0.1130952380952381, "y": 0.01851851851851852, "ox": 0.1130952380952381, "oy": 0.01851851851851852, "term": "server", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 19, "s": 0.14870395634379266, "os": -0.09380331796565869, "bg": 2.859088782673163e-07}, {"x": 0.13690476190476192, "y": 0.05555555555555556, "ox": 0.13690476190476192, "oy": 0.05555555555555556, "term": "principles", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 23, "s": 0.17871759890859484, "os": -0.08066214106799288, "bg": 2.054113633758792e-06}, {"x": 0.10119047619047619, "y": 0.04938271604938272, "ox": 0.10119047619047619, "oy": 0.04938271604938272, "term": "involved", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 17, "s": 0.2892223738062756, "os": -0.051294151813264605, "bg": 8.401278956940219e-07}, {"x": 0.07738095238095238, "y": 0.19753086419753088, "ox": 0.07738095238095238, "oy": 0.19753086419753088, "term": "programs", "cat25k": 16, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 13, "s": 0.9079126875852661, "os": 0.11961375104367081, "bg": 6.311143718998654e-07}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "file", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 7.438911657468866e-08}, {"x": 0.04761904761904762, "y": 0.02469135802469136, "ox": 0.04761904761904762, "oy": 0.02469135802469136, "term": "formats", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5368349249658936, "os": -0.022579591244055616, "bg": 1.2572018019473427e-06}, {"x": 0.011904761904761908, "y": 0.04938271604938272, "ox": 0.011904761904761908, "oy": 0.04938271604938272, "term": "analyzes", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.7455661664392906, "os": 0.03746324463643954, "bg": 1.1023918043783694e-05}, {"x": 0.07738095238095238, "y": 0.09876543209876544, "ox": 0.07738095238095238, "oy": 0.09876543209876544, "term": "develops", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 13, "s": 0.6459754433833561, "os": 0.021454241841216817, "bg": 9.56220121212441e-06}, {"x": 0.06547619047619048, "y": 0.1358024691358025, "ox": 0.06547619047619048, "oy": 0.1358024691358025, "term": "assess", "cat25k": 11, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 11, "s": 0.8472032742155525, "os": 0.07009837731876428, "bg": 5.188295017774627e-06}, {"x": 0.1130952380952381, "y": 0.04320987654320988, "ox": 0.1130952380952381, "oy": 0.04320987654320988, "term": "requests", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 19, "s": 0.213506139154161, "os": -0.0692634406650452, "bg": 1.4306747956508036e-06}, {"x": 0.01785714285714286, "y": 0.08024691358024692, "ox": 0.01785714285714286, "oy": 0.08024691358024692, "term": "output", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.829467939972715, "os": 0.06222093149889281, "bg": 5.72181275897014e-07}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "conceptualize", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 5.64622849580944e-05}, {"x": 0.08333333333333334, "y": 0.04938271604938272, "ox": 0.08333333333333334, "oy": 0.04938271604938272, "term": "rapidly", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 14, "s": 0.43656207366984995, "os": -0.03354267252332377, "bg": 3.64667770670716e-06}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "investment", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 2.898644867581859e-07}, {"x": 0.05952380952380953, "y": 0.02469135802469136, "ox": 0.05952380952380953, "oy": 0.02469135802469136, "term": "comprehensive", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.4195088676671214, "os": -0.03441391077068283, "bg": 7.436714950158208e-07}, {"x": 0.04761904761904762, "y": 0.00617283950617284, "ox": 0.04761904761904762, "oy": 0.00617283950617284, "term": "401k", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3417462482946794, "os": -0.04098449921951574, "bg": 0.0}, {"x": 0.20238095238095238, "y": 0.05555555555555556, "ox": 0.20238095238095238, "oy": 0.05555555555555556, "term": "get", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 34, "s": 0.07708049113233288, "os": -0.14575089846444259, "bg": 1.4189284403880552e-07}, {"x": 0.10119047619047619, "y": 0.01851851851851852, "ox": 0.10119047619047619, "oy": 0.01851851851851852, "term": "done", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 17, "s": 0.1759890859481583, "os": -0.08196899843903148, "bg": 3.8865414394513574e-07}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "having", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 2.0097431066884826e-07}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "participating", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 1.116170325359309e-06}, {"x": 0.14285714285714288, "y": 0.04938271604938272, "ox": 0.14285714285714288, "oy": 0.04938271604938272, "term": "activities", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 24, "s": 0.15006821282401092, "os": -0.09271427015645987, "bg": 4.819573178298106e-07}, {"x": 0.125, "y": 0.07407407407407408, "ox": 0.125, "oy": 0.07407407407407408, "term": "analyzing", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 21, "s": 0.2933151432469304, "os": -0.050422913565905544, "bg": 1.4811111205862328e-05}, {"x": 0.07142857142857144, "y": 0.03703703703703704, "ox": 0.07142857142857144, "oy": 0.03703703703703704, "term": "standard", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.43110504774897684, "os": -0.033978291647003306, "bg": 2.444259842474243e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "difference", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 3.5831479446178797e-07}, {"x": 0.0, "y": 0.0925925925925926, "ox": 0.0, "oy": 0.0925925925925926, "term": "algorithm", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.8799454297407913, "os": 0.09224234943914038, "bg": 1.8113842967110271e-06}, {"x": 0.011904761904761908, "y": 0.0617283950617284, "ox": 0.011904761904761908, "oy": 0.0617283950617284, "term": "bias", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.7939972714870396, "os": 0.04973318328674629, "bg": 2.6924193586746833e-06}, {"x": 0.09523809523809525, "y": 0.01851851851851852, "ox": 0.09523809523809525, "oy": 0.01851851851851852, "term": "experienced", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 16, "s": 0.19372442019099592, "os": -0.07605183867571787, "bg": 1.5503336419993218e-06}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "xgboost", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 0.00013129882675119812}, {"x": 0.1130952380952381, "y": 0.04320987654320988, "ox": 0.1130952380952381, "oy": 0.04320987654320988, "term": "resources", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 19, "s": 0.213506139154161, "os": -0.0692634406650452, "bg": 2.3921763541114364e-07}, {"x": 0.03571428571428572, "y": 0.05555555555555556, "ox": 0.03571428571428572, "oy": 0.05555555555555556, "term": "manipulation", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 6, "s": 0.6371077762619373, "os": 0.019929574908338472, "bg": 6.18465142686093e-06}, {"x": 0.02380952380952381, "y": 0.06790123456790124, "ox": 0.02380952380952381, "oy": 0.06790123456790124, "term": "excel", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.7721691678035472, "os": 0.04403383308527245, "bg": 2.1067263347744042e-06}, {"x": 0.01785714285714286, "y": 0.0617283950617284, "ox": 0.01785714285714286, "oy": 0.0617283950617284, "term": "ensures", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7694406548431105, "os": 0.04381602352343268, "bg": 3.767650173101789e-06}, {"x": 0.07142857142857144, "y": 0.04320987654320988, "ox": 0.07142857142857144, "oy": 0.04320987654320988, "term": "free", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.4877216916780355, "os": -0.027843322321849932, "bg": 3.7467439500968026e-08}, {"x": 0.08333333333333334, "y": 0.14814814814814817, "ox": 0.08333333333333334, "oy": 0.14814814814814817, "term": "four", "cat25k": 12, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 14, "s": 0.8369713506139155, "os": 0.06461683667913022, "bg": 5.878996270921724e-07}, {"x": 0.13690476190476192, "y": 0.08024691358024692, "ox": 0.13690476190476192, "oy": 0.08024691358024692, "term": "tasks", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 23, "s": 0.26125511596180084, "os": -0.05612226376737939, "bg": 3.532870340861634e-06}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "demonstrate", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 1.3410442483584444e-06}, {"x": 0.03571428571428572, "y": 0.0925925925925926, "ox": 0.03571428571428572, "oy": 0.0925925925925926, "term": "thought", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 6, "s": 0.8158253751705321, "os": 0.05673939085925872, "bg": 5.142782121211015e-07}, {"x": 0.005952380952380953, "y": 0.05555555555555556, "ox": 0.005952380952380953, "oy": 0.05555555555555556, "term": "manufacturing", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7905866302864939, "os": 0.049515373724906525, "bg": 4.891400275097242e-07}, {"x": 0.375, "y": 0.2716049382716049, "ox": 0.375, "oy": 0.2716049382716049, "term": "web", "cat25k": 22, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 64, "s": 0.13369713506139155, "os": -0.10262460522016914, "bg": 3.4856795688068493e-07}, {"x": 0.005952380952380953, "y": 0.19753086419753088, "ox": 0.005952380952380953, "oy": 0.19753086419753088, "term": "semantic", "cat25k": 16, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 1, "s": 0.9542974079126877, "os": 0.19061966820343412, "bg": 1.4046531049537646e-05}, {"x": 0.05952380952380953, "y": 0.00617283950617284, "ox": 0.05952380952380953, "oy": 0.00617283950617284, "term": "linked", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.27489768076398363, "os": -0.052818818746142956, "bg": 9.57343764434949e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "ontologies", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.430018471071918e-05}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "assemble", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 5.170684288358721e-06}, {"x": 0.08333333333333334, "y": 0.08641975308641976, "ox": 0.08333333333333334, "oy": 0.08641975308641976, "term": "concept", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 14, "s": 0.5934515688949523, "os": 0.0032671434275964722, "bg": 1.761671015424971e-06}, {"x": 0.07738095238095238, "y": 0.05555555555555556, "ox": 0.07738095238095238, "oy": 0.05555555555555556, "term": "framework", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 13, "s": 0.5504774897680764, "os": -0.021490543434856797, "bg": 1.3526606820244277e-06}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "fluent", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 1.3435114503816794e-05}, {"x": 0.08333333333333334, "y": 0.04938271604938272, "ox": 0.08333333333333334, "oy": 0.04938271604938272, "term": "pandas", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 14, "s": 0.43656207366984995, "os": -0.03354267252332377, "bg": 8.656083948275964e-05}, {"x": 0.10714285714285715, "y": 0.04938271604938272, "ox": 0.10714285714285715, "oy": 0.04938271604938272, "term": "commercial", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 18, "s": 0.25852660300136426, "os": -0.05721131157657822, "bg": 5.735616029096693e-07}, {"x": 0.029761904761904767, "y": 0.04938271604938272, "ox": 0.029761904761904767, "oy": 0.04938271604938272, "term": "fundamentals", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.6330150068212824, "os": 0.01971176534649871, "bg": 4.64666242748079e-06}, {"x": 0.01785714285714286, "y": 0.19135802469135804, "ox": 0.01785714285714286, "oy": 0.19135802469135804, "term": "nlp", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 3, "s": 0.946793997271487, "os": 0.17265037935165353, "bg": 5.472920712670858e-05}, {"x": 0.13690476190476192, "y": 0.08641975308641976, "ox": 0.13690476190476192, "oy": 0.08641975308641976, "term": "vision", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 23, "s": 0.2946793997271487, "os": -0.04998729444222602, "bg": 1.6998900607588947e-06}, {"x": 0.05952380952380953, "y": 0.02469135802469136, "ox": 0.05952380952380953, "oy": 0.02469135802469136, "term": "willingness", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.4195088676671214, "os": -0.03441391077068283, "bg": 7.4437280742331115e-06}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "dental", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 1.3706056402221015e-06}, {"x": 0.125, "y": 0.05555555555555556, "ox": 0.125, "oy": 0.05555555555555556, "term": "office", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 21, "s": 0.21759890859481584, "os": -0.06882782154136566, "bg": 2.2480643716147522e-07}, {"x": 0.09523809523809525, "y": 0.04938271604938272, "ox": 0.09523809523809525, "oy": 0.04938271604938272, "term": "paid", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 16, "s": 0.3267394270122783, "os": -0.04537699204995099, "bg": 7.946813039899028e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "generous", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 2.756246515932139e-06}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "vacation", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 4.722717880545935e-07}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "credit", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 1.022592679775983e-07}, {"x": 0.0, "y": 0.04938271604938272, "ox": 0.0, "oy": 0.04938271604938272, "term": "forecast", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.786493860845839, "os": 0.04929756416306676, "bg": 5.379234701658231e-07}, {"x": 0.1130952380952381, "y": 0.08641975308641976, "ox": 0.1130952380952381, "oy": 0.08641975308641976, "term": "changes", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 19, "s": 0.4972714870395635, "os": -0.026318655388971574, "bg": 5.050624707178554e-07}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "portfolio", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 6.836120653291135e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "segments", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 1.6312080550141228e-06}, {"x": 0.10119047619047619, "y": 0.01851851851851852, "ox": 0.10119047619047619, "oy": 0.01851851851851852, "term": "metadata", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 17, "s": 0.1759890859481583, "os": -0.08196899843903148, "bg": 6.002652271906342e-06}, {"x": 0.33333333333333337, "y": 0.03703703703703704, "ox": 0.33333333333333337, "oy": 0.03703703703703704, "term": "engineer", "cat25k": 3, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 57, "s": 0.017053206002728513, "os": -0.2943333212328021, "bg": 5.547175358496113e-06}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "advertising", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 2.2144240295831993e-07}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "its", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 6.086723719442871e-08}, {"x": 0.10714285714285715, "y": 0.07407407407407408, "ox": 0.10714285714285715, "oy": 0.07407407407407408, "term": "plan", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 18, "s": 0.44201909959072305, "os": -0.03267143427596472, "bg": 3.7301162887377087e-07}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "campaigns", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 1.597126996693947e-06}, {"x": 0.09523809523809525, "y": 0.03703703703703704, "ox": 0.09523809523809525, "oy": 0.03703703703703704, "term": "collaborating", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 16, "s": 0.2564802182810368, "os": -0.05764693070025774, "bg": 2.8310712644979803e-05}, {"x": 0.053571428571428575, "y": 0.07407407407407408, "ox": 0.053571428571428575, "oy": 0.07407407407407408, "term": "utilizing", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.6446111869031379, "os": 0.02058300359385777, "bg": 8.852206096219264e-06}, {"x": 0.14285714285714288, "y": 0.1234567901234568, "ox": 0.14285714285714288, "oy": 0.1234567901234568, "term": "robust", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 24, "s": 0.5559345156889496, "os": -0.019094638254619378, "bg": 1.2106762383463814e-05}, {"x": 0.07738095238095238, "y": 0.0925925925925926, "ox": 0.07738095238095238, "oy": 0.0925925925925926, "term": "hand", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 13, "s": 0.6255115961800819, "os": 0.01531927251606345, "bg": 4.612724353337025e-07}, {"x": 0.11904761904761905, "y": 0.04320987654320988, "ox": 0.11904761904761905, "oy": 0.04320987654320988, "term": "desire", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 20, "s": 0.19781718963165076, "os": -0.07518060042835881, "bg": 3.0184750798051267e-06}, {"x": 0.10119047619047619, "y": 0.12962962962962962, "ox": 0.10119047619047619, "oy": 0.12962962962962962, "term": "program", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 17, "s": 0.6984993178717599, "os": 0.028460449413729258, "bg": 2.4772351784175344e-07}, {"x": 0.18452380952380953, "y": 0.08641975308641976, "ox": 0.18452380952380953, "oy": 0.08641975308641976, "term": "used", "cat25k": 7, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 31, "s": 0.14324693042291953, "os": -0.09732457254873489, "bg": 2.135004373105735e-07}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "indicators", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 1.155684664794024e-06}, {"x": 0.06547619047619048, "y": 0.00617283950617284, "ox": 0.06547619047619048, "oy": 0.00617283950617284, "term": "mentorship", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.24283765347885405, "os": -0.05873597850945657, "bg": 5.106154831379874e-05}, {"x": 0.2142857142857143, "y": 0.11728395061728396, "ox": 0.2142857142857143, "oy": 0.11728395061728396, "term": "improvements", "cat25k": 10, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 36, "s": 0.14529331514324695, "os": -0.09623552473953607, "bg": 5.840412222666034e-06}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "looking", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 2.123187159770869e-07}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "identifies", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 3.009035984609985e-06}, {"x": 0.005952380952380953, "y": 0.05555555555555556, "ox": 0.005952380952380953, "oy": 0.05555555555555556, "term": "communicates", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7905866302864939, "os": 0.049515373724906525, "bg": 1.648710831782858e-05}, {"x": 0.05952380952380953, "y": 0.030864197530864203, "ox": 0.05952380952380953, "oy": 0.030864197530864203, "term": "operate", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.48362892223738063, "os": -0.028278941445529455, "bg": 1.6342880608120766e-06}, {"x": 0.0, "y": 0.0617283950617284, "ox": 0.0, "oy": 0.0617283950617284, "term": "envision", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8233287858117326, "os": 0.061567502813373504, "bg": 1.5019108060229628e-05}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "considerable", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 1.1293792385838111e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "ideation", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 6.175364925471064e-05}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "milestones", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 4.89924012785617e-06}, {"x": 0.005952380952380953, "y": 0.09876543209876544, "ox": 0.005952380952380953, "oy": 0.09876543209876544, "term": "types", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.883356070941337, "os": 0.09246015900098013, "bg": 4.4054360645089036e-07}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "labels", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 6.474570127660718e-07}, {"x": 0.0, "y": 0.0617283950617284, "ox": 0.0, "oy": 0.0617283950617284, "term": "attempt", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8233287858117326, "os": 0.061567502813373504, "bg": 7.886890631225715e-07}, {"x": 0.06547619047619048, "y": 0.04320987654320988, "ox": 0.06547619047619048, "oy": 0.04320987654320988, "term": "documenting", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.5463847203274215, "os": -0.02192616255853632, "bg": 1.6561882092280966e-05}, {"x": 0.029761904761904767, "y": 0.07407407407407408, "ox": 0.029761904761904767, "oy": 0.07407407407407408, "term": "autonomy", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.7742155525238745, "os": 0.044251642647112205, "bg": 1.1168376413702942e-05}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "accepted", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 2.5516373793272205e-07}, {"x": 0.0, "y": 0.06790123456790124, "ox": 0.0, "oy": 0.06790123456790124, "term": "wireless", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.8390177353342428, "os": 0.06770247213852688, "bg": 2.5749019971817694e-07}, {"x": 0.09523809523809525, "y": 0.030864197530864203, "ox": 0.09523809523809525, "oy": 0.030864197530864203, "term": "orchestration", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.23328785811732608, "os": -0.06378190002541112, "bg": 7.949742485127356e-05}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "ups", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 3.802451009221197e-07}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "six", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 1.9587826518399693e-07}, {"x": 0.0, "y": 0.11728395061728396, "ox": 0.0, "oy": 0.11728395061728396, "term": "reinforcement", "cat25k": 10, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 0, "s": 0.9031377899045021, "os": 0.11678222673975387, "bg": 1.697074823582373e-05}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "networking", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 4.345459039844318e-07}, {"x": 0.01785714285714286, "y": 0.04938271604938272, "ox": 0.01785714285714286, "oy": 0.04938271604938272, "term": "organized", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 3, "s": 0.7135061391541611, "os": 0.031546084873125935, "bg": 1.2215889640764558e-06}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "trained", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 8.749717821600253e-07}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "published", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 1.6464726986189705e-07}, {"x": 0.011904761904761908, "y": 0.07407407407407408, "ox": 0.011904761904761908, "oy": 0.07407407407407408, "term": "conference", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 2, "s": 0.8281036834924965, "os": 0.062003121937053034, "bg": 2.7425618417086354e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "resume", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 5.65654504921972e-07}, {"x": 0.005952380952380953, "y": 0.1358024691358025, "ox": 0.005952380952380953, "oy": 0.1358024691358025, "term": "billion", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.9154160982264665, "os": 0.12926997495190037, "bg": 1.451395936836006e-06}, {"x": 0.15476190476190477, "y": 0.08024691358024692, "ox": 0.15476190476190477, "oy": 0.08024691358024692, "term": "day", "cat25k": 7, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 26, "s": 0.20122783083219647, "os": -0.07387374305732022, "bg": 1.7475357231420556e-07}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "forecasts", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 1.8736547744236757e-06}, {"x": 0.07738095238095238, "y": 0.01851851851851852, "ox": 0.07738095238095238, "oy": 0.01851851851851852, "term": "entire", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.24965893587994545, "os": -0.058300359385777045, "bg": 5.674699315860024e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "advising", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 3.830402237720987e-06}, {"x": 0.05952380952380953, "y": 0.02469135802469136, "ox": 0.05952380952380953, "oy": 0.02469135802469136, "term": "mission", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.4195088676671214, "os": -0.03441391077068283, "bg": 5.300725993111366e-07}, {"x": 0.09523809523809525, "y": 0.012345679012345682, "ox": 0.09523809523809525, "oy": 0.012345679012345682, "term": "towards", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 16, "s": 0.17462482946793997, "os": -0.08218680800087123, "bg": 9.75469647990939e-07}, {"x": 0.06547619047619048, "y": 0.0925925925925926, "ox": 0.06547619047619048, "oy": 0.0925925925925926, "term": "success", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 11, "s": 0.6930422919508867, "os": 0.027153592042690675, "bg": 8.79965154733665e-07}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "predictions", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 3.109524716057205e-06}, {"x": 0.03571428571428572, "y": 0.0617283950617284, "ox": 0.03571428571428572, "oy": 0.0617283950617284, "term": "supports", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.684174624829468, "os": 0.026064544233491846, "bg": 1.1026676632025672e-06}, {"x": 0.08928571428571429, "y": 0.08024691358024692, "ox": 0.08928571428571429, "oy": 0.08024691358024692, "term": "roadmap", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 15, "s": 0.5709413369713506, "os": -0.008784985660870506, "bg": 2.0626713214282818e-05}, {"x": 0.07738095238095238, "y": 0.01851851851851852, "ox": 0.07738095238095238, "oy": 0.01851851851851852, "term": "practice", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.24965893587994545, "os": -0.058300359385777045, "bg": 3.8029842159754667e-07}, {"x": 0.029761904761904767, "y": 0.04938271604938272, "ox": 0.029761904761904767, "oy": 0.04938271604938272, "term": "retail", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.6330150068212824, "os": 0.01971176534649871, "bg": 4.460295725497893e-07}, {"x": 0.04761904761904762, "y": 0.154320987654321, "ox": 0.04761904761904762, "oy": 0.154320987654321, "term": "answer", "cat25k": 13, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 8, "s": 0.8969986357435199, "os": 0.10625476458416525, "bg": 1.0192072541118873e-06}, {"x": 0.04761904761904762, "y": 0.0617283950617284, "ox": 0.04761904761904762, "oy": 0.0617283950617284, "term": "down", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.616643929058663, "os": 0.014230224706864629, "bg": 1.5998397884883812e-07}, {"x": 0.08333333333333334, "y": 0.02469135802469136, "ox": 0.08333333333333334, "oy": 0.02469135802469136, "term": "volume", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.252387448840382, "os": -0.05808254982393726, "bg": 4.83935303818415e-07}, {"x": 0.10119047619047619, "y": 0.05555555555555556, "ox": 0.10119047619047619, "oy": 0.05555555555555556, "term": "recommend", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 17, "s": 0.3274215552523875, "os": -0.04515918248811123, "bg": 1.2604998119310037e-06}, {"x": 0.11904761904761905, "y": 0.01851851851851852, "ox": 0.11904761904761905, "oy": 0.01851851851851852, "term": "ensuring", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 20, "s": 0.13847203274215555, "os": -0.0997204777289723, "bg": 4.385964075903113e-06}, {"x": 0.10714285714285715, "y": 0.030864197530864203, "ox": 0.10714285714285715, "oy": 0.030864197530864203, "term": "reliable", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 18, "s": 0.19577080491132334, "os": -0.07561621955203834, "bg": 2.248460012629502e-06}, {"x": 0.35714285714285715, "y": 0.030864197530864203, "ox": 0.35714285714285715, "oy": 0.030864197530864203, "term": "redshift", "cat25k": 3, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 61, "s": 0.01432469304229195, "os": -0.32413692961120993, "bg": 0.00017997185894569212}, {"x": 0.053571428571428575, "y": 0.030864197530864203, "ox": 0.053571428571428575, "oy": 0.030864197530864203, "term": "javascript", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5409276944065484, "os": -0.022361781682215844, "bg": 1.082215108805134e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "isn", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 3.8479493215069684e-08}, {"x": 0.07738095238095238, "y": 0.04938271604938272, "ox": 0.07738095238095238, "oy": 0.04938271604938272, "term": "right", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 13, "s": 0.49113233287858116, "os": -0.02762551276001017, "bg": 1.5343754965143208e-07}, {"x": 0.10119047619047619, "y": 0.06790123456790124, "ox": 0.10119047619047619, "oy": 0.06790123456790124, "term": "approach", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 17, "s": 0.4413369713506139, "os": -0.03288924383780448, "bg": 8.713919607400576e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "something", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 1.6673886838619762e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "strive", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 3.287243054994637e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "grasp", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 3.5538170808887268e-06}, {"x": 0.04761904761904762, "y": 0.20987654320987656, "ox": 0.04761904761904762, "oy": 0.20987654320987656, "term": "throughout", "cat25k": 17, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 8, "s": 0.9413369713506139, "os": 0.16146948851054563, "bg": 1.6384720093779896e-06}, {"x": 0.011904761904761908, "y": 0.14197530864197533, "ox": 0.011904761904761908, "oy": 0.14197530864197533, "term": "noise", "cat25k": 12, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.9167803547066848, "os": 0.12948778451374016, "bg": 1.977991677797815e-06}, {"x": 0.08333333333333334, "y": 0.08024691358024692, "ox": 0.08333333333333334, "oy": 0.08024691358024692, "term": "produce", "cat25k": 7, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 14, "s": 0.581855388813097, "os": -0.0028678258975568943, "bg": 1.5215674579324636e-06}, {"x": 0.16071428571428573, "y": 0.07407407407407408, "ox": 0.16071428571428573, "oy": 0.07407407407407408, "term": "around", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 27, "s": 0.16575716234652116, "os": -0.08592587214578722, "bg": 4.359570877825083e-07}, {"x": 0.029761904761904767, "y": 0.05555555555555556, "ox": 0.029761904761904767, "oy": 0.05555555555555556, "term": "then", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.6800818553888132, "os": 0.025846734671652084, "bg": 7.566840026124515e-08}, {"x": 0.08928571428571429, "y": 0.02469135802469136, "ox": 0.08928571428571429, "oy": 0.02469135802469136, "term": "than", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.23192360163710782, "os": -0.06399970958725087, "bg": 7.558941131909343e-08}, {"x": 0.07142857142857144, "y": 0.08024691358024692, "ox": 0.07142857142857144, "oy": 0.08024691358024692, "term": "growing", "cat25k": 7, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.607094133697135, "os": 0.008966493629070316, "bg": 1.2874662992821498e-06}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "trying", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 2.0844455804672858e-07}, {"x": 0.05952380952380953, "y": 0.03703703703703704, "ox": 0.05952380952380953, "oy": 0.03703703703703704, "term": "taking", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5450204638472033, "os": -0.022143972120376082, "bg": 4.564642708369129e-07}, {"x": 0.04761904761904762, "y": 0.0617283950617284, "ox": 0.04761904761904762, "oy": 0.0617283950617284, "term": "emerging", "cat25k": 5, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 8, "s": 0.616643929058663, "os": 0.014230224706864629, "bg": 2.644980374612979e-06}, {"x": 0.01785714285714286, "y": 0.09876543209876544, "ox": 0.01785714285714286, "oy": 0.09876543209876544, "term": "message", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.8656207366984994, "os": 0.08062583947435292, "bg": 1.018253859106427e-07}, {"x": 0.15476190476190477, "y": 0.0925925925925926, "ox": 0.15476190476190477, "oy": 0.0925925925925926, "term": "jobs", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 26, "s": 0.2387448840381992, "os": -0.06160380440701348, "bg": 4.5258292552760096e-07}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "fwd", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 2.5024743214853686e-06}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "author", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 8.892837605213524e-08}, {"x": 0.08928571428571429, "y": 0.07407407407407408, "ox": 0.08928571428571429, "oy": 0.07407407407407408, "term": "leads", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 15, "s": 0.562755798090041, "os": -0.014919954986023887, "bg": 1.7639019235252481e-06}, {"x": 0.13095238095238096, "y": 0.04938271604938272, "ox": 0.13095238095238096, "oy": 0.04938271604938272, "term": "familiar", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 22, "s": 0.1780354706684857, "os": -0.08087995062983265, "bg": 3.6369424612668662e-06}, {"x": 0.10119047619047619, "y": 0.012345679012345682, "ox": 0.10119047619047619, "oy": 0.012345679012345682, "term": "unix", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 17, "s": 0.15893587994542974, "os": -0.08810396776418485, "bg": 1.612348005968912e-06}, {"x": 0.06547619047619048, "y": 0.03703703703703704, "ox": 0.06547619047619048, "oy": 0.03703703703703704, "term": "100", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 11, "s": 0.48567530695770805, "os": -0.028061131883689694, "bg": 0.0}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "coverage", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 4.5737088397076953e-07}, {"x": 0.125, "y": 0.012345679012345682, "ox": 0.125, "oy": 0.012345679012345682, "term": "tx", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 21, "s": 0.10982264665757162, "os": -0.11177260681743928, "bg": 7.597891816261173e-07}, {"x": 0.005952380952380953, "y": 0.12962962962962962, "ox": 0.005952380952380953, "oy": 0.12962962962962962, "term": "pioneer", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.9106412005457026, "os": 0.12313500562674701, "bg": 3.0700680229389897e-06}, {"x": 0.005952380952380953, "y": 0.12962962962962962, "ox": 0.005952380952380953, "oy": 0.12962962962962962, "term": "enormous", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.9106412005457026, "os": 0.12313500562674701, "bg": 6.3124248606245274e-06}, {"x": 0.11904761904761905, "y": 0.20370370370370372, "ox": 0.11904761904761905, "oy": 0.20370370370370372, "term": "has", "cat25k": 17, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 20, "s": 0.8717598908594816, "os": 0.08432860202562892, "bg": 1.0129711364950445e-07}, {"x": 0.0, "y": 0.12962962962962962, "ox": 0.0, "oy": 0.12962962962962962, "term": "correlations", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133697135061392, "os": 0.12905216539006062, "bg": 1.826267995262139e-05}, {"x": 0.0, "y": 0.14197530864197533, "ox": 0.0, "oy": 0.14197530864197533, "term": "signals", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 0, "s": 0.9276944065484312, "os": 0.14132210404036738, "bg": 3.575048029993099e-06}, {"x": 0.0, "y": 0.12962962962962962, "ox": 0.0, "oy": 0.12962962962962962, "term": "amongst", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133697135061392, "os": 0.12905216539006062, "bg": 5.409322813316671e-06}, {"x": 0.02380952380952381, "y": 0.12962962962962962, "ox": 0.02380952380952381, "oy": 0.12962962962962962, "term": "push", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.8949522510231924, "os": 0.10538352633680619, "bg": 2.394430516308179e-06}, {"x": 0.0, "y": 0.12962962962962962, "ox": 0.0, "oy": 0.12962962962962962, "term": "zoominfo", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133697135061392, "os": 0.12905216539006062, "bg": 0.00021430430190373656}, {"x": 0.0, "y": 0.14814814814814817, "ox": 0.0, "oy": 0.14814814814814817, "term": "stages", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 0, "s": 0.9311050477489768, "os": 0.14745707336552075, "bg": 3.7678206140605647e-06}, {"x": 0.011904761904761908, "y": 0.154320987654321, "ox": 0.011904761904761908, "oy": 0.154320987654321, "term": "sourcing", "cat25k": 13, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 2, "s": 0.9283765347885403, "os": 0.1417577231640469, "bg": 1.2856435753176432e-05}, {"x": 0.005952380952380953, "y": 0.14197530864197533, "ox": 0.005952380952380953, "oy": 0.14197530864197533, "term": "ground", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.9236016371077763, "os": 0.13540494427705377, "bg": 7.89657812151363e-07}, {"x": 0.011904761904761908, "y": 0.14197530864197533, "ox": 0.011904761904761908, "oy": 0.14197530864197533, "term": "truth", "cat25k": 12, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.9167803547066848, "os": 0.12948778451374016, "bg": 1.4103349912291267e-06}, {"x": 0.03571428571428572, "y": 0.14197530864197533, "ox": 0.03571428571428572, "oy": 0.14197530864197533, "term": "normalization", "cat25k": 12, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 6, "s": 0.8963165075034107, "os": 0.10581914546048572, "bg": 4.393836205361844e-05}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "crm", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 1.4663038248975163e-06}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "engines", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 6.330812575225386e-07}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "weekly", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 4.4385269017451066e-07}, {"x": 0.10714285714285715, "y": 0.04938271604938272, "ox": 0.10714285714285715, "oy": 0.04938271604938272, "term": "need", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 18, "s": 0.25852660300136426, "os": -0.05721131157657822, "bg": 1.6276266944775483e-07}, {"x": 0.0, "y": 0.0925925925925926, "ox": 0.0, "oy": 0.0925925925925926, "term": "verizon", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.8799454297407913, "os": 0.09224234943914038, "bg": 3.1932317112177486e-06}, {"x": 0.005952380952380953, "y": 0.04938271604938272, "ox": 0.005952380952380953, "oy": 0.04938271604938272, "term": "represent", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.7646657571623465, "os": 0.04338040439975315, "bg": 6.884520431191284e-07}, {"x": 0.08333333333333334, "y": 0.1358024691358025, "ox": 0.08333333333333334, "oy": 0.1358024691358025, "term": "leading", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 14, "s": 0.8021828103683493, "os": 0.05234689802882346, "bg": 1.2786779720032393e-06}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "universities", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 6.318853452466189e-07}, {"x": 0.03571428571428572, "y": 0.06790123456790124, "ox": 0.03571428571428572, "oy": 0.06790123456790124, "term": "relationship", "cat25k": 6, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.7210095497953616, "os": 0.032199513558645226, "bg": 7.073549288377017e-07}, {"x": 0.011904761904761908, "y": 0.06790123456790124, "ox": 0.011904761904761908, "oy": 0.06790123456790124, "term": "partnership", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.8096862210095498, "os": 0.05586815261189967, "bg": 8.295624138032804e-07}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "systematically", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 8.445038516413167e-06}, {"x": 0.011904761904761908, "y": 0.05555555555555556, "ox": 0.011904761904761908, "oy": 0.05555555555555556, "term": "practicing", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7673942701227832, "os": 0.04359821396159291, "bg": 4.917530773684058e-06}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "chatbot", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 9.171320262503565e-05}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "influencing", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 9.084454998562385e-06}, {"x": 0.04166666666666667, "y": 0.0617283950617284, "ox": 0.04166666666666667, "oy": 0.0617283950617284, "term": "shell", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.6391541609822646, "os": 0.02014738447017824, "bg": 1.642039693411871e-06}, {"x": 0.01785714285714286, "y": 0.05555555555555556, "ox": 0.01785714285714286, "oy": 0.05555555555555556, "term": "script", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.746930422919509, "os": 0.0376810541982793, "bg": 7.3492134320594e-07}, {"x": 0.06547619047619048, "y": 0.02469135802469136, "ox": 0.06547619047619048, "oy": 0.02469135802469136, "term": "cutting", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3553888130968622, "os": -0.04033107053399644, "bg": 1.5085232316851829e-06}, {"x": 0.08333333333333334, "y": 0.030864197530864203, "ox": 0.08333333333333334, "oy": 0.030864197530864203, "term": "edge", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 14, "s": 0.28581173260572984, "os": -0.05194758049878389, "bg": 9.052779778720045e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "commitment", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 6.075337570707707e-07}, {"x": 0.16666666666666669, "y": 0.030864197530864203, "ox": 0.16666666666666669, "oy": 0.030864197530864203, "term": "maintaining", "cat25k": 3, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 28, "s": 0.08867667121418828, "os": -0.1347878171851744, "bg": 4.899984268080813e-06}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "tier", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 2.7793284114206493e-06}, {"x": 0.16666666666666669, "y": 0.04320987654320988, "ox": 0.16666666666666669, "oy": 0.04320987654320988, "term": "architect", "cat25k": 4, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 28, "s": 0.09959072305593451, "os": -0.12251787853486767, "bg": 7.725460103571933e-06}, {"x": 0.04166666666666667, "y": 0.0617283950617284, "ox": 0.04166666666666667, "oy": 0.0617283950617284, "term": "massive", "cat25k": 5, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.6391541609822646, "os": 0.02014738447017824, "bg": 1.7381189473522238e-06}, {"x": 0.20238095238095238, "y": 0.11728395061728396, "ox": 0.20238095238095238, "oy": 0.11728395061728396, "term": "implementing", "cat25k": 10, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 34, "s": 0.16916780354706687, "os": -0.08440120521290885, "bg": 7.740169236609744e-06}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "store", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 6.863051984118624e-08}, {"x": 0.029761904761904767, "y": 0.05555555555555556, "ox": 0.029761904761904767, "oy": 0.05555555555555556, "term": "predict", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.6800818553888132, "os": 0.025846734671652084, "bg": 4.409852997550326e-06}, {"x": 0.011904761904761908, "y": 0.04938271604938272, "ox": 0.011904761904761908, "oy": 0.04938271604938272, "term": "post", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 2, "s": 0.7455661664392906, "os": 0.03746324463643954, "bg": 5.088241997277862e-08}, {"x": 0.07142857142857144, "y": 0.04938271604938272, "ox": 0.07142857142857144, "oy": 0.04938271604938272, "term": "corporate", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 12, "s": 0.5497953615279673, "os": -0.02170835299669656, "bg": 4.716741123373263e-07}, {"x": 0.01785714285714286, "y": 0.05555555555555556, "ox": 0.01785714285714286, "oy": 0.05555555555555556, "term": "behaviors", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.746930422919509, "os": 0.0376810541982793, "bg": 4.683625032102347e-06}, {"x": 0.053571428571428575, "y": 0.01851851851851852, "ox": 0.053571428571428575, "oy": 0.01851851851851852, "term": "context", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4154160982264666, "os": -0.0346317203325226, "bg": 5.900424292135317e-07}, {"x": 0.17857142857142858, "y": 0.02469135802469136, "ox": 0.17857142857142858, "oy": 0.02469135802469136, "term": "guidance", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 30, "s": 0.06821282401091404, "os": -0.15275710603695503, "bg": 2.8585234759393248e-06}, {"x": 0.053571428571428575, "y": 0.06790123456790124, "ox": 0.053571428571428575, "oy": 0.06790123456790124, "term": "finance", "cat25k": 6, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6193724420190996, "os": 0.014448034268704404, "bg": 4.1381820080867524e-07}, {"x": 0.005952380952380953, "y": 0.05555555555555556, "ox": 0.005952380952380953, "oy": 0.05555555555555556, "term": "economic", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7905866302864939, "os": 0.049515373724906525, "bg": 2.1470530365899918e-07}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "formulation", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 2.4133026066282564e-06}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "resulting", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 6.378098993016621e-07}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "cox", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 1.4568565967143097e-06}, {"x": 0.029761904761904767, "y": 0.06790123456790124, "ox": 0.029761904761904767, "oy": 0.06790123456790124, "term": "schedule", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.7503410641200546, "os": 0.03811667332195884, "bg": 4.952705454821569e-07}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "mixed", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 4.705571969723409e-07}, {"x": 0.2619047619047619, "y": 0.05555555555555556, "ox": 0.2619047619047619, "oy": 0.05555555555555556, "term": "enable", "cat25k": 5, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 44, "s": 0.03751705320600273, "os": -0.2049224960975787, "bg": 2.6606639410758316e-06}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "roadmaps", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 6.874312568743126e-05}, {"x": 0.10714285714285715, "y": 0.02469135802469136, "ox": 0.10714285714285715, "oy": 0.02469135802469136, "term": "scaling", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 18, "s": 0.17735334242837655, "os": -0.08175118887719171, "bg": 1.1320494520366085e-05}, {"x": 0.08928571428571429, "y": 0.0617283950617284, "ox": 0.08928571428571429, "oy": 0.0617283950617284, "term": "overall", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 15, "s": 0.4938608458390178, "os": -0.027189893636330634, "bg": 6.321169414318875e-07}, {"x": 0.053571428571428575, "y": 0.01851851851851852, "ox": 0.053571428571428575, "oy": 0.01851851851851852, "term": "informed", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4154160982264666, "os": -0.0346317203325226, "bg": 6.264018678050894e-07}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "addressing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 1.8977326628151832e-06}, {"x": 0.05952380952380953, "y": 0.02469135802469136, "ox": 0.05952380952380953, "oy": 0.02469135802469136, "term": "consistent", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.4195088676671214, "os": -0.03441391077068283, "bg": 1.2330871636286848e-06}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "completion", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 1.0519119051129177e-06}, {"x": 0.08333333333333334, "y": 0.030864197530864203, "ox": 0.08333333333333334, "oy": 0.030864197530864203, "term": "tables", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 14, "s": 0.28581173260572984, "os": -0.05194758049878389, "bg": 1.0498345405509643e-06}, {"x": 0.06547619047619048, "y": 0.07407407407407408, "ox": 0.06547619047619048, "oy": 0.07407407407407408, "term": "custom", "cat25k": 6, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.6043656207366985, "os": 0.008748684067230547, "bg": 6.896411211051937e-07}, {"x": 0.06547619047619048, "y": 0.02469135802469136, "ox": 0.06547619047619048, "oy": 0.02469135802469136, "term": "scripts", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3553888130968622, "os": -0.04033107053399644, "bg": 1.7198824081999866e-06}, {"x": 0.053571428571428575, "y": 0.01851851851851852, "ox": 0.053571428571428575, "oy": 0.01851851851851852, "term": "efficiently", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4154160982264666, "os": -0.0346317203325226, "bg": 3.7258975182572857e-06}, {"x": 0.2261904761904762, "y": 0.03703703703703704, "ox": 0.2261904761904762, "oy": 0.03703703703703704, "term": "maintenance", "cat25k": 3, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 38, "s": 0.0470668485675307, "os": -0.18782444549315716, "bg": 1.688628916791563e-06}, {"x": 0.07142857142857144, "y": 0.03703703703703704, "ox": 0.07142857142857144, "oy": 0.03703703703703704, "term": "resolve", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.43110504774897684, "os": -0.033978291647003306, "bg": 3.4128434209242753e-06}, {"x": 0.11904761904761905, "y": 0.030864197530864203, "ox": 0.11904761904761905, "oy": 0.030864197530864203, "term": "policies", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 20, "s": 0.16030013642564805, "os": -0.08745053907866554, "bg": 5.688380245556908e-07}, {"x": 0.07738095238095238, "y": 0.030864197530864203, "ox": 0.07738095238095238, "oy": 0.030864197530864203, "term": "significant", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.3226466575716235, "os": -0.04603042073547029, "bg": 6.159609020870602e-07}, {"x": 0.06547619047619048, "y": 0.02469135802469136, "ox": 0.06547619047619048, "oy": 0.02469135802469136, "term": "release", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3553888130968622, "os": -0.04033107053399644, "bg": 2.3914390118363555e-07}, {"x": 0.011904761904761908, "y": 0.04320987654320988, "ox": 0.011904761904761908, "oy": 0.04320987654320988, "term": "issue", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 2, "s": 0.7087312414733971, "os": 0.031328275311286166, "bg": 1.4004100167127264e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "fixes", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.57477380342781e-06}, {"x": 0.125, "y": 0.01851851851851852, "ox": 0.125, "oy": 0.01851851851851852, "term": "tuning", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 21, "s": 0.126193724420191, "os": -0.10563763749228591, "bg": 6.326376164745162e-06}, {"x": 0.10714285714285715, "y": 0.00617283950617284, "ox": 0.10714285714285715, "oy": 0.00617283950617284, "term": "developer", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.1364256480218281, "os": -0.10015609685265184, "bg": 1.0556473675529947e-06}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "informatica", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 2.8132837632734242e-05}, {"x": 0.005952380952380953, "y": 0.08641975308641976, "ox": 0.005952380952380953, "oy": 0.08641975308641976, "term": "elements", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.8649386084583902, "os": 0.08019022035067339, "bg": 6.774024416293605e-07}, {"x": 0.03571428571428572, "y": 0.11111111111111112, "ox": 0.03571428571428572, "oy": 0.11111111111111112, "term": "organize", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.8587994542974079, "os": 0.07514429883471883, "bg": 6.045291322588835e-06}, {"x": 0.0, "y": 0.08641975308641976, "ox": 0.0, "oy": 0.08641975308641976, "term": "conjunction", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.8744884038199182, "os": 0.086107380113987, "bg": 3.1966572098890876e-06}, {"x": 0.02380952380952381, "y": 0.08641975308641976, "ox": 0.02380952380952381, "oy": 0.08641975308641976, "term": "delivered", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.8308321964529333, "os": 0.062438741060732564, "bg": 1.2313142083330284e-06}, {"x": 0.0, "y": 0.08024691358024692, "ox": 0.0, "oy": 0.08024691358024692, "term": "replicable", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.8628922237380628, "os": 0.07997241078883363, "bg": 0.00012962602890660444}, {"x": 0.029761904761904767, "y": 0.08641975308641976, "ox": 0.029761904761904767, "oy": 0.08641975308641976, "term": "abreast", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.815143246930423, "os": 0.05652158129741895, "bg": 4.408194136869788e-05}, {"x": 0.005952380952380953, "y": 0.08024691358024692, "ox": 0.005952380952380953, "oy": 0.08024691358024692, "term": "promising", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.8533424283765347, "os": 0.07405525102552002, "bg": 5.2439563403177844e-06}, {"x": 0.08928571428571429, "y": 0.00617283950617284, "ox": 0.08928571428571429, "oy": 0.00617283950617284, "term": "ssis", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.1732605729877217, "os": -0.082404617562711, "bg": 0.00013722185772666265}, {"x": 0.09523809523809525, "y": 0.00617283950617284, "ox": 0.09523809523809525, "oy": 0.00617283950617284, "term": "logic", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.1582537517053206, "os": -0.08832177732602461, "bg": 1.4109915996615112e-06}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "sap", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 1.7945402650612881e-06}, {"x": 0.05952380952380953, "y": 0.012345679012345682, "ox": 0.05952380952380953, "oy": 0.012345679012345682, "term": "adherence", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185538881309686, "os": -0.046683849420989576, "bg": 1.2021994238459261e-05}, {"x": 0.029761904761904767, "y": 0.05555555555555556, "ox": 0.029761904761904767, "oy": 0.05555555555555556, "term": "performs", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 5, "s": 0.6800818553888132, "os": 0.025846734671652084, "bg": 4.492619588719931e-06}, {"x": 0.053571428571428575, "y": 0.02469135802469136, "ox": 0.053571428571428575, "oy": 0.02469135802469136, "term": "target", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4788540245566167, "os": -0.02849675100736922, "bg": 4.970701728110339e-07}, {"x": 0.0, "y": 0.03703703703703704, "ox": 0.0, "oy": 0.03703703703703704, "term": "probability", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 0, "s": 0.7291950886766712, "os": 0.03702762551276001, "bg": 9.205522024441582e-07}, {"x": 0.07738095238095238, "y": 0.01851851851851852, "ox": 0.07738095238095238, "oy": 0.01851851851851852, "term": "running", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.24965893587994545, "os": -0.058300359385777045, "bg": 4.384398763698198e-07}, {"x": 0.05952380952380953, "y": 0.012345679012345682, "ox": 0.05952380952380953, "oy": 0.012345679012345682, "term": "enhancements", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185538881309686, "os": -0.046683849420989576, "bg": 3.741992720264996e-06}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "consistency", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 2.809196560419732e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "factory", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 4.4751167287582894e-07}, {"x": 0.10119047619047619, "y": 0.08641975308641976, "ox": 0.10119047619047619, "oy": 0.08641975308641976, "term": "manner", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 17, "s": 0.5634379263301501, "os": -0.014484335862344364, "bg": 2.184982369482985e-06}, {"x": 0.10714285714285715, "y": 0.04938271604938272, "ox": 0.10714285714285715, "oy": 0.04938271604938272, "term": "flexible", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 18, "s": 0.25852660300136426, "os": -0.05721131157657822, "bg": 2.6214862254006285e-06}, {"x": 0.06547619047619048, "y": 0.02469135802469136, "ox": 0.06547619047619048, "oy": 0.02469135802469136, "term": "selected", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3553888130968622, "os": -0.04033107053399644, "bg": 5.104220353139909e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "regarding", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 3.167291820787582e-07}, {"x": 0.07738095238095238, "y": 0.04320987654320988, "ox": 0.07738095238095238, "oy": 0.04320987654320988, "term": "direction", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 13, "s": 0.4351978171896317, "os": -0.033760482085163544, "bg": 1.1773289649326976e-06}, {"x": 0.08333333333333334, "y": 0.012345679012345682, "ox": 0.08333333333333334, "oy": 0.012345679012345682, "term": "workflow", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.20941336971350616, "os": -0.07035248847424401, "bg": 5.970666117365384e-06}, {"x": 0.02380952380952381, "y": 0.04938271604938272, "ox": 0.02380952380952381, "oy": 0.04938271604938272, "term": "above", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 4, "s": 0.6780354706684857, "os": 0.025628925109812323, "bg": 1.6901259433966118e-07}, {"x": 0.08928571428571429, "y": 0.05555555555555556, "ox": 0.08928571428571429, "oy": 0.05555555555555556, "term": "employee", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.4379263301500682, "os": -0.03332486296148401, "bg": 1.047429666516288e-06}, {"x": 0.05952380952380953, "y": 0.012345679012345682, "ox": 0.05952380952380953, "oy": 0.012345679012345682, "term": "remote", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185538881309686, "os": -0.046683849420989576, "bg": 4.725334051584109e-07}, {"x": 0.04761904761904762, "y": 0.02469135802469136, "ox": 0.04761904761904762, "oy": 0.02469135802469136, "term": "contributions", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5368349249658936, "os": -0.022579591244055616, "bg": 1.0916188416323047e-06}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "rapid", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 1.147189466631465e-06}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "easy", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 1.432491954408938e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "construct", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.39889487305029e-06}, {"x": 0.08333333333333334, "y": 0.04320987654320988, "ox": 0.08333333333333334, "oy": 0.04320987654320988, "term": "disparate", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.3608458390177353, "os": -0.03967764184847714, "bg": 3.0363560257569742e-05}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "break", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 2.7264089336475223e-07}, {"x": 0.07738095238095238, "y": 0.030864197530864203, "ox": 0.07738095238095238, "oy": 0.030864197530864203, "term": "compliance", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.3226466575716235, "os": -0.04603042073547029, "bg": 9.622516689052382e-07}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "single", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 1.4830102673323988e-07}, {"x": 0.011904761904761908, "y": 0.03703703703703704, "ox": 0.011904761904761908, "oy": 0.03703703703703704, "term": "hypothesis", "cat25k": 3, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 2, "s": 0.6643929058663028, "os": 0.025193305986132793, "bg": 2.4881424461550426e-06}, {"x": 0.011904761904761908, "y": 0.05555555555555556, "ox": 0.011904761904761908, "oy": 0.05555555555555556, "term": "variables", "cat25k": 5, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7673942701227832, "os": 0.04359821396159291, "bg": 9.118250277402042e-07}, {"x": 0.09523809523809525, "y": 0.03703703703703704, "ox": 0.09523809523809525, "oy": 0.03703703703703704, "term": "also", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 16, "s": 0.2564802182810368, "os": -0.05764693070025774, "bg": 7.132015737946816e-08}, {"x": 0.13095238095238096, "y": 0.04320987654320988, "ox": 0.13095238095238096, "oy": 0.04320987654320988, "term": "bs", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 22, "s": 0.16234652114597548, "os": -0.08701491995498603, "bg": 4.383114760751082e-06}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "aggregation", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 6.537514437011049e-06}, {"x": 0.10714285714285715, "y": 0.03703703703703704, "ox": 0.10714285714285715, "oy": 0.03703703703703704, "term": "via", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 18, "s": 0.21282401091405184, "os": -0.06948125022688496, "bg": 4.934526441516014e-07}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "ecommerce", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 1.3023229161763575e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "bases", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.3683027424549792e-06}, {"x": 0.07142857142857144, "y": 0.02469135802469136, "ox": 0.07142857142857144, "oy": 0.02469135802469136, "term": "fully", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.32196452933151437, "os": -0.04624823029731005, "bg": 6.567676997047911e-07}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "defining", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 3.4010762948869194e-06}, {"x": 0.02380952380952381, "y": 0.05555555555555556, "ox": 0.02380952380952381, "oy": 0.05555555555555556, "term": "proper", "cat25k": 5, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.7175989085948159, "os": 0.031763894434965696, "bg": 9.217826638561749e-07}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "advances", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 2.3979764566671484e-06}, {"x": 0.08928571428571429, "y": 0.00617283950617284, "ox": 0.08928571428571429, "oy": 0.00617283950617284, "term": "administration", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.1732605729877217, "os": -0.082404617562711, "bg": 4.122552616042417e-07}, {"x": 0.10119047619047619, "y": 0.012345679012345682, "ox": 0.10119047619047619, "oy": 0.012345679012345682, "term": "7", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 17, "s": 0.15893587994542974, "os": -0.08810396776418485, "bg": 0.0}, {"x": 0.029761904761904767, "y": 0.04938271604938272, "ox": 0.029761904761904767, "oy": 0.04938271604938272, "term": "education", "cat25k": 4, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 5, "s": 0.6330150068212824, "os": 0.01971176534649871, "bg": 9.743494962637463e-08}, {"x": 0.005952380952380953, "y": 0.030864197530864203, "ox": 0.005952380952380953, "oy": 0.030864197530864203, "term": "fraud", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 1, "s": 0.6521145975443383, "os": 0.02497549642429303, "bg": 7.74924627279013e-07}, {"x": 0.005952380952380953, "y": 0.04938271604938272, "ox": 0.005952380952380953, "oy": 0.04938271604938272, "term": "sr", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.7646657571623465, "os": 0.04338040439975315, "bg": 1.1036052330507694e-06}, {"x": 0.053571428571428575, "y": 0.030864197530864203, "ox": 0.053571428571428575, "oy": 0.030864197530864203, "term": "manager", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5409276944065484, "os": -0.022361781682215844, "bg": 3.19823318633416e-07}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "less", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 1.374223094139291e-07}, {"x": 0.22023809523809523, "y": 0.04320987654320988, "ox": 0.22023809523809523, "oy": 0.04320987654320988, "term": "streaming", "cat25k": 4, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 37, "s": 0.051841746248294684, "os": -0.17577231640469018, "bg": 9.243854779881758e-06}, {"x": 0.0, "y": 0.04938271604938272, "ox": 0.0, "oy": 0.04938271604938272, "term": "fusion", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.786493860845839, "os": 0.04929756416306676, "bg": 1.4623429329444477e-06}, {"x": 0.1488095238095238, "y": 0.0617283950617284, "ox": 0.1488095238095238, "oy": 0.0617283950617284, "term": "tool", "cat25k": 5, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 25, "s": 0.16439290586630287, "os": -0.08636149126946674, "bg": 1.0749982346993276e-06}, {"x": 0.0, "y": 0.08024691358024692, "ox": 0.0, "oy": 0.08024691358024692, "term": "llnl", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.8628922237380628, "os": 0.07997241078883363, "bg": 5.979031994720055e-05}, {"x": 0.0, "y": 0.05555555555555556, "ox": 0.0, "oy": 0.05555555555555556, "term": "sponsor", "cat25k": 5, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.8042291950886767, "os": 0.05543253348822013, "bg": 5.279403483719977e-07}, {"x": 0.005952380952380953, "y": 0.04320987654320988, "ox": 0.005952380952380953, "oy": 0.04320987654320988, "term": "fulfill", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 1, "s": 0.7387448840381993, "os": 0.03724543507459978, "bg": 3.3379306373841133e-06}, {"x": 0.01785714285714286, "y": 0.04320987654320988, "ox": 0.01785714285714286, "oy": 0.04320987654320988, "term": "mechanisms", "cat25k": 4, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 3, "s": 0.6718963165075035, "os": 0.025411115547972558, "bg": 1.4375880837173671e-06}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "vendors", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 1.3535140473101886e-06}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "person", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 1.4980653269382193e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "flexibility", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 1.249329071247207e-06}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "compute", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 3.840419066528539e-06}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "made", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 4.870248780100086e-08}, {"x": 0.13095238095238096, "y": 0.01851851851851852, "ox": 0.13095238095238096, "oy": 0.01851851851851852, "term": "stored", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 22, "s": 0.11050477489768076, "os": -0.11155479725559952, "bg": 2.7029222968763356e-06}, {"x": 0.35714285714285715, "y": 0.04320987654320988, "ox": 0.35714285714285715, "oy": 0.04320987654320988, "term": "amazon", "cat25k": 4, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 61, "s": 0.015688949522510234, "os": -0.3118669909609032, "bg": 2.3143293539801464e-06}, {"x": 0.04761904761904762, "y": 0.00617283950617284, "ox": 0.04761904761904762, "oy": 0.00617283950617284, "term": "speed", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3417462482946794, "os": -0.04098449921951574, "bg": 2.1623601382459306e-07}, {"x": 0.05952380952380953, "y": 0.02469135802469136, "ox": 0.05952380952380953, "oy": 0.02469135802469136, "term": "repeatable", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.4195088676671214, "os": -0.03441391077068283, "bg": 3.7347126872191464e-05}, {"x": 0.0, "y": 0.04320987654320988, "ox": 0.0, "oy": 0.04320987654320988, "term": "draw", "cat25k": 4, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 0, "s": 0.7578444747612552, "os": 0.04316259483791338, "bg": 6.137121351667662e-07}, {"x": 0.11904761904761905, "y": 0.04938271604938272, "ox": 0.11904761904761905, "oy": 0.04938271604938272, "term": "feedback", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 20, "s": 0.21555252387448842, "os": -0.06904563110320543, "bg": 3.7057936782031844e-07}, {"x": 0.10714285714285715, "y": 0.04320987654320988, "ox": 0.10714285714285715, "oy": 0.04320987654320988, "term": "timely", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 18, "s": 0.2353342428376535, "os": -0.06334628090173158, "bg": 5.159869793781676e-06}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "recruiters", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 3.57025639796322e-06}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "perl", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 6.553376266685716e-07}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "values", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 4.5679751821908355e-07}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "leave", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 3.940089032503258e-07}, {"x": 0.06547619047619048, "y": 0.00617283950617284, "ox": 0.06547619047619048, "oy": 0.00617283950617284, "term": "because", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.24283765347885405, "os": -0.05873597850945657, "bg": 8.842038594186894e-08}, {"x": 0.05952380952380953, "y": 0.00617283950617284, "ox": 0.05952380952380953, "oy": 0.00617283950617284, "term": "live", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.27489768076398363, "os": -0.052818818746142956, "bg": 1.3244103394066376e-07}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "only", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 3.323510120076985e-08}, {"x": 0.06547619047619048, "y": 0.012345679012345682, "ox": 0.06547619047619048, "oy": 0.012345679012345682, "term": "3rd", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28035470668485674, "os": -0.05260100918430319, "bg": 0.0}, {"x": 0.07142857142857144, "y": 0.00617283950617284, "ox": 0.07142857142857144, "oy": 0.00617283950617284, "term": "spirit", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22578444747612553, "os": -0.06465313827277018, "bg": 7.012184695766658e-07}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "someone", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 3.668821990394893e-07}, {"x": 0.22023809523809523, "y": 0.04320987654320988, "ox": 0.22023809523809523, "oy": 0.04320987654320988, "term": "stores", "cat25k": 4, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 37, "s": 0.051841746248294684, "os": -0.17577231640469018, "bg": 6.227130465325084e-07}, {"x": 0.16666666666666669, "y": 0.012345679012345682, "ox": 0.16666666666666669, "oy": 0.012345679012345682, "term": "flows", "cat25k": 1, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 28, "s": 0.06753069577080491, "os": -0.15319272516063454, "bg": 5.502660719880086e-06}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "deployed", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 3.2038798985571525e-06}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "lakes", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 1.0628340163481144e-06}, {"x": 0.25595238095238093, "y": 0.01851851851851852, "ox": 0.25595238095238093, "oy": 0.01851851851851852, "term": "warehousing", "cat25k": 2, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 43, "s": 0.030013642564802184, "os": -0.23581515228518532, "bg": 2.7977131735798563e-05}, {"x": 0.14285714285714288, "y": 0.03703703703703704, "ox": 0.14285714285714288, "oy": 0.03703703703703704, "term": "warehouses", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 24, "s": 0.1275579809004093, "os": -0.10498420880676662, "bg": 3.482272043217318e-05}, {"x": 0.15476190476190477, "y": 0.012345679012345682, "ox": 0.15476190476190477, "oy": 0.012345679012345682, "term": "daily", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 26, "s": 0.08185538881309685, "os": -0.14135840563400734, "bg": 5.285791165659008e-07}, {"x": 0.09523809523809525, "y": 0.06790123456790124, "ox": 0.09523809523809525, "oy": 0.06790123456790124, "term": "microsoft", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 16, "s": 0.4945429740791269, "os": -0.026972084074490865, "bg": 5.280335608744029e-07}, {"x": 0.05952380952380953, "y": 0.01851851851851852, "ox": 0.05952380952380953, "oy": 0.01851851851851852, "term": "responsibility", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.35402455661664395, "os": -0.04054888009583621, "bg": 4.680041914455386e-07}, {"x": 0.2380952380952381, "y": 0.04938271604938272, "ox": 0.2380952380952381, "oy": 0.04938271604938272, "term": "service", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 40, "s": 0.047748976807639835, "os": -0.18738882636947762, "bg": 1.8474190084290221e-07}, {"x": 0.10119047619047619, "y": 0.04938271604938272, "ox": 0.10119047619047619, "oy": 0.04938271604938272, "term": "necessary", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 17, "s": 0.2892223738062756, "os": -0.051294151813264605, "bg": 6.876533896842364e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "guidelines", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 3.089559988397378e-07}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "pm", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 3.307512055281954e-08}, {"x": 0.04761904761904762, "y": 0.00617283950617284, "ox": 0.04761904761904762, "oy": 0.00617283950617284, "term": "coordination", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3417462482946794, "os": -0.04098449921951574, "bg": 1.6809929289032444e-06}, {"x": 0.19642857142857142, "y": 0.00617283950617284, "ox": 0.19642857142857142, "oy": 0.00617283950617284, "term": "scrum", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 33, "s": 0.045702592087312414, "os": -0.18891349330235596, "bg": 0.00012769377530402385}, {"x": 0.053571428571428575, "y": 0.01851851851851852, "ox": 0.053571428571428575, "oy": 0.01851851851851852, "term": "streams", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4154160982264666, "os": -0.0346317203325226, "bg": 2.6698295247102738e-06}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "looker", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 6.868249798245163e-05}, {"x": 0.07738095238095238, "y": 0.012345679012345682, "ox": 0.07738095238095238, "oy": 0.012345679012345682, "term": "traditional", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.22919508867667124, "os": -0.06443532871093041, "bg": 6.255965323601275e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "dynamodb", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 0.00011254290698328738}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "advice", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.6812527249428932e-07}, {"x": 0.16666666666666669, "y": 0.01851851851851852, "ox": 0.16666666666666669, "oy": 0.01851851851851852, "term": "git", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 28, "s": 0.07435197817189632, "os": -0.14705775583548117, "bg": 3.8599365974285356e-05}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "inspire", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 3.2643556841953497e-06}, {"x": 0.04166666666666667, "y": 0.01851851851851852, "ox": 0.04166666666666667, "oy": 0.01851851851851852, "term": "mobile", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5300136425648022, "os": -0.022797400805895377, "bg": 1.3736578941691236e-07}, {"x": 0.11904761904761905, "y": 0.030864197530864203, "ox": 0.11904761904761905, "oy": 0.030864197530864203, "term": "postgres", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 20, "s": 0.16030013642564805, "os": -0.08745053907866554, "bg": 5.869598648583607e-05}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "teradata", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 4.404882077636047e-05}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "task", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 4.006074310324888e-07}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "responsibilities", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 7.136944524173367e-07}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "sdlc", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 5.6020142353406185e-05}, {"x": 0.08333333333333334, "y": 0.012345679012345682, "ox": 0.08333333333333334, "oy": 0.012345679012345682, "term": "operating", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.20941336971350616, "os": -0.07035248847424401, "bg": 4.988032074355597e-07}, {"x": 0.10714285714285715, "y": 0.00617283950617284, "ox": 0.10714285714285715, "oy": 0.00617283950617284, "term": "emr", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.1364256480218281, "os": -0.10015609685265184, "bg": 4.994302552220033e-05}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "informatics", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 3.815903376965462e-06}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "rest", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 2.995209387225836e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "gain", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 4.986470148215969e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "node", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 7.145233570986779e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "premise", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 4.940838095838054e-06}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "messaging", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 1.3639936369696835e-06}, {"x": 0.005952380952380953, "y": 0.03703703703703704, "ox": 0.005952380952380953, "oy": 0.03703703703703704, "term": "targeting", "cat25k": 3, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 1, "s": 0.7019099590723056, "os": 0.031110465749446405, "bg": 2.7867090697626737e-06}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "producing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 1.031007855838001e-06}, {"x": 0.07142857142857144, "y": 0.01851851851851852, "ox": 0.07142857142857144, "oy": 0.01851851851851852, "term": "handle", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.2830832196452933, "os": -0.05238319962246343, "bg": 9.252034676625968e-07}, {"x": 0.04761904761904762, "y": 0.01851851851851852, "ox": 0.04761904761904762, "oy": 0.01851851851851852, "term": "align", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.47066848567530695, "os": -0.02871456056920899, "bg": 3.938929406149099e-06}, {"x": 0.11904761904761905, "y": 0.00617283950617284, "ox": 0.11904761904761905, "oy": 0.00617283950617284, "term": "faster", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.10914051841746249, "os": -0.11199041637927905, "bg": 1.3271032366562852e-06}, {"x": 0.09523809523809525, "y": 0.01851851851851852, "ox": 0.09523809523809525, "oy": 0.01851851851851852, "term": "go", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 16, "s": 0.19372442019099592, "os": -0.07605183867571787, "bg": 9.021992357579487e-08}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "providers", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 3.599505911250017e-07}, {"x": 0.06547619047619048, "y": 0.030864197530864203, "ox": 0.06547619047619048, "oy": 0.030864197530864203, "term": "gaps", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.42360163710777626, "os": -0.03419610120884307, "bg": 5.599014993287306e-06}, {"x": 0.10119047619047619, "y": 0.00617283950617284, "ox": 0.10119047619047619, "oy": 0.00617283950617284, "term": "loading", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.14665757162346524, "os": -0.09423893708933823, "bg": 2.0994217317791627e-06}, {"x": 0.07142857142857144, "y": 0.00617283950617284, "ox": 0.07142857142857144, "oy": 0.00617283950617284, "term": "salesforce", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22578444747612553, "os": -0.06465313827277018, "bg": 0.0001017839596309157}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "bachelors", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 8.504781459009436e-06}, {"x": 0.14285714285714288, "y": 0.01851851851851852, "ox": 0.14285714285714288, "oy": 0.01851851851851852, "term": "attention", "cat25k": 2, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 24, "s": 0.09890859481582537, "os": -0.12338911678222675, "bg": 1.2037678200270807e-06}, {"x": 0.10714285714285715, "y": 0.012345679012345682, "ox": 0.10714285714285715, "oy": 0.012345679012345682, "term": "tooling", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 18, "s": 0.1480218281036835, "os": -0.09402112752749846, "bg": 2.6839801278111335e-05}, {"x": 0.05952380952380953, "y": 0.00617283950617284, "ox": 0.05952380952380953, "oy": 0.00617283950617284, "term": "excited", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.27489768076398363, "os": -0.052818818746142956, "bg": 2.2638913142825512e-06}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "status", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 1.7040891881861628e-07}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "possess", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 3.250285528555113e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "clarify", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 2.342124693547622e-06}, {"x": 0.13690476190476192, "y": 0.02469135802469136, "ox": 0.13690476190476192, "oy": 0.02469135802469136, "term": "specifications", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 23, "s": 0.1111869031377899, "os": -0.11133698769375976, "bg": 1.711974005893946e-06}, {"x": 0.04761904761904762, "y": 0.00617283950617284, "ox": 0.04761904761904762, "oy": 0.00617283950617284, "term": "audits", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3417462482946794, "os": -0.04098449921951574, "bg": 4.6277775663854696e-06}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "minimal", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 1.849588752348137e-06}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "video", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 7.66038973146534e-08}, {"x": 0.11904761904761905, "y": 0.01851851851851852, "ox": 0.11904761904761905, "oy": 0.01851851851851852, "term": "reliability", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 20, "s": 0.13847203274215555, "os": -0.0997204777289723, "bg": 2.7049806396891105e-06}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "fit", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 4.7114882479464036e-07}, {"x": 0.07142857142857144, "y": 0.012345679012345682, "ox": 0.07142857142857144, "oy": 0.012345679012345682, "term": "meeting", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.24488403819918145, "os": -0.0585181689476168, "bg": 2.3155625485544524e-07}, {"x": 0.08333333333333334, "y": 0.00617283950617284, "ox": 0.08333333333333334, "oy": 0.00617283950617284, "term": "sense", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 14, "s": 0.19099590723055934, "os": -0.07648745779939739, "bg": 5.688684682806908e-07}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "functionality", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 1.654858499067412e-06}, {"x": 0.053571428571428575, "y": 0.012345679012345682, "ox": 0.053571428571428575, "oy": 0.012345679012345682, "term": "accountable", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3458390177353342, "os": -0.040766689657675964, "bg": 6.09724664970154e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "roles", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 9.03741512455102e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "update", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 1.5861715030489783e-07}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "documented", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 2.546262080899584e-06}, {"x": 0.19642857142857142, "y": 0.00617283950617284, "ox": 0.19642857142857142, "oy": 0.00617283950617284, "term": "s3", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 33, "s": 0.045702592087312414, "os": -0.18891349330235596, "bg": 0.0}, {"x": 0.05952380952380953, "y": 0.00617283950617284, "ox": 0.05952380952380953, "oy": 0.00617283950617284, "term": "architecting", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.27489768076398363, "os": -0.052818818746142956, "bg": 9.054764864219685e-05}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "read", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 4.3419147049278336e-08}, {"x": 0.07738095238095238, "y": 0.00617283950617284, "ox": 0.07738095238095238, "oy": 0.00617283950617284, "term": "gcp", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.20873124147339703, "os": -0.07057029803608379, "bg": 8.591882512143705e-05}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "succeed", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.555969991044096e-06}, {"x": 0.06547619047619048, "y": 0.012345679012345682, "ox": 0.06547619047619048, "oy": 0.012345679012345682, "term": "foundation", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28035470668485674, "os": -0.05260100918430319, "bg": 4.008624895654338e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "library", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 9.865397563837492e-08}, {"x": 0.2619047619047619, "y": 0.00617283950617284, "ox": 0.2619047619047619, "oy": 0.00617283950617284, "term": "devops", "cat25k": 1, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 44, "s": 0.0252387448840382, "os": -0.2540022506988057, "bg": 0.0008437631837997469}, {"x": 0.05952380952380953, "y": 0.00617283950617284, "ox": 0.05952380952380953, "oy": 0.00617283950617284, "term": "fix", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.27489768076398363, "os": -0.052818818746142956, "bg": 6.781607515944639e-07}, {"x": 0.04761904761904762, "y": 0.012345679012345682, "ox": 0.04761904761904762, "oy": 0.012345679012345682, "term": "querying", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4092769440654843, "os": -0.034849529894362366, "bg": 2.2419262630452084e-05}, {"x": 0.1130952380952381, "y": 0.00617283950617284, "ox": 0.1130952380952381, "oy": 0.00617283950617284, "term": "architectural", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.12414733969986358, "os": -0.10607325661596544, "bg": 3.871331647110312e-06}, {"x": 0.10714285714285715, "y": 0.00617283950617284, "ox": 0.10714285714285715, "oy": 0.00617283950617284, "term": "architects", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.1364256480218281, "os": -0.10015609685265184, "bg": 5.419553291893717e-06}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "satisfy", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.3134277520060146e-06}, {"x": 0.06547619047619048, "y": 0.00617283950617284, "ox": 0.06547619047619048, "oy": 0.00617283950617284, "term": "#", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.24283765347885405, "os": -0.05873597850945657, "bg": 0.0}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "recognize", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 8.329500606179407e-07}, {"x": 0.08333333333333334, "y": 0.00617283950617284, "ox": 0.08333333333333334, "oy": 0.00617283950617284, "term": "load", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 14, "s": 0.19099590723055934, "os": -0.07648745779939739, "bg": 7.894142911721458e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "bugs", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 7.918725370289497e-07}, {"x": 0.16071428571428573, "y": 0.00617283950617284, "ox": 0.16071428571428573, "oy": 0.00617283950617284, "term": "airflow", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 27, "s": 0.06684856753069576, "os": -0.15341053472247432, "bg": 4.875105881205857e-05}, {"x": 0.4166666666666667, "y": 0.00617283950617284, "ox": 0.4166666666666667, "oy": 0.00617283950617284, "term": "kafka", "cat25k": 1, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 72, "s": 0.006139154160982265, "os": -0.4078484045449595, "bg": 0.00024222394765308221}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "setup", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 5.921796334619561e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "everyone", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 2.1771388262369676e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "infrastructures", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 9.407805521844253e-06}, {"x": 0.14285714285714288, "y": 0.00617283950617284, "ox": 0.14285714285714288, "oy": 0.00617283950617284, "term": "restful", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 24, "s": 0.08799454297407913, "os": -0.13565905543253348, "bg": 9.898618350850588e-05}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "memory", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 9.958284745202347e-08}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "ready", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 1.9116151360539502e-07}, {"x": 0.04166666666666667, "y": 0.012345679012345682, "ox": 0.04166666666666667, "oy": 0.012345679012345682, "term": "challenge", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4631650750341064, "os": -0.02893237013104875, "bg": 4.861586578779984e-07}, {"x": 0.07142857142857144, "y": 0.00617283950617284, "ox": 0.07142857142857144, "oy": 0.00617283950617284, "term": "logging", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22578444747612553, "os": -0.06465313827277018, "bg": 2.8623022355571256e-06}, {"x": 0.053571428571428575, "y": 0.00617283950617284, "ox": 0.053571428571428575, "oy": 0.00617283950617284, "term": "checks", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.3130968622100955, "os": -0.046901658982829345, "bg": 8.851460194408162e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "personnel", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 4.792641139162872e-07}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "integrations", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 3.2939432618273146e-05}, {"x": 0.029761904761904767, "y": 0.00617283950617284, "ox": 0.029761904761904767, "oy": 0.00617283950617284, "term": "ec2", "cat25k": 1, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.4993178717598909, "os": -0.02323301992957491, "bg": 0.0}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "offerings", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 1.8902861385199324e-06}, {"x": 0.04761904761904762, "y": 0.00617283950617284, "ox": 0.04761904761904762, "oy": 0.00617283950617284, "term": "purpose", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3417462482946794, "os": -0.04098449921951574, "bg": 2.7940947304599683e-07}, {"x": 0.03571428571428572, "y": 0.012345679012345682, "ox": 0.03571428571428572, "oy": 0.012345679012345682, "term": "forms", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.519099590723056, "os": -0.023015210367735146, "bg": 2.513027218974763e-07}, {"x": 0.03571428571428572, "y": 0.00617283950617284, "ox": 0.03571428571428572, "oy": 0.00617283950617284, "term": "meets", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.44815825375170537, "os": -0.029150179692888523, "bg": 7.835663305129795e-07}, {"x": 0.05952380952380953, "y": 0.00617283950617284, "ox": 0.05952380952380953, "oy": 0.00617283950617284, "term": "marts", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.27489768076398363, "os": -0.052818818746142956, "bg": 5.913390334832288e-05}, {"x": 0.04166666666666667, "y": 0.00617283950617284, "ox": 0.04166666666666667, "oy": 0.00617283950617284, "term": "facilitating", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.40040927694406553, "os": -0.03506733945620213, "bg": 5.716913452719161e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "teammates", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.0409004482811264e-05}, {"x": 0.14285714285714288, "y": 0.0, "ox": 0.14285714285714288, "oy": 0.0, "term": "docker", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 24, "s": 0.08049113233287858, "os": -0.14179402475768688, "bg": 0.00022938081516207188}, {"x": 0.05952380952380953, "y": 0.0, "ox": 0.05952380952380953, "oy": 0.0, "term": "etls", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2401091405184175, "os": -0.05895378807129633, "bg": 0.00018756447528838036}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "administrators", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.0455414323238022e-06}, {"x": 0.07738095238095238, "y": 0.0, "ox": 0.07738095238095238, "oy": 0.0, "term": "troubleshoot", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18758526603001366, "os": -0.07670526736123716, "bg": 2.5194042577931957e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "conceptual", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 3.066683303423588e-06}, {"x": 0.10714285714285715, "y": 0.0, "ox": 0.10714285714285715, "oy": 0.0, "term": "snowflake", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.12073669849931787, "os": -0.1062910661778052, "bg": 3.680394538294506e-05}, {"x": 0.10119047619047619, "y": 0.0, "ox": 0.10119047619047619, "oy": 0.0, "term": "elt", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 17, "s": 0.13574351978171897, "os": -0.10037390641449159, "bg": 3.7208813673582396e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "dw", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 3.1866790442591876e-06}, {"x": 0.05952380952380953, "y": 0.0, "ox": 0.05952380952380953, "oy": 0.0, "term": "terraform", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2401091405184175, "os": -0.05895378807129633, "bg": 0.00014180977636598266}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "mentality", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 7.176677234323894e-06}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "maintainable", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 5.3334146044130194e-05}, {"x": 0.125, "y": 0.0, "ox": 0.125, "oy": 0.0, "term": "houston", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 21, "s": 0.09618008185538882, "os": -0.12404254546774603, "bg": 1.1730617758630306e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "parental", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.9054439327092475e-06}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "lunches", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 1.1455869128151079e-05}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "accessibility", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 7.32642712913168e-07}, {"x": 0.07142857142857144, "y": 0.0, "ox": 0.07142857142857144, "oy": 0.0, "term": "automating", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.2080491132332879, "os": -0.07078810759792355, "bg": 2.1319291026976898e-05}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "fault", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 1.361646193937906e-06}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "tolerant", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 7.096212219322631e-06}, {"x": 0.10714285714285715, "y": 0.0, "ox": 0.10714285714285715, "oy": 0.0, "term": "kubernetes", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.12073669849931787, "os": -0.1062910661778052, "bg": 0.0003375907275080178}, {"x": 0.22023809523809523, "y": 0.0, "ox": 0.22023809523809523, "oy": 0.0, "term": "kinesis", "cat25k": 0, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 37, "s": 0.03342428376534789, "os": -0.2187171016807638, "bg": 0.000259603578319593}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "massively", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.1137232045157764e-05}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "oozie", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 0.00011254290698328738}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "rds", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 1.1558898003561587e-05}, {"x": 0.08928571428571429, "y": 0.0, "ox": 0.08928571428571429, "oy": 0.0, "term": "lambda", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 15, "s": 0.15688949522510234, "os": -0.08853958688786437, "bg": 3.1961942276093008e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "upgrades", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.2200589898521596e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "internally", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 3.1673367343491332e-06}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "secure", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 2.8848745147701166e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "makes", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.3646253448408248e-07}, {"x": 0.05952380952380953, "y": 0.0, "ox": 0.05952380952380953, "oy": 0.0, "term": "technically", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2401091405184175, "os": -0.05895378807129633, "bg": 5.0956050151963685e-06}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "segregation", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 9.731376173374198e-06}, {"x": 0.06547619047619048, "y": 0.0, "ox": 0.06547619047619048, "oy": 0.0, "term": "tdd", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.22305593451568898, "os": -0.06487094783460993, "bg": 1.3824835563007002e-05}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "timescales", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 3.737463923091298e-05}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "sophos", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 2.1104269159159018e-05}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "possesses", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 8.392719595508217e-06}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "inception", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 7.195964503106658e-06}, {"x": 0.125, "y": 0.0, "ox": 0.125, "oy": 0.0, "term": "ci", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 21, "s": 0.09618008185538882, "os": -0.12404254546774603, "bg": 4.743730679886163e-06}, {"x": 0.11904761904761905, "y": 0.0, "ox": 0.11904761904761905, "oy": 0.0, "term": "cd", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 20, "s": 0.10300136425648021, "os": -0.11812538570443241, "bg": 2.094617144988027e-07}, {"x": 0.07738095238095238, "y": 0.0, "ox": 0.07738095238095238, "oy": 0.0, "term": "kanban", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18758526603001366, "os": -0.07670526736123716, "bg": 0.0001365696846816088}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "recommended", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 3.4218504580251395e-07}, {"x": 0.1488095238095238, "y": 0.0, "ox": 0.1488095238095238, "oy": 0.0, "term": "columnar", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 25, "s": 0.07298772169167804, "os": -0.1477111845210005, "bg": 0.00014281511440918814}, {"x": 0.10714285714285715, "y": 0.0, "ox": 0.10714285714285715, "oy": 0.0, "term": "hg", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.12073669849931787, "os": -0.1062910661778052, "bg": 7.174054315163779e-06}, {"x": 0.1130952380952381, "y": 0.0, "ox": 0.1130952380952381, "oy": 0.0, "term": "svn", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.10709413369713505, "os": -0.1122082259411188, "bg": 9.75110059132727e-06}, {"x": 0.10714285714285715, "y": 0.0, "ox": 0.10714285714285715, "oy": 0.0, "term": "maven", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.12073669849931787, "os": -0.1062910661778052, "bg": 1.933968928640307e-05}, {"x": 0.10714285714285715, "y": 0.0, "ox": 0.10714285714285715, "oy": 0.0, "term": "sbt", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.12073669849931787, "os": -0.1062910661778052, "bg": 0.00010726640505582323}, {"x": 0.15476190476190477, "y": 0.0, "ox": 0.15476190476190477, "oy": 0.0, "term": "jenkins", "cat25k": 0, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 26, "s": 0.06616643929058663, "os": -0.1536283442843141, "bg": 1.2707641642550658e-05}, {"x": 0.1130952380952381, "y": 0.0, "ox": 0.1130952380952381, "oy": 0.0, "term": "bamboo", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.10709413369713505, "os": -0.1122082259411188, "bg": 8.9552077007245e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "outputs", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.951297246800889e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "glue", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 2.641879380153454e-06}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "consume", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 4.396878593074225e-06}, {"x": 0.07738095238095238, "y": 0.0, "ox": 0.07738095238095238, "oy": 0.0, "term": "elasticsearch", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18758526603001366, "os": -0.07670526736123716, "bg": 0.000243826957883582}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "implementations", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 3.154190223429693e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "workload", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 4.093492642970348e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "latency", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 4.081510485400438e-06}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "conventions", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 2.3690924643462693e-06}, {"x": 0.06547619047619048, "y": 0.0, "ox": 0.06547619047619048, "oy": 0.0, "term": "schema", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.22305593451568898, "os": -0.06487094783460993, "bg": 3.2880223920303124e-06}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "flink", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 9.907120743034055e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "unlimited", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 6.694124290977183e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "updates", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 2.4507151983431206e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "ssms", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 9.360593461625466e-05}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "react", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 2.834867547900993e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "adobe", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 4.3406889563214914e-07}, {"x": 0.08333333333333334, "y": 0.0, "ox": 0.08333333333333334, "oy": 0.0, "term": "ideally", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.17189631650750342, "os": -0.08262242712455076, "bg": 6.739094280169662e-06}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "enough", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 1.9116062744843907e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "really", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 7.40101074123491e-08}, {"x": 0.08333333333333334, "y": 0.0, "ox": 0.08333333333333334, "oy": 0.0, "term": "want", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.17189631650750342, "os": -0.08262242712455076, "bg": 1.0819291673416327e-07}, {"x": 0.1130952380952381, "y": 0.0, "ox": 0.1130952380952381, "oy": 0.0, "term": "manual", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.10709413369713505, "os": -0.1122082259411188, "bg": 7.629758346478772e-07}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "moving", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 3.0844857020111134e-07}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "dbt", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 3.579866828953963e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "storm", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 7.159223639470088e-07}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "weeks", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 1.900150459342372e-07}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "near", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 1.956961643368325e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "parquet", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 2.8023306049531196e-05}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "jvm", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 1.9154193384992905e-05}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "databricks", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 0.00011254290698328738}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "container", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 9.765173615021701e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "micro", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 5.374764887628447e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "dataflow", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 3.33592794395641e-05}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "kimball", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 9.451291755300655e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "biological", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 5.955297748510356e-07}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "chance", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 3.2133480642389594e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "raise", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 5.658623985803645e-07}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "ads", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 2.929305138429624e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "standardized", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 2.8684534422516976e-06}, {"x": 0.06547619047619048, "y": 0.0, "ox": 0.06547619047619048, "oy": 0.0, "term": "profile", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.22305593451568898, "os": -0.06487094783460993, "bg": 1.0893171969674042e-07}, {"x": 0.05952380952380953, "y": 0.0, "ox": 0.05952380952380953, "oy": 0.0, "term": "board", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2401091405184175, "os": -0.05895378807129633, "bg": 9.413195998945515e-08}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "growflow", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 0.00011254290698328738}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "talend", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 0.00011254290698328738}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "error", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.340735669035646e-07}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "lineage", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 6.426890108212762e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "regardless", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 8.44883239248544e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "adf", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.0483828694239136e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "populate", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 1.7455738792324716e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "000", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 0.0}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "exclusive", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 5.483939494872961e-07}, {"x": 0.07738095238095238, "y": 0.0, "ox": 0.07738095238095238, "oy": 0.0, "term": "devex", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18758526603001366, "os": -0.07670526736123716, "bg": 0.000243826957883582}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "highlighted", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 2.652925397416515e-06}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "recruiter", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 5.338132314283874e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "sla", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 7.222645883904393e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "eco", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.5299992694253488e-06}, {"x": 0.04166666666666667, "y": 0.0, "ox": 0.04166666666666667, "oy": 0.0, "term": "recovery", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3362892223738063, "os": -0.0412023087813555, "bg": 3.7084662376995474e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "screened", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 3.4712599921665227e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "scams", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 4.333359574232977e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "junk", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.8234574120124818e-06}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "routines", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 1.945044706852587e-06}, {"x": 0.06547619047619048, "y": 0.0, "ox": 0.06547619047619048, "oy": 0.0, "term": "eg", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.22305593451568898, "os": -0.06487094783460993, "bg": 3.4001707998524695e-07}, {"x": 0.053571428571428575, "y": 0.0, "ox": 0.053571428571428575, "oy": 0.0, "term": "chembl", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.2680763983628922, "os": -0.05303662830798272, "bg": 0.0001688096108938469}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "demonstrable", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 2.56595026760723e-05}, {"x": 0.04761904761904762, "y": 0.0, "ox": 0.04761904761904762, "oy": 0.0, "term": "faculty", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2994542974079127, "os": -0.04711946854466911, "bg": 3.023295455157414e-07}, {"x": 0.03571428571428572, "y": 0.0, "ox": 0.03571428571428572, "oy": 0.0, "term": "nweh", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.3731241473396999, "os": -0.035285149018041896, "bg": 0.00011254290698328738}, {"x": 0.09523809523809525, "y": 0.0, "ox": 0.09523809523809525, "oy": 0.0, "term": "ed", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 16, "s": 0.1459754433833561, "os": -0.09445674665117798, "bg": 6.166229840667704e-07}], "docs": {"categories": ["data scientist", "data engineer"], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "texts": ["Lead effort to determine overpayments from contracts and from other sources of information.,Automate procedures for obtaining, reading, parsing, and understanding contracts.,Improve pricing procedures when contracts are not available.,Support initiatives for data and process quality.,Interface with others in our Data Science and Data Analytics teams.,Explain insights to team members and other Equian employees,Provide technical expertise on other aspects of claim scoring as needed.,Bachelor\u2019s degree in a quantitative field.,Strong understanding of Natural Language Processing.,Experience in Python or R.,Proficiency in basic SQL commands and queries.,Strong analytical and quantitative skills.,Self-starter and eager to learn.,Excellent communication skills, both written and verbal.,Demonstrated ability to build relationships and work as part of a team.", "Analysis:,Deep dive analysis into Airbnb\u2019s vast data to uncover opportunities.,Partner with data scientists and analysts to help \u201ctell the story\u201d behind the data.,Product Strategy:,Partnering with leadership to prioritize areas of opportunity to drive growth/quality.,Evaluate and define product and business metrics.,Product Execution:,Communicate state of the business to stakeholders.,Digest and interpret experiments.,Decision Tools:,Democratize data by building and socializing decision tools (dashboards, reports).,Build key data sets/pipelines to empower operational and exploratory analysis.", "Lead projects to Identify candidate therapeutic targets and biomarkers for immunology, through integrated analysis of high-throughput molecular data, real-world data and clinical phenotypes,Explore opportunities to re-position existing drugs in novel indications,Collaborate with wet-lab and computational scientific teams and champion projects from discovery to early clinical development.,Develop innovative and creative computational approaches to understand complex immune conditions using multi-dimensional \u2013omics and clinical data,Establish interactions with collaborators within and outside the Immunology and Inflammation Research and Early Development Therapeutic Area.,Prepare publications, research and clinical/regulatory documentation in accordance with Sanofi quality standards.", "10% Network with business stakeholders to develop a pipeline of data science projects aligned with business strategies. Translate complex and ambiguous business problems into project charters clearly identifying technical risks and project scope.,10% Work in a cross-disciplinary project team of database specialists, data scientists, and business subject-matter experts to complete project deliverables on-time and within budget.,10% Design strategies and propose algorithms to analyze and leverage data from existing as well as new data sources.,50% Develop and code models by applying algorithms to large structured as well as unstructured data sets for our more complex projects. Develop visualization products to share analysis across a large group of business users.,20% Continuously seek out industry best practices and develop skills to create new capabilities for data analytics at Cargill to improve business decisions.", "Design, coding, testing and monitoring of new classifiers in production environments that drive our analytical and SaaS product platforms,Maintain our data assets,Perform large-scale data analysis,Work closely with other groups like product, engineering and customer operations to acquire requirements and develop new systems that solve their problems", "Analysis, design, development, test, troubleshooting, and documentation of complex data and data management systems that may involve one or more of the following: predictive score models, feature extraction/feature engineering, data-driven analysis, application of machine learning algorithms, decision making, and related utilities.,Complete written documentation and reports of results in the form of business reports, internal technology white papers, and statistical system documentation.,Independent resolution of problems under changing priorities.,Devise new approaches to solve difficult statistical modeling problems. This includes new model concepts and designs, such as creative targets generation, objective functions modification, and new or different uses of data sources.,Provide insights from data experiments to ensure accuracy, completeness, feasibility, and to provide analytical opinions to help business decision.,Identify and deploy new algorithms or combinations of algorithms in creative ways to achieve performance improvement.,Work in team environment, interact with multiple departments including internal and external customers.,Monitor system performance and proactively prevent system degradation.", "Safety evaluation is of fundamental importance to Waymo, and measuring it is subtle. This role works with a wide variety of teams across the company (onboard software engineering, product, safety, systems engineering) to evaluate the driving and safety performance of Waymo's self driving car. This is based on a wide variety of data from real and simulated driving from Waymo\u2019s self-driving system as well as from external data sources.,The Planner Team is responsible for \"motion planning\" for the self driving car. This includes gracefully handling many complex situations involving social interactions (merging, negotiating narrow roads, etc), while dealing with a noisy and uncertain environment. Evaluation of this onboard decision-making system is a complex, open-ended problem with tremendous potential impact. The quality of the road maneuvers we execute is subjective, contextual, and geometrically subtle, and evaluation needs apply to a wide variety of conditions with an extremely long tail.,Analysis and improvement of existing data sources and metrics,Development of novel approaches to measuring and improving,Collaborate with engineers and other data scientists to evaluate and improve core,Waymo technologies, with a focus on bringing statistical depth, general analytical rigor, and accurate causal interpretation of data.,PhD in statistics, math, or other quantitative area,Progressive statistics background either in academia or industry,Data science and system evaluation experience,Willingness to understand a complex system and its various components,Experience with tools for manipulating big data,Experience with R/Python and statistical libraries,Experience with C++,Familiarity with Spark/MapReduce,Training or interest in geometry or classical physics", "Using Big Data tools (Hadoop, Spark, H2O, AWS) to conduct the analysis of billions of customer transaction records,Writing software to clean and investigate large, messy data sets of numerical and textual data,Integrating with external data sources and APIs to discover interesting trends,Building machine learning models from development through testing and validation to our 30+ million customers in production,Designing rich data visualizations to communicate complex ideas to customers or company leaders,Curious. You ask why, you explore, you're not afraid to blurt out your disruptive idea. You probably know Python, Scala, or R and you\u2019re constantly exploring new open source tools.,Wrangler. You know how to programmatically extract data from a database and an API, bring it through a transformation or two, and model it into human-readable form (ROC curve, map, d3 visualization, Tableau, etc.).", "Apply your analytics and data science expertise to better serve our users.,Create and manage key data pipelines, including developing new solutions for batch and real-time data and analytics use cases.,Demonstrated ability to do data science, data engineering, and / or user analytics.,Passion and persistence for solving hard problems.,Degree in Engineering, Computer Science, Stats, Mathematics, or related quantitative discipline, followed by graduate degree or 2 years of equivalent experience.,Experience in requirements analysis, design, implementation, and testing of software solutions (especially data related using Python, R, or similar programming languages).,Experience with relational databases and SQL,Ability to oversee and work simultaneously on different projects with a variety of timelines,A team and customer centric mindset.,Demonstrated ability to solve hard mathematical, algorithmic, and statistical problems.,Experience with applied Machine Learning and AI techniques.,Experience working with cloud platforms like AWS, real-time data processing, and agile software development.,Drive to understand key issues and subtleties in complex systems.", "Lead projects to Identify candidate therapeutic targets and biomarkers for immunology, through integrated analysis of high-throughput molecular data, real-world data and clinical phenotypes,Explore opportunities to re-position existing drugs in novel indications,Collaborate with wet-lab and computational scientific teams and champion projects from discovery to early clinical development.,Develop innovative and creative computational approaches to understand complex immune conditions using multi-dimensional \u2013omics and clinical data,Establish interactions with collaborators within and outside the Immunology and Inflammation Research and Early Development Therapeutic Area.,Prepare publications, research and clinical/regulatory documentation in accordance with Sanofi quality standards.", "Work with a team of data scientists, machine learning engineers, software engineers and QA engineers.,Perform data collection, preprocessing, feature engineering, data visualization and analysis.,Build automation of data collection and preprocessing.,Build models to address business problems.,Engage with lines of business, users and analysts to explore and prototype opportunities and use-cases exploiting data and the application of cognitive and machine learning technologies.,Modern, object-oriented or functional programming experience, (Java, C++, Python, Scala, SQL, R),Data Science and Machine Learning Frameworks (\u201cR\u201d, Apache Spark / MLlib, TensorFlow, Scikit-learn, etc\u2026),Natural Language Processing (NLTK, CoreNLP, Gensim, Spacy, etc..),Experience with Big-Data technologies and cloud (AWS, other),Linux / Bash scripting,Relational databases (Oracle, PostgreSQL, MySQL, etc.),Agile development methodology,CI/CD Development environments and tools (GIT, Maven, Jenkins, etc),RESTful Microservice APIs", "Identify customers' needs and requirements, and quantify them into statistical problems.,You will collect and cleanse large amount of real-world costumers\u2019 usage and commerce data in various product environments,You will develop predictive models on large-scale datasets to address various business problems through demonstrating advanced statistical modeling, machine learning, or data mining techniques.,You will deliver informative and effective findings, results and recommendations from machine learning models to stakeholders, be able to build dashboards of critical statistics and metrics", "Leverage large sets of structured and unstructured data to develop tactical and strategic insights.,Collaborate with analytic and data teams to set objectives, approaches, and work plans.,Research and evaluate new analytical methodologies, approaches, and solutions.,Develop and validate statistical forecasting models and tools.,Interpret and communicate analytic results to analytical and non-analytical business partners and executive decision makers.,Travel is required Monday-Friday,Advanced degree (Masters at a minimum) in a quantitative field in one or more of the following:,Statistics,Computer Science,Data Science,Engineering,Physics,Mathematics,Computational Social Science,[or similar academic pedigree],2+ years of experience in advanced analytics and machine learning model development and validation,2+ years of experience working on analytic-based projects and delivering results within scope, funding, and duration,Experience with a statistical package and various programming languages such as R, Python, Java, C++, SAS, Matlab, etc.,Experience with standard classification and regression techniques, including dimension reduction, feature selection and hypothesis testing,Strong communication skills with ability to explain modeling results to non-technical users,Capacity for a high degree of autonomy and out-of-the-box thinking", "Serve as a technical lead on our most demanding projects that involves data science and ML knowledge and experience. Exert technical influence over multiple teams and across organizations, increasing their productivity and effectiveness by sharing your knowledge and experience,Leverage knowledge of internet and industry prior art in design decisions,Contribute intellectual property through patents,Assist in career development of other, actively mentor individuals and community on the advanced technical issues; helping managers guide career growth of their team members,10+ years of industry experience in predictive modeling, data science and analysis,Experience with handling terabyte-size datasets, using big data platforms, such as Spark, Hadoop/Hive, Impala, Presto/Athena, etc.,Deep hands-on technical expertise in at least one major technical area: statistical analysis, ML and Deep Learning (DP) with established tools such as R, Scikit-learn, MLlib, SparkML, Tensorflow, etc.,Proven track records in leading the delivery of statistical analysis and ML solutions for solving real customer use cases with significant business impact,Proven problem solving, diving into data to discover hidden platforms,Comfortable with ambiguity", "Work with engineers, geoscientists and management to frame open ended problems in terms of deliverables that can be generated using data science techniques.,Work with subject matter experts to evaluate multiple internal and external datasets for applicability and usefulness in achieving model objectives.,Work with management and user community to scope and prioritize project efforts using risk vs value criteria.,Explore and examine data from a variety of angles using appropriate profiling techniques to understand the data and determine hidden weaknesses, trends, and/or opportunities.,Employ appropriate visual, machine learning, and statistical methods to prepare data for use in predictive and prescriptive modeling.,Create data driven solutions that provide structure, assist in the decision making process, and facilitate automation.,Devise new algorithms to solve problems and tools to automate work efforts.,Appropriately evaluate and tune models to insure business objectives are achieved.,Effectively communicate recommendations to stakeholders and management.,Periodically evaluate existing models to insure continued effectiveness.,Help champion the appropriate use of machine learning and statistical methods as an effective means to decision making and automation.,Creates training materials, and delivers formal and informal training sessions to increase awareness and utilization of quantitative methods for evaluating data and decision making.,Provides mentoring for those seeking to machine learning and statistical methods.,Assist in the evaluation of tools for doing visual and quantitative analysis, as well as tools and techniques for deploying predictive and prescriptive solutions.,Excellent interpersonal and customer-facing communication skills,Ability to interact with a wide range of people,Excellent analytical problem solving skills,Strong mentoring/training skills,Must be a self-starter with ability to follow through on projects assigned,Must be willing to travel within the US up to 15% of the time,4-year degree or equivalent experience required, advanced degree beneficial,5+ years? experience developing models for decision support and process automation,Excellent understanding of machine learning techniques and algorithms such as Regression, Decision Trees, k-NN, Na\u00efve Bayes, and Neural Networks.,Experience identifying patterns in high frequency time series data.,Working knowledge of common data science tools such as R, Weka, NumPy, SAS, and MatLab, with excellence in at least one.,Experience using visual analytics tools required with knowledge of Tibco Spotfire a plus.,Proficiency in using query languages such as SQL and Hive.,Experience with NoSQL databases such as MongoDB, Cassandra, or HBase a plus.", "Use machine learning to eliminate advanced security threats - our models are built to detect anomalies in the intricacies of human-to-human communications,Brainstorm ideas with your team and quickly mock up prototypes in Jupyter Notebooks,Be thrown into the exciting unknown - we like to focus on new problems so there is little prior research to rely on,Be responsible for getting your work into production (see here),Work closely with the backend engineering team to deploy, monitor and iterate on your models,You\u2019ll work alongside amazing, high-performing colleagues,We offer a competitive salary and equity options with every role, with annual salary reviews.,Everyone gets 25 days of paid annual leave (33 days including bank holidays),Company contributions are made towards your pension,You\u2019ll get a high-spec tech kit to work on & get to choose your OS,Flexible morning start-times on the engineering teams,One week of remote working from abroad per year,Fully stocked kitchen with plenty of office snacks including fruit, nuts, bread, cereals, and amazing coffee,We offer a cycle to work scheme & eye-care vouchers,Parents and carers are guaranteed one day per week of work-from-home, and we'll give you an extra day of annual leave to take your child to their first day of nursery and primary school,Generous enhanced Maternity & Paternity leave", "A cover letter explaining your motivation to apply for the position, your interest in the announced position/topic, and any job-related experience you think is important (including applied methods),A detailed curriculum vitae including a detailed account of relevant technical skills/prior experience,access to one of your recent statistics/modelling projects relevant for this position (e.g. paper, GitHub repository),Copies of certificates (high school/ B.Sc./ M.Sc./ Ph.D.), and a transcript of all covered subjects and grades (B.Sc./ M.Sc.), publication list,At least one relevant recent own publication,Two references,Excellent technical facilities which are without parallel,The freedom you need to bridge the difficult gap between basic research and close to being ready for application,Work in interdisciplinary, multinational teams and excellent links with national and international research networks including the project partners,A vibrant region with a high quality of life and a wide cultural offering for a balance between family and professional life,Interesting career opportunities and an extensive range of training and further education courses,Remuneration up to TV\u00f6D public-sector pay grade 13 including attractive public-sector social security benefits", "Work collaboratively with business stakeholders and team members to deliver value through advanced analytic and modeling techniques,Analyze, visualize, and model data using advanced statistical techniques and tools against structured and unstructured data,Perform machine learning, natural language, and statistical analysis, such as classification, regression, recommendation, sentiment analysis, and deep learning,Work with organizations to move from descriptive analytics to predictive and prescriptive analysis,Work with a variety of tools to implement advanced analytic solutions,Create quality deliverables to communicate technical solutions to appropriate audiences,Work with the sales team to create and deliver client proposals and demonstrations,Present to client, industry and internal peer groups,Provide mentoring and leadership to more junior team members,High level understanding of advanced analytics techniques,Ability to quickly understand and adapt to new industries, situations, and business requirements,Development experience with R, Python, Azure Machine Learning, SAS, SPSS or related languages and tools.,Advanced knowledge of data analysis with transactional SQL,Knowledge of Big Data techniques and methodologies,Degree or equivalent experience in Statistics, Mathematics, Computers Science, or related fields,Eagerness to learn new tools and technologies,Strong communication skills", "Working within the analytics / data science team tasked with optimising product performance,Analyse a combination of product / customer / user behaviour data,Implement statistical techniques to increase the quality of your analysis,Conduct statistical segmentations,Provide strategic insights and recommendations for improvement,Identify optimisation opportunities, generating experimentation / A/B tests ideas and analysing results,Degree educated (Maths / Stats / Engineering /Economics / Physics / Chemistry preferred),Hands on experience using SQL to analyse / manipulate large data-sets,Experience with Google Analytics is highly desirable,Experience working for an online platform / on an online product is highly desirable,Experience using R or Python is highly desirable,Experience using visualisation tools (e.g. Tableau / Looker ) is highly desirable", "Apply (interpretable) machine learning to better understand climate impacts such as floods, crop failure, low carbon uptake or wildfires,Follow new developments in (interpretable) machine learning, translate and apply them to pressing research questions in Earth system sciences,Support the group with expertise on state-of-the-art data science and machine learning methods,Excellent data science and machine learning skills including deep learning (e.g. CNNs),Experience with common machine learning toolboxes such as TensorFlow, PyTorch, Keras or similar,Interest in interpretable machine learning,Experience with code sharing and version control e.g. via GitHub,Self-driven personality able to work both independently and in a team,Prepared to work in a highly interdisciplinary environment,PhD in Computer Science, Applied Mathematics, Engineering, Environmental Sciences or related field,Excellent oral and written communication skills in English,Excellent technical facilities which are without parallel,Work in interdisciplinary, multinational teams and excellent links with national and international research networks,A vibrant region with a high quality of life and a wide cultural offering for a balance between family and professional life,Interesting career opportunities and an extensive range of training and further education courses,Remuneration at public-sector pay grade TV\u00f6D EG13 (100%) including attractive public-sector social security benefits", "Develop and maintain forecasting and optimization algorithms for AutoGrid's machine learning and predictive control systems,Implement and deploy your algorithms on big-data platforms, in production,Benchmark and debug critical issues with the algorithms and software as they arise,Prototype new methods and analyses on customer data sets,Work closely with product management, engineering and QA teams to manage the full product lifecycle including requirements, architecture, algorithmics and QA,MS. or Ph.D. in computer science, operations research, statistics or related field,Exposure to very large forecasting and optimization problems and techniques, including large-scale time series forecasting and stochastic linear programming,Good understanding of algorithms, data structures and performance optimization techniques,Excellent programming skills in Python, Java or C++,Experience building production python applications. Specifically, knowledge of the Celery and Django frameworks is a plus,Hands-on experience with the energy industry,Understanding of and experience with Big Data / NoSQL frameworks, Spark, HBase, Hadoop / MapReduce, HDFS, etc.,Knowledge of relational databases (SQL),MS. or Ph.D. in computer science, operations research, statistics or related field,Exposure to very large forecasting and optimization problems and techniques, including large-scale time series forecasting and stochastic linear programming,Good understanding of algorithms, data structures and performance optimization techniques,Excellent programming skills in Python, Java or C++,Experience building production python applications. Specifically, knowledge of the Celery and Django frameworks is a plus,Hands-on experience with the energy industry,Understanding of and experience with Big Data / NoSQL frameworks, Spark, HBase, Hadoop / MapReduce, HDFS, etc.", "Advanced knowledge of statistical and machine learning methods, particularly in the areas of modeling and business analytics,Strong programming skills,Experience with statistical languages and packages, such as R, SAS, Matlab, and/or Mahout,Experience working with relational databases and/or distributed computing platforms, and their query interfaces, such as SQL, MapReduce and Hive.,Excellent written and verbal communications skills, with a proven ability to translate complex methodologies and analytical results to higher-level business insights and key take-aways,Ability to travel as-needed to meet with customers,**This role will support US government clients that require US citizenship. Given this, US citizenship is required for you to apply.,A proven passion for generating insights from data, with a strong familiarity with the higher-level trends in data growth, open-source platforms, and public data sets.,Experience working hands-on with large-scale data sets,Familiarity with visualization software and techniques (including Tableau), and business intelligence (BI) software, such as Microstrategy, Cognos, Pentaho, etc.", "Maintain the team's data collection, validation, and distribution processes.,Act as a subject matter expert for data problems.,Work to automate and streamline processes involving data and deployment.,Experience with data integrity, regression, and back testing,Programming skills in R, Java, Python, PHP,Database knowledge within SQL, MySQL, etc.,Ability to problem solve, organize, and multitask,Competitive salary and bonus structure,401K+ health benefits, educational reimbursements, and generous PTO", "Excellent health and retirement benefits,Anual Bonus,Great work/life balance,Very friendly company culture,Experience using Machine Learning,2+ years of experience using R and Python", "3+ years of proven experience as a Data Scientist or Data Analyst,Knowledge of R, SQL and Python,Identify valuable data sources and automate collection processes,Undertake preprocessing of structured and unstructured data,Analyze large amounts of information to discover trends and patterns,Build predictive models and machine-learning algorithms,Combine models through ensemble modeling,Present information using data visualization techniques,Propose solutions and strategies to business challenges,Collaborate with engineering and product development teams Requirements", "As PhD student you will be supervised jointly by Prof. Breteler and a senior PostDoctoral researcher in our group. Additionally, you will participate in the Clinical and Population Science Graduate School (BIGS-CPS) and participate in regular trainings and scientific retreats,The position is initially limited to three years with the possibility of extension", "Develop and maintain forecasting and optimization algorithms for AutoGrid's machine learning and predictive control systems,Implement and deploy your algorithms on big-data platforms, in production,Benchmark and debug critical issues with the algorithms and software as they arise,Prototype new methods and analyses on customer data sets,Work closely with product management, engineering and QA teams to manage the full product lifecycle including requirements, architecture, algorithmics and QA,MS. or Ph.D. in computer science, operations research, statistics or related field,Exposure to very large forecasting and optimization problems and techniques, including large-scale time series forecasting and stochastic linear programming,Good understanding of algorithms, data structures and performance optimization techniques,Excellent programming skills in Python, Java or C++,Experience building production python applications. Specifically, knowledge of the Celery and Django frameworks is a plus,Hands-on experience with the energy industry,Understanding of and experience with Big Data / NoSQL frameworks, Spark, HBase, Hadoop / MapReduce, HDFS, etc.,Knowledge of relational databases (SQL),MS. or Ph.D. in computer science, operations research, statistics or related field,Exposure to very large forecasting and optimization problems and techniques, including large-scale time series forecasting and stochastic linear programming,Good understanding of algorithms, data structures and performance optimization techniques,Excellent programming skills in Python, Java or C++,Experience building production python applications. Specifically, knowledge of the Celery and Django frameworks is a plus,Hands-on experience with the energy industry,Understanding of and experience with Big Data / NoSQL frameworks, Spark, HBase, Hadoop / MapReduce, HDFS, etc.", "Prepare Data and Data pipelines for model development,Develop novel approaches for exploration, analysis and feature selection from large-scale multidimensional data that include neurophysiological data, behavioral, wearable and clinical/patient data,Build, train, test and deploy machine learning models,Develop visualization tools to interpret and communicate results,Work closely with software developers to integrate models into software products,Document code, results and details of the approaches in a thorough and systematic way in order to promote knowledge sharing and code re-use,Collaborate with other data scientists, SMEs to ideate and solve complex issues pertinent to data ingestion/curation, model performance, model generalization.", "literature research with the aim of critically evaluating new experimental and theoretical methods, making them usable for our own work or evaluating the problems, difficulties and disadvantages that arise, deriving new approaches and solutions,independent research and expansion of scientific knowledge at the highest international level in the field of uncertainty assessment, interpretability and traceability of neural networks,review and further development of existing methods for robustness analysis and uncertainty modeling of machine learning models. New and further development of approaches for (visual analysis) of model components or for self-explanatory models,research and development of procedures for the verification and stabilization of machine learning models, e.g. through prior knowledge or through the combination of multiple data sources,perform experiments to validate the developed methods. Evaluation and interpretation of the experimental results to compare different methods,implementation of the procedures in prototypical algorithms,detailed preparation and presentation of the new approaches to scientific users,presentation and discussion of the research results at conferences, meetings, etc. at national and international levels,publication of the research results in suitable publications,participation in topic-related scientific committees, working groups and interest groups. Representation and presentation of the scientific results internally and externally,provide guidance for doctoral students as part of the research project", "Identify, ingest, and enrich a wide range of structured and unstructured data into datasets for analysis,Learn to apply statistical and advanced techniques (e.g. segmentation, machine learning) to develop both prototypes and scalable (i.e. efficient, automated) data analyses,Conduct basic ad hoc analysis, building models, forecasting viewing behavior,Extensive code development, debugging, optimizing, and productionizing,Support the development of machine learning pipeline architecture,Help build out frameworks for quickly rolling out new data analysis for standalone data-driven products and services to support our network of media brands.,Be able to transform unstructured raw data in to formats suitable for modeling,Be able to work in teams and collaborate with stakeholders to define requirements", "Solid understanding of basic statistics (significance testing, correlation analysis, confidence intervals, descriptive statistics),Experiment Design (sample size*, power*, randomization*, matched market),Forecasting, Prediction Models (linear/nonlinear), Nonparametric models,Large scale multivariate optimization,Methodology writeups, interpretation of results,Applications and generalization (Use cases, automation and scalability evaluation),POVs, 1-pager, vendor evaluation and assessment,Lead commercialization of tools and techniques,Data prep and cleanup (missing data, outlier detection, duplications) from internet scale digital advertisement/survey response data,Data Integration (merge*, join*, subset*),Data visualization (Tableau, R, Matplotlib, ggPlot etc.),Data mining, data reduction*,Classification (Decision Tree, clustering, bagging, boosting, logit regression),Prediction (Neural Network) & validation (cross validation),Utilization of statistical programming tools (R,SAS, SciPy), coding languages (Python, Java, C++), and Google tools (BigQuery*, TensorFlow),Comfortable with data retrieval and processing with SQL and NoSQL,Solid understanding of relational and non-relational database technology, cloud based data lake, ETL, data pipeline,Understanding of code version management system,Some management experience, enjoys mentoring, managing direct reports,Highly collaborative individual with great communication skill,Develop communication styles focusing on technical details for non-technical audiences,Degree / equivalent experiences in applied quantitative field (Statistics, Mathematics, Econometrics, Engineering or CS).,At least 5 years of industry experience ( or 3 with graduate degree) in a data focused role,Experience with building end-to-end data products with ability to use the story to create new analytical products or change an existing product in a way that improves company goals and metrics.,Knowledge of the data science lifecycle - acquiring data from different sources, exploratory data analysis, creating and optimizing modes including validation and interpretation, and finally onward communication of results in a clear and succinct fashion.,Hands-on Experience with statistical software packages (R, Python SciPy), programming languages (SQL, Python or JAVA),Experience with statistical models for multivariate testing, time series analysis, logistic regression and linear/non-linear regression.,Baseline understanding of all the five core competencies: programming, statistics, machine learning, data munging, and data visualization.,Ability to communicate your findings effectively to non-technical audiences.,Preferred: Knowledge of popular ad serving technologies and supporting analytical and research tools (Doubleclick, Atlas, comScore, Compete, etc.),Preferred: Experience with machine learning algorithms (random forest, SVM, etc.) and/or Bayesian methods.", "Develop and maintain forecasting and optimization algorithms for AutoGrid's machine learning and predictive control systems,Implement and deploy your algorithms on big-data platforms, in production,Benchmark and debug critical issues with the algorithms and software as they arise,Prototype new methods and analyses on customer data sets,Work closely with product management, engineering and QA teams to manage the full product lifecycle including requirements, architecture, algorithmics and QA,MS. or Ph.D. in computer science, operations research, statistics or related field,Exposure to very large forecasting and optimization problems and techniques, including large-scale time series forecasting and stochastic linear programming,Good understanding of algorithms, data structures and performance optimization techniques,Excellent programming skills in Python, Java or C++,Experience building production python applications. Specifically, knowledge of the Celery and Django frameworks is a plus,Hands-on experience with the energy industry,Understanding of and experience with Big Data / NoSQL frameworks, Spark, HBase, Hadoop / MapReduce, HDFS, etc.,Knowledge of relational databases (SQL),MS. or Ph.D. in computer science, operations research, statistics or related field,Exposure to very large forecasting and optimization problems and techniques, including large-scale time series forecasting and stochastic linear programming,Good understanding of algorithms, data structures and performance optimization techniques,Excellent programming skills in Python, Java or C++,Experience building production python applications. Specifically, knowledge of the Celery and Django frameworks is a plus,Hands-on experience with the energy industry,Understanding of and experience with Big Data / NoSQL frameworks, Spark, HBase, Hadoop / MapReduce, HDFS, etc.", "Shaping and innovating personalized digital healthcare solutions, and,Understanding and evaluating the new benchmarks of a new ecosystem for digital medicine.,Tailoring multimodal and longitudinal data from patients with Parkinson\u2019s disease to individual patient`s needs,Shaping with us \u201cpredictive individualized reference modelling\u201d,Integration and analysis of multi-parametric data,Development of data model(s) and contribute to new solutions in digital healthcare,Engage in publications and knowledge sharing,Interacting with the group and our wider research community, especially with PhD students and engage in grant writing,Hold a Ph.D. degree in life science, data science or engineering,Ideal candidates would have a good understanding and proven experience in biomedical statistics and machine learning is of advantage,Analytical skills with the ability to translate complex research data and to meet patients\u2019 needs,A cross-disciplinary curiosity, strong organizational and interpersonal skills and seeking collaborative interdisciplinary digital medicine research,Team player with good communication skills,A track record of previous publications in medical data science is advantageous,Working knowledge of modern programming languages (Python, R), and Linux,Excellent working knowledge of English is required. Knowledge of German, French and/or Luxembourgish is advantageous,Contributing to an innovative research project and a fast developing research group,A highly interdisciplinary research environment integrating biologists, physicists, mathematicians, and clinical researchers working in the area of systems biomedicine,An international environment with competitive compensation & benefits.,State-of-the-art research facilities and computational equipment,Contract Type: Fixed Term Contract 24 Month (with the possibility of renewal for up to 5 years in total),Work Hours: Full Time 40.0 Hours per Week,Starting date: as soon as possible,Location: Belval,Internal Title: Postdoctoral researcher/Research associate,Job Reference: UOL04177,A detailed Curriculum vitae,A motivation letter that includes a description of past research experience an future interests, as well as of how you would contribute to the research group,,Copy of PhD degree,Name and contact details of three references,Multilingual and international character. Modern institution with a personal atmosphere. Staff coming from 90 countries. Member of the \u201cUniversity of the Greater Region\u201d (UniGR).,A modern and dynamic university. High-quality equipment. Close ties to the business world and to the Luxembourg labour market. A unique urban site with excellent infrastructure.,A partner for society and industry. Cooperation with European institutions, innovative companies, the Financial Centre and with numerous non-academic partners such as ministries, local governments, associations, NGOs \u2026", "Build Machine Learning models that power our apps and inventory classification. Our preferred language is Python.,Research and develop predictive algorithms using various regression techniques.,Work with internal or external stakeholders to optimize our products.,Use statistics to describe data. Communicate with other teams through data visualization.,Bachelor's degree in Statistics, Math, or Economics,Experience building Machine Learning models using messy data,Strong programming ability with Python, Jupyter, and Spark,Experience with Data Visualization,A self-starter. You love what you do and you keep yourself motivated.,A learner. You have an insatiable thirst for knowledge and greater understanding.,A conversationalist. You speak the language of statistics, and you can explain what you're doing to our grandmas.,An innovator. You're interested in the \"why\" and the solutions that make things more interesting and efficient.,With others. We collaborate cross-functionally to solve problems and deliver the best products for our customers.,With transparency. We have an open team room. No cubicles, no private offices.,With agility. We don\u2019t believe in following a process for process\u2019s sake. We ship frequently and focus on delivering incremental value.,With open minds. We are committed to building a diverse team of people with unique perspectives. This encourages a healthy and inclusive environment that builds a more sustainable, successful company.,With pride. We value our people most of all. We invest in ourselves by applying our own strengths and interests to company needs.,Lunch-and learns,Annual stipend for continuous education,Tech all-hands lunches every other Friday,Hack days,Team outings,Nordstrom discount,Flexible work environment", "Integrate with interdisciplinary research teams and creatively develop/apply modern data science, statistics, and machine learning techniques to advance research.,Coding/algorithmic prototyping of relevant analysis methods, including setting clear goals, measuring progress, and the creation of appropriate documentation.,Collaborate with, educate, convene, and support a broad community of researchers on campus in how to best leverage data science in their teaching and research. This may include contributing to mini-courses and workshops on data science.,Communicate results and impact to all stakeholders. This may include presenting research at academic conferences and workshops.", "Master\u2019s degree in Healthcare, Science, Computer Engineering or related scientific, technical, or clinical discipline,At least 4 years of professional experience (12 years if degree requirement is not met),Experience with visualization and dashboard creation,Additional qualifications: Experience with VA OIT", "Perform scientific work in the area of quantum physics and quantum information.,Develop simulations of materials for energy storage together with our partners.,Transfer theoretical concepts into experiments.,Present the obtained results in scientific journals as well as national and international conferences.", "Retrieve, process and prepare a rich data variety of data sources such as social media, news, internal/external documents, emails, financial data, and operational data,Analyze and model structured data and implement algorithms to support analysis using advanced statistical and mathematical methods from statistics, machine learning, data mining, econometrics, and operations research,Perform Statistical Natural Language Processing to mine unstructured data, using methods such as document clustering, topic analysis, named entity recognition, document classification, and sentiment analysis,Utilize a diverse array of technologies and tools as needed, to deliver insights, such as R, SAS, Python, Spark, Hadoop, Qlikview, and Tableau,Participate in client engagements focused on Big Data and advanced business analytics, in diverse domains such as risk management, product development, marketing research, supply chain, public policy; communicate results and educate others through reports and presentations,2+ years of professional experience working as a Data Scientist,Master\u2019s degree, or PhD in Computer Science, Statistics, Mathematics, Econometrics, or related fields from an accredited college or university,Strong mathematical background with strong knowledge in at least one of the following fields: statistics, data mining, machine learning, statistics, operations research, econometrics, natural language processing, and/or information retrieval,Deep experience in extracting, cleaning, preparing and modeling data,Experience with command-line scripting, data structures, and algorithms; ability to work in a Linux environment and proficiency in analysis (e.g. R, SAS, Matlab) packages as well as proficiency in programming languages (e.g. Python, Ruby, Java, Scala)", "Identify strong AI/ML use cases in cloud security, data security and adjacent domains, leveraging Netskope\u2019s rich set of data sources;,Define scalable data acquisition and labeling strategy for specific use cases;,Work with data engineers to retrieve, clean and normalize data. Ensure scalable and continuous high-quality data stream;,Work closely with threat research and development team in feature engineering. Make sure high-quality feature sets are chosen in a systematic way;,Select ML models for defined use cases. Implement KPIs to ensure optimal algorithms and results;,Conduct strict internal testing to ensure high efficacy, low false positive and false negative rate;,Interpret results and communicate findings;,Document use case, data acquisition, feature engineering, training, validation, deployment, future improvement opportunity and other important aspects;,Work closely with development and QE team in productization;,Be an evangelist of AI/ML within Netskope. Promote AI/ML wherever applicable, beyond security use cases;,Collaborate with data analytics team to define new platform requirements and continuously improve our horizontally scalable data lake.", "Combine and synthesize data from multiple sources,Select appropriate analytical methods to explore, develop, and test hypotheses,Develop and apply predictive models,Communicate results effectively and clearly to a range of audiences,Stay up to date with techniques and tools,Analytical, articulate, inquisitive, and insightful,Skilled problem solver,Receptive to peer review, enjoy working collaboratively,Self-motivated and proactive, work productively without supervision,Critical thinker and quick learner,Attentive to detail,Interested in understanding challenges in healthcare delivery and building data products to meet the needs of the healthcare system", "Drive the implementation of database curation tools to enhance efficiency and effectiveness for the team,Lead outsourced content curation,Drive the implementation of bioinformatics software and pipelines,Contribute to analyses of public and internal data,Collaborate closely with interdisciplinary teams across the organization to understand business needs and address through technical solutions,Publish scientific or methodological results in top journals", "Data curation,Develop statistical modeling techniques for pattern recognition problems.,Develop code in R, Python, Java, or other languages for statistical analysis, optimization, and simulation.,Build models that maximize performance and accuracy.", "Work directly with biomarker scientists to benefit both future clinical trials and the patients,Share your discoveries and propose new hypotheses on cancer immunotherapy with the cancer research community through publications.,Work with a vibrant global data science to learn to implement production-level analysis pipelines and libraries for clinical data.", "Master the data flowing through NPR\u2019s digital platforms and find opportunities for analyses that inform key decisions in content, revenue, and digital product development.,Serve diverse needs across the organization, from foundational trend analyses to big questions that require your data science expertise.,Uncover insights on audience usage across NPR platforms, including website, apps, podcasts, and social media.,Drive the evolution of personalization algorithms and A/B testing in NPR platforms.,Innovate how we use behavioral data to engage more deeply with our audiences and drive loyalty and donations to stations.,Independently perform data mining techniques and statistical analyses on large data sets to uncover critical patterns and predictive models.,Assure data quality and integrity for NPR\u2019s digital metrics systems.,Create meaningful dashboards for executives, content leaders and digital newsroom staff, including historical reporting.,Develop metrics to measure business performance.,Build models to understand media behavior online to discover actionable insights.,Communicate effectively with small teams, formal presentations, and at business unit events with engaging storytelling.,Develop written communications to address varied styles and information needs.,Build strong working relationships at all levels with internal clients and build problem-solving partnerships with clients and colleagues.", "Lead and participate in the application of analytic and machine learning approaches to HBO\u2019s understanding of the drivers of content performance to help HBO make smarter decisions in the acquisition, scheduling, and promotion of content,Mine HBO and other third party data to better understand how consumers make choices with regards to content consumption and how those change based on various factors (including availability, seasonality, promotion etc.),Develop compelling data-driven stories to influence key content decisions at all levels within the organization,Provide analytic consultation on related components of the insights environment including but not limited to audience modeling, promotional effectiveness, consumer engagement, and product optimization.,Bachelor Degree or higher in quantitative field of study (mathematics, physical sciences, computer science, operations research etc.) from an accredited institution,Inquisitive, conceptual thinker comfortable working on complex analytic problems related to how, why, when, where and what people choose to watch HBO content,Must be capable of telling compelling data centric stories that inform key strategic and tactical decisions in content development, scheduling and promotion to varying internal and external stakeholders, of varying seniority,Prior experience in the application of analytic programming (R or SAS) and SQL in the manipulation of data and execution of analyses preferably in an entertainment or media company,1+ years of relevant experience,Position based in New York or Seattle", "Collaborate with the SMEs from the client side to understand the current workflows and help design autonomous solutions empowered by Beyond Limits\u2019 Artificial Intelligence IP stack,Act as an in-house reservoir engineering expert to ensure the accuracy of the O&G knowledge/physics implemented in our agents,Contribute to the solution design, usually in collaboration with data scientists", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Build the next level of data science capability at Chipotle from hardware and software to best practices to hands on deep diving analysis and models.,Analyze existing data to generate insights for internal (marketing, product, coaching ops, member ops, sales) audiences.,Partner with internal customers to define experimental questions and scope for each analytics investigation.,Own top-notch, first-in-class computational science projects studying internal data from start to finish.,Perform exploratory and targeted data analyses using descriptive statistics and other methods,Translate data analytics outcomes into clear visualizations understandable to laymen.,Report on analytics findings to senior leadership to inform key business decisions.,Solve challenging analytical problems, which requires working with large and complex data sets and applying analytical methods including machine learning and statistical approaches as needed.,End-to-end analysis including data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.,Prototype and build analytical pipelines using various data sources to provide insights at scale.,Interact cross-functionally with a wide variety of teams and work closely with them to identify opportunities to improve on platform.,Developing analytical solutions, forecasting, and optimization methods to improve the quality of user experience. Areas include search quality, product recommendation, end-user behavioral modeling, user engagement, pricing, etc.,Make business recommendations (e.g. cost-benefit, forecasting, experiment analysis) based on rigorous analysis.,Work alongside software developers, software engineers and data engineers to translate algorithms into viable solutions,Hold an advanced scientific degree (CS, Engineering, or [Applied] Mathematics), including training in statistical analysis and experimental design. Skilled in critically thinking through a research question, optimal methodologies, and clear operationalization\u2019s of hypotheses.,Expertise and hands on experience with statistical modeling and machine learning software (Python, R, etc.),Advanced knowledge of database query languages (SQL) and an understanding of relational algebra and set theory,Comfort with common applied ML strategies and ensemble methodologies (e.g. train/test/validation sets, feature engineering, accuracy assessment).,Experience building simple backend data infrastructure/pipelines. Know how to connect the dots between data sources (e.g. SaaS or other APIs), ETL/storage solutions (better if you\u2019ve built these yourself), analysis environments (e.g. R or Python scripting), and (preferred but not required) front-end reporting (e.g. dashboards, apps).,Strong knowledge of a variety of persistence layers, e.g. SQL (MS, Oracle), NoSQL (MongoDb, Neo4j), both on-prem, cloud, elastic or otherwise.,Passionate about storytelling with data. Ability to port analytics outcomes into interactive- or presentation-based media (e.g. D3, Shiny, PowerPoint/Keynote).,Understand technical basics of data visualization and common pitfalls. Ability to make clean, elegant, informative graphs (knowledge of ggplot2 visualization package).,Exceptional written and verbal communication skills. Ability to present data science findings to sizable audiences of laymen and specialists alike.,Outstanding leadership and organizational skills.,Ability to work collaboratively as part of a fast-paced, customer oriented team,Desire to use or create standards, consider reusability, and have a long-term view while understanding short-term needs,Self-starter who provides ideas and solutions and understands various approaches within the BI discipline", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Data collection, data cleansing and entity extraction,Building dashboards and reports,Data Visualizations; Statistical inference,Doing ad-hoc analysis and presenting complex results to non-technical partners,Working with stakeholders to understand questions and apply data as part of the solution,Communicating results to decision makers to aid in decision-making,Selecting features, building and optimizing classifiers using machine learning techniques,Data mining using state-of-the-art methods and algorithms.,Integrating with third party sources of information when needed,Creating data applications that are measured and KP driven,Work with the team to drive value from data and help to define appropriate data governance and security models that serve multiple business outcomes.,A minimum of 3-5 years of progressively responsible experience in a directly related area, during which both professional and management capabilities have been clearly proven.,Experience with common data science toolkits, such as Python and Tensor Flow,Experience in R, Weka, NumPy, MatLab, Julia not required but desirable,Understanding of machine learning techniques, image processing and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests and Waterfalls,Experience with NoSQL and Graph database platforms, such as CosmoDB (preferred), HortonWorks, Cloudera, MongoDB, Cassandra, HBase, Neo4J etc.,Extensive expertise in RDBMS systems like Oracle/MySQL database, as well as data modeling, both logical and physical.,Extensive experience in multidimensional data modeling, Data Warehouse and BI systems including star schemas, snowflakes, normalized and de-normalized models, handling \"slow-changing\" dimensions/attributes.,Knowledge of Time Series Databases and associated algorithms and technologies,Great written and oral communication skills,Excellent verbal, written and visual presentation skills,Extremely strong analytical and problem-solving skills,Experience with data visualization tools, such as D3.js, GGplot, Tableau, PowerBI etc.,Proficiency in using query languages such as SQL, Hive, Pig,Good scripting and programming skills,Good negotiating and consensus building abilities.,Demonstrated skills to work effectively across internal functional areas in ambiguous situations.,Structured thinker and effective communicator, comfortable with interacting with staff at all levels of the organization.,Ability to work independently, establishing strategic objectives, project plans and milestone goals.,Minimum of a Bachelor\u2019s Degree (Computer Science, Mathematics, Statistics, Data Science).,Advanced degree in Applied Mathematics, Business Analytics, Statistics, Machine Learning, Computer Science, Data Science are a plus", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research and develop AI/Deep Learning algorithms for electronic health care applications.,Implement neural network models (e.g. CNNs, RNNs), machine learning algorithms, and statistical analyses in Python using PyTorch, TensorFlow, scipy, sklearn, etc.,Work expertly with Big Databases.,Maintain breadth of knowledge in latest developments in AI/Deep Learning and industry best practices.,Clearly communicate the design and operation of implemented algorithms along with their results using concise reports and visualizations.,Contribute to and maintain documentation on the use of developed tools, machine learning cluster infrastructure, software, and algorithms.,Use internal ticketing system tools to report and document work according to CCI standards and protocols.", "completed scientific university studies (diploma/master) in data science, computer science, physics, mathematics, electrical engineering, technical cybernetics, geophysics or a comparable course of studies,doctorate or comparable proof of scientific qualification including relevant publications (German/English) in the field of data science and applications of respective methods for engineering problems,deep knowledge in data science methods (e.g. supervised and unsupervised learning, time series analysis in time and frequency domain, deep neural networks, time series prediction, Bayesian networks, ...),profound experience in the application of data science methods to large and/or heterogeneous data sets with time and location reference (preferably with Python),several years of professional experience in research and development (e.g. in the field of condition monitoring, cyber physical systems, system identification, anomaly detection, IoT concepts, etc.),project experience in science, preferably with experience in acquiring and managing nationally and internationally (e.g., EU) funded research projects,very good German and English language skills, both written and spoken,preferably good self-organisation and a high level of independent initiative in the processing of complex scientific and technical issues,ability to quickly familiarize yourself with unknown circumstances and complex interrelationships including new tools and methods in data science welcome,preferably practical experience in agile working (e.g. SCRUM)", "Develop, implement, and refine core marketing measurement frameworks and models, including attribution, churn, CLV, ROI, experiment methodology, and segmentation,Play a foundational role in deciding how we can best engage and grow Etsy users by determining and helping execute on opportunities for growth through analysis of behavioral and transactional data,Transform raw data into meaningful and impactful analysis characterized by strong data governance, technique transparency, and aggressive documentation and communication,Identify key metrics that drive the performance of marketing channels and marketing products, make these accessible to partners through dashboards and reports, and drive these metrics through statistical analyses of experiments,Continually evaluate and refine your technical toolkit; teach what you learn to the team,You have 1-3 years experience as a data scientist/analyst or in a quantitative role in which you extracted meaning from big data sets with little engineering support. Experience with acquisition and retention marketing operations, channel attribution, churn and lifetime value models, analytical tools and infrastructure, and finding cost-effective growth strategies a plus.,You are proficient in SQL and either R or Python. Bonus points for experience with the Hadoop ecosystem or additional scripting languages (Scala, PHP, Ruby, etc.),You can communicate your insights verbally, visually, and in writing. You care deeply about the quality of your data and the integrity of your work.,You are a strong, independent, analytical thinker who enjoys thinking about and solving complex and nuanced problems.,You care about reproducible work and have picked up tools to this end; you can teach others how to make their own work better and more efficient.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Real-World Impact \u2013 No project is ever the same; we work across multiple sectors, providing unique learning and development opportunities internationally.,Fusing Tech & Leadership \u2013 We work with the latest technologies and methodologies and offer first class learning programmes at all levels.,Multidisciplinary Teamwork - Our teams include data scientists, engineers, project managers, UX and visual designers who work collaboratively to enhance performance.,Innovative Work Culture \u2013 Creativity, insight and passion come from being balanced. We cultivate a modern work environment through an emphasis on wellness, insightful talks and training sessions.,Striving for Diversity \u2013 With colleagues from over 40 nationalities, we recognise the benefits of working with people from all walks of life.,Healthcare Efficiency \u2013 We helped a healthcare provider improve their clinical trial practices by identifying congestion in diagnostic testing as a key indicator of admissions breaches.,Environmental Impact \u2013 We designed and built the first data-driven application for a state of the art centre of excellence in urban innovation by collecting real-time data from environmental sensors across London and deploying proprietary analytics to find unexpected patterns in air pollution.,Product Development \u2013 We worked with the CEO of an elite automotive organisation to reduce the 18-month car development timeframe by improving processes, designs and team structures.,Learn how successful projections on real world problems across a variety of industries are completed through referencing past deliveries of end to end machine learning pipelines,Build products alongside the Core engineering team and evolve the engineering process to scale with data, handling complex problems and advanced client situations,Be able to focus on modelling by working alongside the Data Engineering team which focuses on the wrangling, clean-up and transformation of data.,Learn best practices in software development and productionise machine learning by working with our Machine Learning Engineering teams which optimise code for model development and scale it,Work with our UX and Visual Design teams to interpret your complex models into stunning and user-focused visualisations,Learn to use new technologies and problem-solving skills in a multicultural and creative environment,Work on complex and extremely varied data sets from some of the world\u2019s largest organisations to solve real world problems,Develop data science products and solutions for clients as well as for our data science team,Write highly optimized code to advance our internal Data Science Toolbox,Work in a multi-disciplinary environment with specialists in machine learning, engineering and design,Add real-world impact to your academic expertise, as you are encouraged to write \u2018black\u2019 papers and present at meetings and conferences should you wish,Attend conferences such as NIPS and ICML as one global team as well as Data Science retrospectives where you will have the opportunity to share and learn from your co-workers.,Work within one of the largest and most advanced data science teams, support the Lead Data Scientists to develop data science products", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with customers to capture data and analytics requirements,Develop, verify, and validate analytics to address customer needs and opportunities,Work alongside software developers and software engineers to translate algorithms into commercially viable products and services,Work in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics,Perform exploratory and targeted data analyses using descriptive statistics and other methods,Work with data engineers on data quality assessment, data cleansing and data analytics,Generate reports, annotated code, and other projects artifacts to document, archive, and communicate the work and outcomes,Communicate methods, findings, and hypotheses with stakeholders\nQualifications/Requirements:", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Demonstrated knowledge in the following: software business intelligence, data analysis data mining, compiler/decompiler, user interface, enterprise application integration, operation job scheduling and server and network monitoring.,Knowledge of design techniques, tools, and principles involved in the production of precision data models, queries, and programs.,Advance knowledge of SQL and relational database management systems.,Demonstrated knowledge of programming, data modeling, simulation, as well as statistics and mathematics.,Ability to parse data in common file formats such as csv and xml files, applying regular expressions and mathematical transforming data into information.,Ability to communicate verbally and in writing information concepts and methods to non-technical users.,Analyzes problems and develops solutions involving computer software programs and applications by designing programs or queries to retrieve complex data from databases in one or more programming languages.,Evaluates project plans and proposals to assess feasibility issues. Coordinates with stakeholders including executives, project sponsors, IT Teams, clinicians, and clinical and operational staff to determine analytic requirements for project requests.,Applies theoretical expertise and innovation to create or apply new technology.", "Work with a team of data scientists and cross functional partners to collaboratively develop solutions,Apply ML and other statistical approaches to generate insights on structured and unstructured data,Provide ML expertise in the design and delivery of data products for broad consumption by business partners,Mentor and support the training of technical and non-technical teams in data science and machine learning,Participate in the broader data science community to stay current with methodology, software, and data development and availability,Communicate and visualize the output of analyses, including both written and verbal communication, to business leaders and non-technical audiences,Conceptualize and deploy data science solutions for business questions", "Develop and implement machine learning models,Collaborate closely with product, business, and engineering teams to identify data science opportunities and solutions,Conduct in-depth statistical analyses, including A/B testing,Effectively communicate (both written and verbally) technical concepts across multiple teams,Think creatively and be able to test out new ideas and iterate rapidly", "Our biggest investment is in people like you. At Meredith, we offer comprehensive health benefits and 401k matching, plus career-boosting opportunities like tuition reimbursement, learning sessions and attendance at tech conferences to help you thrive.,For this role at Bizrate Insights, you can expect the fast-paced excitement of a startup with the rock-solid support from Meredith, an industry leader, with some nice perks on the side. If you are a passionate technologist who can get a lot done while having fun, come join a team of like-minded, skilled professionals where you can learn and share your knowledge!,Participating in activities across the data science lifecycle: business case definition, data collection, cleaning, integration, analysis, visualization, feature engineering, modeling with machine learning algorithms, interpreting the models/results, and reporting.,Communicating and coordinating effectively with product managers, software engineers, analytical groups and customer-facing teams in an agile environment.,Working with data processing pipelines for analyzing gigabyte- to terabyte-scale data from multiple sources.,Designing actionable data products based on statistical analysis and machine learning methods and working on their deployment to enterprise systems to move the needle for business at Meredith / Bizrate Insights.,Learning continuously in the areas of data science, machine learning, ad technology, marketing operations, data ecosystems, content operations, social media analysis and industry standard measurements.,Degree in computer science, engineering, statistics, or other quantitative fields. (Master's degree is preferred),Solid understanding of popular machine learning algorithms including supervised, un-supervised, and semi-supervised ones. What is the difference between algorithms? What algorithm would you choose for a given problem and why? What are the pros and cons of these algorithms? What is cross validation, hold-out sample, RMSE, overfitting, bias, \u2026?,Proficient in one or more core data science programming languages (Python or R) and the ability to become functional in emergent languages and APIs as needed (Java, Scala, \u2026).,Practical experience of building a full cycle machine learning engine from cleansing data to training and hyper parameter optimization to methods for evaluating the accuracy to identifying the ways to improve the accuracy of the model.,Exposed to and experienced with state-of-the-art machine learning toolkits such as Spark ML, TensorFlow, scikit-learn, xgboost, \u2026 preferably in Big Data frameworks (e.g. Apache Spark).,A self-learner with capability to switch from the independent contributor and leader of the project to a collaborative contributor on the team (and vice versa) depending on the demand of the project and availability of the resources.,Collaborate with stakeholders, partners and clients to ensure that all parties are aligned and supportive of the project's priorities, requirements, timeline and objectives.,Familiarity with the MapReduce paradigm and Apache Spark project.,Proficient with SQL as a means of gathering data from relational databases.,Practiced data manipulation and mining of structured and unstructured data.,Experience in visualizations for business stakeholders using a variety of visualization tools from MS Excel, libraries in R/Python.", "Developing the next generation of AI that protects the privacy of our customers and ensures that our modeling practices are ethical and bias free.,Bachelor\u2019s degree or four or more years of work experience.,Coding proficiency in Python, Java, R, etc.,Knowledge of AI/ML models, frameworks(keras, pytorch, etc.) and libraries/packages/apis (e.g., scikit),Bachelor\u2019s/Master\u2019s degree or PHD in mathematics, statistics, physics, engineering, computer science, or economics.,Advanced knowledge of math, probability, statistics, and models", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Manages project tasks, timelines and deliverables,Manages and mentors junior analysts, contributing to their technical and career development,Manages relationships with other Merkle business units,Develops analytical solutions to client business problems,Predictive statistical models,Customer profiling,Segmentation Analysis,Campaign Analysis,Data Analysis and Mining,Presents ideas and findings to clients,Participates in the sales process through solution development and responses to RFPs,Demonstrate thought leadership", "Design and implement effective data parsing and acquisition using Big Data processing methodology,Select, refine and develop features and algorithms for machine learning and deep learning techniques to solve various problems in semiconductor manufacturing and supplier chain environment.,Define and execute data collection and cleaning procedures,Perform adhoc analysis deliver clear presentations,Design, test, review, and monitor performance of algorithms in a team and shared ownership environment,Develop software tools and implement automation for data management, visualization, and advanced analytics", "Employ web retrieval and text mining techniques to facilitate the collection of publically-available unstructured data,Perform natural language, machine learning, and statistical analysis methods, such as classification, sentiment analysis, topic modelling, regression, statistical inference,Develop statistical and predictive models using software such as Apache Spark MLlib, Python, and/or R,Write regular research reports to describe novel ways on how unstructured data and machine learning approaches may enhance company and macroeconomic insights,A PhD in Information Retrieval/Natural Language Processing with a solid background in statistical learning techniques (e.g. CNNs, CRFs, HMMs, LDA, SVMs),Experience with Semantic Web, RDF/OWL/SPARQL, linked data, and/or taxonomies/ontologies,Strong programming skills in at least one object oriented programming language (Java, Scala, C++, Python, etc.),Excellent time management skills, with the ability to prioritize and multi-task, and work under shifting deadlines in a fast-paced environment,Excellent oral and written communication skills, including the ability to present complex technical information to a generalist audience,A track record of publications in one or more of the following: ACL, EMNLP, COLING, NIPS, NAACL, SIGIR, ICML KDD, WWW etc,Work with cloud and big data technologies including: Elasticsearch, Hadoop, AWS, C2S, and others", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Define problems, research solutions, and figure out which data to use in your solution,Query, assemble, clean, and refine datasets in preparation for analysis,Analyze data using a variety techniques in order to build a proof of concept for your solution,Build high-quality prototypes and collaborate with the engineering team to implement the solution in a scalable framework,Communicate findings to stakeholders and a broader audience of technical and non-technical co-workers and clients,Extremely fluent in Python, Pandas, and Scikit Learn,1+ years of work experience in machine learning in a commercial setting,Deep expertise in statistics, algebra, geometry and mathematical modeling,Fluent in data fundamentals including SQL and data manipulation using a procedural language,Multiple pieces of evidence that you've successfully been able to scope, deliver and sell your own research and prototypes,Excellent written and verbal communication skills on quantitative topics for a variety of audiences including clients (both technical and non-technical), product managers and engineers,Experience with NLP or computer vision is a plus,Willingness to learn new tools as they become needed,Bachelor's degree or higher in natural sciences / computer science / mathematics,Stock options,Full benefits, including medical, dental and vision insurance,Working Environment: Open-office space in SF, equipped with snacks, working remotely is optional,10 company paid holidays and generous vacation policy", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop and test pricing strategies that optimize profitability through the improvement of revenue, credit risk, and cost per customer acquisition;,Leverage pricing models to simulate profitability from pricing strategies and forecast changes to SoFi\u2019s customer credit portfolio;,Build intuitive and analytical models to help decision makers understand the price elasticity of customer segments;,Run significance tests and continuously discover trends in customer behavior to inform decision making with a high level of confidence;,Build reporting pipelines and dashboards to deliver results of pricing strategies to the business.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collecting and combining data from multiple sources,Uncovering and exploring anomalous data (including metadata),Applying the scientific process to data evaluation, performing statistical inference, and data mining,Developing analytic plans, engineer supporting algorithms, and design and implement solutions which execute analytic plans.,Designing and developing tools and techniques for analysis,Analyzing data using mathematical/statistical methods,Evaluating, documenting, and communicating research processes, analyses, and results to customers, peers, and leadership,Completed a degree program in the fields of mathematics, statistics, computer science, computational sciences, or a passion for rigorous analysis of data,Tenacity, integrity, persistence, and willingness to learn,Ability to solve complex problems,Use critical thinking and reasoning to make analytic determinations,Works effectively in a collaborative environment,Strong communications skills to both technical and non-technical audiences", "Marketing Strategy: Qualitative and quantitative market research services that deliver unique reports and marketing strategy insights to advertisers,Media Optimization: Data science solutions that facilitate efficient, transparent and profitable advertising placement for media publishers,Marketing and Media Activation: Connects Schireson's audience segmentation capabilities with its campaign optimization technology to allow clients to better plan and execute their advertising campaigns,Conducting data mining,Building predictive models,Developing proprietary algorithms and assets for clients,Drawing key insights to drive critical decision making,Creating high quality data driven presentations,Collaborating with peers and managers to consistently exceed client expectations,Participating in the full cycle of strategic client analysis projects from client facing kick-off to code delivery or implementation,Answering key strategic client questions utilizing robust data analysis,3-5+ years of hand-on experience doing quantitative analysis/modeling, technical/coding work,A desire to work on complex data sets, predictive modeling, and machine learning projects (but not alone!),Superb communication and presentation skills,Ability to think strategically, analytically, and proactively about diverse business problems,Passion for managing the quality & accuracy of analytics, including checking your and others' work,Experience multi-tasking in a fast-paced \u2013 and a fast-growing \u2013 small company or professional services/consulting firm environment is a plus,Proficiency with Python,Experience with software engineering best practices (OOP, Version Control and/or functional programming),Expertise in SQL and relational databases,Familiarity with cloud tools, AWS a plus,Exposure to big data tools such as Hadoop, Hive, Sqoop, Pig, Impala or Spark is a big plus,Experience in advertising or media & entertainment are a plus", "Leverage data and business principles to create and drive large scale FB Data Center programs,Define and develop the program for metrics creation, data collection, modeling, and reporting the operational performance of Facebooks data centers,Work cross-functionally to define problem statements, collect data, build analytical models and make recommendations,Be a self-starter, motivated by a passion for developing the best possible solutions to problems,Identify and implement streamlined processes for data reporting and communication,Use analytical models to identify insights that are used to drive key decisions across the organization,Routinely communicate metrics, trends and other key indicators to leadership,Provide leadership and mentorship to other members of the team,Lead and support various ad hoc projects, as needed, in support of Facebooks Data Center strategy,Build and maintain data driven optimization models, experiments, forecasting algorithms and capacity constraint models,Leverage tools like R, Tableau, PHP, Python, Hadoop & SQL to drive efficient analytics", "Defines opportunities where predictive analytics can provide continuous improvement to the inspection process.,Defines opportunities for the introduction and use of real-time or closed loop predictive analytics that can drive continuous improvement in the inspection process of Public Accounting Firms driving continuous audit improvement.,Designs, develops, and implements advanced predictive analytical models/applications that address improvements in business processes and lead to insightful and forward looking solutions.,Utilizes machine learning techniques to design and build data models and execute analyses.,Uses existing tools and/or coordinate acquisition of new tools to identify, analyze, and interpret trends or patterns in large, complex datasets.,Identifies problematic areas and conduct research to determine the best course of action and develops recommendations.,Uses experience in visualization techniques to drive model results and depict relevant process trends.,Demonstrates analytic capabilities with an understanding of correlation, optimization, and time series analysis.,Effectively communicates complex findings into actionable and understandable result for high-level discussions with management and executives.,Demonstrates a strong understanding of analysis and computational complexity and ability to programmatically solve.", "Creatively leverage both new and existing data to increase the effectiveness and efficiency of our risk infrastructure,Work with engineers to design machine learning solutions that operate effectively at scale,Partner with operatives to quickly respond to rapidly evolving threats,Apply good software development practices and actively contribute to production code", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Collaborating actively across functional partners and senior management to accomplish various project objectives.,Executing research and analytics, including data source identification, processing, model/algorithm development.,Turning insights into meaningful recommendations which will drive business value creation.,Ongoing coaching and career development \u2013 you will work with passionate people and have access to best in class training through our P&G Leadership Academy as well as day-to-day mentoring from your manager,We provide a market-competitive salary benchmarked against the finest companies, so you'll be able to spend your generous vacation time doing more of the things you love with the people you love,We offer a suite of benefits, including but not limited to: flexible working arrangements, generous paid vacation increasing with service, generous parental leave policies, group life insurance, health insurance, and dedicated support to help you find the right child care or elder care,Additional perks include discounted P&G products from our company shops and a discount platform offering you unbeatable savings on everything from groceries to exotic holidays,What's more, your financial package might include things like interest-free loans, a tax-advantageous share purchase plan, a contributory pension plan, and financial education and advisement on topics including purchasing real estate and generating wealth", "You are comfortable with data analysis, wrangling, and curation,You have a degree in a quantitative field with coursework in statistics (e.g. Math, Linguistics, Physics, Chemistry, Engineering),You feel at home on the command line and with text processing,You are eager to learn new technologies and skills in data processing and analysis,1+ years of full-time work experience or domain-relevant internships,Proficient with Unix commands and Ruby/Python scripting,Familiar with regular expressions, DOM/CSS/HTML, parsing JSON, and information extraction,Experience in reporting, analytics and databases,Experience with implementing machine learning pipelines,Experience with Spark or MapReduce", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Envision new network (engineering) AI/ML projects and present well-formed ideas to the team. Utilize considerable creativity to merge ideation, knowledge of AI/ML model uses, insights from SMEs/translators and constraints to propose ML projects that are meaningful and actionable.,Provide timelines, milestones, project plans and types of AI/ML models to be attempted for new prototype projects. Adapt and communicate needed changes as datasets, labels or models may not function as expected.,Partner with team and non-team data scientists to teach complex concepts, assess project feasibility, select input features and validate project output. Envision and test for corner cases.,Utilize large amounts of GPU effectively to train and attempt multiple models, documenting progress.,Guide AI/ML projects with high autonomy so that they will be accepted by the business and SMEs who will use them.,Pull sample data sets from NS tools and VGRID to attempt prototypes on new topics.,Learn the wireless domain (5G/LTE, xLPT datasets, RF Planning, Orchestration, etc) and toolsets to better communicate and understand the needs of engineers for AI/ML automation.,Publish blogs, create documentation, perform presentations, submit intellectual property write-ups and lead efforts for external publications.,Bachelor\u2019s degree in Electrical Engineering, Computer Science or four or more years of work experience.,Six years of relevant work experience.,A Degree.,Masters/PhD in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science, or a related field.,Broad experience in training multiple types of AI/ML models such as KNN, and XGBoost, CNN/RNN, reinforcement learning, deep learning models, GANs, etc.,Knowledge of Relational databases in SQL or R.,Previous experience in wireless networking is a plus.,Recognized as a contributor to Software Organized Networks (SON) platforms.,Experience in advanced AI/ML such as ensembles, deep learning, reinforcement learning, and NLP.,Four or more trained models moved to production or completed in a research environment.,Experience with Spark and Big Data deployments.,Attached examples of filing IP, Published Papers or Conference Presentation titles to your resume.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with a team of data scientists and cross functional partners to collaboratively develop solutions,Apply ML and other statistical approaches to generate insights on structured and unstructured data,Provide ML expertise in the design and delivery of data products for broad consumption by business partners,Mentor and support the training of technical and non-technical teams in data science and machine learning,Participate in the broader data science community to stay current with methodology, software, and data development and availability,Communicate and visualize the output of analyses, including both written and verbal communication, to business leaders and non-technical audiences,Conceptualize and deploy data science solutions for business questions", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Analyze one of the most interesting datasets in the world, which consists of over 20 billion user actions and 4 trillion events per day,Work with the product, marketing, engineering, and operations teams to launch and evolve high-impact products,Define what metrics we will use to analyze and optimize new products,Analyze feature adoption to ensure we are developing products that will drive the most impact for our customers,Help craft dashboards, product scorecards, and forecasts to keep the entire team up-to-date,Report to senior management on product adoption and forecasting,Research the latest data science techniques to apply to our product analytics methodologies", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Support and conduct projects that increase graduation and retention rates, reduce time to degree, support improved academic advising, and improve student learning and course performance;,Help improve our ability to assess the outcomes of strategic planning decisions by supporting the development of business intelligence dashboards that monitor strategic goals and key performance indicators;,Work with campus stakeholders to identify, prioritize, and conduct analytical projects that support the university\u2019s mission, goals, and strategic plan;,Use appropriate data and methods to uncover fact patterns and test hypotheses that support progress towards campus goals;,Prepare high-quality and clear written reports, exhibits, and visualizations that present and interpret findings;,Work with campus stakeholders to improve data quality and usefulness, including support of the development of business intelligence dashboards that monitor strategic goals and key performance indicators;,Support, operationalize and assess our student success initiatives including supporting committees charged with student success and persistence and the development and assessment of student advising tools, case management tools, nudging campaigns, and first-year alerts;,Support the implementation, training, and effective use of vendor-provided analytics systems;,Support collaborative, innovative, use of learning analytics tools;,Perform other duties as assigned.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Individuals in this role identifies and addresses complex business problems.,Designs and drives the creation of new data products and analytical capabilities embedded in multiple business applications.,Gathers and analyzes large volumes of data, evaluates scenarios to make predictions on future outcomes and supports decision making.,Comfortable in using advanced statistical data modeling techniques and tools.,Uses business acumen to solution strategy and roadmap problems and make best practice recommendations.,Responsible for using analytic techniques like advanced data visualizations, machine learning, Natural Language Processing and large scale optimization to improve Asurion\u2019s wireless and retail customers\u2019 experiences.,Work closely with product managers to identify and answer important product questions that help improve outcomes.,Interpret problems and provide solutions using appropriate data modeling techniques,Develop prototypes for new data product ideas using Machine Learning, Deep Learning & AI.,Design large scale models using Logistic Regression, Decision Trees, Conjoint Analysis, Spatial models, Time-series models , and Machine Learning algorithms.,Utilize Natural Language Processing to analyze speech and social data.,Communicate findings to product managers and development groups.,Drive the collection of new data and the refinement of existing data.,Analyze and interpret the results of product experiments.,Regularly invents new and novel approaches to problems; Takes initiative and breaks down barriers to solve problems; recognized within team as the source of solutions.,Comfortable manipulating and analyzing complex, high-volume, high dimensionality data from multiple sources.,A strong passion for empirical research and for answering hard questions with data.", "Statistical Computing: R, NumPy, SciPy,Modern Programming: Python, Scala, Java, C++,,Machine Learning Sub-Specialties: graph analytics, natural language processing, computer vision", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Partner with cross-functional teams focused on product improvements to insight,Dive deep into a wide range of data (behavioral, financial, etc.) to identify opportunities and recommend solutions,Contribute to data team initiatives focused on ensuring fast, reliable, and comprehensive data,Serve as a trusted consultant and promote data literacy across the company,Work with Redshift, Python, R, JavaScript/CoffeeScript, and MongoDB to unblock data questions. Experience in these isn't a requirement for applying, but you'll get exposure to them on the job.,Do your best work in a fun, respectful, and supportive environment,You have experience analyzing data in a business setting. You know how to pick the right approach for a question, from statistical models to A/B tests to data visualizations. You know that sometimes the simplest solution is the best.,You are creative and resourceful, with a passion for deriving insights from data and telling a story.,You're great at collaborating with non-technical consumers of data.,You're curious about the business side of \u2013 well business. You're eager to get to the guts of our customer's data in order to answer questions across the company.,You're self-motivated and thrive when working autonomously. You know when to run with something, and when to ask for help.,You're extremely proficient when it comes to writing SQL and Python/R, and you know how to write code to answer complicated questions and expose useful data.,You strive to master any technology or language that is needed or useful.,You're a fast learner, adept at prioritizing, and can bring good opinions to the table.,You have experience working ina consulting capacity", "Are you passionate about data and the science of the sample?,Do you enjoy working even harder to find a solution when it is just within your grasp?,Do you enjoy balancing innovation and applied project work for clients?,Are you naturally curious? Do you love puzzles?,Are you intrigued with becoming a \"student\" of successful individuals throughout the world?,Do others call you a perfectionist?,Do you seek to find the signal among the noise?,Are you very dependable and reliable?,Do you have an upbeat, positive attitude?,Do you produce twice the work of those around you?,Do you verify, verify, verify\u2026and then verify again?,An opportunity to work with those who appreciate and reward performance,An opportunity to work with a diverse portfolio of clients in a variety of industries,A stimulating growth environment and highly engaged company culture \u2013 we are a five time winner by Great Place to Work\u00ae as one of the best small and medium workplaces in the United States,An opportunity to make a difference for clients through insightful and consultative research,An opportunity to express your creativity", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build production web apps that improve how merchandisers make decisions and manage risk.,Innovate on algorithms that optimize our merchandise portfolio.,Partner across the business to make an impact on our bottom line and our client experience.,Contribute to a culture of technical collaboration and scalable development.,You have experience working collaboratively in the modern web ecosystem.,You have experience finding creative uses for the toolkits of financial engineering, operations research, optimization, etc (with desire to grow as a DS/ML generalist).,You are generous with your ideas and experience, and eager to seek the ideas and experience of others.,You believe that things are rarely black and white; there are multiple good paths to follow, and multiple valuable perspectives to consider.,You believe that working is better than perfect, and innovative is better than incremental.,You have a relevant degree and relevant industry experience.,We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!,We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation,We are a technologically and data-driven business,We are committed to our clients and connected through our vision of \"Transforming the way people find what they love\",We love solving problems, thinking creatively and trying new things,We believe in autonomy & taking initiative,We are challenged, developed and have meaningful impact,We take what we do seriously. We don't take ourselves seriously,We have a smart, experienced leadership team that wants to do it right & is open to new ideas,We offer competitive compensation packages and comprehensive health benefits", "Begin developing relationships one level up and two levels down. Identify and support opportunities to expand project scope beyond current deliverable and actively share information about clients\u2019 emerging support needs and trends with team members and management.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Analyze results of WSJ\u2019s ongoing A/B testing efforts.,Source, query, and clean data required for larger scale data science projects.,Experiment with using techniques like machine learning to power personalization in our products.", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Previous message: [Jobs] Please circulate widely: Apply now for the IIASA Young Scientists Summer Program 2014 (fwd),Next message: [Jobs] Brown U Demography postdoc,Messages sorted by: [ date ] [ thread ] [ subject ] [ author ],Previous message: [Jobs] Please circulate widely: Apply now for the IIASA Young Scientists Summer Program 2014 (fwd),Next message: [Jobs] Brown U Demography postdoc", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Previous message: [Jobs] Northwestern University Postdoctoral Fellowships in Global, Comparative, or International Affairs,Next message: [Jobs] Fwd: URGENT-Demographer Vacancy Announcement posted,Messages sorted by: [ date ] [ thread ] [ subject ] [ author ],Previous message: [Jobs] Northwestern University Postdoctoral Fellowships in Global, Comparative, or International Affairs,Next message: [Jobs] Fwd: URGENT-Demographer Vacancy Announcement posted", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Assist in building a scalable machine learning pipeline,Assist in designing and building robust data processing pipelines,Build high volume services that are reliable at scale,Collaborate with and explain complex technical issues to Product and Project Leads,Optimize and enhance existing products,1-2 years of Python experience in a data science environment,Experience building machine learning pipelines,Knowledge of machine learning algorithms and their applicability to specific use cases,Familiar with primary component analysis and feature extraction,Comfortable in a Linux/UNIX environment,Honest,Hungry,Humble,Happy,Knowledge of neural networks, including TensorFlow,Experience with H2O,Familiarity with Java and high performance SQL,Collaborative work environment with friendly people,Beautiful office with an open floor plan and huge windows,Transparency-we share very detailed financial info at company meetings,Investment portfolio- Acorns contribution from SteelHouse,Competitive compensation,100% healthcare coverage,Open-ended vacation policy with an annual vacation stipend!,401k plan,Flexible Spending Account (FSA) for dependent, medical and dental care,Weekly food deliveries, food trucks and outdoor BBQ", "Texas Health Resources (612 E. Lamar, Arlington, TX 76011),Full Time; 1st Shift,Minimum - $35.48,Maximum - $56.00,Please note salary is determine by the number of years of experience", "Georgia", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Merchant ranking: Determining which merchants to call first based on multiple objectives,User behavior modeling: Predicting user propensity for click, conversion, and attrition,Personalization: Recommendation algorithms for both users and merchants,Forecasting: Modeling historical data to forecast demand, purchase propensity, merchant risk, etc.,Marketing and CRM: Customer life-stage modeling, large-scale advertising on search engines and social media for customer acquisition and continued engagement,Exposure - Being a household name gives employees the opportunity to work in startup-like environment and focus on deeply interesting work that you wouldn\u2019t get to do anywhere else; you will have a large-scale impact in a short amount of time,Professional Development - Participate in weekly \u201clunch and learns\u201d with other engineering and business functions, skill building workshops, and networking sessions,Fun - Explore the city through our planned intern social outings, and develop your network while getting to know your peers and Groupon\u2019s leaders,You should be currently enrolled in a MS or PhD program in Computer Science, Software Engineering, Math, Statistics or related field,Preferred emphasis on algorithms, machine learning, data mining, statistics, applied mathematics, or similar field,You have strong coding skills in one or more programming language, such as Java/C++/Python/Ruby etc.,You have an understanding of applied math topics, such as probability and statistics, linear algebra, basic optimization techniques,Experience in SQL, Hadoop, R and scripting preferred,You are an excellent and efficient communicator", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Helping farmers in India get what they need most: insurance in tough years.,Giving produce growers in California the tools to optimize yields and minimize waste,Helping irrigated farmers in Nebraska manage water more efficiently and sustainably to protect our water supply,Perform deep dive analyses to understand and optimize environmental measurements,Provide expertise on statistical and mathematical concepts in a production environment,Develop and implement state-of-the-art analytical algorithms for time series segmentation, classification, and recognition,Research, develop, and prototype measurements for real-time crop monitoring and decision agriculture,Collaborate with team members from prototyping through production to rolling out big data capabilities, analytic frameworks, and best practices inside of data science,Advanced degree in a relevant computer or physical science or engineering discipline using any of these technologies,Substantial experience working on strategy and full-life cycle data science in Python, as well as experience working with data mining tools with Python and/or R,Experience with machine learning libraries such as xgboost, sklearn, Tensorflow, Keras, etc,An understanding of large datasets, the application of calibrations and analytics in a wide range of environments, and the implications of scalability across the globe,Team experience, specifically in cross-group collaborations with outstanding communication skills,Ability to obtain work authorization in the United States in 2018", "Lead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design, development, and deployment of AI solutions that drive business growth and better customer experience.,Contribute to the development of AI/ML strategy and roadmap for the company.,Contribute to research and development of AI/ML techniques and technology that fuels the business innovation and growth of Verizon.,Represent Verizon in AI/ML research and the industry through publications, conference speeches, collaboration with leading researchers and universities.,Build strong influence both among AI/ML community and with senior business leaders and actively promote the effective applications of AI/ML technology.,Lead engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or in a related field.,Ph.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.,Eight or more years of experience in practicing machine learning and data science in business.,Accomplished researcher and expert in machine learning, neural networks, reinforcement learning, chatbot technology, and NLP.,Strong communication and interpersonal influencing skills.,Excellent problem solving and critical thinking capabilities.,Experience in leading large scale data science projects and delivering from end to end.,Strong computing/programming skills; Proficient in Python, Spark, SQL, Linux shell script.,Strong experiencein Big Data and Cloud technology.", "Be immersed in a team of innovative and high caliber computer scientists, engineers and machine intelligence experts in a collegial and fun environment.,Work on solving unique and hard problems with data science, machine learning and Artificial Intelligence that are of value in the real world.,Create world-class products, solutions and cutting edge concepts.,Build on Comcast's tools, platforms and vast technical resources.,Work in a highly visible, dynamic team that provides continuous opportunities for learning and growth.,Have access to a wide range of data, exciting projects and technologies,Join a company with a strong commitment to our teams maintaining a healthy work life balance and providing a top tier benefits program,Research, analyses, architect and develop emerging concepts related to AI/ML and HMI,Engage and partner with top tier universities, companies, research labs and start-ups across the technology landscape to explore new concepts and theories,Design and develop, and analyze data, systems, methods, tools and technologies,Apply machine learning, deep learning, and NLP methods to massive data sets and complex systems.,Be a respected SME acting as an internal technology consultant to the Senior leadership of Comcast.,Design, implement, and deploy full-stack solutions for millions of Comcast customers.,Investigate and solve exciting and difficult challenges in data science, machine learning, classification, content analysis, and deep learning.,Research and develop innovative, scalable and dynamic solutions to hard problems.,Review analyze and report on technologies, methods, research and trends,Be a valued contributor in shaping the future of our products and services.,Collaborate witha cross functional agile team of software engineers, data engineers, ML experts, and others to address challenges head on.,Help to drive thought, strategy and process across out Technology, Products and Experiences.,Bring your experience designing, developing and implementing data pipelines, systems and solutions at scale.", "Working with warehouse team, ensure that inventory management software is implemented with data that is well formatted and easily accessible,Working with engineering team, extract demand data from online store,Predict supply levels with variable supply data,Marry supply data with demand data to forecast shortfalls and overages,Produce a business analytics dashboard to view current and future supply levels,Identify patterns and propose solutions,Regularly meet with supply team to communicate understocked areas to affect inbound supply,Regularly communicate to senior management about analysis results", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Develop and implement effective/strategic business solutions through research and analysis of data,Harvest insights from data and present findings that will impact business objectives,Facilitate effectiveness measurement through test design, implementation support, and post-analysis for new and existing programs and initiatives,Assist in managing day-to-day results reporting and visualization for major corporate initiatives,Analyze, review, forecast, and trend complex data,Develop statistical models, forecasts and conduct statistical analysis to better understand trends, population segments and predict behaviors/outcomes", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Conduct recurring web reporting to monitor and drive KPIs,Proactively develop site performance reports and analysis such as segmentation analysis ; highlight observations and business context to deliver actionable recommendations to business leads, not just data,Perform short-term and ad hoc analyses,Translate business requirements into a Solution Design Document used for implementation support, guidance, and documentation,Participate in the design, set up, and evaluation of A/B and multivariate testing,Ensure quality and timeliness of deliverables that meet expectations while balancing business needs with the appropriate level of analytical rigor,Provide insights to support senior management with decision making,Complete and maintain distribution of web analytics reports including contextualization,Bachelor\u2019s degree in business, finance, economics, statistics, or related field,Expert proficiency in web analytics tools including Omniture and Google Analytics,Strong business acumen and ability to present to senior management,Strong quality assurance skills,Must be detailed oriented with logical problem solving skills,Outstanding organizational skills and dedication to quality and integrity,Clear understanding of basic financial, statistical, and economic concepts,Ability to work collaboratively acting as a subject matter expert within a team environment to help define and meet measurement criteria and goals,Must be a self-motivator and enjoy sifting through large amounts of data,Ability to contribute effectively in a fast paced environment with limited supervision,Benefits package,Veterinary discounts,Website and retail discount,Bring your pet to work Friday,High-tech environment,Bachelor's (Required),Union City, CA 94587 (Preferred)", "The Senior Data Scientist anticipates future business needs and identifies opportunities for complex analysis.,The Senior Data Scientist, will gather and analyze data to solve and address highly complex business problems and evaluate scenarios to make predictions on future outcomes and provide prescriptive solutions that support decision making.,The Senior Data Scientist will be involved in all phases of (Big Data) analytics projects including question formulation, research, development, implementation and testing. The Senior Data Scientist will be able to explore and understand data and build advanced analytical models, then present and discuss the resulting models to any level of audience.,The Senior Data Scientist will design and drive the creation of new standards and best practices in the use of statistical data modeling, big data and optimization tools for Cox Automotive. Identify and direct special studies and analyses for unique business problems and scenarios. Proactively identify algorithms or products with high intellectual property content, assist in the evaluation of their potential for patents, where appropriate assist in the patent applications, and contribute to the intellectual property protection for Cox Automotive.,Development, research, and exploration in the areas of statistics, machine learning, experimental design, optimization, simulation, and operational research,Interprets problems and develops solutions to business problems using data analysis, data mining, optimization tools, and machine learning techniques and statistics,Leverage big data to solve strategic, tactical, structure and unstructured business problems,Collaborate with client and Enterprise Data Products team to set analytic objectives, approaches and work schedule,Research and evaluate new analytical methodologies, approaches and solutions,Analyze customer and economic trends that impact business performance and recommend ways to improve outcomes,Developing advanced statistical models utilizing typical and atypical methodologies.,Developing and updating data models for statistical modeling purposes, tracking results against forecasts, and re-specifying when required.,Design and deploy data-science and technology based algorithmic solutions to address business needs for Cox Automotive. Identify, understand and evaluate new commerce analytic and data technologies to determine the effectiveness of the solution and its feasibility of integration with Cox Automotive\u2019 s current platforms,Design large scale models using Logistic Regression, Linear Models Family (Poisson models, Survival models, Hierarchical Models, Na\u00efve-Bayesian estimators), Conjoint Analysis, Spatial Models, Time-Series Models,Design large scale models using linear and mixed integer optimization; non-linear methods; and, heuristics,Design large scale discrete-event and Monte Carlo simulation models,Interpret and communicate analytic results to analytical and non-analytical business partners and executive decision makers.,Develop, coach and mentor team members within the department.", "Synthesize raw data into recommendations on how products should evolve,Build machine learning algorithms to enable decision making in order to optimize customer experience/reduce cost/maximize profits or customer value,Work with product management and engineering and operations teams to develop KPIs for new features and instrumentation to quantify the adoption of new product initiatives,Quantify the value of existing product improvements or the potential value of new product hypotheses.,Design and support large-scale multivariate tests to prove out new ideas,Create product scorecards for teams and managers to track product performance, business impact and discover opportunities,Deep-dive analyses to identify root causes of product or operational issues", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work within administrative, existing legacy systems to extract data for consumption into models. Create bounds of model accuracy as may result from data vintage and manage perceived accuracy of model based on historical data quality.,Identify opportunities to enhance Operations businesses using econometrics and advanced analytical methods, perform analyses, and deliver findings/recommendations.,Work on complex models to answer business optimization questions in LTC and Life Insurance Operations, both on a forecasting basis as well as an ad hoc basis to meet changing business demands.,Work closely with partners (Actuaries, Ops Leaders, Senior Leaders, Shared Services) to model business problems dealing with Operations and make clear recommendations. Explain clearly impact of Ops decisions/process changes on partner areas and vice versa.,Collaborate with thought leaders and functional leadership within Insurance Operations around effective use of models to address business needs; coach/teach partner areas on creation and ownership of their models to address business needs.", "Collaborate with AI Center Data Scientist teams and senior leaders to ensure model architecture and data architecture roadmaps are designed to scale beyond a business area. Anticipate connections, dependencies, and risks in scaling and plan for these in the design phase as opposed to solving for these down the road once the solutions are in production where these could be far more expensive.,Design, develop, and deploy large-scale AI products with closed loop optimization in a production environment. Find ways to optimize and improve our AI products by looking across various business applications and making the connections that may not be visible to teams working on a specific problem space.,Experiment and bring relevant data science techniques, algorithms, and tools to the organization and embed into ways of working of data science teams.,Operate as a thought partner to the Enterprise Architecture team on our overall enterprise northstar architecture to ensure that AI products and platforms requirements are accurately represented in all our business and technology applications and roadmap.,Lead in designing AI that protects the privacy of our customers and ensures that our modeling practices are ethical and bias free. Bring thought leadership in this space by introducing ways of identifying and testing for biases in our models. Introduce new methods to protect the confidentiality of our data.,Operate as an expert both within Verizon and externally within industry,Serve as a technical thought leader and advisor to the organization,Serve as trusted advisor to our business partners,Attract top talent with deep technical expertise to Verizon,Partner with academia and startup ecosystem to identify and potentially bring in new AI applications that can be of value to Verizon.,Stay informed on the latest advancements in AI and technology space, finding ways to deliver value by applying and customizing these to our specific problem space.,Help create an environment where our scientists are engaged in pioneering work that can be shared with the world in a number of ways like open source, publications, etc.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Design and develop tools to enhance current and future data requirements from multiple data sources,Interact with project stakeholders to provide solutions addressing business data analytic requirements,Ensure that development requirements are clearly understood and that work is consistent with requirements,Provide accurate completion deadlines,Work with SQL and other database access tools and methods to address business or technical requirements,Design, implement and support data movement solutions across multiple platforms and data sources (Database tables/views and external files) using custom code and ETL tools and frameworks,Contribute to UI modernization actively developing functionalities and participating in design discussions,Conduct PL/SQL, shell scripting and advanced SQL developments,Ability to generate, modify and version database object scripts (DML/DDL),Become proficient in the data model and data structures to be able to efficiently respond to business queries and requests,Provide ongoing maintenance and support of database schemas and views,Conduct root cause analysis and resolve production problems and data issues,Work within the defined software development lifecycle, following rigorous change control policies,Create and maintain up to date documentation of the data model, data flow and field level mappings,Bachelor degree and 3 - 5 years of experience in database developments for various transactional and/or analytical database systems,Significant experience in the development process with hands on experience of basic design, structured programming, work reviews, testing cycles from unit to acceptance testing, release build, implementation and post release bug/issue fixes,Excellent information management skills (architecture, design, development and support) in ETL, Database, Reporting & Big Data,Extensive experience programming analytic models using R, Python, Java, MATLAB, SAS, or other programming language,Experience in modeling behavior of financial instruments by either statistical and/or machine learning methods,Advanced proficiency with Oracle SQL (e.g. complex views/joins, analytical functions, regular expressions, table partitioning, materialized views, distributed transactions) and Oracle SQL Performance/Tuning,Advanced proficiency with PL/SQL Packages/Functions/Procedures programming and debugging, Oracle Collection and Object Types,Experience with Oracle Data Pump, SQL*Loader and external tables framework,Experience working with large volumes of data and relational / dimensional data models,Good understanding of Oracle Database Architecture,Experience with Oracle development tools (TOAD, Oracle SQL Developer\u2026) and Source Control Software,Advanced knowledge in scripting in Unix/Linux environment,Experience with business intelligence reporting tools (SQL query, Business Objects), data integration tools (Informatica, etc.), and metadata repository processes and tools,Experience with financial reporting is a plus,Experience with Oracle Application Express is a plus,Experience with Snaplogic and/or Talend ETL is a plus,Must be resourceful, self-motivated, and comfortable working under pressure in an unstructured environment with little supervision,Strong written and oral communications skills,Proven ability and initiative to learn and research new concepts, ideas, and technologies quickly,Strong systems/process orientation with demonstrated analytical thinking, organization skills and problem solving skills", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Prototyping optimization solutions for advertising applications,Developing and implementing machine learning algorithms,Researching and building new analytical solutions,Developing methods for advertising attribution analyses,Data visualization and analysis,Data processing and statistical algorithm development,Graduate degree in Statistics, Operations Research, Applied Math, Engineering, Computer Science, or other quantitative discipline preferred,Experience in implementing optimization methods,Experience in conducting attribution analyses,Experience with Media Math & Economics,Experience with R and Python a plus,Experience with Machine Learning Implementation,Experience working with relational databases. SQL experience a plus.,Excellent oral and written communication skills are a must.,Must be highly analytical with the ability to resolve complex issues independently.,Must be detail oriented with a passion for providing informative documentation.,Strong interest in TV advertising and advanced media platforms.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build data products that solve hard problems and reveal opportunities through creative sourcing, model development, and visualization.,Four or more years of experience in a data science role, with a concentration on social sciences, demographic analysis, and anthropology.,Experience with the creation of visual data displays.,Expert in Python; R; Scala,MA in Applied Mathematics, Computer Science, Engineering, or related field; or experience. Self-taught coders are welcome. Bring your portfolio and impress us.,Credentials in machine learning, AI, probabilities, statistical inference, and linear models,Familiarity with Unix shell scripting", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Lead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design, development, and deployment of AI solutions that drive business growth and better customer experience.,Contribute to the development of AI/ML strategy and roadmap for the company.,Contribute to research and development of AI/ML techniques and technology that fuels the business innovation and growth of Verizon.,Represent Verizon in AI/ML research and the industry through publications, conference speeches, collaboration with leading researchers and universities.,Build strong influence both among AI/ML community and with senior business leaders and actively promote the effective applications of AI/ML technology.,Lead engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or in a related field.,Ph.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.,Eight or more years of experience in practicing machine learning and data science in business.,Accomplished researcher and expert in machine learning, neural networks, reinforcement learning, chatbot technology, and NLP.,Strong communication and interpersonal influencing skills.,Excellent problem solving and critical thinking capabilities.,Experience in leading large scale data science projects and delivering from end to end.,Strong computing/programming skills; Proficient in Python, Spark, SQL, Linux shell script.,Strong experiencein Big Data and Cloud technology.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Analysis and validation of data within the Oracle and SSIS platforms,Understand, write and debug complex SQL logic against large tables,Develop & maintain Webi Reports and Dashboards in SAP Business Objects,Ensures adherence to locally defined standards for all developed components,Performs extensive data analysis & validations for both Source and Target tables/columns,Maintain technical documentation for all supported platform applications,Supports the development and design of the internal data integration framework,Participates in design and development reviews,Works with System owners to resolve source data issues and refine transformation rules,Collaborate proactively with DBA teams to triage performance issues,Ensures performance metrics are met and tracked,Write and maintain unit tests,Support QA Reviews", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Ideation of data models and modeling (i.e. historical as well as predictive models),Working with the Games team and partners to understand what are the needed KPIs in addition to what selected BI tools provide,Help design and monitor A/B tests,Ideate cross-promotion campaigns,Work across teams to identify needs and translate them to modeling mandates,Explain player behavior and campaign results", "Design experiments and create case studies to apply the findings into production code,Employ predictive modeling, data mining, graph algorithms and other data science techniques to your project with the end goal of increasing our platform\u2019s ability to drive customer value while sustaining healthy margins,Responsible for implementing data mining and statistical machine learning solutions to various business problems such as lead scoring, demand forecasting, and target marketing segmentation,Model train and improve our machine learning algorithms for audience discovery and apply bid optimization strategies for real-time-bidding,Provide analytical /technical leadership in a team environment for the design and developed of analysis systems to extract meaning from large scale data,Advanced degree or MA/MS,3 years experience in applied Machine Learning techniques, advanced analytics and statistical modeling,Strong background in foundational mathematics such as statistics, probability and linear algebra,Experience manipulating large data sets, using SQL, Hadoop, MapReduce, Hive,Understanding of programming concepts and experience with programming languages, such as Python, MATLAB, R, Java, Scala or C", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with a team of data scientists and cross functional partners to collaboratively develop solutions,Apply ML and other statistical approaches to generate insights on structured and unstructured data,Provide ML expertise in the design and delivery of data products for broad consumption by business partners,Mentor and support the training of technical and non-technical teams in data science and machine learning,Participate in the broader data science community to stay current with methodology, software, and data development and availability,Communicate and visualize the output of analyses, including both written and verbal communication, to business leaders and non-technical audiences,Conceptualize and deploy data science solutions for business questions", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a Subject Matter Expert for all aspects of data analytics applications from descriptive through to prescriptive solutions. Design, build, deploy and maintain highly scalable analytical solutions that support those running the operations to make more efficient and effective decisions.,Deep and broad knowledge in the application of advanced analytical techniques to solve business problems.,Identify additional sensors to include missing information that is relevant for building analytics systems.,Use data analysis to identify interactions between process and product quality, leverage process capability to optimize quality, and identify process enhancements to improve product quality and consistency.,Validates mathematical modeling results with design of experiments or empirical data in manufacturing processes, pilot studies or lab experiments.,Works with cross functional teams of scientists, engineers, and operations in R&D, manufacturing process, and data infrastructure.,Communicates from factory floor to business leaders, sees options uniquely across all businesses, and has the ability to interface/collaborate in a business sensitive manner that is flexible and unbiased.,Understands manufacturing data, manufacturing process, and fundamental meaning of process measurements.,Understands applied physics (enthalpy, heat transfer, etc),Develops solutions that can be understood and used by operations to extract insights and make decisions,Standardizes, simplifies and improves the user friendliness of analytics platform for more widespread use of tools across OC.,Stays current with respect to statistical/mathematical methodology, to maintain proficiency in applying different methods and justifying methods selected.,Leads the assessment and introduction of new statistical/mathematical technology and methodology to apply in broader practice.,Commits to evolving how projects are managed to match the business and project needs, change management challenges and communication requirements to successfully deliver impactful results.,Defines, develops and communicates the technology standards for manufacturing analytics.,Builds a community for growing the capability within Owens Corning through personal development, mentoring and teaching opportunities with peers.", "Own the definition and tracking of important metrics regarding adoption, utilization and impact of Xpring initiatives. How do we know we are making an impact?,Be the expert for technical deep dives of emerging blockchain technologies, such as DeFi instruments, novel scaling technologies and digital exchange performance.,Understand and model digital assets, their liquidity and how they can support different applications and use cases (BTC/ETH/XRP and beyond).,Identify and collect important datasets to support analytics and data science research across Xpring and Ripple.,You have significant hands-on experience applying quantitative methods to real world data, big and small,You are experienced with Python/R/SQL and cloud data services,You love delivering tools and APIs in addition to presentations and dashboards,You are a collaborative coder, comfortable with Git and code reviews,You have excellent written and verbal communication skills,You are excited about blockchains: the technical details and the potential benefits,The chance to work in a fast-paced start-up environment with experienced industry leaders,A learning environment where you can dive deep into the latest technologies and make an impact,Competitive salary and equity,100% paid medical and dental and 95% paid vision insurance for employees starting on your first day,401k (with match), fully paid parental leave, commuter benefits,Generous wellness reimbursement and weekly onsite programs,Flexible vacation policy - work with your manager to take time off when you need it,Employee giving match,Modern office in San Francisco\u2019s Financial District,Fully-stocked kitchen with organic snacks, beverages, and coffee drinks,Weekly company meeting - ask me anything style discussion with our Leadership Team", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Use data to inform and influence the direction of team roadmaps and inform business decisions,Work with partner teams to define goals and identify metrics for improving existing features and new releases,Deepen our understanding of patterns/trends in user behavior, design experiments, and uncover new opportunities,Build dashboards and reports to drive awareness and understanding of metrics and experiment results,Work closely with Data Engineering and IT to author and develop core data sets that empower operational and exploratory analyses", "Develop data science methodologies to classify images,Utilize data science tools such as Python, TensorFlow, Keras, etc.,Work as part of a team to evaluate and refine new modeling approaches,Implement, train and deploy Deep Learning / Convolutional Neural Network models,Student pursuing an MS or PhD degree in Computer Science, Computer Science & Engineering, or Electrical Engineering with experience in applied image processing and machine learning,Strong interpersonal and communication skills,Highly analytical and imaginative thinking ability,Demonstrated skills in Python, TensorFlow, and Keras,Experience with Linux systems", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Work across massive enterprise email datasets,Deploy models to production that impacts directly our customers and products,Improve our ML workflow,Collaborate with engineering and product leaders to develop a product suite that is continually detecting more advanced threats for our customers,Make your mark on leading ambitious projects,Mentor and knowledge share with data scientists at Tessian,4+ years Data Science experience,Has strong NLP experience (in production),Has worked with, or understands the principles behind working with data at scale,Can balance longer term projects alongside impactful quick wins,Cares deeply about the impact their team hasIs a creative at heart, and can encourage novel ways of thinking in their team. This has always been a core part of the data science team\u2019s DNA,Reads Rules of Machine Learning - Martin Zinkevich and it resonates with how they think about machine learning and data science,It\u2019s important to us that all Tessians are part of the journey we\u2019re on, so we offer equity options with every role and benchmark to provide above market rate salaries - there\u2019s plenty more too\u2026.,25 days of paid holiday (plus 8 bank holidays, and an additional day for every year you've worked at Tessian!),Private health insurance provided through Vitality Health and mental health support through our Employee Assistance Program,Classpass - subsided access to gym time and classes all across London,Choice First: Do your best work, in the way that works best for you,Flexible working hours and working from home (if you're not already remote!),Work-from-home subsidy upon joining, so you can kit out your home office,Enhanced pension contributions, matched up to 5%,We\u2019re family friendly, with policies built to support you in all stages of life,High-quality tech kit provided for you to work on, plus Tessian ANC headphones,If you're relocating to join the team, we'll provide a contribution to help with your costs,Elite membership of the Tessian House System...,Every other Wednesday we stop at 5 and share team updates and drinks", "Advocate, evangelize and build data-driven design and manufacturing processes that drive efficiency.,Develop a process design space exploration framework which enables design parameter sensitivities used to inform initial design of electromagnetic induction heating processes, large scale deformation and structural mechanics.,Research, design, implement and validate cutting-edge algorithms to analyze diverse sources of data to achieve targeted outcomes.,Provide insight into leading analytic practices, design and lead iterative learning and development cycles, and ultimately produce new and creative analytic solutions that will become part of our core deliverables.,Construction of isogeometric sensitivity analysis and surrogate models to enable rapid design space exploration.,Develop numerical techniques to optimize steady and non-steady forming processes utilizing inverse problem formulation together with evolutionary search schemes.,Application of isogeometric analysis to model the elastic deformation and electromagnetic heating of multi-phase metallic materials.,Clear communicator with strong ability to communicate insights in a clear, concise, and valid, data-driven way, so that others in the company can effectively act on those insights.,Strong desire for a fast paced, data-driven, collaborative and iterative engineering environment. Strong love of learning, especially complementary physical sciences including Mechanics, Physics and Material Science.,Excel at making complex concepts simple and easy to understand by those around you.,Passionate about asking and answering questions in large datasets, and able to communicate that passion to product managers, Manufacturing managers and engineers.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Applies machine learning/deep learning/artificial intelligence techniques,Develops and implements Markov decision process models and healthcare economic models,Uses advanced analytics methods to extract value from business data,Performs large-scale experimentation and builds data-driven models to answer business questions,Creates hypotheses and experiments to identify hidden relationships and construct new analytics methods,Articulates a vision and roadmap for the utilization of data as a valued corporate asset,Visualizes information and develops reports on results of data analysis,Influences product teams through presentation of data-based recommendations,Spreads best practices to analytics and product teams", "Explore client data holdings to establish relationships and identify connections across disparate systems.,Determine statistically significant patterns in data and identify measurements that have no statistical validity.,Develop statistical models to predict relevant results.,Design and execute experiments to compare the performance of different systems.", "Utilize analytical, statistical, and programming skills to collect, analyze, and interpret large data sets.,Able to understand, break down and explain complex algorithms,Conducts advanced analytics leveraging predictive modeling, machine learning, simulation, and other techniques to deliver insights or develop analytical solutions to better manage compliance risk.,Lead projects to enhance data analysis and drive predictive analytics,Research and explore data sources; at times, work with data and systems experts to understand the data and what it represents,Build strong working relationships and translate complex analytical and technical concepts to non-technical employees to enable understanding and drive informed decisions.,Familiarity with Machine Learning Algorithms and willingness to learn skills and interest in data-mining large datasets for insights extraction,Present findings in a format that is understood and actionable by the business partners and stakeholders.", "Bringing the relevant data into a central system to create the single version of truth,Creating key metrics, reports and dashboards to support business health,Opportunity analysis and hypothesis generation for stages throughout the end-to-end user lifecycle,Building advanced analytical models (behavior segmentation, churn prediction, purchase propensity, recommendation engine, etc.) that spans engineering, marketing, partners and finance,Design, prototype, implement and test descriptive and predictive analytics models,Work with data engineers to architect and develop operational models that run at scale,Partner with teams to identify and explore opportunities for the application of machine learning and predictive analysis", "Working directly with clients to develop analytic questions and define data and technology opportunities,Driving the exploration of data sources and analytic techniques to create new variables and modeling capabilities,Working with large and complex data sets to solve difficult and non-routine problems,Applying a variety of statistical methods/approaches to develop analytic models", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Parameterization and Abstraction: Use data-driven methodologies to discover/develop and abstract key security and malware metrics across multiple models and network segments to support network traffic due to malware and security threat evaluation and analysis, including evaluation of alternate threat detection and network bandwidth consumption due to malware models and future business scenarios,Modeling: Architect, Design, Build and Support data models using both real time and historical analysis approaches to support security, business, technical planning, and decision-making.", "5+ years of financial industry experience,Natural programmer, and demonstrated industry experience with statistics and data modeling,Extensive experience in written and oral communications/presentations, and ability to produce a variety of business documents (business requirements, technical specs, slide presentations, etc.) that demonstrate command of language, clarity of thought, and orderliness of presentation,You will be responsible for providing Project/Program Management oversight and direction for Artificial Intelligence and other analytics projects across client s Institutional.,You will work with a variety of partners and stakeholders to help identify, execute, and track the progress of multiple projects at the same time.,As part of a Product organization, you will also have the opportunity to work closely with Product Managers on various aspects of product rollout.", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "BA/BS degree and 3+ years of experience as a data scientist, or MS/PhD and 2+ years of industry experience in a quantitative role.,Fluency with SQL and Python or R for data analysis.,Solid understanding of statistical inference, experimental design and analysis.,Enthusiasm for clean code and sharing reproducible results.,Communication skills to work with partners on engineering, product and business teams.,An eye for great data visualization with Matplotlib, Plotly, ggplot, or Tableau.,Define key metrics to track Yelp\u2019s performance and inform product decisions.,Assess and frame questions from partners into actionable deliverables.,Design, execute, and analyze complex experiments impacting millions of users.,Devise and evaluate models for diverse business needs, such as identifying growth opportunities, personalizing user experience, and matching consumers to businesses.,Own analyses start-to-finish and communicate key insights to stakeholders.", "Interact with and advise Management regarding credit risk issues; formulate product strategy recommendations, and evaluate the risk in the overall loan portfolio,Autonomous end-to-end statistical model creation, Including but not limited to identifying objectives, compiling data, sampling/prepping data, feature selection, model comparison/selection, deployment and monitoring,Ensure adequate internal control processes around model development, implementation and validation are established,Develop, monitor and maintain of custom risk scorecards. Recommend and implement model changes to improve performance of credit functions,Apply intermediate to advanced knowledge of financial processes and procedures, and routine modeling theories and techniques to create effective modeling solutions for a single or multiple business functions", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "You are currently enrolled in a BS, MS or MBA program or equivalent in Computer Science, Mathematics, Economics, Information Management, Statistics or a related field,You have earned a minimum of a 3.0 GPA,You know how to work a problem from beginning to end with data science tools and techniques, including data manipulation (SQL, Hadoop, etc.) and programming (R, Python, XML, Javascript, or ETL) frameworks,You have experience with Oracle databases, strong knowledge of and experience with reporting packages (Business Objects etc), and knowledge of statistics for analyzing large datasets (Excel, SPSS, SAS etc).,You possess technical knowledge regarding data models, database design development, data mining and segmentation techniques,You have strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy,You are proficient in queries, report writing and presenting findings", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Works with stakeholders throughout the organization to identify opportunities for leveraging data to drive business solutions,Munges, cleans, and prepares data,Mines and analyzes data from state databases to drive optimization and improvement of program development, marketing techniques and business strategies,Assesses the effectiveness and accuracy of new data sources and data gathering techniques,Performs exploratory data analysis in order to understand state data,Interprets results from multiple sources using a variety of techniques, ranging from simple data aggregation via statistical analysis to complex data mining independently,Develops analytic and predicative models for business needs,Coordinates with different functional teams to implement models and monitor outcomes,Create meaningful visualizations that communicate information and answers,Performs related duties as assigned", "Design, develop and maintain software solutions to business problems using data engineering and machine learning techniques.,Collaborate with both business and engineering teams.,Assist, train and mentor other members of the data team as needed.,Other duties as assigned by the Chief Technology Officer.,Experience with recommendation systems, machine learning algorithms such as linear regression, SVM, decision trees, and clustering algorithms.,Experience in Python, R.,Experience reading cross-disciplinary research and applying to unique problems.,Proficiency in SQL.,Bachelor's or higher degree in Mathematics, Engineering, Economics, or Statistics.,Knowledge of the key KPI\u2019s that are important to an ecommerce business.,Ability to work in both a team and individual environment.,Experience with Java Scala or other object oriented development is a plus.,Knowledge of distributed computing (Spark/Hadoop) is a plus.,We\u2019re a relatively small but rapidly growing team.,We like cats. Dogs are okay too.,We\u2019re generally a build over buy technology focused group that has built, maintains, and manages all of Wantable's homegrown software solutions to solve the business needs.,We work in reasonable weekly sprints; following an agile process.,New/emerging technologies, modern techniques, and finding new solutions excite us the most.,We\u2019re hard workers but we don\u2019t let that get in the way of having a good time.,We make consistent, meaningful impact on the direction and health of the business.,We believe in transforming the way that people shop through a combination of technology and experienced stylists.,First 30 days...,Become familiar with the Wantable business model, data structure and existing machine learning initiatives. Begin working with our python and R code bases.,First 90 days...,Establish standards and practices for data analysts to use with Tableau. Complete a project within the python code base. Become fully involved and integrated in active machine learning projects.,First 6 months...,This is where things start to get interesting. Once there are established successes this position will move more into building, tuning, maintaining and reporting on the success of recommender systems, natural language processing, and any other data science that we're able to use to improve customer and employee experience.,One year and beyond...", "Develop statistical algorithms for large data sets,Apply machine learning and recommendation systems techniques for digital marketing use-cases,Design A/B testing experiments, and do end-to-end data analysis,Effectively communicate with other teams, and tell stories with data,Experience with statistical analysis and algorithm development, data mining, and machine learning,Experience with Python/R and SQL,Excellent communication and collaboration skills,Masters degree in a relevant field", "Conduct financial data analysis on large datasets to generate profitable trading alphas.,Define key metrics for model validation,Back-testing and production performance monitoring,Contribute to in-house data analysis packages and research framework development,Acquisition of new data sets and cleaning/maintenance of new and existing data sets.", "You can move the needle here.,We have the tools (and data) to truly do interesting work in Data Science.,We're tackling the problems that drive global business.,There is an exciting journey ahead, and we're going to have fun along the way.,8+ years of experience applying advanced AI techniques (machine learning, predictive analytics, optimization, semantic analysis, time-series analysis, advanced visualization) to real-world problems.,You have a demonstrated ability to iteratively conceptualize, design and build data-driven analytical models, and have taken a project leadership role in shaping these solutions.,Strong capabilities in modern analytics languages/tools.,Balancing breadth, you also bring a particular depth in at least one of the following areas: Deep Learning, Optimization or Operations Research, NLP, Image Processing, or Machine Learning. Given the problems we are tackling, there is a place at Noodle for experts in various fields.,You have an advanced degree in a relevant field (Computer Science, Operations Research, Statistics, Applied Math, Electrical Engineering, or other Computational Science),You have experience manipulating and preparing large, heterogeneous data sets (\"Big Data\") to support advanced analytics.,Collaborative, open, and respectful working style,Intellectual curiosity and the hunger and humility to always keep learning\u2026 and to share that learning. You enjoy mentoring other intellectually curious data scientists.,Experience with (and excitement for) interdisciplinary collaboration", "Leads the organization's planning of GEODIS\u2019 data model and data architecture through continual redesign of the data model and/or transition to a common model,Recommends strategies to management on approaches to optimize business success based on changing business needs,Evaluates applicability of leading edge technologies and uses information to significantly influence future business strategy,Analyzes complex business and competitive issues and discerns the implications for systems support; including identifying, defining, directing, and performing project issue analysis of the technical and economic feasibility of proposed data solutions,Designs projects with the proper tools and methodologies to successfully address cross-technology and cross-platform issues within the business and/or the future architecture of the organization,Proposes and leads projects required to support the development or the organization's data infrastructure needs and provides intelligence on advances in database technologies,Develops partnerships with senior users to understand business needs and define future data requirements,Interacts with internal and external audiences to develop effective networks to ensure business needs are met,Other duties as required and assigned,Master\u2019s degree in Data Science, Computer Science, Statistics or Business Administration,Minimum 7 years of related experience and/or training; or an equivalent combination of education and experience,Experience with analytical and data mining efforts which could include but not limited to clustering, segmentation, logistic and multivariate regression, decision/CART trees, neural networks, time-series analysis, sentiment analysis, and topic modeling,Experience with optimization algorithms; Bayesian Optimization, Genetic Algorithm, and Particle Swarm Optimization, etc.,Knowledge of Experience in modeling and an ability to implement models or model components from scratch.,Experience using statistical computer languages; Python, R, etc.,PC literate with experience with Microsoft Outlook, Word, Access and Excel,Excellent written and oral communication skills that demonstrate a proven ability to develop relationships with a range of internal and external customers,Excellent planning and organizational skills,Ability to read and interpret documents such as safety rules, operating and maintenance instructions, and procedure manual,Ability to write routine reports and correspondence", "Build production fraud and credit machine learning models; your models will decide who we lend to in real time,Passion and drive to change consumer banking for the better,5+ years industry experience or PhD in a related field and 2+ years experience,Deep understanding of and experience with machine learning and data analysis,Strong programming ability, preferably in python", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "50%-Design and develop algorithms and models to use against large datasets to create business insights,20%-Establish scalable, efficient processes for large scale data analyses, model development and model implementation,20%-Present analysis and resulting recommendations to senior management; Leverage data to present a compelling business case to optimize investments and operations,10%-Communicate and educate technical and non-technical employees on analytics and data-driven decision making,This position reports to Director of Data Science, or Sr. Manager, Data Science,This position has no direct reports,Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.,Typically requires overnight travel less than 10% of the time.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collaborate with scientists and researchers in one or more of the following areas: data intensive applications, text processing, graph analysis, machine learning, statistical learning, information visualization, low-level data management, data integration, data streaming, scientific data mining, data fusion, massive-scale knowledge fusion using semantic graphs, database technology, programming models for scalable parallel computing, application performance modeling and analysis, scalable tool development, novel architectures (e.g., FPGAs, GPUs and embedded systems), and HPC architecture simulation and evaluation.,Work with other LLNL scientists and application developers to bring research results to practical use in LLNL programs.,Assess the requirements for data sciences research from LLNL programs and external government sponsors.,Carry out development of data analysis algorithms to address program and sponsor data sciences requirements.,Engage other developers frequently to share relevant knowledge, opinions, and recommendations, working to fulfill deliverables as a team.,Contribute to technical solutions, participate as a member of a multidisciplinary team to analyze sponsor requirements and designs, and implement software and perform analyses to address these requirements.,Develop and integrate components-such as web-based user interfaces, access control mechanisms, and commercial indexing products-for creating an operational information and knowledge discovery system.", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "You'll be using your heavy statistical and econometric background to continue enhancing our state-of-the-art credit model,Work directly with vendors to source and evaluate cutting-edge data for inclusion into our model,Aggregate data from a variety of datasets, automate data collection and processing,Develop optimization algorithms and guide integration with existing technology,Communicate analytical findings on your credit model to the finance team and executive leadership,Work with our Data Engineers to ensure your models are implemented correctly and efficiently,2+ years of experience doing quantitative analysis, with proven experience delivering results on research or analytics projects,B.S. in a quantitative discipline such as Math, Statistics, Economics, or Econometrics. Graduate degree heavily preferred.,Proven fluency in SQL,Experience with at least one statistical package such as R, SAS, MATLAB, etc.,Strong understanding of econometrics and predictive modeling methods, knowledge of machine learning techniques is a plus,Knowledge of other programming languages (e.g., Python, Java, or .Net) and/or experience with large data platforms (e.g., Hadoop) is a plus,Ability to prioritize and manage projects to completion without guidance,Strong communication skills,A full-time, salaried position,Medical (employee medical fully paid by Carvana), Dental, and Vision benefits,A 401K with company match,All the perks your heart desires (gym, snacks, iced coffee on tap)", "Master\u2019s degree or equivalent experience in a technical field, such as data science, urban planning, computer science, economics, finance, or mathematics,0-3 years of experience and demonstrated skill in data science, or similar field using advanced analytic methods and tools,Solid understanding of and proficiency with Python (pandas, numpy, scipy, scikit-learn), R and/or SQL,Efficient manager of workflows and processes with the proven ability to manage multiple competing projects, priorities, and deadlines,Proficient in statistical analysis, data management, and presentation,Ability to write clearly and succinctly in a variety of formats (e.g. email, technical documentation, instructions),Effective in a variety of communication settings: one-to-one and in groups; with peers, supervisors, staff and clients; in-person and long distance.,Experience applying and interpreting travel forecasts particularly around transit usage.,Proficiency using network analysis software such as TransCAD, Cube, EMME, and/or VISUM and their associated scripting languages,Solid understanding and experience applying open-source GIS packages (e.g., OSGeo, QGIS, GeoPandas, etc.),Experience with software development tools such as GitHub, version control, issue tracking, testing, etc.,Understanding of statistical analysis/database software/packages (e.g., SQL, SPSS, scikit-learn, R, SAS),Demonstrated ability to manage projects successfully,Solid background, formal or informal, in urban planning theory,Spanish Proficiency or Fluency,100% employee-owned company with employee stock ownership plan,Award winning culture and workplace flexibility: https://www.whenworkworks.org/workplace-awards/rsg-2, https://reviews.greatplacetowork.com/rsg?,Competitive 401(k) with matching, medical with HSA, dental, vision, disability and other insurance coverage", "Build scalable backend data applications to support the growing needs of the business,Build data pipelines that collect, process, and compute business metrics from marketplace activity,Leverage best practices in continuous integration and delivery.,Collaborate with other engineers,Expertise in building data or training pipelines using Spark or Hadoop,Experience in building production grade API to expose results,Extensive programming experience in Python/Scala/Java,A BS or MS in Computer Science or other related technical fields,At least 3 years experience in production environment,Experience and interest in ML,Qualified applicants receive a few questions to answer,Selected candidates will be invited to schedule a 30 minute intro call with our CTO,Next, Candidates will be invited to schedule a behavioral interview with the CEO,Next, candidates will be invited to schedule a technical interview with our CTO. The technical interview can be split between a technical discussion and a technical test.,Next, candidates will be invited to review the technical test with our CTO and other software engineers.,Candidates will then be invited to schedule an additional interview with (CEO, CTO, COO),Successful candidates will subsequently be made an offer via email.", "Define key metrics to track Yelp\u2019s performance and inform product decisions,Assess and frame questions from internal partners and stakeholders into actionable deliverables,Design, execute, and analyze complex experiments impacting millions of users,Devise and evaluate models for diverse business needs such as identifying growth opportunities, personalizing user experience, and matching consumers to businesses,Own analyses start-to-finish and communicate key insights to stakeholders,Share your technical skills to develop and maintain high-quality, reusable analysis tools,BA/BS degree and a few years of experience as a data scientist, or MS/PhD and a few years of industry experience in a related quantitative role,Fluency with SQL and Python or R for data analysis,Solid understanding of statistical inference, experimental design, and analysis,Enthusiasm for clean code and sharing reproducible results,Communication skills to work with partners on engineering, product and business teams", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Design, build, and launch new data models for TV usage and associated online activity.,Apply your expertise in quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how users interact with our products.,Partner with Product and Engineering teams to solve problems, build products based on data, and identify trends and opportunities.,Mine massive amounts of data to extract actionable insights that inform, influence, support, and help execute our product decisions and product launches.,Design and implement reporting and metrics that track and monitor the performance of our products, the quality of our data, and overall health of the business.,Data Infrastructure,Work with SQL, Map Reduce, and no-SQL databases.,Design, build and launch new ETL processes.,Build data sets to empower operational and exploratory analysis.,Automate analyses.,Product Operations & Leadership,Design and evaluate experiments monitoring key product metrics, and understand root causes of changes in metrics.,Build and analyze dashboards and reports.,Influence product teams through presentation of findings.,Communicate the state of business, experiment results, etc. to product and management teams.,Exploratory Analysis & Statistical Modeling,Understand ecosystems, user behaviors, and long-term trends.,Identify levers to help move key metrics.,Evaluate and define metrics.,Build models of user behaviors for analysis and powering production systems.,Identify and correct for any biases or errors in our data sets.", "Create predictive analytics using the latest machine learning algorithms on relatively \u201cbig\u201d data (hundreds of features on > 100 million records),Provide analytic insights for partners throughout the lifecycle of their campaign(s); build, review, and recommend custom models; provide real-time visualizations and analysis of campaign work; and produce post-campaign analytics to help our partners constantly learn from their efforts,Work with Senior Data Engineer to help build and optimize our predictive analytics pipeline to perform well on large datasets stored in Amazon Web Services (AWS),Help develop and maintain Python analytics code base to build and automate custom analytic products,Analyze the performance of our production machine-learning models,Work closely with the data engineering team to analyze and augment our data,PhD in political science, sociology, mathematics, statistics or other related field or 4-5 years of experience building production level machine learning models on large datasets,Excellent general data science skills (data cleaning, munging, data visualization),Excellent Python skills (with additional experience in R desired),Experience with Amazon Web Services, in particular applying analytics in that setting,Experience identifying appropriate statistical techniques and applying them to a variety of real-world datasets,Strong problem-solving skills as well as the ability to manage several tasks/projects concurrently and prioritize work effectively,Experience with databases and knowledge of SQL preferred", "Identify critical business problems and create analytical/modeling solutions,Maintain the right balance between speed to market and analytical soundness when designing solutions,Translate complex findings and results into a compelling narrative,Define and evaluate key metrics and understand what moves them and why,Investigate challenging questions around user experience to understand the voice of our user,Ownership of conceptualizing, developing, and maintaining dashboards and visualizations,Communicate analyses and recommendations to cross functional stakeholders for decision making,Strategize and execute on making analyses easily repeatable and accessible", "Develop scalable solutions using state-of-the-art machine learning models,Draw meaningful insights from large datasets, define key metrics to track performance, and develop actionable solutions to tackle business problems,Use the Dow Jones terabyte-scale data lake to identify opportunities to enhance an understanding of the Dow Jones customer base and content consumption,Use visualization tools to present complex model and business insights in a simple, engaging manner for business stakeholders,Work closely with engineers to develop and deploy scalable machine learning applications,Identify opportunities for new data projects. Ask smart questions, diagnose the real problem, and develop a long-term solution", "Work with the data analytics team to define and implement machine learning solutions or advanced analytics use cases for Fortune 500 clients,Always curious about data, constantly asking questions about extracting key insights to understand the domain,Manage data received from multiple sources and cleanse data as needed,Integrate data from multiple sources and develop data storage solutions,Develop data analytics model for descriptive and predictive analytics using client provided data,Manage the data quality feedback loop so that all issues are corrected in a timely manner,Bachelor\u2019s or Master\u2019s Degree in Computer Science, Engineering, Mathematics, or a related field is preferred,At least 1 year Data Analytics or IT implementation project experience,Software experience with R and/or Python,Understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests \u2026etc.,Able to apply statistics skills, such as distributions, statistical testing, regression\u2026etc. into understanding key features of the data,Familiar with data cleaning/processing techniques via R or Python,Automotive IoT architecture and solution experience,An entrepreneurial mindset and has the ability to work independently or within a team,The ability to think critically and understand complex processes", "At least 4 years of experience with Data Analysis, Predictive Analytics, or Machine Learning,At least 2 year or experience with at least one programming language, such as Java or Python, etc.,At least 2 year of experience with at least one statistics/data analysis package, such as Python or R", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Lead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design, development, and deployment of AI solutions that drive business growth and better customer experience.,Contribute to the development of AI/ML strategy and roadmap for the company.,Contribute to research and development of AI/ML techniques and technology that fuels the business innovation and growth of Verizon.,Represent Verizon in AI/ML research and the industry through publications, conference speeches, collaboration with leading researchers and universities.,Build strong influence both among AI/ML community and with senior business leaders and actively promote the effective applications of AI/ML technology.,Lead engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or in a related field.,Ph.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.,Eight or more years of experience in practicing machine learning and data science in business.,Accomplished researcher and expert in machine learning, neural networks, reinforcement learning, chatbot technology, and NLP.,Strong communication and interpersonal influencing skills.,Excellent problem solving and critical thinking capabilities.,Experience in leading large scale data science projects and delivering from end to end.,Strong computing/programming skills; Proficient in Python, Spark, SQL, Linux shell script.,Strong experiencein Big Data and Cloud technology.", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Collaborate with scientists and researchers in one or more of the following areas: data intensive applications, text processing, graph analysis, machine learning, statistical learning, information visualization, low-level data management, data integration, data streaming, scientific data mining, data fusion, massive-scale knowledge fusion using semantic graphs, database technology, programming models for scalable parallel computing, application performance modeling and analysis, scalable tool development, novel architectures (e.g., FPGAs, GPUs and embedded systems), and HPC architecture simulation and evaluation.,Work with other LLNL scientists and application developers to bring research results to practical use in LLNL programs.,Assess the requirements for data sciences research from LLNL programs and external government sponsors.,Carry out development of data analysis algorithms to address program and sponsor data sciences requirements.,Engage other developers frequently to share relevant knowledge, opinions, and recommendations, working to fulfill deliverables as a team.,Contribute to technical solutions, participate as a member of a multidisciplinary team to analyze sponsor requirements and designs, and implement software and perform analyses to address these requirements.,Develop and integrate components-such as web-based user interfaces, access control mechanisms, and commercial indexing products-for creating an operational information and knowledge discovery system.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "You build and lead a great team of Data Scientists.,You create Machine Learning services & Statistics & Data mining.,You promote Machine Learning, its architecture and the incredible advantages it could bring.,You help bring Data Science at Coolblue to the next level by delivering high performing scalable data driven solutions.,By using your technical skills to closely collaborate with your team and take on the challenges of a rapidly growing e-commerce company.,By contributing to a solid data science infrastructure and selecting the right tools for the job.,By coaching and providing feedback to your team members.,By selecting and interviewing the best candidates for your team, together with our recruiters.,By working with the other leads on incorporating Data Science into the IT software development process.,A Master's degree (or PhD.) in Econometrics, Computer Science, Machine Learning, Mathematics, Statistics or another related field.,Relevant experience in a data-driven environment: leveraging analytics and large amounts of (streaming) data to drive significant business impact.,Expertise in Statistics & Data mining solutions and implementing Machine Learning services.,Prior experience in managing projects and seeing them through to completion (managerial experience is a huge plus).,Experience in writing and deploying robust, structured code within extended production environments (preferably Java, Scala, or Perl).,Experience in preparing scripts in languages like Python, R, Matlab, EViews, Julia, or Octave.,You lead the way with 'no ego, no fear & no politics' as core values to nurture autonomy within your data scientist teams.,You can\u2019t stop smiling when you tell us about one of your pet projects.,Money.,Travel allowance and a pension plan.,Plenty of space for creativity.,Over 30 training courses at our own Coolblue University.,25 leave days. As long as you'll promise to come back.,Discount on a new bicycle. Because that's how we roll.,Relocation assistance, from A to Z (if you live abroad).,An office at the best possible location. It's only a short stumble away from Rotterdam Central Station. Or a 2-minute walk.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Contribute to the development of the data model for the Business Intelligence tool in collaboration with the key users, the subsidiaries and the IT partner.,Support the data team in understanding and analysing the data base and process data coming from different data sources.,Work on consistency of the data coming from various sources.,Support the data team in extracting, combining and verifying the data needed for the Business Intelligence tool.,Support the data, IT and business teams in specifying and documenting all processes needed for the above two tasks.,You are in the 3rd or 4th year of your studies with focus on IT and Business Analyst,You have a good team spirit, perseverance and appreciate collaboration with different countries and cultures.,You have strong analytical and data manipulation skills.,You are interested in business intelligence applied to logistics.,Working language is English. Written and spoken English on a professional level is key; French and/or German conversation level is needed as well.,Skills: advanced excel \u2013 ETL (Pentaho) programming,Workplace in Paris or Bern", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Bogota,Boston,Buenos Aires,Chicago,Denver,Lima", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "We have an immature data infrastructure, but mature datasets and we\u2019d like someone to interrogate this data relentlessly.,We need you to create models from scratch ensuring sustainability to where the addition of new variables is as simple as updating a query; most of the dirty work is already done. Think Jupyter notebooks and meetings with internal business partners.,We need you to work directly with our product, building data scientific solutions that empower our retail stores and company as a whole.,We need you to build machine learning models and productionize them yourself (This is Key!).,These models should predict customer retention, analyze sales & employee data in real time (sales, demo's, actions); offer insight into how stores compare with each other (and what the variables are), improve sales, employee operational flows, and much more. The sky\u2019s the limit!!!,We hope the outcome of your contributions will offer true dashboards resulting from data consolidation from the variety of sources that we have driving the Retail vertical forward into the 21st century. From store to country level, customer to employee - we have data points that need to be dash-boarded from the minutia to the state-region-country level.,To accomplish our goals we need you to design and implement statistical experiments around business decisions & build pipelines to pull in new sources of data for your models.,1-4 years of Data Science experience in a large progressive and technology driven company.,Relative fluency with Python / Pandas / Scikit-Learn and a deep desire to improve. We value Data Scientists who can build models as well as put them into production.,Experience building predictive models,Strong understanding of probability, simulation, and statistical inference,Experience writing SQL,A git-based workflow,Experience working with Qualtrics is a plus,NLP and word2vec,Time Series Analysis,Data visualization / exploration tools (especially Looker),Data Flow Tools : Think Qualtrics,Columnar databases (especially Redshift),Document databases (especially Elasticsearch),AWS,Postgres,Apache Spark", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with and coordinate with Supervision team members, Model Risk Management, Compliance, Risk and Audit to build sophisticated models, determine proper empirical methodology, organize data collection, write unique programs, prepare written reports, and summarize the results of analytic studies in formal and informal presentations.,Design, develop, test, and implement advanced predictive models and risk analytics tools utilizing data from Supervision, Compliance, and other key partners across GWIM.,Work with Supervision leads to understand business data requirements and translate them into predictive models and analytical tools.,Conduct research to apply techniques in natural language processing, financial engineering methodologies, and applied mathematics to suggest process improvements and risk mitigation analytics where applicable.,Develop state-of-the-art software tools to collect, and analyze large volumes of structured and/or unstructured data to streamline business processes and improvements and enhance the quality and efficiency of supervision and oversight", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Design, build, and launch new data models for TV usage and associated online activity.,Apply your expertise in quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how users interact with our products.,Partner with Product and Engineering teams to solve problems, build products based on data, and identify trends and opportunities.,Mine massive amounts of data to extract actionable insights that inform, influence, support, and help execute our product decisions and product launches.,Design and implement reporting and metrics that track and monitor the performance of our products, the quality of our data, and overall health of the business.,Data Infrastructure,Work with SQL, Map Reduce, and no-SQL databases.,Design, build and launch new ETL processes.,Build data sets to empower operational and exploratory analysis.,Automate analyses.,Product Operations & Leadership,Design and evaluate experiments monitoring key product metrics, and understand root causes of changes in metrics.,Build and analyze dashboards and reports.,Influence product teams through presentation of findings.,Communicate the state of business, experiment results, etc. to product and management teams.,Exploratory Analysis & Statistical Modeling,Understand ecosystems, user behaviors, and long-term trends.,Identify levers to help move key metrics.,Evaluate and define metrics.,Build models of user behaviors for analysis and powering production systems.,Identify and correct for any biases or errors in our data sets.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Interviewing key project stakeholders, documenting findings and making detailed recommendations,Assess and frame the business problem, success criteria, and risks,Acquire and blend disparate data sources, which could be any format of internal structured, unstructured, external, or high volume sources,Exploratory data mining & analysis,Guide clients through processes to identify meaningful predictors and then engineer representative features into the data set,Understand best practices on core set predictive use cases & be familiar with model libraries", "Lead and guide data engineers in implementing our methodologies on different platforms, such as R, Spark, H2O, tensorflow, while evaluating models and making trade-off decisions on the best solutions.,Creative use of existing data, as well frequent integration of new types of data, for feature engineering that will results in more predictive, accurate, and insightful models.,Deep dive into the internal mechanics of machine learning algorithms, suggesting modifications to our current implementation and pipeline.,Lead and guide data engineers in comparing the performance of different machine learning methods, such as kernel methods, XGBoost, regularized regression, and deep nets.,Develop causal models.,Develop and implement practical approaches to deal with real life data challenges, such as incomplete data, high signal to noise ratio.,Accelerate research-to-production cycle by developing new and novel experimental frameworks and metrics.,Integrate learning-to-rank techniques and collaborative filtering methods.,Ability to write clean and concise code, especially in R or Python.,Solid understanding of statistics.,Keen eye for detail and thoughtful investigation of data before relying upon it.,Intuition for data science best practices, stemming from proven experience.,Steadfast focus on creating impactful change and ability to prioritize between many tasks to maximize the improvement of the business.,5+ years industry experience and/or Ph.D. in a quantitative field.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, develop, operate, and extend advanced marketing attribution models with Machine Learning algorithms to be deployed in production,Collaborate closely with client services and engineering to deliver insights to clients on time,Define and implement features to our marketing analytics products in Python,Degree in Computer Science, Statistics, Mathematics, Physics, or Engineering,Experience in predictive modeling with Machine Learning algorithms,Understanding of probability, statistics, and linear algebra,Solid coding skills in Python,Excellent teamwork and communication ability", "Retrieve, prepare, and process a rich variety of data sources such as social media, CRM data, paid media data, sales data and account data,Create attribution models that quantify return on investment in marketing spend and channel selection and correlate closely to actual sales and/or campaign performance,Mine data to generate new insights around channel, audience & account performance,Partner with Brand, Consumer Planning and Sales teams to develop hypotheses and run experiments, build forecasts and report on relevant performance variables,Collaborate with Data Architect, Growth Manager and IT teams to create data lakes and sustainable data infrastructure,Consult other teams in their exploration of Data Science for choices around pricing, distribution, A&P resourcing, etc.,Extend our first party data with second and third party data when needed,Enhance data collection procedures to include information that is relevant for building analytic systems,Process, cleanse, and verify the integrity of data used for analysis,Understanding of marketing channels including digital media, social media, offline media, CRM and trade media,Ability to translate project objectives into a project plan with milestones, and resource/technology requirements, and teach, lead, and manage projects to successful execution,Experience with tools such as R, SQL, Spark, Python, Hadoop, RedShift, Matlab, Tableau, Oracle and various DMPs for data analysis, warehousing and visualization,Ability to communicate complex concepts in simple, jargon-free English,Familiarity with machine learning techniques (preferred),3+ years of Data Science experience (working as a Data Scientist, Statistician, Predictive Analyst or related).,Masters degree in Applied Mathematics, Economics or Computer Science,Experience in attribution modeling with large data sets source from disparate sources (e.g. Data Warehouses, 3rd Party APIs, etc.)", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Degree CS, Applied Math, Statistics, or other scientific field. Graduate degree a plus,Experience building models on real world problems,Software development experience in python,Familiarity with hardware / robotics,Understanding of supply chains,Previous experience at a start-up,100% company-paid Medical, Dental, and Vision for you and 75% for your dependents,Ownership via Stock Options,Flexible Time Off,Daily catered lunch,Free and discounted pizza!,The opportunity to work with an incredibly supportive team of thinkers and innovators", "Georgia", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, develop, and lead the implementation of AI solutions in application projects for Microsoft\u2019s products and services.,Select and apply appropriate statistical and machine learning to large-scale, high-dimensional data.,Stay current with the latest research and technology and communicate your knowledge throughout the enterprise.,Take responsibility for preparing data for analysis, review data preparation/ETL code and provide critical feedback on issues of data integrity.,Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.,Patent and publish relevant IP and scientific research.,Research, develop, and implement AI solutions in application projects for Microsoft\u2019s products and services.,Select and apply appropriate statistical and machine learning to large-scale, high-dimensional data.,Integrate AI solutions into the overall product or service.,Stay current with the latest research and technology and communicate your knowledge throughout the enterprise.,Take responsibility for preparing data for analysis, review data preparation/ETL code and provide critical feedback on issues of data integrity.,Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.,Lead the architecture design and the implementation of AI solutions in application projects for Microsoft\u2019s products and services.,Test and review all new and modified code and data pipelines.,Build out new API infrastructure and endpoints, data pipelines, as necessary.,Build reporting and monitoring mechanisms into our solutions.,Analyze technology industry and market trends and choose their potential impact on the solutions.,Develop patterns, standards and guidelines necessary to uphold our design principles and maintain integrity of the product architecture.,Participate in key project design reviews.,Patent and publish relevant IP and scientific research.,Collaborate with PM, development, and marketing teams across Microsoft to translate business needs into technical solutions.,Create and manage end-to-end project plans.,Ensure timely and high-quality of delivery.,Provide hands-on program management during analysis, design, development, testing, implementation, and post implementation phase.,Perform risk planning and management in projects.,Provide day-to-day coordination in our Scrum teams.,Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.,Evangelize group work with stakeholders.", "Previous message: [Jobs] Fwd: [inedinfo] Fwd: JOB: Statistician 0.6 fte,Next message: [Jobs] NPD Group", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop models that drive personalization of digital experiences for Nike consumers \u2013 e.g. suggesting products with an aesthetic match to products a consumer has shown interest in through computer vision, consumer behavior patterns, or some new approach you devise.,Develop models that help us understand and describe our customers \u2013 e.g. learning how to extract deep interests and tendencies from event streams.,Develop production code for both batch and real time models in collaboration with our data and service engineering teams.,Manage an experimentation portfolio that validates and feeds back into our core customer understandings.,Build processes to support fast, iterative experimentation, both for model creation and for customer-facing products.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "You will design and develop processes and systems to consolidate and analyze unstructured, diverse data sources to generate actionable insights,You will work with product and service teams to identify questions and issues for data analysis and experimentation,You will use machine learning to build models that can address areas like user segmentation, churn, affinity, and conversion rates,You will develop software programs, algorithms, and automated processes to cleanse, integrate, and evaluate large datasets from multiple disparate sources,MS or PhD program in Computer Science, Software Engineering, Math, Statistics or related field,Understanding of applied math topics, such as probability and statistics, linear algebra, basic optimization techniques, etc.,You're an expert in SQL and Excel with experience in data warehouse technologies, such as Teradata,You have hands-on experience with predictive analytics tools and machine learning techniques/algorithms,Hands on experience with tools such as R, Python and familiarity with Teradata/Hadoop environment,Experience in designing and analyzing multivariate tests,Experience with data visualization tools such as Tableau, Splunk, or Kibana a plus", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Independently lead data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design and development of machine learning/statistical models and ensure best performance.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Assist engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Be a subject matter expert on machine learning and predictive modeling.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or relevant.,A Ph.D. in statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science or a relevant field.,Five or more years of experience in practicing machine learning and data science in business.,Strong foundational quantitative knowledge and skills; extensive training in math, statistics, physical science, engineering, or other relevant fields.,Experience in leading data science projects and delivering from end to end.,Strong technical experience in machine learning and statistical modeling.,Strong on computing/programming skills; proficiency in Python, R, and Linux shell script.,Experience in data management and data analysis in relational database and in Hadoop.,Strong communication and interpersonal skills.,Excellent problem solving and critical thinking capabilities.,Experience with NLP and chatbot technology.,Experience with Hadoop, Spark, C++, scala, or Java.", "Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions,Generate reports and analysis on key product metrics,Develop custom data models and algorithms to apply to data sets,Help identify and assess metrics and KPIs that can be tracked to measure impacts and business outcomes,Excellent problem solving and communication skills,Strong statistics and mathematical fundamentals,Familiar with reporting and analytics platforms and tools such as Looker, Google Analytics, and Excel,B.S. in Computer Science, Statistics, Mathematics or similar field,Ability to lift up to 50 pounds,Ability to stand for extended periods,Ability to work on a computer in front of a display monitor for extended periods,Ability to consistently operate a computer and other office productivity machinery, including but not limited to a calculator, copy machine, and computer printer,Occasional night and weekend work are a natural part of this position", "at least 4 years experience,Extensive experience with machine learning and statistical modeling are imperative; text mining and topic modeling are a plus,The team is heavily investing in modern machine learning methods and technologies, so experience with Deep Learning, Neural Networks and corresponding technologies such as Tensorflow, Caffe, PyTorch, Scikit Learn, Numpy etc. is a plus,Python (preferred), R or other languages is a must,Typical data sources are truly \"big\", and thus working knowledge of Big Data technologies & NoSQL databases in addition to traditional Relational Databases is a must. Experience using streaming data processing techniques is a plus,Bachelor's degree from four-year college or university; four years related experience; or equivalent combination of education and experience", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop a thorough understanding of\nbusiness problems through interviewing stakeholders and subject matter experts,Derive what is critical and what is extraneous to solving the business problem. Understand the data requirements for formulating the solution,Research new data sets and determine their quality and feasibility,Manipulate data to support various statistical techniques,Build machine learning models appropriate for each business problem,Collaborate with data engineers to ensure solutions are successfully operationalized,Effective story telling with data. Interpret and communicate analytic results to analytical and non-analytical business partners and executive decision makers,Passionate about machine learning. A thought leader who researches latest tools, techniques, and trends and looks for innovative approaches,Always learning \u2013 about data science, industry, market, business, etc.,Critical thinking skills. Ability to assess a situation from multiple points of view,High personal and professional standards; unassailable integrity and ethics,High-energy and self-starter personality.,Strong structured coding skills.,Team Oriented- able to train, develop, and mentor other data scientists.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Engage broadly with the business to frame, structure and prioritize business problems where analytic projects or tools can have the biggest impact on Nordstrom\u2019s business,Perform large-scale statistical research, analysis, and modeling in the areas of web analytics, supply chain optimization, and forecasting,Communicate insights and recommend areas for further data discovery,Master\u2019s degree in Computer Science, Mathematics, Statistics, or equivalent education and experience,1+ year of corporate experience in Data Science and Analytics,3+ years of corporate experience using SQL in a variety of RDMS environments,Scripting skills in Bash and at least one analytic programming language (e.g. R or Python),Experience working with NoSQL data environments and tools such as Hadoop, Spark, DynamoDB,Fluency with statistical and machine learning algorithms such as decision trees, neural networks, collaborative filtering, clustering, survival analysis, graph theory, etc.,Commuter Benefits,100% Paid Parental Leave,Charitable Giving and Volunteer Match,Merchandise Discount", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collaborate with data scientists and engineers to design, develop and operationalize machine-learning based decision making systems,Solve complex problems and propose solutions in collaboration with the data science team,Keep abreast with developments in machine learning, data science, game monetization, by attending meetings, conferences and reading current publications and media,Develop and implement machine learning models and pipelines with Python, Spark ,TensorFlow/Keras and PyTorch", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "BA/BS in Data Science, Machine Learning, Statistics, or related STEM field,Proficiency in Python, R, SQL or another programming language,Experience or coursework in machine learning and/or natural language processing,Statistical modeling through software (e.g. SPSS) or programming language (e.g. Python),Understanding of database and analytical technologies in the industry,Demonstrated ability to think strategically about business, product and technical challenges in an enterprise environment,Excellent oral and written communication skills,Ability to collaborate in a team environment,MS in Data Science, Machine Learning, Statistics, or related STEM field,Quantitative data analysis,Experience and/or coursework in Deep Learning,Advanced MS Excel skills (e.g. VLOOKUP, Pivot tables),Professional experience with Spark and/or TensorFlow,Hands-on experience deploying analytical models to solve business problems,Ability to develop experimental and analytical plans for data modeling processes", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "A top shopping app on iTunes and Google Play,Geekwire App of Year,50+ Million Downloads,Train machine learning/deep learning models to solve problems in our products,Collaborate with stakeholders across the company to make data-driven algorithms a core part of our marketplace,Make individual technical contributions where possible,Develop end-to-end machine learning solutions: offline training, production implementation, A/B testing, and communication of results", "Work on every aspect of modeling and statistical problems: data wrangling, feature engineering, target definition, machine learning models (specification, estimation, and comparison), performance evaluation, policy development, implementation and monitoring,Dive into quantitative problems in every part of the organization, including credit, growth, engineering, and operations, to drive end-to-end implementation and business outcomes,Extract actionable insights by analyzing complex, multi-dimensional data sets and interpreting their implications for our business,Provide advice and education in the usage and interpretation of solutions to end users, including on-boarding and training new team members,Deep understanding of statistical methods and machine learning,Proficiency in Python, scikit-learn, Pandas, NumPy, R, or similar,Experience working with Git,Experience with SQL on structured data and tools for dealing with unstructured data,A good understanding of software engineering: you can write unit tests, refactor code, and use appropriate design patterns,Expertise in working with data: cleaning data, building datasets, integrating with machine learning models, performing statistical analysis, data visualization,An analytical mindset,A detail-oriented nature,The ability to manage projects across multiple stakeholders and prioritize vigilantly,Superb communication and collaboration skills,A proven track record in solving business problems through fact-based methods,The ability to persuade, inspire, and motivate others,An ability to summarize key insights for both technical and non-technical audiences,Experience working with large volumes of data,Experience with credit bureau and/or financial services data", "supervised learning methods for classification such as binary and multinomial Logit, Na\u00efve Bayes, and support vector machines.,unsupervised learning methods for clustering such as Gaussian mixtures and K-Means.,linear and nonlinear regression modeling.", "Analyze client proprietary and public domain datasets leveraging advanced analytics and statistics to work on scheduling and network optimization, macroeconomic forecasting, pricing, and routing,Source, clean, and process a diverse set of data streams,Identify and understand business needs and contextualize results of the analysis to drive impact for the organization as a whole,Dynamically collaborate with operations and product teams to refine project goals, scope, and timelines,Provide consulting services on both internal and external analytic questions,Bachelor\u2019s degree in mathematics/statistics, engineering, computer science, or other technical disciplines,Expert knowledge in a mathematical programming language of choice (preferred: R, Python, Matlab),Experience with ETL processes for manipulating data sets,Detail-oriented, proactive problem-solving skills,Ability to communicate complex findings in a clear, precise, and actionable manner. Able to translate high-level ideas into well-defined problems,2+ years work experience (non-academic),Experience with API integration,Corporate headquarters in sunny Austin, TX,Incredible growth opportunity at one of the fastest growing companies in Austin,The kitchen is fully stocked with local coffee, drinks, and snacks,Company sponsored healthcare insurance and dental insurance,Team building events and office competitions,Partial cell phone reimbursement,Receive an elite technology package to include a brand new MacBook Pro!,Competitive salary,Opportunity to put your thumbprint on a high-growth company in the early stages", "A successful candidate will have hands-on experience with graph database and web applications and the desire to work in a high performing and faced-paced team. Excellent analytical and problem-solving skills are a must as well as a track record of taking complex concepts and implementing practical solutions.,Lead the design, development, testing, and deployment of a graph database solution.,Design and develop graph data models in accordance with leading practices for our use cases.,Work with system owners and data engineers to identify and integrate with data sources used to hydrate the knowledge graph.,Create and manage queries to pull insights from the knowledge graph.,Optimize for high performance, efficiency, scalability, and stability of the database.,Define the database architecture and development best practices.,Assume the role of a trusted-advisor, offering technical insights to the team and business stakeholders.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop data assets using statistics/machine learning packages & Microsoft BI platform to solve real-world ecommerce, CRM and inventory offer management problems,Collaborate with IT teams to integrate the data assets with business applications,Develop analytical insights through complex data analysis and financial modeling to identify the product improvement opportunities and support subsequent business case development,Perform A/B test design, implementation and analysis on the web/mobile site in order to determine the effectiveness of efforts,Stay up to date with the latest data science technologies and build business case to demonstrate the benefits of onboarding the respective technology or techniques,5 years of professional experience in data scientist role especially in ecommerce, CRM and inventory management,Excellent written and oral communication skills, including an ability to communicate across business areas,Ability to conceptualize business problems and solving them through data analysis,Expert data visualization, presentation skills and ability to present analysis to product owners,Expert SQL/data manipulation skills required including cleaning and managing data.,Experience with statistics/machine learning packages such as R, SAS etc.,Degree qualified in quantitative field such as Computer Science, Mathematics, Statistics, Machine Learning / AI,Willingness to work with unstructured, messy data.", "Working in an interdisciplinary field, together with computer scientists, business spocs, and telecom network engineers.,Integrating multiple data sources, models, and software tools with business line specific decision support and data analysis. This will include the use of distributed computing and utilization of both cloud internal computing resources.,Developing new automation methods to analyze and evaluate business strategies across various business geographies, technologies, drivers and scales.,Modeling complex systems by integrating large varied datasets of economic, demographic and telecom related information,Creating and walking through executive presentations that explain the complex data and algorithms used in simple easy-to-understand visually striking layman terms that leave a residual impact on an executive audience.,Solving complex geospatial problems utilizing robust and repeatable solutions.,Keeping current on latest computing trends and analysis methodologies through tech journals and ongoing research.,Work will involve challenging existing constructs and business paradigms and creatively solving problems that result in a business transformation of a process or a program or a current approach,Bachelor\u2019s degree or four or more years of work experience.,Six or more years of relevant work experience.,Experience using Python in automation of analytics tasks.,Experience using / development of Web Front Ends (JavaScript / html / nginx / php).,Experience in management of data in SQL and/or NoSQL databases (Postgres, Oracle, Teradata, Hadoop, Casandra, MongoDB).,Leadership experience in one or more areas of team, task or project lead responsibilities.,Experience in project management.,Masters of Science degree in Geo Information Systems.,Experience with parallel computing in distributed environments and familiarity with GIS and spatial databases (e.g., Postgres / PostGIS, Oracle/ESRI sde).,Demonstrated complete understanding and wide application of technical procedures, principles, theories and concepts in the Telecommunications field. General knowledge of other related disciplines.,Good interpersonal and communication skills. Ability to convey complex information to upper level executives.,Ability to adapt to quickly changing business environment and requests that come in related to upper level executive requests.,Programming experience in one or more programming languages (e.g., Python, JavaScript).,Experience in analysis of large spatial and non-spatial datasets.,Experience with a multitude of databases in general (Oracle, Postgres).,Knowledge of R, SPSS, or SAS for statistical analysis.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Builds best-in-class predictive and prescriptive models taking into account business constraints.,Performs data integration, data set cleansing, analysis, and deploys analytical models through the development of scripts and custom workflows.,Monitors the performance and accuracy of existing models.,Advises IT Data Engineers to build, automate, and/or maintain a set of enterprise-wide data services (data lake) and connectors for internal and external data sources that can be rapidly deployed to business analysts.,Creates proactive and reactive custom reports based on client needs,Troubleshoots issues, prepares summaries, documents processes and workflows, proposes solutions, and collaborates and communicates with internal team members.,Builds and maintains trusted relationships with information managers, architects, and business units for knowledge sharing, mentoring, and training", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Working directly with clients to develop analytic questions and define data and technology opportunities,Driving the exploration of data sources and analytic techniques to create new variables and modeling capabilities,Working with large and complex data sets to solve difficult and non-routine problems,Applying a variety of statistical methods/approaches to develop analytic models", "Build and deploy predictive models to enable smart decision making,Manage the production scoring process with support from IT,Provide technical and analytical support to marketing programs and consumer research,Serve as department expert on data access and manipulation of large and complex structured and unstructured data,Support projects that advance L.L.Bean\u2019s data infrastructure and environment both on-premise and cloud,Investigate and onboard new data sources to provide valuable customer insights,Help build advanced analytical capabilities through an awareness of industry best practices", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with stakeholders throughout the organization (Operations/Supply Chain, Sales/Marketing, Finance, etc.) to identify opportunities for leveraging company data to drive business solutions,Develop scalable statistical, machine learning or mathematical solutions for complex business problems such as Pricing, Inventory Reordering, and Demand Forecasting,Perform analytical studies and generate actionable recommendations. Use statistical techniques and hypothesis testing to validate your findings", "Operations & Maintenance (O&M) Data Science Consultant key to leading requirements/design with new client, determining the optimum data reporting and visualizations to assist their business objectives.,Work in collaboration with a team of 4-6 data team members through SDLC: development, requirements/design, user acceptance testing, and delivery.,Works under general direction. Develops, implements, and maintains complex business, accounting, and management information systems.,Translates designs into computer software programs and code.,Tests, debugs and refines the computer software to produce the required product.,Prepares required documentation, including those required by the relevant SDLC process.,Enhances software to reduce operating time or improve efficiency.,Works with Business Analysts and users to define existing or new system scope and objectives.,Performs modifications to and maintenance of operational programs and procedures.", "Cash Vouchers for the top 100 winners in each contest,Top 100 participants will get Certificates of Merit", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collaborate with scientists and researchers in one or more of the following areas: data intensive applications, text processing, graph analysis, machine learning, statistical learning, information visualization, low-level data management, data integration, data streaming, scientific data mining, data fusion, massive-scale knowledge fusion using semantic graphs, database technology, programming models for scalable parallel computing, application performance modeling and analysis, scalable tool development, novel architectures (e.g., FPGAs, GPUs and embedded systems), and HPC architecture simulation and evaluation.,Work with other LLNL scientists and application developers to bring research results to practical use in LLNL programs.,Assess the requirements for data sciences research from LLNL programs and external government sponsors.,Carry out the development of data analysis algorithms to address program and sponsor data sciences requirements.,Engage with other developers frequently to share relevant knowledge, opinions, and recommendations, working to fulfill deliverables as a team.,Design technical solutions independently, participating as a member of a multidisciplinary team to analyze sponsor requirements and designs, and implementing software and performing analyses to address these requirements.,Develop and integrate components-such as web-based user interfaces, accessing control mechanisms, and commercial indexing products-for creating an operational information and knowledge discovery system.,Lead multiple parallel tasks and priorities of customers and partners to ensure complex deadlines are met.,Manage various complex projects, using team members\u2019 skills to complete complex projects/tasks, and solve abstract complex problems/ideas and convert them into useable algorithms/software modules.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Verifiable experience working with health-data from EMR (preferably Epic), Clinical databases and clinical or enterprise data warehouses.,Verifiable experience working with and optimizing SPARQL queries and and graph (RDF) based database (triple stores),Verifiable experience working with R and Python analytic languages,Formal Healthcare Economics education/degree,Experience in clinical informatics, clinical terminologies (ICD9-10, LOINC, CPT, etc).,Excellent interpersonal and writing skills to communicate and document user interactions and requirements,Strong analytical skills to analyze, categorize and translate business/user requirements to functional requirements,Excellent presentation skills for demo and user training, writing skills for user documentations", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Leverage data to perform intensive analysis across all areas of our business to drive product development,Design experiments and interpret the results to draw detailed and actionable conclusions,Generate ideas for exploratory analysis to shape future projects and provide recommendations for actions,Perform time-series analyses, hypothesis testing, and causal analyses to statistically assess relative impact and extract trends,Build models to enhance understanding of user behavior and predict future performance of cohorts,Create dashboards and reports to regularly communicate results and monitor key metrics,Present findings to senior management to strengthen business decisions,Minimum 3 years of experience in a quantitative analysis role,MS or PhD in Math, Economics, Statistics, Engineering, Computer Science, or other quantitative field (advanced degrees are a plus),SQL skills and the ability to use tools such as Python, R to work efficiently at scale with large data sets,Advanced knowledge of experimentation and statistical methods,Experience in modeling and machine learning,Working knowledge of big data technology,Work closely with cross-functional teams to execute on decisions,Self-driven with the ability to work in a self-guided manner", "Building efficient machine learning models to solve or automate various business problems.,Developing strong relationships with business teams and product owners.,Extracting, cleansing, mining, analyzing and visualizing healthcare data.,Clear concise presentation of findings/recommendations to business teams and senior leadership.,Participating and assisting in business conversations to identify potential use cases and business opportunities for the application of data science and data analysis.", "Pursuing a B.S., M.S., or Ph.D. in a scientific or quantitative field,Excellent statistical intuition and knowledge of various analytical approaches,Curiosity and passion for Quora,Superb communication skills and ability to explain your analysis clearly,Proficiency in SQL,Familiarity with Python or similar scripting language,Analyze external traffic patterns over time to better understand how changes in outside products cause increases or decreases in visits to Quora,Study the usage differences between the English and Spanish Quora products to understand how various mechanics of Quora translate across different languages,Develop methodology and prototype features in our internal experimentation platform to identify and capture novelty effects in experiments,Investigate the effect of how \u201csatisfying\u201d a question page is on user engagement and clickthrough patterns through the rest of the product,Understand effects of positional bias on home feed ranking to identify opportunities to improve ranking and feed engagement,Improve the accuracy of a user engagement prediction model and understand what factors are important in determining new users' long term usage.,Build a recommender system to suggest Quora topics to a user based on provided interests,Evaluate statistical properties of various methods used to determine significance in ratio metrics, considering coverage, power, and feasibility", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Reach out and talk to data scientists in the area you are located in (using LinkedIn or other networking platforms)", "This position is to be used strictly for the conversion of current IBM Data Science interns. Please do not apply to this role if you are not a current IBM intern.,You will implement and validate predictive models, and create and maintain statistical models with a focus on big data.,You\u2019ll communicate with internal and external clients to understand business needs and provide analytical solutions.,You will use statistical concepts such as regression, time series, mixed model, Bayesian methods, clustering, etc., to analyze data and provide insights.,You are great at solving problems; debugging; troubleshooting; and designing & implementing solutions to complex technical issues.,You thrive on teamwork and have excellent verbal and written communication skills.,You have strong technical and analytical abilities, a knack for driving impact and growth, and some experience with programming/scripting in a language such as Java or Python.,You have a basic understanding of statistical programming in a language such as R, SAS, or Python.,You have an interest in, understanding of, or experience with Design Thinking Methodology", "Collaborate with the clinical team to understand the clinical scenario,Perform pre-processing and validation on new and existing data sources,Design and develop models to address customer and/or business needs,Communicate results and create data visualizations,Work closely with the development team to select technologies with implementation in mind,Document the model development process,Contribute to scientific conference research submissions,Stay up-to-date on emerging data science tools and methods", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Begin developing relationships one level up and two levels down. Identify and support opportunities to expand project scope beyond current deliverable and actively share information about clients\u2019 emerging support needs and trends with team members and management.", "Identifying and flagging potential data sources in support of client needs,Sourcing, restructuring and cleaning of data to make them amenable to analysis,Supporting case teams with their analytics work in either a \u201cconsultancy/expert\u201d mode, or as part of active case work,Train and coach L.E.K. staff on the use of our software and proposed statistical and machine learning methods,Develop proprietary data solutions by matching and merging of proprietary as well as public data sources,Contribute to commercial proposals, as required by Managing Directors", "Use Machine Learning and AI to model complex problems, discover insights, and identify opportunities.,Integrate and prepare large, varied datasets; architect specialized database and computing environments; and communicate results.,Develop experimental design approaches to validate finding or test hypotheses.,Solve client analytics\u2019 problems and communicates results and methodologies in an understandable manner.,Research new approaches/methods to improve, optimize, and test targeted questions.,Work closely with business analysts to gain an understanding of client business and problems.,Additional duties as assigned to ensure client and company success.,M.S., or PhD in a quantitative discipline: computer science, statistics, operations research, applied mathematics, engineering, mathematics or related quantitative fields.,Proficient in programming environment and languages such as: Node.js, Python, R, Javascript, SQL, and deep knowledge of analytics packages available for above languages.,Prior research or development experience working with data, solving problems with data, and experience building advanced analytic models.,Strong working knowledge of machine learning and statistics.,Ability to communicate your ideas (verbal and written) so that team members and clients can understand them.,Ability to defend your professional decisions and organize proof that your ideas and processes are correct.,Resourceful in getting things done, self-starter, productive working independently or collaboratively\u2014ours is a fast-paced entrepreneurial environment with performance expectations and deadlines.,Inquisitiveness and an eagerness to learn new technologies and apply concepts to real-world problems.,Share our values: growth, relationships, integrity, and passion.,Experience with various BI tools.", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Independently lead data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design and development of machine learning/statistical models and ensure best performance.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Assist engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Be a subject matter expert on machine learning and predictive modeling.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or relevant.,A Ph.D. in statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science or a relevant field.,Five or more years of experience in practicing machine learning and data science in business.,Strong foundational quantitative knowledge and skills; extensive training in math, statistics, physical science, engineering, or other relevant fields.,Experience in leading data science projects and delivering from end to end.,Strong technical experience in machine learning and statistical modeling.,Strong on computing/programming skills; proficiency in Python, R, and Linux shell script.,Experience in data management and data analysis in relational database and in Hadoop.,Strong communication and interpersonal skills.,Excellent problem solving and critical thinking capabilities.,Experience with NLP and chatbot technology.,Experience with Hadoop, Spark, C++, scala, or Java.", "\u2026 train and deploy new ML models to structure new facts from news documents,\u2026 scale clustering algorithms to work on millions of documents at once,\u2026 build summarization algorithms that work on document types we\u2019ve never seen before,\u2026 discover patterns of disinformation across news and social media, and propose product features that make it easier to uncover,\u2026 bring our algorithms to many different languages,Curiosity and enthusiasm, and a love for teaching and learning,Strong programming skills, including in Python, with at least 3 years experience in a production engineering team,Masters/Ph.D. in a quantitative field or at least 5 years building analytical/data driven products in an industry setting,Skills in data exploration, visualization, and cleaning,Experience applying machine learning algorithms to real data sets, preferably including text analysis and NLP,Experience in using one or more of the following: NumPy, SciPy, Scikit-Learn, NLTK, SpaCy, TensorFlow, Keras, PyTorch,Experience taking ambiguous problem statements through to delivered products,Experience interacting with end users/clients or strong interest in doing so,Experience with ElasticSearch and Postgres,Foreign language proficiency / fluency,Rapidly growing company, with opportunities for growth into leadership roles,Health, Dental, Vision Benefits,Unlimited Paid Time Off,Smart, engaged co-workers who are at the top of their game,Honest and open environment for exchange of ideas,Real customers with global name recognition + healthy sales pipeline,Proactive learning and teaching opportunities via individual book allowances, tech talks, and brown bag lunches,Twice weekly catered lunches,Team outings and bi-weekly company happy hours", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Lead data science projects independently while collaborating with team members and multi-organizational stakeholders.,Perform hands-on advanced analysis in marketing science and business analytics on big data to promote growth and business development,Leverage state-of-the-art data processing tools and/or analytical methodologies to drive improved decisions", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Gather insights about customer behavior and the ways they use energy from quantitative and qualitative research in order to better inform our product solutions.,Use our wealth of data sources, such as energy usage, marketing data, and premise data to personalize the customer experience.,Create an approach to target customer messaging so that we can provide customers with accurate and timely energy saving actions.,Use the latest behavioral science techniques to engage customers in their energy usage and keep them engaged.,Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.,Develop company A/B testing framework and test model quality.,Develop processes and tools to monitor and analyze model performance and data accuracy.,Extract and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies.,Create models to help us measure the overall energy savings impact of our products.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "50% - Establish scalable, efficient processes for large scale data analyses, model development and model implementation,30% - Design and develop algorithms and models to use against large datasets to create business insights,20%-Present analysis and resulting recommendations to senior management; Leverage data to present a compelling business case to optimize investments and operations,This position reports to Sr Manager, Data Science Online,This position has no Direct Reports,Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.,Typically requires overnight travel less than 10% of the time.", "Work with data analysis team members across the enterprise to accomplish the verification and validation of the Joint Simulation Environment.,Write tools to assist in mining data from flight tests and simulation runs.,Write tools to assist in comparative analysis of flight tests and simulation runs.,Assess the effectiveness and accuracy of new data sources and data gathering techniques.,Strong problem-solving skills with an emphasis on producing data to drive decisions.,Experience using statistical computer languages (R, Python, MATLAB, or similar etc.) to manipulate data and draw insights from medium and large data sets.,Experience working with and creating data architectures.,Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.,Excellent written and verbal communication skills for coordinating across teams.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Conduct analyses to learn from our vast amount of data,Apply statistical techniques to model suspicious user behavior, identify mechanisms for on-platform manipulation, and size mitigation opportunities.,Write complex data flows using SQL, Spark, Scalding, R and Python scripts.,Capable of operation at senior level or above as a Data Scientist or ML Engineer.,You have 3 plus years of industry or graduate level research experience working on political motivated manipulation, or relevant security issues.,You are a self-starter who is capable of learning on the job, takes initiative, and can thrive within a large team. You can pivot from blockers and develop a new approach when there are no precedents.,You form sound hypotheses for largely unknown problems (through a combination of product domain knowledge and logical reasoning) and quickly iterate on data exploration.", "Familiarity with agile project methodologies and LEAN project methodologies,Deep knowledge of experimental design, with emphasis on A/B testing in a digital environment.,Strong proficiency in programming languages, such as Python, R, and SAS. Familiarity with Tensorflow, scikit learn, pytorch,Fluency in SQL for data access, manipulation, and validation", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "B.S., M.S., Ph.D. in a scientific or quantitative field,4+ years work experience,Excellent statistical intuition and knowledge of various analytical approaches,Curiosity and passion for Quora,Superb communication skills and ability to explain your analysis clearly,Proficiency in SQL,Familiarity with Python or similar scripting language,Passion for learning and always improving yourself and the team around you", "Help frame the issue, establish a framework to solve it by breaking it down in sub-issues,Establish early hypothesis,Design analysis to test hypothesis and answer the key questions,Develop insights, methods or tools using various analytic methods such as predictive modeling, regressions, machine learning, time series analysis, simulations, etc.,Handle large amounts of data from multiple and disparate sources,Analyze the data and the results to ensure data quality, statistical relevance, etc. with a critical mindset,Communicate in a simple, conclusive manner, focusing on the key messages,Always tie back the analysis to the business issue. Focus on the \u2018so-what\u2019 and how to make the results actionable,Ensure buy-in from key stakeholders and clients,Carry the work in a responsible and autonomous manner.,Make sure to leverage adequately all the resources available (incl. managers, colleagues, etc.),Own stakeholder relationship and help shape their agenda, ensure constant alignment on priorities,Bring and build expertise in advanced analytics, data management, data visualization/reporting, and the associated tools (R, SAS, Tableau, Alteryx, Python, etc.),Align with data owners and department managers, contribute to the development of data models and architecture for analytics,Analyze and explore data improvement opportunities (existing and new sources),PhD or MSc. in a scientific field,5 years (8 if MSc.) of advanced analytics experience with significant exposure and appetite to strategic issues, and a proven ability to think \u201clike a strategist\u201d Or 5 years (8 if MSc.) of strategy consulting (can be internal) with significant exposure to data science and advanced analytics,Strong problem-solving skills,Ability to deliver action-oriented, clear key messages emanating from complex analysis,Ability to explain complex mathematical concepts, algorithms and data structures,Proficient in data mining and advanced analytical methods,Proficiency in data analysis and visualization languages/tools such as Python, SAS, R, Tableau, Alteryx,Experience in Big Data platforms and related technologies such as Hadoop, Hive, Pig, Apache Spark", "Working with key stakeholders and subject-matter-experts to understand business problems and consider solution approaches,Enhancing data collection procedures to include information that is relevant for building analytic systems,Processing, cleansing, and verifying the integrity of data used for analysis and draw insights from large data sets,Presenting information using data visualization techniques,This position requires an onsite presence in our Fort Worth office.,Masters or Ph.D. students pursuing a major or minor in one of the following: Statistics, Operations Research, Applied Mathematics, Industrial Engineering, Computer Science, or Transportation. Ph.D. students are preferred.,An overall GPA of 3.0 is required. An overall GPA of 3.5 is preferred.,Strong analytical and problem solving skills,Strong interpersonal and communication skills,Proficiency in using statistical computer language R (experience in Shiny is a plus), SAS, and query languages such as SQL,Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.,Experience using business intelligence tools (e.g. Tableau)", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build an organization to scale the impact of data science and analysis in Geo.,Forecast product and business growth and model opportunities for strategic investments.,Enable teams across Geo to do better/faster analysis on their own.,Ensure that we are rigorous in our analysis, but also striking the right balance on when to go deep versus when a rough and quick answer suffices.,Provide thought leadership on what matters to Geo and why, through a metrics-driven lens.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Envision new network (engineering) AI/ML projects and present well-formed ideas to the team. Utilize considerable creativity to merge ideation, knowledge of AI/ML model uses, insights from SMEs/translators and constraints to propose ML projects that are meaningful and actionable.,Provide timelines, milestones, project plans and types of AI/ML models to be attempted for new prototype projects. Adapt and communicate needed changes as datasets, labels or models may not function as expected.,Partner with team and non-team data scientists to teach complex concepts, assess project feasibility, select input features and validate project output. Envision and test for corner cases.,Utilize large amounts of GPU effectively to train and attempt multiple models, documenting progress.,Guide AI/ML projects with high autonomy so that they will be accepted by the business and SMEs who will use them.,Pull sample data sets from NS tools and VGRID to attempt prototypes on new topics.,Learn the wireless domain (5G/LTE, xLPT datasets, RF Planning, Orchestration, etc) and toolsets to better communicate and understand the needs of engineers for AI/ML automation.,Publish blogs, create documentation, perform presentations, submit intellectual property write-ups and lead efforts for external publications.,Bachelor\u2019s degree in Electrical Engineering, Computer Science or four or more years of work experience.,Six years of relevant work experience.,A Degree.,Masters/PhD in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science, or a related field.,Broad experience in training multiple types of AI/ML models such as KNN, and XGBoost, CNN/RNN, reinforcement learning, deep learning models, GANs, etc.,Knowledge of Relational databases in SQL or R.,Previous experience in wireless networking is a plus.,Recognized as a contributor to Software Organized Networks (SON) platforms.,Experience in advanced AI/ML such as ensembles, deep learning, reinforcement learning, and NLP.,Four or more trained models moved to production or completed in a research environment.,Experience with Spark and Big Data deployments.,Attached examples of filing IP, Published Papers or Conference Presentation titles to your resume.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Collaborate on required analytic projects,In conjunction with data owners and subject matter experts, develop new ways of getting insights out of data that allow us to improve supply chain logistics,Understanding root causes of changes in metric,Develop algorithms that can go from prototype to production quickly,Use data mining, model building, and other analytical techniques to develop and maintain predictive model", "Residential broadband service \u2013 over 700K customers and growing,Viasat Satellite technology holds the Guinness World Record for highest capacity communication satellite,Our in-flight Wi-Fi connectivity is presently streaming Amazon Prime and Netflix on providers like JetBlue and United Airlines", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Build models, quantitative analysis and design queries for all clients and internal teams. Add incremental value and extend capabilities of the packages,Manage weekly, monthly, and quarterly reporting packages for all clients and internal,teams.,Manage population health analysis program based on claims and other data sets. Be able to translate gaps in care into clinically actionable information.,Help design and implement online version of reporting tools,Collaborate with clinical team on population health workflows.,Manage reporting process to help clinicians track and follow at-risk patients.,Bachelor's Degree in Mathematics, Physics, Economics or Computer Science, or,equivalent experience. Master's Degree preferred.,Experience with analytic software Python (libraries such as Pandas, Scikit-learn and SciPy) as well as data visualization experience (Tableau and Keynote).,Experience in data extraction, transformation and loading. . Must have excellent Excel and data analytics skills.,Must have a passion for organizing and representing data in valuable ways.,Demonstrated ability to quantify and present results in laymen's terms.,Must have the ability to work with complex data sets from multiple sources.,Demonstrated ability to represent healthcare return on investment and compare patient cohorts by multiple groupings (e.g., disease, zip, age, gender, health plan, etc.),Must have working understanding of the healthcare industry.,Working knowledge of Salesforce or similar CRM to manage workflows.,Must have excellent interpersonal skills; especially the ability to present complex ideas in a fun and creative format.,Ability to quickly learn new procedures and processes.,Strong organizational and follow-through.,High level of ownership, accountability and initiative", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Data discovery, data preparation and blending, and data visualization,Utilize software tools such as Microsoft SQL Server, Alteryx Designer and Microsoft PowerBI,Research and develop statistical learning models for data analysis,SVMs,Na\u00efve-Bayes Classifiers,Logistic and Linear Regressions,K Means Clustering,Random Forest Classifier,Collaborate with business partners and data engineers to understand company needs and devise possible solutions,Build and maintain effective and collaborative relationships with a diverse range of staff,Bachelors of Data Analytics, Computer Science, Information Technology or similar degree program,Requires 5+ years\u2019 practical experience with ETL, data processing, database programing, and Analytics,Extensive background in data mining and statistical analysis,Able to understand various data structures and modeling techniques,Excellent pattering recognition and predictive modeling skills,Passionate about data and analytics,High attention to data accuracy,Ability to work in an agile team,Critical thinking to ask questions, determine best course and offer solutions,Complete work independently,Act as a change agent,Continuous improvement mindset,Ability to understand the big picture,Effective analytical and decision-making skills,Strong interpersonal skills to build relationships and communicate effectively with managers, staff, and vendors,Strong written and verbal communication skills,Teamwork skills within the department and on project teams,Demonstrated ability to work effectively in a fast-paced, complex, and dynamic business environment,Working with an energetic team focused on making our members wildly successful,An opportunity to work with others that have your back every step of the way,Opportunities to make a difference both inside and outside of our walls", "US citizenship required (dual national US citizens are eligible).,All positions require relocation to the Washington, DC metropolitan area.,All applicants must successfully complete a thorough medical and psychological exam, a polygraph interview, and a comprehensive background investigation.", "Demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans,Develops analytical models to drive analytics insights,Leads small and participates in large data analytics project teams,Models compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity,Participates in the continuous improvement of data science and analytics,Presents data insights and recommendations to key stakeholders", "Partner with Finance, Revenue, Ops, Marketing & Product teams to define business problems, scope Data Science solutions and represent Data\u2019s voice in the room.,Build predictive models to optimize Pricing, Marketing, Operations and Sales.,Build analyses and tools to understand our partners and renters and their pain points.,Explore and test new data sources to improve our Risk models and marketing targeting.,Actively contribute to Rhino\u2019s Data culture by building out core data models, tooling and best practices as well as training other Rhinos on using our data effectively.,You have an advanced degree in a quantitative field or 2+ years of work experience in an analytical role.,You use data science tools like SQL and Python (pandas, numpy) and you have a track record of picking up new tools along the way.,You are able to zoom in and out of problems and navigate comfortably between high level strategic thinking and communication and granular methods of problem solving.,You have experience in one or more of the following areas: experimental design, web tracking and attribution, predictive modeling (supervised learning), customer segmentation models, frequentist and Bayesian statistics, building custom web apps.,You are excited to join a rapidly growing company and to work cross functionally to drive business impact.,Competitive compensation and 401k,Unlimited PTO to give our employees a little extra R&R when they need it,Stock option plan to give our employees a direct stake in Rhino\u2019s success,Comprehensive health coverage (medical, dental, vision),Remote Work Program to allow for flexibility between home and the office,Generous Parental Leave to create a family-friendly culture,Wellness Perks (Gym, Classpass, & Citibike Memberships)", "Independently lead data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design and development of machine learning/statistical models and ensure best performance.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Assist engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Be a subject matter expert on machine learning and predictive modeling.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or relevant.,A Ph.D. in statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science or a relevant field.,Five or more years of experience in practicing machine learning and data science in business.,Strong foundational quantitative knowledge and skills; extensive training in math, statistics, physical science, engineering, or other relevant fields.,Experience in leading data science projects and delivering from end to end.,Strong technical experience in machine learning and statistical modeling.,Strong on computing/programming skills; proficiency in Python, R, and Linux shell script.,Experience in data management and data analysis in relational database and in Hadoop.,Strong communication and interpersonal skills.,Excellent problem solving and critical thinking capabilities.,Experience with NLP and chatbot technology.,Experience with Hadoop, Spark, C++, scala, or Java.", "Translate business problems into hypotheses and questions that can be tested with data,Develop processes, approaches the data science team uses to develop data products,Thinks like an owner \u2013 approaches work with eye toward creating value vs. taking and completing jobs,Teach others about data science", "Partner with product, engineering, operations and finance teams to find opportunities, make smart trade-offs, and understand customer impact of new features,Use data to inform, influence, support, and execute our product decisions,Craft statistical models to gain insights from data and communicate results to partners,Build pipelines to automate generation of metrics to guide the business,3+ years of relevant work experience,Comfort and experience building complex SQL queries,Deep understanding of statistics(e.g., hypothesis testing, regressions),Familiarity with ETL design and analytics workloads, preferably experience in building maintainable pipelines,Highly self-motivated with the desire to solve open-ended problems,Resourcefulness, pragmatism and ability to drive business impact", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Identifying, structuring, and integrating internal and external datasets and processes to unlock the potential of the data.,Developing dashboards, reports, and analytical insights to support the effective delivery of health care services and improve outcomes for our members.,You are eager to learn what you don\u2019t know yet and build your own knowledge base.,You\u2019re a roll up your sleeves and jump right in kind of person.,You believe that data science is a team sport.,3-5 years of total experience,Some experience with Python and/or SQL and a desire to build up your own skills in these languages.,Some experience with Amazon Web Services.,Some experience working with diverse and messy data sets.,Some experience communicating data and technical analyses to non-technical audiences", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Manage large customer accounts by optimally balancing transaction approval rates and loss rates,Analyze large volumes of data to understand consumer behavior, identify patterns of good and fraudulent activities, and eventually take effective actions,Monitor for newly developing fraud patterns and determine when to take further actions,Develop and implement predictive models for fraud detection,Analyze and implement decision rules to combat developing fraud attacks,Be capable of making decisions with limited information and ambiguous situations", "Identify opportunities where machine learning can have an impact across the entire Cash App", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Primary Location: United States,New York,Education: Bachelor's Degree,Job Function: Technology,Schedule: Full-time,Shift: Day Job,Employee Status: Regular,Travel Time: No,Work with stakeholders to refine requirements, communicate progress and create effective feature roadmap,Work with the team to develop a system for semantic search, ontology, knowledge graph creation and query execution,Independently work on end-to-end development of NLP models to build sophisticated and extendable natural language-based search engine,Train deep learning models with internal and external training datasets,Develop highly scalable classifiers and tools leveraging machine learning, data regression and rules-based models,Adapt standard machine learning methods to best exploit modern parallel environments (e.g. distributed clusters, multicore SMP, and GPU),Code deliverables in tandem with the team and mentor junior members of the team", "Enter, clean, and conduct basic data manipulation and analysis,Build and disseminate databases and spreadsheets designed to record POTFF related programmatic data,Provide consultation and assistance to supported units and POTFF staff to identify opportunities and methods for capturing data relating to POTFF programs and initiatives,Prepare reports and presentations that accurately convey data trends and associated analysis,Possess an active DoD Secret Security Clearance,Bachelor\u2019s Degree in quantitative science, social science or related discipline,Proficient with the suite of Microsoft Office programs, including Word, Excel and Access,Prior experience using statistical software application such as SPSS, SAS, and R", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research and develop statistical learning models for data analysis,Collaborate with product management and engineering departments to understand company needs and devise workable solutions,Keep up-to-date with latest technology trends,Communicate results and ideas to key decision makers,Optimize joint development efforts through appropriate database use and project design,Evaluates research concepts to develop appropriate statistical methods of analysis.,Advises on the development of research information, data gathering techniques, and data reduction to decrease bias and/or errors in data gathering and analysis.,Obtains and analyzes raw data from multiple sources to perform core job duties and to answer questions posed by internal / external customers.,Interacts with customers to clarify needs and in reviewing results. Obtains proper approvals on each of the deliverables.,Handles the reconciliation and analysis of internal and external data related issues.,Works with the data warehouse teams to insure data sets and reports are built to specifications.,Audits database contents for accuracy and validity.,Formats output of data analysis for internal / external customer readiness.,Provides overall quality assurance oversight from requirements through post-release.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Responsible for designing and identifying solutions for various business problems,Create & make presentations to explain business problems, analysis approach and recommendations,Data preparation and data mining in SQL, MySQL and Postgres,Handle multiple tasks with different priorities,Work under minimal supervision and display strong independent behavior,Building Analytics assets,Ensure timely and accurate delivery of the projects", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Conduct analysis and modeling in the area of audio/video call quality (both fidelity and reliability). Perform exploratory data analysis and build predictive models that fit subjective call quality metrics.,Develop fundamental metrics and tools for measuring call quality and validate these metrics/tools", "Modifying ranking algorithms to support a healthy marketplace with dynamic commissions.,Create models to understand traveler preferences with respect to price, geography, traveler preferences, images, and amenities,Work on larger programs/initiatives encompassing several projects that have a company-wide impact,Apply best-in-class machine learning techniques to solve the business problem at hand,Produce novel insights with machine learning to inform company strategy,Assist others with data discovery and data preparation phase,Help build the core feedback loops and incentive structures in our marketplace.", "Be a part of an product team and drive the product roadmap with responsibility of statistical learning algorithms, modeling, and productization.,Leverage research data such as competitive intelligence, market insights, business/product performance data to support strategic decision making,Deep understanding of a wide variety of ML techniques and algorithms: Statistical NLP, Scalable Time Series Modeling (Prophet), supervised regression and classification based models (GBM, Neural Networks etc) and unsupervised learners.,Develop classifiers to categorize marketing leads,Extract and visualize subtle features from multidimensional marketing data,Implement new and improve existing analysis algorithms and further develop software for automated generation of reports and analysis,Collaborate with other team members to ensure data are acquired, processed, and stored according to appropriate procedures,Exemplify G5 Core Values and Behaviors,MS or PhD degree in Machine Learning, Applied Statistics, Econometrics, Mathematics or a related field with demonstrated academic excellence,Academic background in deep learning, natural language processing, machine learning,Experience with NLP toolkits such as NLTK, OpenNLP, Stanford CoreNLP etc.,Experience with Time Series Modeling techniques,Familiarity with open sources framework such as H2O and Tensorflow,5+ years of programming experience, proficiency in R and Python,Experience with visualization solutions such as Shiny,Strong presentation and communication skills", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Partner closely with leadership on our (external) partnerships, business development, payments, and product management teams on causal inference (including observational studies/natural or quasi-experiments) and incrementality, predictive modeling, and analytical problems that drive key decisions globally. Excellent communications and understanding of business strategy is key.,Develop statistical/machine learning models and related analysis that explain user acquisition, retention, and engagement on our various partner initiatives and devices.,Research and develop key metrics and performance indicators. How do we measure the impact of marketing dollars that a partner company spends to promote Netflix? What is the value of implementing Dolby Atmos audio?,Translate analytic insights into concrete, actionable recommendations for business or product improvement, and communicate these findings.", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Analyze results of WSJ\u2019s ongoing A/B testing efforts.,Source, query, and clean data required for larger scale data science projects.,Experiment with using techniques like machine learning to power personalization in our products.", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "5+ years experience in artificial intelligence with a focus on machine learning and neural networks,Masters degree in Computer Science, Data Science, Mathematics, or Statistics or related degree.,Experience in cloud computing platforms such as Google Cloud Platform,Experience with deep learning framework (ex: Tensorflow, Torch, etc),Proven command of R or Python or Java,Excellent interpersonal, verbal, and written communication skills - must be able to communicate complex ideas in both technical and user-friendly language.,Excellent understanding of machine learning techniques and algorithms,Experience using statistical computer languages to manipulate data and draw insights from large data sets.,Capable of working in a dynamic environment, handling multiple projects, meeting deadlines, prioritizing appropriately and responding to issues quickly and creatively with an open and positive attitude.,Advanced knowledge of statistical techniques and concepts (regression, statistical tests and proper usage, etc.) and experience with their application,Energetic and self-motivated - must be extremely organized and detail oriented with an ability to make proactive recommendations.,Experience building and deploying autonomous/AI programs,Cloud/Big Data platforms,Tensorflow or similar tool,R, Python, Java,Must be able to see the \u201cbig picture\u201d and drive towards business results,Strong sense of curiosity.,Dynamic, fast-paced office environment and remote stakeholders.", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Leverage common statistical packages to bring learning methods to the user\u2019s data,Optimize the workflow of scalable model-creation in a healthcare environment,Productize healthcare-specific functionality into packages to commoditize machine learning,Consult with other teams to inculcate best practices, help with model interpretation, and promote a culture of machine learning competency throughout the firm,Contribute data science thought leadership to healthcare via Health Catalyst\u2019s tech blog,Familiarity with supervised and unsupervised learning methods,Facility with common techniques needed to preprocess raw data, avoid model over-fitting, tune hyperparameters, and perform feature selection,Knowledge of common R/Python machine learning packages,Ability to explain how a learning method actually learns and not just how to interact with its API,Software engineering experience (especially object-oriented code, unit tests, version control),Fluency with healthcare data,Ability to productize your R or Python code into a package,Knowledge of mixed models, glm regularization, and modeling for inference as well as accuracy,Ability to create visualizations via Shiny, Bokeh, or D3.js", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, design, and implement new quantitative trading strategies. This includes generating alphas from a variety of traditional and alternative datasets using rigorous statistical methods,Experience with Excel (VBA), Python, R, Microsoft Azure ML, Amazon ML, etc.", "Anticipate future business needs and identify appropriate opportunities for modeling, simulation, or machine learning,Gather and analyze data to solve and address highly complex business problems, make predictions on future outcomes, and provide prescriptive solutions that support decision making,Be involved in all phases of analytics projects including question formulation, research, development, implementation, testing, and maintenance,Explore data and build advanced analytical models, then present and discuss the resulting models to any level of audience", "Collaborate with scientists and researchers in one or more of the following areas: data intensive applications, text processing, graph analysis, machine learning, statistical learning, information visualization, low-level data management, data integration, data streaming, scientific data mining, data fusion, massive-scale knowledge fusion using semantic graphs, database technology, programming models for scalable parallel computing, application performance modeling and analysis, scalable tool development, novel architectures (e.g., FPGAs, GPUs and embedded systems), and HPC architecture simulation and evaluation.,Work with other LLNL scientists and application developers to bring research results to practical use in LLNL programs.,Assess the requirements for data sciences research from LLNL programs and external government sponsors.,Carry out the development of data analysis algorithms to address program and sponsor data sciences requirements.,Engage with other developers frequently to share relevant knowledge, opinions, and recommendations, working to fulfill deliverables as a team.,Design technical solutions independently, participating as a member of a multidisciplinary team to analyze sponsor requirements and designs, and implementing software and performing analyses to address these requirements.,Develop and integrate components-such as web-based user interfaces, accessing control mechanisms, and commercial indexing products-for creating an operational information and knowledge discovery system.,Lead multiple parallel tasks and priorities of customers and partners to ensure complex deadlines are met.,Manage various complex projects, using team members\u2019 skills to complete complex projects/tasks, and solve abstract complex problems/ideas and convert them into useable algorithms/software modules.", "Design, build and own predictive models from being able to predict customer churn to identifying fraudulent patterns in a large scale event stream,Design and build scalable data pipelines for managing and analyzing billions of transactions a day,Design experiments and evaluation to validate models in production,Work closely with data engineers and data scientists to build scalable and performant data models,Work closely with product teams to ensure data is actionable and aligned with our products,3+ years industry experience in data science, with designing, building and evaluating predictive models,Proficiency with statistics, predictive modeling and experimentation,Familiarity with applied machine learning and data modeling,Proficient in Python,Proven record of using predictive models to drive key metrics in production,Collaborate with product management and engineering departments to understand the company needs and devise possible solutions,Strong communicator able to summarize and communicate results and ideas to key decision makers,A leader, able to drive data drive decisions and products across the organization,Nice to have: Familiarity with Tableau or other data visualization tools and able to produce reports that clearly summarize findings,Nice to have: Familiar with Spark, SQL and data pipelines", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Design and prototype models with artificial intelligence, optimization, and statistics in order to solve challenging analytics problems influencing farmer decisions,Collaborate with engineers to automate analytical models and deliver value at scale,Conduct scientific presentations regularly with leadership team,Collaborate with engineering teams to support product launch in FieldViewTM", "Work closely with various organizational entities/key business stakeholders to understand our fans and customers to ensure that data science projects fulfill the evolving needs of the organization.,Manipulate data from various sources using advanced data engineering techniques.,Apply advanced analytic techniques such as machine learning, recommendation systems, statistics, deep learning, mathematical models, and algorithms in production environments.,Provide ad-hoc analysis that improves decision-making and business performance.,Work with the data platform services team to transform prescriptive and predictive models into production-grade software.", "You are proficient at assessing business requirements, modeling, and gaining an understanding of business metrics,You are comfortable communicating technical issues to non-technical users,You are excellent at communicating over written, verbal, organizational, and presentational channels,You excel at being flexible and juggling multiple projects in a fast-paced environment", "Under limited direction, research, develop, design and implement machine learning algorithms for cyber threat detection in operational technology environments.,Identify data types that should be collected in Operational Technology (OT) environments to enable detection of cyber events.,Test and validate developed algorithms on real OT data.,Identify, define and scope moderately complex data analytics problems in cybersecurity domain.,Develop cross-domain strategies for increased network security and resiliency of critical infrastructure, working with researchers in other disciplines.,Lead multidisciplinary teams in the areas of modeling and simulation for critical infrastructure cyber security, information security, and network security. Continue building LLNL\u2019s machine learning and data analytics capabilities.,Pursue program development opportunities by co-authoring proposals and proposing ideas that will address sponsor needs. Identify program growth opportunities for existing customers, understanding the customer space and needs.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Envision new network (engineering) AI/ML projects and present well-formed ideas to the team. Utilize considerable creativity to merge ideation, knowledge of AI/ML model uses, insights from SMEs/translators and constraints to propose ML projects that are meaningful and actionable.,Provide timelines, milestones, project plans and types of AI/ML models to be attempted for new prototype projects. Adapt and communicate needed changes as datasets, labels or models may not function as expected.,Partner with team and non-team data scientists to teach complex concepts, assess project feasibility, select input features and validate project output. Envision and test for corner cases.,Utilize large amounts of GPU effectively to train and attempt multiple models, documenting progress.,Guide AI/ML projects with high autonomy so that they will be accepted by the business and SMEs who will use them.,Pull sample data sets from NS tools and VGRID to attempt prototypes on new topics.,Learn the wireless domain (5G/LTE, xLPT datasets, RF Planning, Orchestration, etc) and toolsets to better communicate and understand the needs of engineers for AI/ML automation.,Publish blogs, create documentation, perform presentations, submit intellectual property write-ups and lead efforts for external publications.,Bachelor\u2019s degree in Electrical Engineering, Computer Science or four or more years of work experience.,Six years of relevant work experience.,A Degree.,Masters/PhD in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science, or a related field.,Broad experience in training multiple types of AI/ML models such as KNN, and XGBoost, CNN/RNN, reinforcement learning, deep learning models, GANs, etc.,Knowledge of Relational databases in SQL or R.,Previous experience in wireless networking is a plus.,Recognized as a contributor to Software Organized Networks (SON) platforms.,Experience in advanced AI/ML such as ensembles, deep learning, reinforcement learning, and NLP.,Four or more trained models moved to production or completed in a research environment.,Experience with Spark and Big Data deployments.,Attached examples of filing IP, Published Papers or Conference Presentation titles to your resume.", "This influential role in the Marketing team will be accountable for supporting in achieving our business goals through innovative techniques that measure and enhance the effectiveness of Marketing activities;,The role will have a clear understanding of ThinkMarkets\u2019 strategic objectives and the roles Marketing plays in achieving them;,Analyse and interpret behavioural data collected from desktop and mobile analytics software to understand opportunities in the customer journey;,Run analytics to measure key performance indicators;,Process, cleanse and verify the integrity of data used for analysis;,Apply data mining techniques to digital data to identify key insights;,Provide insight and analytical support for digital projects across the business;,Use statistical techniques to analyse digital data;,Deploy new methodologies to ensure better value for data;,Leverage models to address key growth challenges such as cross-channel spend allocation and response modelling;,Develop processes for repeatable tasks, such as data extraction, reporting and standard statistical validations;,Work with cross-functional teams to implement and deploy models and enhance analytical solutions by providing data-driven recommendations;,Create detailed documentation outlining design and technical specifications of each solution.,Be able to extract insights from unstructured data sets using NLP and Machine Learning Techniques.", "Collect, manage, and maintain data stores for the company,Produce regular reports and analysis based on the data,Communicate findings and propose data-based projects or initiatives,Inform business processes and strategies based on data analysis and findings,Find out what challenges and pain points exist for each department and determine if those challenges can be investigated through the data science process,You are an experienced programmer and statistician,Comfortable working in large, varying data sets and environments,You can adapt to different systems and gather data from those systems as needed,Strong understanding of Revenue Operations processes and the challenges faced by Revenue Operations organizations,Translate sales and marketing requirements and challenges into actionable data projects", "Researches and identifies Machine Learning (ML) and Natural Language Processing (NLP) methods and algorithms to solve specific problems to improve user experience on Clarivate's data and websites,Implements these methods and devises appropriate test plans to validate and compare the different approaches,Identifies new applications of ML and NLP in the context of Clarivate's extensive sets of content and data,Explores existing data for insights and recommends additional sources of data for improvements,Excellent understanding of ML, NLP and statistical methodologies,Excellent programming skills ( Java/Scala ),Ability to test ideas and adapt methods quickly end to end from data extraction to implementation and validation,Experience with search engines, classification algorithms, recommendation systems and relevance evaluation methodologies,Bachelor's degree in Computer Science, Statistics, Technology, or Engineering, strongly preferred, or equivalent work experience", "Develop, test and validate statistically sound predictive models.,Design and develop analytical solutions/reports to support Risk Management, Finance and Operations departments using business intelligence tools such as Python, R or Qlikview.,Analyze large data sets to discover trends, patterns and actionable insights.,Apply research and evaluation standards, descriptive statistics and data management principles to translate complex data into meaningful content.,Design, create, test and maintain a portfolio of reports, scorecards and dashboards.Manage individual projects as assigned to ensure adherence to schedule, as well as quality and accuracy of delivered results.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Lead the design, deployment and management of insight and analytic end points in cloud-based infrastructure and manage update/refresh cycles,Oversee management of the Consumer Insights & Analytics workspace within AWS,Work with big data engineers in Technology to facilitate publishing of analytic end points in enterprise data lake,Manage the creation of analytic data sets in support of operational analytics and data science team initiatives,Collaborate with both internal teams within Consumer Insights & Analytics and other business units to enforce robust analytic process, continuously build on data engineering learnings and continuous improvement in processes and tooling,Evaluate, acquire and integrate additional sources of data that can enhance analytic capability,Work with appropriate teams to ensure solutions comply with the HBO data governance and security policies.,BS or higher in computer science or similar experience,Demonstrated expertise with cloud based, big-data analytic infract support advanced analytic and research functions,Strong hands on expertise with handling structured and unstructured data analytic workflows within a big data platform setting.,Experience with analytic scripting in R, Python and SAS,Excellent communication and project management skills,Flexibility and comfort working in a dynamic organization with minimal documentation and process,Strong problem solving and critical thinking skills,5+ years of relevant experience", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "You will help to architect, build and maintain databases to support research and development.,Work with individual scientists to understand their needs and prioritize software development.,Interface with an existing IT team of two people to define systems,As the team grows, their is the potential for this role to grow into a technical leadership and/or data science work.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Design, train and deliver solutions from a wide range of datasets:,Genome sequences,Electronic medical records,Claims data,Longitudinal data,Physiological data (e.g. ECG, PulseOx),Implement, test and validate new algorithms in a way that can be used by the data science team,Stay up-to-date with the latest ML/AI techniques by reviewing literature, attending conferences, etc", "Design and implement analytical frameworks to measure and report out the ROI of marketing campaigns,Generate prescriptive insights, tactical and strategic, to improve marketing effectiveness,Execute and maintain long-term modelling initiatives,Evaluate opportunities for international marketing investments,Design A/B tests and analyse their results,Communicate results to marketing and product management partners,6+ years experience in Data Science,Extensive experience in predictive analytics and machine learning for marketing, including time series analysis, supervised, unsupervised, and reinforcement learning,Advanced SQL skills,Advanced R or Python skills,Ability to create clear data visualizations for different audiences,Confidence to take the lead on a project and see it through to completion,Ability to effectively communicate highly-technical insights to a non-technical audience,Experience with Tableau,Experience cleansing data sets,Passion to stay up-to-date on the latest industry trends and methods,Bachelor\u2019s degree in computer science, engineering, statistics, mathematics or related field,Master\u2019s degree in computer science, mathematics, statistics, marketing science or related field", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Use data mining skills to develop new metrics, insights and segmentation on both internal and external datasets,Support projects from start to finish & produce data-driven results with appropriate techniques to answer key business questions,Work closely with a variety of teams to deploy tests, drive deployment and adoption of new algorithms,Generate high level summary reports and trends,Communicate analyses and field questions from both internal and external stakeholders,4+ years of work or educational experience in data science, machine learning, and statistics.,Hands on Experience in Scala, Java, Spark, Python or R,Proven technical database knowledge (Hadoop, NoSQL, data modeling) and experience optimizing SQL queries on large data,Advanced knowledge of statistics and or machine learning techniques.,Knowledge of Tensorflow, Pytorch, Keras LSTM or other types of RNN a plus,Proven experience in predictive analytics, segmentation, experimental design and related areas,Exceptional communication and presentation skills", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop and implement effective/strategic business solutions through research and analysis of data,Harvest insights from data and present findings that will impact business objectives,Facilitate effectiveness measurement through test design, implementation support, and post-analysis for new and existing programs and initiatives,Assist in managing day-to-day results reporting and visualization for major corporate initiatives,Analyze, review, forecast, and trend complex data,Develop statistical models, forecasts and conduct statistical analysis to better understand trends, population segments and predict behaviors/outcomes", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Create R/Python programming modules for custom insights required by our clients. Leverage statistical analysis to understand what is \"acceptable\" versus \"outliers\".,Drive actionable insights from data and present them through client-facing Tableau and/or Microsoft Powerpoint in a compelling manner,Contribute to Data Exploration needed to enhance the cleanliness and effectiveness of our data sources,Create well documented coding and analytics packets to ensure reusability as the team expands.,Contribute to Analytics Data Management by understanding and implementing the right level of summary tables needed to answer repeated business questions, working off Big Data platforms like Spark, AWS, etc.,Programming skills in form of R and/or Python, SQL. Concise and organized programming skills will be highly valuable,Familiarity with the works of Spark, AWS S3 will be valuable,Familiarity with the Retail/CPG/Shopper domain and data will be a plus,Familiarity with Tableau or experience with any other visualization tool will be useful,Comfortable working in a fast-paced environment,Strong written/verbal communication and presentation skills.,Our Clients express delight in consuming insights through our analytics powered Business Intelligence solutions as well as custom analytics that we provide to them,Our Codes, Solutions are well documented and are reusable for rest of the team", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Ensures rigorous compliance to standard safety procedures and OC corporate policies,Develops rigorous design of experiments based on statistical basis to generate better understanding of product and process related performance,Partners with global Innovation, Business and Marketing teams to provide robust testing plans that measure the relevant characteristics with appropriate sample sizes and utilizes advanced data analytics to drive decision,Analyzes product development and manufacturing processes to identify root causes for performance gaps, and recommends solutions to impact change.,Validates mathematical modeling results with empirical data in product development processes, pilot studies or lab experiments.,Works in a team environment to advance understanding of the product performance and showcases the benefits of advanced analytics through executed projects,Standardizes, simplifies and improves the user friendliness of leveraging analytics platform for more widespread use within CSB S&T organization,Stays current with respect to statistical/mathematical methodology, to maintain proficiency in applying different methods and justifying methods selected,Leads the assessment and introduction of new statistical/mathematical technology and methodology within CSB S&T,Commits to evolving how projects are managed to match the business and project needs, change management challenges and communication requirements to successfully deliver.,Defines, implements and communicates the technology standards for advanced analytics,Builds a community for growing the capability within the organization through personal development, mentoring and teaching opportunities with peers.,Technical Expertise \u2013 Experience with modeling tools & platforms (like MiniTab, R, Python, IBM SPSS, SAS or other), data management/data mining skills, visualization techniques and descriptive statistics to solve complex problems required. Experience with optimization techniques and applications preferred.,Technology Leadership - Strong working knowledge of contemporary analysis technology, software platforms, and methodologies with the ability to apply it to our business processes. The ability to educate senior leaders on the impact and benefit of analytics (descriptive, predictive, prescriptive and cognitive) in our operations.,Project Leadership \u2013 Small to medium scale project management experience, including, but not limited to scope, schedule, cost, risk, resource, and change management. Possibly including initiatives with global reach, technology, processes, cross-functional teams and partner team members.,Consulting skills - Proven track record of influencing the decision and problem-solving processes. The ability to understand business and economic drivers and align goals across functional lines and organizational boundaries for execution.,Analytics: Has demonstrated experience in applying statistical techniques to solve business problems.,Visualization: Has experience in the effective utilization of visualization techniques to explore data to find root causes as well as presentation of results.,Leadership: Recognized expert in their field with the ability to help others define the problem and move quickly to resolution. Someone sought out to bring resolution to an issue in timely, cost effective manner.,Interpretive Skills: Understand customers fitness for use related to specifications, procedures, and products and partner with the international team of product development experts to build solutions,Consultative Skills: Ability to influence business partners in their decision-making. Shape solutions by helping partners articulate what they need.,Optimization: Experience in various optimization techniques including linear programming, integer programming, non-linear programming and dynamic programming.,Diversity: Understands, communicates with, and effectively interacts with people across cultures. Effectively achieves business results working across and with multi-national teams.,Communication: Clearly conveys relevant information and ideas with confidence and in a manner that inspires the audience. Adjusts approach to capture audience attention and ensures there is an understanding of the message. Seeks to understand others through active listening.", "Can connect data to business problems and clearly communicate the benefits, risks, and trade-offs,Believes that data integrity is paramount and obsesses about figuring things out,Is self-motivated, eager to create something new, and always looking to improve upon existing processes, tools and methodologies,Loves working with both people and numbers to help teams answer complex questions,Is passionate about building community all over the world,Collaborate with strategy, product, and data teams to map business needs to existing metrics or possible solutions, and then surface this information in our reporting platform,Advise on and promote good experimental design and evaluation, as we quickly iterate on product changes,Rapidly prototype and validate new metrics and models to generate new impactful insights about our members so we can better serve them and their communities,Craft clear and comprehensible data visualizations that enable teams to easily understand and act upon the output of an analysis,Analyze performance bottlenecks and recommend optimizations as we continually improve the performance of our tools,3+ years working with data analytics, data warehousing practices, ETL pipelines, experimentation, and modeling,Proficiency in statistics and experiment design,Partnering directly with clients (either internal or external) and leading projects with multiple work streams,Meticulous attention to detail and an obsession with 'getting it right',Advanced SQL and proficiency with one or more scripting languages: Python, Scala or R preferred,An optimistic outlook on what is possible to achieve and a passionate drive toward getting to that place,Experience with data technologies like Hadoop, Hive, and Spark", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Works with teams to discover insights from analyzing operational, financial, quality, and peer comparison data.,Under direct supervision, models complex business problems, discovers business insights and identifies opportunities through the use of statistical, mathematical, algorithmic, data mining and visualization techniques.,Proficiency in integrating and preparing large, varied datasets, architecting specialized database and computing environments, and communicating results.,Works closely with subject matter experts and technical experts to understand proper data extraction and interpretation.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "is an analytic leader who can mentor and teach to help enhance, build, and grow Hallmark\u2019s analytic capabilities.,will have proven experience over multiple years conducting analysis and developing algorithms for consumers and/or stores from a consumer packaged goods (CPG) company or retailer.,can hit the ground running and bring new analytical approaches not currently practiced by Hallmark.,is flexible, has strong initiative and is highly collaborative.,will be innately curious and a strong critical thinker.,3+ years of mathematical predictive algorithm development utilizing large-scale multivariate datasets in a business environment,3+ years\u2019 business experience in application of advanced analytics and data mining including predictive algorithms,Business experience working with large-scale consumer analytics datasets,6+ years\u2019 business experience of mathematical predictive algorithm development utilizing large-scale multivariate datasets,6+ years\u2019 experience in business application of advanced analytics and data mining including predictive algorithms in a professional environment,Experience in building big data based IT processes, understanding data science workflows and building pipelines,Strong communication skills: written, verbal, and presentation,PhD or MS in quantitative discipline - Statistics, Physics, Math, Engineering, Economics, Econometrics, Data Science, Computer Science, Operations Research \u2013 highly preferred,Experience with SAS, R, Python, Tableau, SQL, Hadoop, Spark,Proficiency in either R or Python,Experience deriving insight from structured and unstructured data,Experience with a consumer packaged goods (CPG) company or retailer,Demonstrated ability to derive explanatory variables from high-dimensionality collections of data: social, click-stream, SKU-level sales, digital marketing, weather, economic,Experience working with Big Data,Inherently Curious, Self-starter, Proactive, Comfort with Ambiguity, Passion for Problem-solving, Creative, Collaborative, Team-oriented,Demonstrated ability to coach and teach others,Take care of you and yours: We offer comprehensive medical, dental and vision benefits, as well as discounts on elder care, child care, education assistance and adoption assistance.,Save for your future: Through profit sharing, you share in the success of Hallmark. We also offer a 4% match on 401(k) contributions.,Enjoy your time: Maximize your work-life balance through Paid Time Off (PTO), paid holidays, community volunteer opportunities and discounts on product, entertainment venues, amusement parks and sporting events.", "Working closely with engineers and client teams to understand the client\u2019s needs/problems within digital marketing. The Data Scientist will apply their knowledge of machine learning methodologies to propose solutions to these problems and implement the solutions to drive value for clients.,Building models in R or Python using statistical and machine learning techniques.,Creating visualizations and deliverables to communicate technical solutions to appropriate audiences,Working with internal teams to optimize the data science process to better support the discovery team,Collaborating with the sales team to create and deliver client proposals and demonstrations", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Data Scientist", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build, maintain, and improve key decision systems and predictive models that are key to Progressive Leasing\u2019s competitive advantage,Explore new data sources both internal and external to Progressive and use findings to further improve and optimize model performance,Design, implement, and read in-market experiments testing new algorithms, technologies, and processes to better serve our customer and stay ahead of the competition,Recommend and support strategic business changes through rigorous analytics, deep understanding of the business, and creative problem solving,Identify, evaluate, and help implement emerging advanced analytics technologies and methods", "Communicate clearly inside and outside the company, to foster cooperation and development.,Design and implement major new features for our platform.,Effectively estimate time to implement designs.,Write and test code for highly available and high volume workloads.", "Develop production level models for predictions and insights,Able to create / test hypothesis from data based on business needs,Develop machine learning / AI based algorithms and refine based on remodeling. Discover new features based on business needs or curiosity,Able to manipulate data and improve data pipelines,Work closely with product & go to market teams to unlock AI potential in customers", "4+ years of experience applying advanced AI techniques (machine learning, predictive analytics, optimization, semantic analysis, time-series analysis, advanced visualization) to real-world problems,Proficient with SQL, Python; (experience in R is good to have but not required),Ability to define business problem in machine learning / Deep Learning framework,You have a demonstrated ability to iteratively conceptualize, design and build data-driven analytical models, and have taken a project leadership role in shaping these solutions.,Strong capabilities in modern analytics languages/tools.,You have expertise in Machine Learning and Deep Learning and one of the following other areas: Optimization or Operations Research, NLP, Computer Vision,A huge plus if you have hands on experience in one of the following: Spark: pyspark MLlib (running ML algorithms on local Hadoop cluster environment), or GCP or AWS or machine learning on Azure.,Master's in Computer Science, Operations Research, Statistics, Applied Math.,You have experience manipulating and preparing large, heterogeneous data sets (\"Big Data\") to support advanced analytics.,The opportunity to work for one of the coolest companies in global tech \u2013 our awards include Top 10 Most Innovative Enterprise Software Companies (Fast Company 2016) & a double Magic Quadrant Visionary (Gartner, 2015),Highly competitive salaries - we're very serious about attracting the very best technologists in the industry.,A long list of incredible benefits worthy of a leading Silicon Valley tech firm (inc. stock options, bonuses, healthcare, flexible working hours, global travel opportunities and much more.,A rewarding, progressive and tailored career path with a company that values diversity, innovation and invests in you.", "Developing the next generation of AI that protects the privacy of our customers and ensures that our modeling practices are ethical and bias free.,Bachelor\u2019s degree or four or more years of work experience.,Coding proficiency in Python, Java, R, etc.,Knowledge of AI/ML models, frameworks(keras, pytorch, etc.) and libraries/packages/apis (e.g., scikit),Bachelor\u2019s/Master\u2019s degree or PHD in mathematics, statistics, physics, engineering, computer science, or economics.,Advanced knowledge of math, probability, statistics, and models", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Understand insurance data and fraud mechanisms;,Create the associated mathematical models;,Implement, test and optimize the associated algorithms;,Participate in workshops with the clients to gather feedback and integrate it to the models;,You have advanced technical skills in Applied Mathematics, preferably in machine learning and/or operations research;,You combine strong analysis with synthesis abilities and are not afraid to deal with the details;,You can write quality production code;,You don't mind meeting with clients to discuss their needs and integrate their feedback in your projects;", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Previous message: [Jobs] Fwd: OMB seeking applications for Chief Statistician,Next message: [Jobs] Tenure-track position at University of Hawaii,Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop models for user behavior and marketplace dynamics,Design optimization algorithms to improve marketplace efficiency,Apply machine learning for recommendation, prediction, and forecasting,Strong quantitative background,Research, data science modeling, or engineering experience,Familiarity with technical tools for analysis - Python (with Pandas, etc.), R, SQL, etc.,Research mindset with bias towards action - able to structure a project from idea to experimentation to prototype to implementation,Background in Machine Learning, Statistics, Operations Research, Operations Management, Econometrics, or similar", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Process large amounts of data from multiple sources and extract relevant insights.,Research new ways of modeling data for actionable insights and processes improvement.,Perform statistical analyses and build machine learning solutions to support Google Cloud business needs.,Collaborate on complex and technical work with effective communication to develop quantitative strategies.,Design agile and rigorous experiments to measure effectiveness of models, tools and programs.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Developing the next generation of AI that protects the privacy of our customers and ensures that our modeling practices are ethical and bias free.,Operate as an expert and thought leader both within Verizon and externally within industry,Partner with academia and startup ecosystem to identify and potentially bring in new AI approach that can be of value to Verizon.,Bachelor\u2019s degree or four or more years of work experience.,Six or more years of relevant work experience.,Commercial application experience,Minimum of two years of experience in scaling AI in one or more areas like personalization, forecasting, experimentation, computer vision, etc.,Knowledge and ability to explain both the code and the underlying math used in algorithms/models.,Coding proficiency in Python, Java, R, etc.,Knowledge of AI/ML models, frameworks(keras, pytorch, etc.) and libraries/packages/apis (e.g., scikit),Experience developing models and algorithms independently, writing your own code, developing strategy for algorithmic experimentation, and deploying in production,Bachelor\u2019s/Master\u2019s degree or PHD in mathematics, statistics, physics, engineering, computer science, or economics.,Advanced knowledge of math, probability, statistics, and models,Experience with Hadoop, AWS or other distributed compute services,Exposure to data structures, data modeling and software architecture skills,Excellent communication skills, critical thinking, and strategic thinking skills above and beyond the technical knowledge,Excellent cross-functional collaboration skills", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Manipulate large clinical data sets.,Mine patient data for critical events, outcome variables, and treatment actions.,Perform statistical analysis to understand relationships between clinical variables and Etiometry\u2019s Risk AnalyticsTM technology.,Develop models for prediction and classification utilizing machine learning techniques, such as logistic regression, SVMs, neural networks, decision trees etc.,Provide data mining and data analysis support, including data cleaning, to data customers, such as Etiometry\u2019s research and development teams and research collaborators from partnering hospitals.,Research current advances in data science and statistical analysis methods.,Publish analysis findings at data science conferences and in journals.", "Provide thought-leadership in the area of Machine learning and Data Science.,Identify important and interesting questions about large datasets, then translate those questions into concrete analytical tasks,Develop strategies to extract, resolve, and unify information of various types from numerous disparate data sources.,Mine and organize massive data sets of both structured and unstructured data. This would involve exploring data, constructing appropriate transformations, and tracking down the source and meaning of anomalies when and where they arise.,Model building should draw from any approach that enhances accuracy and understanding including statistical modeling, mathematical modeling, econometric modeling, network modeling, machine learning, algorithms, genetic algorithms, and neural networks.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Begin developing relationships one level up and two levels down. Identify and support opportunities to expand project scope beyond current deliverable and actively share information about clients\u2019 emerging support needs and trends with team members and management.", "Work with our investment teams to perform alpha extraction of useful insights from large, messy data sets.,Evaluate and analyze structured and unstructured data sets to model relevant financial metrics and generate quantitative fundamental insights and predictions that can be applied to investment strategies.,Develop and apply statistical methods to solve complex problems within the financial industry.,Apply statistical analysis and machine learning methods to large data sets for data mining, feature engineering, bias correction and prediction.,Be involved in all aspects of the research process including design, data analysis, prototyping, implementation, testing and performance monitoring.,Model and evaluate the fundamentals of companies within the financial industry through analysis of large structured and unstructured data sets utilizing natural language processing techniques.,Design schemas and applications to automate data mining and warehousing processes in a scalable way.,Develop and evaluate novel methods for statistical inference, model selection and data modeling.,Support the investment teams in alpha idea generation, back testing and implementation. Evaluate new datasets for alpha potential.,Utilize multiple languages including Python, Java, C/C++, SQL and R to perform quantitative research and programming as well as write well-structured robust code for both research and production.,Utilize large scale distributed computing technology including Spark and Hadoop to analyze financial datasets.", "Previous message: [Jobs] Fwd: FW: [aapor-net] Job Posting: Study Director at the IU Center for Survey Research,Next message: [Jobs] NIA position,Messages sorted by: [ date ] [ thread ] [ subject ] [ author ],Previous message: [Jobs] Fwd: FW: [aapor-net] Job Posting: Study Director at the IU Center for Survey Research,Next message: [Jobs] NIA position", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Apply advanced statistical and machine learning algorithm to solve critical business problem. Research, recommend, and implement new and/or alternative statistical and other mathematical methodologies appropriate for the given model or analysis.,Use supervised and unsupervised modeling techniques in the development and testing of sophisticated models in all areas of insurance. Examples include claims, growth opportunities, marketing optimization, underwriting, expense reduction and many others.,Lead efforts with our IT partners on the development of modeling datasets, as well as model deployment, validation and problem solving.,Analyze available modeling data files in order to understand the data and identify issues that could potentially have an impact on model results.,Apply design of experiment principles to test new ideas (including marketing campaigns), analyze results and provide actionable insights.,Utilize text mining techniques to extract information from various sources and build models to improve customer experience and optimize operational efficiencies.,Works directly with business partners to identify new advanced modeling opportunities and solutions.,Work with the business partners to resolve any issues that arise from the deployment of the models including the development of sophisticated business rules.,Provide training and support to junior analysts and other coworkers in the areas of business intelligence and advanced analytics.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Identify, retrieve, prepare, and process data from various sources (e.g. internal & external, structured & unstructured, online & offline).,Conduct predictive analytics using advanced statistical and mathematical methods from statistics, machine learning, data mining, econometrics, and operations research.,Ideate potential use cases of Natural Language Processing (NLP) and other Machine Learning techniques to bring client impact, revenue generation and productivity increase.,Perform Statistical Natural Language Processing (NLP) to mine unstructured data, using methods such as document clustering, topic analysis, named entity recognition, and sentiment analysis.,Deliver analytical models in production environment and collaborate with cross-functional teams to drive business results through implementing predictive insights of advanced analytics.,Must have 2 years\u2019 minimum experience with at least one of the programming languages, such as Python, R, Java and Scala.,Experience with NLP techniques to transform unstructured data to structured data to fit free text data into NLP Models.,Experience with NLP toolkits such as NLTK, SpaCy, Stanford CoreNLP etc.,Experience with machine learning and deep learning models, especially clustering/classification models for document clustering, topic analysis, sequence models for name entity recognition, and sentiment analysis.,Experience with data visualization tools to tell the story.,Strong written and oral communications, a can-do attitude and the willingness to tackle hard problems in innovative ways.,Experience with TensorFlow/Keras is preferred.,Experiences working in the industries of financial services and/or commercial real estate preferred.,Experiences with Microsoft Azure preferred.,Bachelor\u2019s Degree in computer science, Statistics, Mathematics, Engineering, Econometrics, or related fields.", "Big data consultants,Big data managers,Solve challenging business problems using advanced statistical methods including AI, ML and quantitative analytics;,Understand business requirements and supporting the development of business cases;,Run discovery analytics to identify new and innovative opportunities;,Partner with developers and engineers to deploy, embed and scale analytics to deliver complex/critical analysis projects;,Create reusable assets, solutions and developing best practices for current and future business problems;,Lead analytical discussions and influence analytical direction of clients\u2019 teams;,Communicate and provide guidance to senior client leadership and teams;,Contribute data science expertise to new sales activities;,Have fun in all of the above.,Shape and drive the full lifecycle of a Big Data solutions;,Assess the current data capabilities, application landscape, data sources and tools;,Define modern data architectures;,Realize or lead proof of concepts;,Estimate, plan and deliver modern analytics solutions;,Provide thought leadership and grow big data skills and capabilities;,Have fun in all of the above.", "Develop and implement data driven business solutions using advanced statistical methods and machine learning techniques.,Performs data analyses, prepare and analyze historical data to identify patterns, apply statistical methods to formulate solutions to complex business problems.,Partners with IT to integrate analysis into the enterprise data environment and business partners to ensure insight is operationalized.", "Selecting the best financial products for your needs,Taking the right actions to improve your credit score,Anticipate your future financial health based on your current financial status and history", "Envision new network (engineering) AI/ML projects and present well-formed ideas to the team. Utilize considerable creativity to merge ideation, knowledge of AI/ML model uses, insights from SMEs/translators and constraints to propose ML projects that are meaningful and actionable.,Provide timelines, milestones, project plans and types of AI/ML models to be attempted for new prototype projects. Adapt and communicate needed changes as datasets, labels or models may not function as expected.,Partner with team and non-team data scientists to teach complex concepts, assess project feasibility, select input features and validate project output. Envision and test for corner cases.,Utilize large amounts of GPU effectively to train and attempt multiple models, documenting progress.,Guide AI/ML projects with high autonomy so that they will be accepted by the business and SMEs who will use them.,Pull sample data sets from NS tools and VGRID to attempt prototypes on new topics.,Learn the wireless domain (5G/LTE, xLPT datasets, RF Planning, Orchestration, etc) and toolsets to better communicate and understand the needs of engineers for AI/ML automation.,Publish blogs, create documentation, perform presentations, submit intellectual property write-ups and lead efforts for external publications.,Bachelor\u2019s degree in Electrical Engineering, Computer Science or four or more years of work experience.,Six years of relevant work experience.,A Degree.,Masters/PhD in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science, or a related field.,Broad experience in training multiple types of AI/ML models such as KNN, and XGBoost, CNN/RNN, reinforcement learning, deep learning models, GANs, etc.,Knowledge of Relational databases in SQL or R.,Previous experience in wireless networking is a plus.,Recognized as a contributor to Software Organized Networks (SON) platforms.,Experience in advanced AI/ML such as ensembles, deep learning, reinforcement learning, and NLP.,Four or more trained models moved to production or completed in a research environment.,Experience with Spark and Big Data deployments.,Attached examples of filing IP, Published Papers or Conference Presentation titles to your resume.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.,Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies.,Assess the effectiveness and accuracy of new data sources and data gathering techniques.,Develop custom data models and algorithms to apply to data sets.,Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.,Coordinate with different functional teams to implement models and monitor outcomes.,Develop processes and tools to monitor and analyze model performance and data accuracy.,Strong problem-solving skills with an emphasis on product development.,Experience using statistical computer languages (R, Python, SLQ, etc.) to manipulate data and draw insights from large data sets.,Experience working with and creating data architectures.,Knowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.,Knowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.,Excellent written and verbal communication skills for coordinating across teams.,A drive to learn and master new technologies and techniques.", "Develop, manage and maintain Machine Learning infrastructure,Utilize Natural Language Processing between users, stylists, and products.,Research, develop, plan and implement predictive algorithm,Use various regression and other data analysis techniques and methods,Work with other team mebers to build upon our data collection, storage, and processing infrastructure,Stay motivated to actively engage with customers,4+ years of experience in Data Science/ Big Data Analytics / Data Engineering,BS degree in Computer Science or Engineering,Expertise in fields such as Mathematical Statistics, Machine Learning Systems and Automated Reasoning Algorithmic Solutions,4+ years of hands on experience with Big Data ETL processes, Data visualization, Data Exploring Algorithms,Proven experience in Data Modeling,4+ years of experience with Data Management, Analytical tools, languages and libraries (e.g. SAS, MATLAB, R, Apache Mahout, Scikit-learn).,Knowledge of computer programming (C++, Java, Ruby, or others),Sense of ownership and pride in your performance and its impact on company\u2019s success,Critical thinker and problem-solving skills,Team player,Good time-management skills,Data Science: 4 years (Preferred)", "A higher degree, MSc in the physical sciences, mathematics or engineering.,A good knowledge of statistical methods and their practical application.,A strong interest in sustainable buildings.,Mathematical modelling experience and understanding gained in any field/application.,Experience in building energy performance evaluation would be desirable as would experience in analytical data science methods.,Gain valuable experience and marketable, highly transferable skills,Responsibility for a high-profile project,Mentoring from the company and an experienced academic team", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Begin developing relationships one level up and two levels down. Identify and support opportunities to expand project scope beyond current deliverable and actively share information about clients\u2019 emerging support needs and trends with team members and management.", "Serve as subject matter expert (SME) responsible for data analysis and implementation efforts for new and emerging sequencing technologies.,Provide high quality professional reports on performed data analysis, developed tools, and approaches.,Support internal and external collaborators with data exchange and custom analysis as needed for specific scientific projects.,B.S. in Biotechnology, Bioinformatics, Computer Science or a related field and a minimum of 8 years related experience in genome assembly or bioinformatics and finishing; or an equivalent combination of education and experience.", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collaborate across business teams,Understand business needs and apply Data Science, Artificial Intelligence (AI), Machine Learning (ML), Machine Vision (MV) technology and methods to architect and apply to an ecommerce marketing platform,Build and optimize models using Data Science, Artificial Intelligence, Machine Learning, including method selection to apply to engineering challenges,Improve rapid marketing optimization strategies for ecommerce,Develop algorithms for optimal exploitation of a marketing initiatives for next-generation ecommerce channels,Be a thought leader for the company to provide technical guidance in the above areas for experiments and projects", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Translate ambiguous business questions and concerns into specific quantitative questions that can be answered with available data using sound methodologies.", "Work with product managers and designers to develop new product features which benefit our users and leverage our huge cognitive training dataset,Support and influence the product managers\u2019 and marketing team\u2019s roadmap by conducting in-depth analyses of Lumos\u2019 data on user behavior patterns and game engagement,Build models to create a personalized user experience in product and marketing, including ELTV (Expected LifeTime Value) modeling, predicting churn, game recommendations, etc.,Have 5+ years of experience working as data scientist (preferably at a product-driven technology company),Are fluent in Python, R, or Julia,Have strong skills in statistics, probability, and/or machine learning, including but not limited to: recommender systems, generalized linear modeling, mixed effects, ensemble models, etc.,Are comfortable working with large and messy datasets,Are comfortable working with open-ended questions and are willing to perform EDA (Exploratory Data Analysis) at scale to find answers in the data,Have experience with big data technologies including SQL, relational databases, and Spark,A collaborative culture where promotion from within is encouraged,Regular creative learning sessions, skill shares, forums, guest speakers, and workshops,Education budget for conferences, professional and skill development,Competitive health benefits (medical, dental, vision)Flexible PTO \u2014 take the time off when you need it,Equinox gym reimbursement (or $ towards the gym or classes of your choice),Catered lunch 4 times a week, fully stocked kitchen & snacks, and dinner if you work late,Weekly happy hours and bi-monthly movie nights,Game room with Smash tournaments, tons of board games, ping pong, chess, and Legos,Team off-sites (past excursions include creative retreats, cooking classes, and terrarium building),We recognize family is important, we host a Bring Your Child to Work Day as well as other kid and pet friendly offsite events", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with senior software engineers, semantic web technologists, ontologists, physicists, chemists, biologists, and related field subject matter experts to implement and deploy systems for analyzing big data within the realm of intelligence analysis. (40%),Conceive and design innovative methods for employing semantic technologies to the counter-WMD and intelligence analysis problem sets. (10%),Research and develop systems concerned with semi-automated or automated ontology construction alignment. (10%),Intersect semantic web / ontology based approaches with Natural Language Processing, Information Retrieval, and Data Mining techniques to make novel advancements in practical application and research of data science and analysis.(15%),Decompose real-world mission gaps into problems sets that can be addressed using set, information, and graph theoretic approaches. (10%),Support system deployment, troubleshooting and user training. (10%),Author and present papers on current research activities. (5%)", "Write programs to automate analyses and data wrangling,Build machine learning models to forecast and understand customer behavior,Maintain and improve reporting in Looker, Metabase, and R,Explain analyses and discoveries with articles and presentations,Strong knowledge of statistics and inference,2+ years writing and maintaining code,2+ years working with SQL,Experience communicating statistical concepts to a broad audience,Programming in R and/or Python,Managing and organizing a large codebase,Experience with Bayesian Methods,Deep experience in some part of statistics (Ex: time series analysis, experimental design, multivariate analysis, natural language processing, etc.),Interest in functional style programming", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Design, manage, and deliver content optimization to ultimately help to improve response and drive sales.,Deliver deep dives with the objective of improving our understanding of the consumer and the content we produce,On occasion, present to clients and attend client meetings, sometimes at client locations,Support Lead Data Scientist in delivering a wider range of data science projects predominantly using python & cloud-based services", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Utilize a large panel of tools corresponding to different steps of the analysis. The most common will be SQL/HiveQL for data extraction, R/Python/Spark for data analysis and Tableau for data visualization.,The team is fully transverse as we are working with production, marketing, CRM, media, licensing, sales and finance.,Based on your strong background on computer science, you will be in charge of the automatization of scores and segmentations in a Hadoop environment.,You will work with the San Francisco teams, as well as with other teams based all around the world, mainly Montreal and Paris.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collecting and combining data from multiple sources,Uncovering and exploring anomalous data (including metadata),Applying the scientific process to data evaluation, performing statistical inference, and data mining,Developing analytic plans, engineer supporting algorithms, and design and implement solutions which execute analytic plans.,Designing and developing tools and techniques for analysis,Analyzing data using mathematical/statistical methods,Evaluating, documenting, and communicating research processes, analyses, and results to customers, peers, and leadership,Completed a degree program in the fields of mathematics, statistics, computer science, computational sciences, or a passion for rigorous analysis of data,Tenacity, integrity, persistence, and willingness to learn,Ability to solve complex problems,Use critical thinking and reasoning to make analytic determinations,Works effectively in a collaborative environment,Strong communications skills to both technical and non-technical audiences", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop and implement effective/strategic business solutions through research and analysis of data,Harvest insights from data and present findings that will impact business objectives,Facilitate effectiveness measurement through test design, implementation support, and post-analysis for new and existing programs and initiatives,Assist in managing day-to-day results reporting and visualization for major corporate initiatives,Analyze, review, forecast, and trend complex data,Develop statistical models, forecasts and conduct statistical analysis to better understand trends, population segments and predict behaviors/outcomes", "Delve deeply into a varying array of large data sources, determining the appropriate data for the analysis to be performed, discovering issues, identifying potential uses, and preparing for analysis,Monitor and provide feedback on model performance and recalibrates models as necessary.,Develop client economic models that provide financial motivation for loyalty program design,Gather requirements from appropriate business partners for project, including necessary data for analysis to be performed.,Work with IT to streamline established data warehouses, initiate new development of databases, production data, and available tools to build strategic datasets in support of key initiatives.,Intellectual curiosity with the ability to utilize, organize and synthesize data of various types and sources,Interest in partnering with the business to understand their needs and utilize advanced analytics to develop effective solutions,Drive to not only succeed personally, but to make an impact and contribute to the success of our rapidly growing business.,Ability to listen and translate requirements back and forth between analytics and IT to communicate back to the business a technical solution in non-technical terms.,Ability to adapt to the changing environment of our business and the marketplace in general, re-prioritize effectively, and integrate new techniques and tools into your everyday work.,2+ years of experience in programming languages, R , Python, SAS, SQL, or Spark, for large scale data and predictive analysis,2+ years of analytics experience in a Big Data environment, such as Hadoop, Azure and/or ElasticSearch", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Solve new and challenging problems using statistical & machine learning to find valuable data insights,Collaborate on data problems with our Engineering, Product, and Design teams to create high-quality, reliable, client-facing products,Dive deep into a wide range of data to identify opportunities and recommend solutions,Communicate clearly to senior leaders with various levels of technical knowledge, educating them and sharing your insights or recommendations with them,Hire and mentor junior Data Scientists so they grow in their own careers,Hold a Masters or PhD Degree in Statistics, Applied Mathematics, Computer Science, Operations Research or a related quantitative field.,Have five or more years of hands-on data science experience in building and delivering data models,Have realized the power of machine learning for other organizations,Possess knowledge and experience with metric design, NLP, and categorical and numeric prediction models,Are highly-proficient in Python and SQL, essential to our Data team\u2019s stack", "Build a foundation of state of the art technical and scientific capabilities to enhance our cutting edge financial intelligence,Propose, build, and execute various data science models to leverage existing data to improve business and create new business avenues utilizing unstructured data,Work on the complex problems at the intersection of finance and technology,Collaborate with the product and technology teams to contribute to the advanced data analytics and product development efforts", "Collaborating with colleagues from multidisciplinary science, engineering, and business backgrounds, to identify and reduce gaps in current inventory availability.,Build statistical, predictive and machine learning models to drive sourcing accuracy.,Ensure data quality throughout all stages of acquisition and processing, including data sourcing/collection, ground truth generation, normalization, and transformation.,Promote data-driven decision making across the team, using tools such as Tableau, SQL, and Excel.,Influence leaders to make thoughtful data decisions to provide the most consistent and usable data for all teams across the company.,Combine expert knowledge of statistical methodology and analysis with advanced programming skills to support complex analyzes.,Share/develop best practices with other Amazon teams on a global scale.", "Work in a digital communication team whose main objective is to solve signal processing / digital communication problems using state-of-the-art techniques,This position requires a strong theoretical digital communication and signal processing foundation,Experience required in Data Science which includes: data-centric problem formulation, data cleaning hands-on experience as well as classification, regression and reinforcement learning knowledge,Conduct specialized technical studies, lead internal research and development efforts,Candidate should be eager to explore modern techniques and unconventional methods for algorithm design and problem-solving,We are looking for a person who can work and lead small groups of engineers. The nature of our work requires the candidate to work on multiple projects simultaneously,We expect the chosen candidate to be able to be an active software developer in some of the programming languages described below and to pass a coding interview", "Conduct ad-hoc analysis on the data set and present patterns and trends to the team,Extract, combine, clean, pre-process, and analyze a corpus of questions and answers from internal and external data sets,Perform feature extraction for the data set and create a training set for a classifier model,Optimize hyper-parameters of a classifier from an existing machine learning library,Analyze performance of a data product and come up with suggestions for scaling it,Suggest enhancements for a data product and articulate them from a systems approach,Find potential opportunities for building new capabilities into the product as well as develop prototypes of new internal tools to make our operations more efficient,Help the operations team to improve the service we provide to our customers,Collaborate with Engineering to ship new capabilities of data products to production,Work with Data Engineering and DevOps teams to improve data pipelines and analytics,Investigate if an algorithm from a scientific paper can be applied to our domain,Implement a new machine learning algorithm in TensorFlow and evaluate its efficacy", "Development, research, and exploration in the areas of statistics, machine learning, experimental design, optimization, and simulation,Interprets problems and develops solutions to business problems using data analysis, data mining, optimization tools, and machine learning techniques and statistics,Will act as a strategic thought partner; propose alternatives/solutions in alignment with requirements/objectives/timelines,Collaborate with client and Enterprise Data Products team to set analytic objectives, approaches and work schedule,Research and evaluate new analytical methodologies, approaches and solutions,Analyze customer and economic trends that impact business performance and recommend ways to improve outcomes,Design large scale models using linear and mixed integer optimization; non-linear methods; and, heuristics,Developing advanced statistical models utilizing typical and atypical methodologies.,Design and deploy data-science and technology based algorithmic solutions to address business needs for Cox Automotive. Identify, understand and evaluate new commerce analytic and data technologies to determine the effectiveness of the solution and its feasibility of integration with Cox Automotive\u2019 s current platforms,Design large scale models using Logistic Regression, Linear Models Family (Poisson models, Survival models, Hierarchical Models, Na\u00efve-Bayesian estimators), Conjoint Analysis, Spatial Models, Time-Series Models,Design large scale discrete-event and Monte Carlo simulation models,Interpret and communicate analytic results to analytical and non-analytical business partners and executive decision makers", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Selecting features, building and optimizing classifiers using machine learning techniques.,Data mining using state-of-the-art methods.,Extending companys data with third party sources of information when needed.,Enhancing data collection procedures to include information that is relevant for building analytic systems.,Processing, cleansing, and verifying the integrity of data used for analysis.,Doing ad-hoc analysis and presenting results in a clear manner.", "Analyze data stored in various heterogeneous formats and develop data insights,Utilize appropriate pre-built models and develop custom models to make predictions and derive insights from heterogeneous data stores,Apply various machine learning techniques to build and train models.,Tune model parameters, validate models, analyze and improve their performance,Prepare relevant datasets to train and test models,Identify useful sources of information and patterns in the data,Work closely with different functional teams on implementing production models,Prepare reports and demonstrations with results of your work,Under limited direction, optimize customer experiences, revenue generation, ad targeting and other business outcomes,Perform other duties and responsibilities as assigned", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Data Scientist with expertise in machine learning, statistical data analytics, open source and proprietary tools and applications. Must have an excellent knowledge of advanced methods, and experience in applying those methods to a variety of applications related to health care,Establish and implement end to end proof of concept for data analyses across functional areas,Create innovative methodologies for data,Build proof of concept systems,Establish strategic partnerships with technical leadership across functional areas,Presenting to senior leadership as well external audience,Demonstrates the ability to create new and different solutions that align to real work problems and opportunities,Develop, maintain and support Business Intelligence (BI) activities,Identify and facilitate implementation of improvement opportunities,Facilitate requirements gathering from non-technical business personnel and suggest efficient reporting solutions based on requirements,Document business requirements in the form of specifications (business and technical specifications),Work independently and provide appropriate recommendations for optimal analysis and development,Perform other job-related duties as assigned", "Have autonomy and impact in your work thanks to our bottoms-up culture and a Chief Algorithm Officer reporting directly to the CEO,Create and test new allocation strategies to maintain healthy inventory across Stitch Fix warehouses,Communicate and collaborate with business partners in order to run allocation strategies across all departments of the company,Maintain fast runtimes for the inventory forecasting and allocation code to promote interactive use cases,Maintain and improve the reliability of the inventory forecasting and allocation code through standard software engineering practices,Cross-functionally add new features to improve the prediction accuracy of the inventory forecasting code,Create and implement service level agreements with the broader inventory optimization team in order to best fulfill business objectives,Continue to grow your skill set through our great company wide development programs in communication and culture, as well as technical programs within the algorithms team,You have a Master's in Statistics/Biostatistics/Math/Physics/Chemistry/Computer Science,You have at least 2-4 years of experience, with 1 year of software engineering experience, 1 year relevant to cross-functional team communication, and 1 year applying statistics and machine learning in production,You have a bias toward action,You have strong cross functional communication skills that help simplify and move complex problems forward with business partners,You have strong standards for software engineering practices such as implementing automated unit tests, are familiar with Computer Science algorithm fundamentals like runtime complexity, and have 1 year of software engineering experience,You have a solid background in statistics: 2-3 stats courses and 1 year of relevant professional experience,You're proficient in machine learning: 2 courses and 1 year of relevant professional experience,You have production data science experience, i.e. not just fitting a model but being held accountable for performance and reliability in production,We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!,We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation,We are a technologically and data-driven business,We are committed to our clients and connected through our vision of \"Transforming the way people find what they love\",We love solving problems, thinking creatively and trying new things,We believe in autonomy & taking initiative,We are challenged, developed and have meaningful impact,We take what we do seriously. We don't take ourselves seriously,We have a smart, experienced leadership team that wants to do it right & is open to new ideas,We offer competitive compensation packages and comprehensive health benefits", "Interest and passion for data analytics, insight extraction, programming, and modeling,Critical thinking to debug programs, create strong variables, iterate modeling techniques, etc.,Curiosity about what the data says and the analytics that extract insights,Perform data manipulation, wrangling, cleansing, and analysis (in Python, R, or SAS),Knowledge of internal and external data sources,Build, iterate, and validate predictive models using multiple statistical techniques,Interpret, understand, and present results to clients,Provide support and assistance for the implementation of predictive models", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Apply expertise in quantitative analysis, data mining, modeling, and the presentation of data to understand how we drive value for our partners.,Partner with cross functional teams, including Measurement, Insights, Partner Analytics, Partner Marketing, Partner Product, and Partner Engineering teams to solve problems, build products based on data, and identify new opportunities.,Report, visualize and communicate results to internal stakeholders and to inform the industry as a whole.,Design, build, and launch new data models of user behaviors for analysis that relates to Partner outcomes and involve in the implementation of the model.,Use various statistical methods to lead research projects from beginning to end, and ensure an efficient execution process.,2+ years of data science experience in the advertising or marketing industry.,Strong knowledge in various statistical methods and/or machine learning.,Expertise in working with relational databases, querying language (SQL, HQL), and statistical languages (R, Python).,Ability to develop a research plan and deliver insightful and actionable results.", "Lead and participate in the application of analytic and machine learning approaches to HBO\u2019s understanding of the drivers of content performance to help HBO make smarter decisions in the acquisition, scheduling, and promotion of content,Mine HBO and other third party data to better understand how consumers make choices with regards to content consumption and how those change based on various factors (including availability, seasonality, promotion etc.),Develop compelling data-driven stories to influence key content decisions at all levels within the organization,Bachelor Degree or higher in quantitative field of study (mathematics, physical sciences, computer science, operations research etc.) from an accredited institution,Inquisitive, conceptual thinker comfortable working on complex analytic problems related to how, why, when, where and what people choose to watch HBO content,Must be capable of telling compelling data centric stories that inform key strategic and tactical decisions in content development, scheduling and promotion to varying internal and external stakeholders, of varying seniority,Prior experience in the application of analytic programming (R or SAS) and SQL in the manipulation of data and execution of analyses preferably in an entertainment or media company,1+ years of relevant experience", "Architecture design, and development experience around SaaS and platform software.,Routine involvement in high level architectural and design discussions providing authoritative technical guidance.,Experience delivering technical collateral including architecture and design documents, technical case studies, conference papers and whitepapers.,Demonstrated track record of successful customer and external engagement driving influence through deep technical product and industry knowledge.,Seasoned working with fellow senior engineers, architects, product management, senior management, and other partners to define the technical roadmap for the product in response to requirements.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Broad knowledge of ML algorithms, where they apply, and how to optimize and evaluate their effectiveness,Deep understanding of the math behind the techniques (superficial usage should not satisfy you!),Desire to see your work solving practical, important problems,Bachelor's degree in Computer Science, Math, Statistics, or Quantitative Science or related field,Math-level comprehension of machine learning techniques and algorithms (e.g., normalization, dimensionality reduction, classification, regularization etc.),3+ years\u2019 experience desired, but is secondary to a real understanding of ML,Ability to provide leadership in practical applications of machine learning on a rich, massive, real-world data set,Strong engineering, analytical, and research capabilities,Experience using ML with Python. A big plus if you\u2019ve also worked in C++,An understanding of practical software development such as code repos, issue tracking, code reviews, etc.,Excellent communication skills,Managing and manipulating large data sets,C#, Java, .Net, Tomcat,Networking and/or mobile systems (TCP/IP stack, cellular, Wi-Fi, Android, iOS),Security (SIEM, UEBA, VPNs),Cloud deployment system (AWS, Azure, Google) and/or microservice architectures,Health and life benefits, along with a competitive 401k plan and a company match,9 company paid holidays, 15 paid vacation days, 10 paid sick days, and 2 floating holidays,Commuting and fitness benefits", "Work on a platform level player matching system that impacts thousands of games,Implement models for anomaly detection,Communicate your findings to a larger business audience,Be the voice of statistical rigor across the entire organization,Simulate new algorithms with bots in a staging environment to understand the impact of your ideas before users see them,Collaborate with other engineering teams to have your algorithms implemented in production,2+ years analyzing customer behavior,Programming experience (Python, R or any other relevant language),Excellent analytical and problem solving skills,Experience building and training predictive models,Working knowledge of statistical mathematics,Prior experience in statistical algorithm design,Ability to produce ad-hoc reports using SQL,Basic understanding of optimization methodologies,Good understanding of A/B testing methodologies,Prior experience in gaming", "Work closely with cross-functional teams of data scientists, marketing managers, and R&D who are passionate about Spotify\u2019s success.,Develop a deep understanding of our audience through exploratory analysis to generate hypotheses and identify growth opportunities.,Work closely with marketing on multi-channel attribution (both short and long-term) and mixed media modeling.,Design and implement experiments (A/B and multivariate) to optimize acquisition, engagement, and reactivation of artists, labels, managers, and podcasters.,You have 4+ years of relevant experience in marketing analytics, with a degree in statistics, mathematics, computer science, economics, psychology or another quantitative discipline.,You have the technical competence to perform advanced analytics: coding skills (such as Python or Scala), experience with dashboarding tools (Tableau, Looker, SQL) and experience performing analysis with large datasets.,You possess statistical competence (such as attribution modelling, reporting, forecasting, significance testing, predictive modelling, etc).,Experience with designing, auditing & implementing the data infrastructure needed for tracking marketing campaigns.", "Develop solutions to support our core business functions.,Collaborate closely in an equal partnership with business and engineering teams.,Have the opportunity for major impact on a thriving business.,Own projects end to end, including strategy and rollout.,Solve business problems by reframing them to enable algorithmic solutions.,An advanced degree from a quantitative field; for example, mathematics, physics, cognitive science, statistics, etc.,Theoretical knowledge and experience applying statistical (or machine learning) models to real-life problems.,Fluency in scripting languages (R or Python).,Experience with relational databases / SQL.,Experience using modern tools to access large distributed datasets (e.g. Presto, Spark.),Interest and skill in data visualization high skill in data visualization (big plus).,Scrappiness and can thrive on autonomy.,Great communication skills.,An innate curiosity and bias to action.,The ability to identify what\u2019s important, and what\u2019s not.,The ability to recognize that simple is sometimes best.,Experience tackling vague problems, transforming unframed problems into algorithmic solutions.,We are a group of bright, kind and goal oriented people. You can be your authentic self here, and are empowered to encourage others to do the same!,We are a successful, fast-growing company at the forefront of tech and fashion, redefining retail for the next generation,We are not just data-driven: We are data first,We are committed to our clients and connected through our vision of \u201cTransforming the way people find what they love\u201d,We love solving problems, thinking creatively and trying new things,We believe in autonomy and taking initiative,We are challenged, developed and have meaningful impact,We take what we do seriously. We don\u2019t take ourselves seriously,We have a smart, experienced leadership team that wants to do it right and is open to new ideas,We offer competitive compensation packages and comprehensive health benefits", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Develop data modeling, machine learning and data visualization solutions to address customer needs,Foster relationships with engineering and product management team members to identify important questions and recommending innovative solutions using data,Communicate project output in terms of customer value, business objectives, and product opportunity,Promote data science methods and processes across solutions,Identify opportunities for new growth and efficiency based on data analyses,Project output in terms of customer value, business objectives, and product opportunity,Responsible for designing, developing, and implementing data driven solutions", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Lead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design, development, and deployment of AI solutions that drive business growth and better customer experience.,Contribute to the development of AI/ML strategy and roadmap for the company.,Contribute to research and development of AI/ML techniques and technology that fuels the business innovation and growth of Verizon.,Represent Verizon in AI/ML research and the industry through publications, conference speeches, collaboration with leading researchers and universities.,Build strong influence both among AI/ML community and with senior business leaders and actively promote the effective applications of AI/ML technology.,Lead engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or in a related field.,Ph.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.,Eight or more years of experience in practicing machine learning and data science in business.,Accomplished researcher and expert in machine learning, neural networks, reinforcement learning, chatbot technology, and NLP.,Strong communication and interpersonal influencing skills.,Excellent problem solving and critical thinking capabilities.,Experience in leading large scale data science projects and delivering from end to end.,Strong computing/programming skills; Proficient in Python, Spark, SQL, Linux shell script.,Strong experiencein Big Data and Cloud technology.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Analyze and mine both structured and unstructured data to drive product centric, as well as user & revenue centric insights and decisions,Create automated reports and data visualizations to track key business metrics tied to our product & release lifecycles,Work with product team to validate functionality and advise on data and analytics tracking,Set up real-time alerts,Craft compelling stories; make logical recommendations; drive informed actions,Analyze historical data and identify opportunities for optimization,Develop and test analytical solutions to be leveraged by both internal and external clients,Fulfill ad-hoc query requests & analysis", "Sr. Data Scientist anticipates future business needs and identifies opportunities for complex analysis.,Sr. Data Scientist will gather and analyze data to solve and address highly complex business problems and evaluate scenarios to make predictions on future outcomes and provide prescriptive solutions that support decision making.,Sr. Data Scientist will be involved in all phases of (Big Data) analytics projects including question formulation, research, development, implementation and testing. Sr. Data Scientist will be able to explore and understand data and build advanced analytical models, then present and discuss the resulting models to any level of audience.,Sr. Data Scientist will be involved in development of the highly performant and scalable cloud-based production data processing and machine learning model training and deployment pipelines and integration of the models with products and services,Sr. Data Scientist will design and drive the creation of new standards and best practices in the use of statistical data modeling, big data and optimization tools for Cox Automotive. Identify and direct special studies and analyses for unique business problems and scenarios. Proactively identify algorithms or products with high intellectual property content, assist in the evaluation of their potential for patents, where appropriate assist in the patent applications, and contribute to the intellectual property protection for Cox Automotive.", "We care about our employees. In fact, The Washington Post and The Washington Business Journal consistently rank us as a \"Best Place to Work.,You'll work with great people who love what they do: our team includes published authors, certified trainers, and internationally renowned speakers.,We have a \"bring your own device\" workplace and will share the cost of a new computer of your choice -- Mac or PC. It's up to you.,We'll invest in your career by providing 3 days of paid professional development every year, including travel and registration fees to attend classes and conferences, in addition to tuition assistance for degrees and certifications.,Starting day one, every employee is bonus eligible and receives 17 days of paid vacation,You can bike, drive, or metro to work -- our commute reimbursement plan has you covered.,We cater dinner once a month, and always have healthy snacks available!,You'll have fun! We hold monthly happy hours and events all year long, including a summer weekend getaway for you and your family,Working directly with client stakeholders to understand and define analysis objectives and then translate these into actionable results.,Obtaining data from multiple, disparate data sources including structured, semi structured and unstructured data.,Using machine learning and data mining techniques to understand the patterns in large volumes of data, identify relationships, detect data anomalies, and classify data sets.,Working with data integration developers to assess data quality and define data processing business rules for cleansing, aggregation, enhancement, supporting analysis and predictive modelling activities.,Designing and building algorithms and predictive models using techniques such as linear and logistic regression, support vector machines, ensemble models (random forest and/or gradient boosted trees), neural networks, and clustering techniques.,Deploying predictive models and integrating them into business processes and applications.,Validating and optimizing model performance upon deployment and tracking over time as necessary.,Presenting complex analysis results tailored to different audiences (e.g. technical, manager, executive) in a highly consumable and actionable form including the use of data visualizations.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build positive, productive relationships with clinical and business partners to understand their information needs, identify data science opportunities, and ensure our products add value. Partners may include SCCA executives, directors and managers, providers, researchers, administrators, clinical staff, and outside institutions or agencies,Develop a robust understanding of SCCAs data and data systems financial, operational and clinical to ensure effective, reliable, and responsible use of data,Build solutions that apply machine learning techniques to address complex business problems, derive actionable insights, and maximize decision-making confidence,Lead or participate in all aspects of solution development, including exploratory data analysis, feature selection engineering, model build, testing and deployment,Create data visualizations and present solutions to technical and non-technical audiences,Identify and implement industry best practices for data science in healthcare,Make recommendations on technical infrastructure, analytic competencies, and project selection to inform and execute the strategy of the Advanced Analytics Program", "Drive scalable and repeatable data science and advanced analytics capabilities within the team,Partner with Marketing teams worldwide, with a focus on email and online marketing.,Develop and implement ROAS methodologies for online marketing channels.,Design A/B tests to optimise our marketing performance, customer targeting, and channel mix.,Work directly with external vendors where appropriate to align on test specification and ensure accurate execution.,Prioritise a complex and substantial work load, ensuring client expectations are handled and works is focused on the most valuable activities.,Interact directly with multiple departments across the organisation. Identify key opportunities that deliver valuable business insight.,You have strong experience in programming (Python, R, etc.) and data preparation (SQL).,You understand advanced statistics including modelling techniques such as regression and clustering.,You can apply a wide variety of technical methods and models to effectively solve business problems.,At least 4-5 years of marketing effectiveness/efficiency measurement experience.,An understanding of the fundamentals of A/B testing in theory and practice.,Knowledge of the online media/marketing landscape and external measurement tools.,You have demonstrated the ability to work through complex business problems and partner with internal clients with a consultative approach.,You have well-rounded commercial knowledge covering product and marketing. Experience of ecommerce strongly preferred.,You have knowledge of and passion for Big Data.,Experience with Hive, Hadoop, Qubole preferred.,You have strong analytical and business modelling skills with the ability to convert raw data into actionable business insights.,You have a high level of proficiency in Excel, especially with large data sets.,You have excellent verbal, written and data visualisation skills.,You have experience working in a global matrix environment and can work remotely.,Experience mentoring or managing junior analysts or data scientists.,You have earned a degree in Economics, Mathematics, Business, or another technical field or related discipline.,Relationship building: Builds effective relationships through positive communication that motivates and influences others. Is an honest, dependable, and valued team member, and actively involved in achieving team objectives.,Handling complexity: Can work effectively in a highly complex, diverse, changing environment. Adapts well to and is energized by change whilst maintaining focus on key business goals and personal objectives.,Innovation: Embraces creativity, innovation and is open to new ideas. Works to improve current practices /technologies to provide new business opportunities and improved results.,Problem solving: Takes initiative to identify current and potential problems and determines the best solution. Involves and/or leads the people and resources required.,Personal Effectiveness: Produces outstanding results both professionally and personally by being proactive and committed. Continually focuses on achieving positive results contributing to Expedia\u2019s business success.,Technical Competence: Uses technical / job knowledge and experience, incorporating functional skills and broad-based business knowledge, to meet and exceed job requirements / customer expectations.,Strategy/Planning: Understands the needs and direction of the business, anticipating and developing business priorities to meet these. Embraces change and drives to improve current working practices / products/technologies to grow the business.,Goal setting/short term planning: Achieves results by setting goals using quality planning, analysis and decision-making. Adapts and copes successfully with changing circumstances.", "Use/build basic statistical models and research methods to analyze data and generate business/product reports with minimal supervision,Building dashboard to visualize and communicate your understanding of our users,Working with engineers and product managers to set up A/B experiment and analyze experiments,Helping to improve data quality by communicating how to track various events on Mobile and OTT platforms,Proactive in addressing new problems as they arise, and supporting Ad-hoc analysis of new features, bugs and growth initiatives,Masters in a Quantitative Field, i.e. Statistics, Computers Science, Operations Research, etc.,SQL, Python, Data Mining, Data Visualization, Statistical Analysis,Basic understanding of Machine learning algorithms: Supervised learning algorithms, unsupervised learning algorithms, dimension reduction techniques.,1+ years experience in a similar role,Strong communication, presentation, collaboration skills,Spark, Airflow, data ETL experiences, product analytics or growth analytics experiences in the tech industry,Video streaming industry experience is a plus,Help build the future of TV,A tight-knit team of passionate people and a tech-first, data-driven business,In addition to VC funding, Tubi generates healthy revenue.,We offer very competitive pay, equity, full medical, dental & vision benefits, catered lunch and dinner, and gym subsidies,Your choice of hardware,Open PTO", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "Planning and implementation of new data products, in close collaboration with the marketing, product management and editorial teams,Development of statistical models and getting them to production,Focus on use cases in the area of customer marketing / customer analytics.,Planning and support in the execution and evaluation of experiments,Monitoring, testing, and optimization of models in production,Close collaboration with the data engineers for continuous optimization/extension of data pipelines,Master's degree in a quantitative discipline (Mathematics, Statistics, Computer Science, Physics, Engineering),Minimum of 2 years of work experience, 5 years for a senior role,Theoretical and practical statistics and machine learning / data science know-how,Practical experience with tools for statistical modeling (e.g. Python scikit-learn, R statistical packages, or SparkML),Significant experience in at least one programming language (Python, Scala, Java),Experience with big data concepts and frameworks (e.g. Apache Spark, Kafka) and cloud platforms (e.g. Google Cloud),Experience with planning and setup of experiments is a plus,Challenging and interesting activities in a diverse task field with a high level of autonomy and the possibility to contribute to the digital transformation of one of the best-known media brands in the German speaking area.,Premium working environment: central location with offices \u00abat Bellevue\u00bb in the heart of Zurich \u2013 with a view to the opera house and the lake \u2013 with a large selection of shopping opportunities and restaurants in walking distance.,Flexible working hours and home office is part of our culture.", "Lead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design, development, and deployment of AI solutions that drive business growth and better customer experience.,Contribute to the development of AI/ML strategy and roadmap for the company.,Contribute to research and development of AI/ML techniques and technology that fuels the business innovation and growth of Verizon.,Represent Verizon in AI/ML research and the industry through publications, conference speeches, collaboration with leading researchers and universities.,Build strong influence both among AI/ML community and with senior business leaders and actively promote the effective applications of AI/ML technology.,Lead engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or in a related field.,Ph.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.,Eight or more years of experience in practicing machine learning and data science in business.,Accomplished researcher and expert in machine learning, neural networks, reinforcement learning, chatbot technology, and NLP.,Strong communication and interpersonal influencing skills.,Excellent problem solving and critical thinking capabilities.,Experience in leading large scale data science projects and delivering from end to end.,Strong computing/programming skills; Proficient in Python, Spark, SQL, Linux shell script.,Strong experiencein Big Data and Cloud technology.", "Responsible for the set-up of the modelling and data science unit to support operational initiatives and company strategy,Source and build datasets to enable Lilium\u2019s market launch,Build and constantly refine complex demand and operational models to inform top-level business decisions,Develop tools and visualisations to support negotiations and discussions with Partners, Governments and Investors,Produce and articulate insights to support global company expansion, from where we launch next to how we optimise existing routes,Work closely with Strategy, Product and company Management, providing the basis for key decision making,Passionate about our mission to make air mobility an accessible and affordable reality for everyone,Background in statistics, maths, computer science or another related field, with a clear and demonstratable record of outstanding academic achievement. Graduate degree a bonus,Exceptional quantitative problem-solving skills and demonstrated aptitude for analytics,Familiarity with SQL and Python (R and other statistics tools a bonus), comfortable with programming concepts,Highly proficient in developing complex (multi-parameter) models from scratch,Comfortable working in a developing, fluid and constantly changing company,Effectively handle multiple priorities, organize workload and meet deadlines", "Work closely with in-house subject matter experts to thoroughly understand the business domain and use that knowledge to help define, design and implement machine learning systems,Help define project goals and timelines,Evaluate new architectures for feature extraction to optimize and extend machine learning models,Take ownership of text analysis, image and form recognition projects,Create systems for evaluating model accuracy and anomaly detection", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Be a major player in a company that's a pioneer in semantic technology,Work with all the latest ML tools and technologies,Work with enormous data sets. Our database has over 10 billion records extracted from the Web,Find correlations and signals amongst all the noise,Push ZoomInfo to the next level of data excellence,Solve interesting and challenging problems alongside a great team of engineers,Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, etc.,Clean, analyze and select data to achieve goals,Build and release models that elevate our data quality and accuracy,Collaborate with colleagues from engineering, QA and business backgrounds,Develop new skills as you push your knowledge - and our technology - to new levels,Work for a profitable, growing company that works with an impressive Fortune 500 client list", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "7+ years' practical experience manipulating data sets, building statistical models, doing database programming and data analytics.,Temporarily due to COVID-19.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Statistical modeling and hypothesis testing using Python and/or R.,Designing, training, and validating a breadth of machine learning algorithms.,Writing complex SQL queries in various RDBMS (e.g., Postgres, MySQL) and distributed frameworks (e.g., Hive).,Building and deploying Python and/or Scala applications.,Deploying applications and interacting with cloud-based infrastructures (e.g., AWS).,Building and validating deep neural networks with modern tools, such as PyTorch or Tensorflow.,Interacting with and building RESTful APIs.,Managing *nix servers.,Writing unit tests.,Using continuous integration.,Collaborating via Git.,Competitive salary,Bonus program,Health, dental, and vision insurance,401k with employer matching,Generous vacation policy", "Understand the Goodreads/Amazon data structures (MySQL/Data Lake/Redshift),Acquire data by building the necessary SQL / ETL queries. Import processes through various company specific interfaces for RedShift and Data Lake storage systems.,Investigate the feasibility of applying scientific principles and concepts to business problems,Analyze data for trends and input validity by inspecting univariate distributions, exploring bivariate relationships, constructing appropriate transformations, and tracking down the source and meaning of anomalies.,Build models using statistical modeling, mathematical modeling, econometric modeling, network modeling, social network modeling, natural language processing, machine learning algorithms, genetic algorithms, and neural networks.,Validate models against alternative approaches, expected and observed outcome, and other business defined key performance indicators.,Develop metrics to quantify the benefits of a solution and influence project resources.,Partner with Engineering/Data Engineering to improve the quality of existing data and bring additional data sources in line,Audit metric data and measure project progress and success.,Build/automate reports/dashboards (in Tableau) that allow the business leaders to get a clear snapshot of their operations,Develop innovative experimental design and measurement methodologies to understand our customer growth and business efficacy,Design and implement scalable and reliable approaches to support or automate decision making throughout the business.", "Ingest and organize data from various sources (e.g. CSV, relational database),Process data using typical data science techniques (e.g. deduping, imputation),Linear / logistic regression,Decision tree algorithms (e.g. random forest),Clustering algorithms (e.g. K-means),Numpy,Pandas,Matplotlib", "Contribute to the development of our data pipelines by implementing novel algorithms for retrieving, analyzing and visualizing data,Extract useful statistics and usage profiles from the existing fleet,Apply statistical analysis on service and vehicle data to drive decision making in the organization,Implement scalable planning tool based on forecasting models for service operations,Create visualization to communicate data in a meaningful and actionable manner", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The PebblePost Graph uses distributed data and graph processing to create connections between online browsing activity and physical location in a privacy-compliant, consumer-centric fashion.,Our Decisioning Engine uses machine learning and predictive analytics to determine an ideal recipient for an offer, and manages delivery intelligence for each personalized PDM\u2122 send.,Our data warehouse delivers real-time insights for brands on campaign effectiveness utilizing first and third-party data.,Architect data science solutions to improve performance within a fast-growing, disruptive first-mover company,Collaborate with product managers and team members to gather requirements, design, and develop solutions within an agile environment.,Participate in code reviews and help maintain high standards of code quality,Stay up to date with relevant science and technologies to create an exceptional product,Work with engineering team to design, implement, and test applications to our cloud environments.,Expertise in regression analysis,Expertise in SQL and one or more relational databases,Expertise in machine learning techniques (e.g. clustering, decision tree learning, artificial neural networks, etc) and the pros/cons of when to use each,Experience with a scripting language ( e.g. Python) and statistical computer languages e.g .R.,High-level verbal and written communications skills,Excellent problem-solving and implementation skills,Self-motivated and passionate about apply data science to real world problems,Master or PhD in Computer Science, Statistics, Math, Industrial research or related fields,Experience in building production-level prediction and optimization systems,Expertise in the Hadoop Ecosystem (e.g. Spark, PySpark, HDFS, Hive, HBase, Impala, etc)", "Work with a team of data scientists and cross functional partners to collaboratively develop solutions,Apply ML and other statistical approaches to generate insights on structured and unstructured data,Provide ML expertise in the design and delivery of data products for broad consumption by business partners,Mentor and support the training of technical and non-technical teams in data science and machine learning,Participate in the broader data science community to stay current with methodology, software, and data development and availability,Communicate and visualize the output of analyses, including both written and verbal communication, to business leaders and non-technical audiences,Conceptualize and deploy data science solutions for business questions", "Identify innovative ways to leverage large datasets to solve industry challenges,Design and implement data analysis, data mining, research, analysis, predictive modeling strategies and best practices,Use analytical expertise to spot key business insights, trends, and opportunities in our vast database,Help interpret and communicate your findings in a way that is easily understood by those less familiar with data,Collaborate with a dynamic Product and Development team in delivering data products and insights to clients,Establish and maintain relationships with distributed cross-functional teams to leverage domain and industry expertise,Identify missing and inaccurate data in our platforms; seek out supplements and replacements to improve our data sets to the greatest extent possible", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "Envision new network (engineering) AI/ML projects and present well-formed ideas to the team. Utilize considerable creativity to merge ideation, knowledge of AI/ML model uses, insights from SMEs/translators and constraints to propose ML projects that are meaningful and actionable.,Provide timelines, milestones, project plans and types of AI/ML models to be attempted for new prototype projects. Adapt and communicate needed changes as datasets, labels or models may not function as expected.,Partner with team and non-team data scientists to teach complex concepts, assess project feasibility, select input features and validate project output. Envision and test for corner cases.,Utilize large amounts of GPU effectively to train and attempt multiple models, documenting progress.,Guide AI/ML projects with high autonomy so that they will be accepted by the business and SMEs who will use them.,Pull sample data sets from NS tools and VGRID to attempt prototypes on new topics.,Learn the wireless domain (5G/LTE, xLPT datasets, RF Planning, Orchestration, etc) and toolsets to better communicate and understand the needs of engineers for AI/ML automation.,Publish blogs, create documentation, perform presentations, submit intellectual property write-ups and lead efforts for external publications.,Bachelor\u2019s degree in Electrical Engineering, Computer Science or four or more years of work experience.,Six years of relevant work experience.,A Degree.,Masters/PhD in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science, or a related field.,Broad experience in training multiple types of AI/ML models such as KNN, and XGBoost, CNN/RNN, reinforcement learning, deep learning models, GANs, etc.,Knowledge of Relational databases in SQL or R.,Previous experience in wireless networking is a plus.,Recognized as a contributor to Software Organized Networks (SON) platforms.,Experience in advanced AI/ML such as ensembles, deep learning, reinforcement learning, and NLP.,Four or more trained models moved to production or completed in a research environment.,Experience with Spark and Big Data deployments.,Attached examples of filing IP, Published Papers or Conference Presentation titles to your resume.", "Collecting and combining data from multiple sources,Uncovering and exploring anomalous data (including metadata),Applying the scientific process to data evaluation, performing statistical inference, and data mining,Developing analytic plans, engineer supporting algorithms, and design and implement solutions which execute analytic plans.,Designing and developing tools and techniques for analysis,Analyzing data using mathematical/statistical methods,Evaluating, documenting, and communicating research processes, analyses, and results to customers, peers, and leadership,Completed a degree program in the fields of mathematics, statistics, computer science, computational sciences, or a passion for rigorous analysis of data,Tenacity, integrity, persistence, and willingness to learn,Ability to solve complex problems,Use critical thinking and reasoning to make analytic determinations,Works effectively in a collaborative environment,Strong communications skills to both technical and non-technical audiences", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Knowledgeable with Data Science tools and frameworks (i.e. Python, Scikit, NLTK, Numpy, Pandas, TensorFlow, Keras, R, Spark).,Basic knowledge of machine learning techniques (i.e. classification, regression, and clustering).,Understand machine learning principles (training, validation, etc.),Knowledge of data query and data processing tools (i.e. SQL),Computer science fundamentals: data structures, algorithms, performance complexity, and implications of computer architecture on software performance (e.g., I/O and memory tuning).,Software engineering fundamentals: version control systems (i.e. Git, Github) and workflows, and ability to write production-ready code.,Mathematics fundamentals: linear algebra, calculus, probability,Interest in reading academic papers and trying to implement state-of-the-art experimental systems,Experience using deep learning architectures,Experience deploying highly scalable software supporting millions or more users,Experience with integrating applications and platforms with cloud technologies (i.e. AWS and GCP)", "Defining and calculating measures of inequity for students within a school schedule,Building recommendations for course placement in a schedule,Proposing and prototyping algorithms for supporting team teaching best practices,Developing metrics and visualizations for student tracking,Build underlying systems that power our data-driven products (e.g., recommendation engines, constraint solvers, and predictive models),Consult directly with school leaders to work on complex problems within Abl's product using your research and rapid prototyping skills to push new features into production,Perform data profiling, complex sampling, and statistical modeling,Design and develop tailored data models for K12 schools,Identify incomplete data, improve the quality of data, and integrate data from several data sources,Work on the challenge of combining data from across schools and districts, who all store things differently, so that we can measure our impact in aggregate,Determine how to evaluate equity, or fairness, for students and for teachers,Propose metrics for evaluating the overall quality of a schedule and methods for comparing multiple schedules' ability to meet school leader priorities,Find trends and insights in complex, human-generated school data", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Curious: You enjoy peeling apart a problem and examining the interrelationships between data and how they can be used to form a solution.,Creative: You try new approaches to solving problems, looking for ways to apply techniques across domains and in new contexts.,Practical: You explore theories with an eye to their real world applications and their potential for improving performance for clients.,Collaborative: You work well on a team and understand how to lead a group of bright, technical coworkers. You can explain your work to your peers and source ideas and improvements from them.,Determined: You enjoy the challenge of finding solutions to difficult problems, testing them to discover what is successful and what should be optimized further, to develop industry-leading tools. You learn from failure and trying again.,Organized: You know how to keep track of both high-level goals and the tasks required to meet those goals. You take personal ownership of your team\u2019s success.", "Develop algorithms on large-scale datasets using deep learning, machine learning, statistical modeling, and data mining techniques,Implement scalable and efficient modeling algorithms that can work with large-scale data in production systems,Collaborate with product management and engineering groups to develop new products and features,Build customer segmentation, personalized recommendation, automated chat bots, image recognition, etc.,Communicate and present your reasoning, methodology and results to peers and leadership team,Continuously seek for opportunities to learn and improve knowledge, skills and processes.,PhD or MS degree in Computer Science, Electrical Engineering, Statistics, Applied Math, Econometrics, Operations Research, or other related fields.,Deep understanding of statistical modeling, machine learning, or data mining concepts, and a track record of solving problems with these methods.,Proficient in one or more machine learning or statistical modeling tools such as ScikitLearn, Tensorflow, and R,Strong analytical and quantitative problem solving ability.,Excellent communication skills with both technical and business people, able to speak at a level appropriate for the audience.,Strong autonomy and team player", "Independently lead data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.,Lead the design and development of machine learning/statistical models and ensure best performance.,Work closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.,Assist engagement with key business stakeholders in discussion on business strategies and opportunities.,Build strong working relationship and develop deep partnership with the business.,Be a subject matter expert on machine learning and predictive modeling.,Bachelor\u2019s degree or four or more years of work experience.,Master\u2019s degree in a quantitative field or relevant.,A Ph.D. in statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science or a relevant field.,Five or more years of experience in practicing machine learning and data science in business.,Strong foundational quantitative knowledge and skills; extensive training in math, statistics, physical science, engineering, or other relevant fields.,Experience in leading data science projects and delivering from end to end.,Strong technical experience in machine learning and statistical modeling.,Strong on computing/programming skills; proficiency in Python, R, and Linux shell script.,Experience in data management and data analysis in relational database and in Hadoop.,Strong communication and interpersonal skills.,Excellent problem solving and critical thinking capabilities.,Experience with NLP and chatbot technology.,Experience with Hadoop, Spark, C++, scala, or Java.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Bring Creativity to Data products.,Analyze transaction data.,Build models to predict possible finance operation inconsistencies.,Use machine learning methods to perform transaction matching (matching algorithms).,Cleanse raw data for use in models.,Perform data munging, data mining, clustering & classification methods, pattern recognition.,Comfortable with Statistics, Calculus & Multivariate analysis.,You will work closely with a small R&D team with Product Managers, Data Engineers & Analysts. The R&D team will experiment with the data and come up with POC's.,Familiar with SQL, Python, R, Spark MLib, AWS, SQL Server, Redshift.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Research, design and prototype robust and scalable models based on machine learning, data mining, and statistical modeling to answer key business problems,Build tools and support structures needed to analyze data, perform elements of data cleaning, feature selection and feature engineering and organize experiments in conjunction with best practices,Work with development teams & business groups to ensure models can be implemented as part of a delivered solution replicable across many clients,Present findings to stakeholders to drive improvements and solutions from concept through to delivery,Keep abreast of the latest developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Apply statistical techniques to complex data sets to understand our users across geographies and their engagement across our platforms.,Identify, analyze, and interpret trends or patterns and provide ongoing reports and data visualisations, in order to make recommendations to the wider business.,Work with internal stakeholders and external business partners to ensure accurate collection and logging of data needed to provide insights on user engagement and help drive business decisions.,Analyze performance of marketing campaigns and customer referral programs. Build data pipelines that import traffic data on a daily basis.,Deploy machine learning models for email targeting, content recommendation algorithms, delivery & targeting infrastructure, and the in-product notification experience across iOS, Android and Web.", "Georgia", "Research and define statistical algorithms that surface interesting and actionable insights into user\u2019s data,Guide a co-located engineering team through implementation and validation of new algorithms and analysis methods,Design elegant solutions that impact our clients' business,3+ years' experience with statistical algorithms and predictive modelling,Expertise in time series analysis", "Work with Stakeholders throughout the organization to identify opportunities for leveraging company and third-part reference data to drive business solutions.,Mine, analyze, and implement data driven solutions focused on deep insights, improving efficiency and creating value.,Performs deep data analysis and data processing in support of reporting/analytic solutions and Artificial Intelligence/Machine Learning.,Utilizes predictive modeling and machine learning approaches to derive actionable insights using state-of-the-art statistical methods.,Enhances data collection procedures to include information that is relevant for building analytic solutions. Processing, cleansing, and verifying the integrity of data used for analysis.,Leads and supports the internal set-up, configuration, data integration, and quality control of data platforms including data warehouses, operational data stores, data lakes and more.,Leverages master data management solutions to continuously improve data quality.,Works closely with business leaders to develop and automate KPIs that lead and monitor our business.,Conducts ad-hoc analysis, builds interactive dashboards that provide business insights, creates business value and presents results in a clear manner.,Extends Bioclinica\u2019s data with third-party sources of information when needed.,Collaborates with stakeholders to ensure data collection supports future business goals.,Develops processes and tools to monitor and analyze model performance and data outcomes.,Ensures data completeness and accuracy through developing and implementing data quality checks. Assesses the effectiveness, quality and accuracy of data sources and data platforms.,Creates and maintains data integration documentation across projects being supported including ongoing changes or amendments to data integration specifications.,Supports and provides solutions or new ideas for tool development that can enhance the operational efficiency of the data department and initiates, creates and drives awareness around new processes that are implemented.,Communicates effectively with team, stakeholders, and other operations personnel on the successful execution of data integration deliverables.,Attends external Client meetings to provide data integration support, as required.,Coordinates multiple sources of data for file transformation, processing, and proper documentation and quality control across assigned projects.,Supports operational issues and troubleshooting directly impacted by data related aspects of the system.,Communicates effectively within the project team and escalates issues to ensure timely delivery.,Maintain expertise with Bioclinica applications and integration solutions and associated data.,Assist internal and external users with troubleshooting and implementing new integrations.,Reading, understanding and adhering to organizational Standard Operating Procedures (\u201cSOP\u201d).,Assisting in establishing and enforcing departmental standards.,Working with internal staff to resolve issues.,Exploring new opportunities to add value to organization and departmental processes.,Helping others to achieve results.,Performing other duties as assigned.,Attending and participating in applicable company-sponsored and industry training.", "Bachelor\u2019s degree with 2+ years of experience or recent Master\u2019s degree graduate in a Data Science related field.,Ability to apply probability, algebra, Bayesian inference, frequentist statistics, machine learning, reinforcement learning, and optimization techniques to solve business objectives.,Entry level proficiency in predictive modeling, including comprehension of theory, modeling/identification strategies, and limitations and pitfalls.,Entry level proficiency in machine learning algorithms and concepts.,Demonstrates basic computational skills and level of experience using statistical programming languages R or Python.,Entry level proficiency in SQL & NoSQL querying and Python coding to wrangle and explore big data.,Entry level proficiency in visualization tools, such as ggplot2, plotly, or Tableau to explore big data.,Strong ability to conduct meta-analysis literature reviews.,Experience with probabilistic graphical models, Bayesian networks, deep learning, reinforcement learning, time series, or active learning is a plus.,Experience with cloud computing platforms, such as Open Stack, Google Compute Engine, or EC2 is a plus.,Experience with big data platforms, such as Neo4j, Spark, Big Query, Hadoop, Azure, or AWS is a plus.,Experience with Google Cloud Platform is a plus.,Experience with the CRISP-DM process is a plus.", "Empathetic to the analytic needs of product owners, engineers, and analysts, our candidate can translate requests to formulate hypotheses and develop models to support their needs.,A self-directed individual who won\u2019t wait for a pristine dataset, but instead is comfortable wrangling their own data and can identify necessary data sources to improve results.,Respectful of established protocols but you also drive change as our business evolves over time.,Proactively engage with various stakeholders and engineers and build strong relationships,You agree that data science is fun! Our new data scientist shares this enthusiasm and it is reflected in his/her work.,Analyze customer data from product warehouses, Success programs, Support, Finance, and Sales ops, and, in partnership with customer-savvy business leaders and their teams, identify the signals of health and the current deficiencies.,Design, build, and manage the necessary models to classify and improve customer health. A key challenge will be distilling dozens and hundreds of possible signs into something simple that allows wise and efficient action to be taken.,Broker data discussions across different business units, where each may have a different set of signals and/or visions of customer health. Work always to find the simplest solution that will serve customers the best, and be willing to set aside perfection when it doesn\u2019t best serve us.,Regularly report on key health metrics to leadership (Directors, VPs, C-suite), and optimize the presentation and process to make it as effective and efficient as possible.,Support business leaders with ad-hoc data delving that will substantiate or disprove health-related hunches, and create work-lists of at-risk customers to drive high-urgency solutions.,Use customer data, cross-referenced against customer success programs, to identify whether (and how much) our offerings are helping our customers. Queue-up data to both celebrate our success, and learn from our gaps and deficiencies.,Track, prioritize, and project manage any additional reporting/data based projects,Self-motivated and driven by the purpose outlined above. Eager to be a leader at Zendesk in understanding what customer health means, and willing to go deep and learn from co-workers and other CX companies about this aspect of the industry.,Demonstrated ability to produce on-time and quality data deliverables, with special skill for visualization and/or process design that enables the recipient to achieve their business objective quickly (and enjoyably!),Well-established personal processes to handle multiple requests from varied stakeholders, in a way that maintains clear priority, adjusts when needed, and is always helpful and pleasant.,Strong communication skills, both written and oral, and an ability to listen and understand root needs across many different business units, each with their own perspectives on our customer experience.,Advanced SQL skills are a must. Deep proficiency in Python, Scala, or R. Familiar with BigQuery and cloud environments a plus,5+ years experience in advanced analytics with at least 2+ years in a data science role", "Be the data expert in a team innovating on data-driven features,Build a deep knowledge of the technical assessment aspect of hiring,Build & curate a prioritized catalog of all data opportunities in our product offering,Guide product roadmap and strategy,Have company-wide impact through data-driven decisions and insights,Drive well-defined data experiments; evaluate results,Bring selected models to production; test and evaluate,4+ years experience as a data scientist, with a proven record of deploying your models in production,Experience framing data problems clearly and relating them to business needs. You are a data scientist who can relate to our product use cases and frame problems and solutions in the context of these use cases.,Experience with a variety of modeling approaches and model evaluation techniques,Good judgment on the types of problems that are feasible to solve and have the largest ROI,Experience working with Subject Matter Experts; in this case, recruiters and hiring managers to improve model precision,100% employee coverage for Medical, Dental, and Vision; with 70% coverage for dependents,Employee stock options, 401k, flexible work hours and time off, commuter benefits,Monthly wellness classes, free lunches, snacks and endless supply of Philz Coffee", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "analyzing the information content in an initial corpus to identify informational elements,structuring logical forms to represent information appropriately,establishing goals for the system and evaluating how well the system meets those goals,Create knowledge extraction system,Develop computational models of language that is readable and appropriate for the domain,Prepare training and testing data,Work with other researchers to understand and integrate with the overall system", "Analyzes Data \u2013 Integrates data from multiple sources and supports/performs complex statistical data analysis including but not limited to multivariate statistical analyses, multilevel modeling, dyadic analysis and/or coarsened exact matching analysis.,Designs \u2013 Supports retrospective outcomes research and analysis by designing and monitoring quasi-experimental and other study designs.,Obtains Data \u2013 aggregates and manages data through all aspect of the cycle; analysis, design, coding, engineering, testing, debugging, documentation, research and development, maintenance, new development, operations and delivery. Leverages various forms of data from clinical EMRs and fully adjudicated medical claims data using SQL/Hadoop/Hive/Tableau/SAS/R/Python or other applications as applicable.,Supports Operations - Ensures the consistent implementation of data generation and collection by all ACM staff as assigned.,Collaborates \u2013 Works with partner organizations in the development, testing and implementation of data marts/views facilitating more efficient and timely measurement.,Visualizes Data \u2013 Creates publication-quality graphical and narrative representations of outcomes analysis performed.,Bachelor's Level Degree.,One year of experience preferred in database ETL operations and/or statistical computer programming.", "Build end-to-end data pipelines, consuming from a variety of sources,Work with data consumers to identify self-service opportunities, and make sure they have access to the data they need to make decisions.,Help guide the team towards data best practices, and mentor teammates.,Contribute to a technical roadmap for the CustOps data program, and set priorities for work across the team,Build and refine processes to make pipeline implementations faster, simpler, and less error-prone for product engineers,Participate in architectural decisions across the organization, particularly with respect to the impact on data collection,Set timeliness and correctness goals for our data pipelines, maintain sufficient monitoring & coverage to ensure that incidents and outages are mitigated appropriately", "Passionate about building a product with a positive impact on the world,5+ years professional experience as a software engineer or data engineer,Strong working knowledge of Python, Docker, Airflow (or a similar data workflow tool), and cloud computing concepts,Willing to learn new tools, languages, and patterns as needed to build a great product,A solid communicator who enjoys collaborating with other engineers, designers, PMs, and scientists", "Build and manage Kafka based streaming data pipelines,Build and manage Airflow based ETLs,Monitor and improve performance of the pipelines,Develop storage layer (relational / non-relational) for hosting data for analytical queries", "Design and engineer solutions that meets business requirements, cyber security practices, and compliance regulations * Provide security guidance in compliance with NIST 800-171r2 and Cyber Maturity Model Certification (CMMC) * Asses technical designs and identify security design gaps in existing and proposed architectures,Identify and communicate current and emerging security threats,Identify risks and provide guidance regarding remediation of gaps to facilitate a hardened and sustainable solutions,Take ownership of solutions, assignments, actions items and issues, and remain accountable for their completion,Work effectively with other team members, customers and key stakeholders and foster team success,Communicate and collaborate with leadership and technical teams to include systems and network administrators, security engineers, and IT Support teams,Security and risk assessments for services, applications, hardware and systems", "Getting hands-on experience with Google Cloud Platform and technology/languages such as BigQuery, Scala, Scio, Luigi, Styx and Docker,Understand what fuels many of Spotify's product features such as Discover Weekly, Daily Mix, Podcast offerings, holiday campaigns and others,Work hand-in-hand with the data science community to understand various user or content trends that influence product changes and customer acquisition strategies,Collaboration on a global scale; our squad offers ongoing opportunities to work in Stockholm with other engineering colleagues,Innovate our data products to create a single coherent platform with sources of truth that serve a plethora of product and data stakeholders,Communicate insights and recommendations to key stakeholders, engineering, data science and product partners,Work in a supportive team that offers engineers the flexibility to be creative and chase interesting ideas,Work closely with the product manager, end-users and stakeholders to understand, document, troubleshoot and analyze requirements for complex data solutions,Lead and mentor engineers as we grow the bigger team,A BS/MS in CS or any other relevant fields of study,You have 5+ years of experience in the development of high-quality database and data solutions.,Strong analytical and problem solving ability,Have worked in a team with both Data Engineers and Data Scientists,You are capable of tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions,You are a self-motivated individual contributor and great teammate with the ability to multitask, prioritize and communicate progress in a rapidly changing environment.,Would like to build skills to further enhance the t-shape within analytics and data engineering,Strong coding skills in preferably Scala, Java and Python,Strong communication and data presentation skills (such as Tableau, PowerPoint, Qlik, etc.),Experience performing analysis with large datasets in a cloud based-environment, preferably with an understanding of Google\u2019s Cloud Platform,You are a communicative person that values building strong relationships with colleagues and multiple stakeholders, and have the ability to explain complex topics in simple terms", "You get people. You have a unique blend of skills in developing deep consumer insights and competitive intelligence through data that drives product innovation to create the right experiences for people\u2019s daily lives that achieve our goals to acquire, engage, and retain them.,You get data. You have a thirst for knowledge and insight. You thrive and strive to present data in ways that product, design, engineering, marketing, and executive teams understand and act upon. Your data is 100% accurate and credible. Your reports are always clear and actionable.,You get growth. You are a consumer-focused, data-driven, and growth-enabling analyst who has supported Growth strategies, roadmaps, scrums, and final product rollouts, across the analytics/insights, acquisition/referrals, activation/onboarding, and adoption/retention loop.,You get mobile/digital. You have significant industry experience \u2013 and a strong understanding of the mobile/digital ecosystem \u2013 from apps to advertising and analytics. You have successfully applied the latest mobile/digital tools to help drive reach, retention, and revenue growth.,Understand the marketplace trends and help answer revenue trends,Analyze supply as well as demand patterns and find revenue opportunities, explain model behaviors, suggest improvements etc.,Gain insights on what drives performance in terms of reach and revenue growth,Create dashboards and reports that provide analysis and commentary, explaining product, sales, and business trends for Executive reporting,Work closely with product and inform and update stakeholders on product performance, plans, and progress towards metrics,Define data testing plans and create methodologies that help teams to iterate fast and release new features for testing and, if successful, rollout to all users globally,Generate and go deep on consumer insights and competitive intelligence to help teams drive product innovation and iteration,Build strong partnerships with product, sales, engineering, and marketing teams and enable them to launch new Growth initiatives for testing/iteration,Provide feedback to product, sales and engineering teams on impact of product launches: target launch metrics, A|B testing, post-launch metrics,Investigate data and monitor data quality \u2013 partner closely with and provide requirements to the Data Engineering teams that can be clearly acted upon,BS/MS in highly-quantitative field (Analytics, Computer Science, Mathematics) is preferred,Data analysis, generating insights for consumer-focused products,B2B or advertising experience is a must,Experience with big data technologies such as Hive, Hadoop, MapReduce, Spark, PIG etc.,Experience with scripting programming languages such as Perl/Python is good to have,Familiarity with Unix/Linux environment highly recommended,Significant experience, proficiency in, and passion for Mobile and/or Web products,Track record of proactively establishing and following through on commitments,Demonstrated use of analytics, metrics, and benchmarking to drive decisions,Excellent interpersonal, organizational, creative, and communications skills,Team player in driving growth results combined with a positive attitude,Strong work ethic and strong core values (honesty, integrity, creativity)", "Design, construct, test, optimize, and deploy solutions.,Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space.,Possess exceptional analytical, conceptual, and problem-solving abilities.,Have experience with both traditional (e.g., MSBI) and modern (e.g., cloud) data architecture.,Collaborate as a part of a team to develop Cloud Data and Analytics solutions.,Assist in designing multi-phased cloud data strategies, including designing multi-phased implementation roadmaps.,Analyze, architect, design and actively develop cloud data warehouse, data lakes and other cloud-based data solutions.,Participate in development of cloud data warehouses and business intelligence solutions.,Exemplify strong focus on back end data integration.,Demonstrate strong focus on the design and development of data warehouses.,Highly self-motivated to deliver both independently and with strong team collaboration.,Ability to creatively take on new challenges and work outside comfort zone.,Solid written and oral communications along with presentation and interpersonal skills.,Comfortable conducting and supporting white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations.,Quantitative background with 3+ years of experience applying data architecture or engineering to solve real-world business problems.,4+ years of data engineering and/or data warehousing experience.,2+ years of building cloud data solutions (AWS, Azure, GCP, Snowflake).,Experience with:,Relational and dimensional database structures, theories, principles, and practices.,Manipulating / mining data from database tables (SQL Server, Redshift, Oracle).,SQL, ETL / ELT optimization, and analytics tools including R, HiveQL, and Python.,Big data application development and/or with cloud data warehousing (e.g. Hadoop, Spark, Redshift, Snowflake, Azure, SQL DW, BigQuery)..,Solution architecture on cloud platforms such as AWS, Azure, an GCP.,Cloud SDKs and programmatic access services.,Practical knowledge of data visualization tools (e.g., Tableau, Power BI) a plus.,Expert programming skills in Python and a software development background.,Experience writing \"infrastructure as code\" deployments e.g. ARM, CloudFormation, Terraform.,Job function\nInformation Technology", "You have good problem-solving skills, a tendency towards simple and effective solutions, and a \u201cgetting things done\u201d mentality.,Analytical thinking, troubleshooting skills, attention to detail.,You are a reliable, trustworthy person that keeps their promises.,Interest in keeping yourself up to date and learning new technologies.,Product-oriented mindset and eagerness to take part in shaping the products we build.,Ability to work autonomously in a fully distributed team.,Good communication skills in verbal and written English.,BS degree in Computer Science or similar technical field.,2+ years of software engineering experience.,Solid understanding of modern back-end systems, microservice architecture, message-driven solutions, distributed processing, and replication.,1+ years of experience with relational databases (Postgres, MariaDB, Oracle).,Background in building data processing pipelines.,Understanding of data streaming concepts and technologies such as Pulsar, Kafka, RabbitMQ, or similar.,Familiarity with Agile methodology, test-driven development, containerization, continuous integration/deployment, cloud environments, and monitoring.,Ability to write clean, efficient, maintainable, and well-tested code; Golang/Python skills are a plus.", "Houston, TX", "Houston, TX", "Design, build and support data processing pipelines and APIs,Ensure that code adheres to defined standards and best practices for performance, speed, scalability, and quality,Practice Agile development methods and exemplify core Agile values of transparency, collaboration, acceptance of change, and iterative development,Routinely deliver working software solutions that meet user story acceptance criteria,Bachelor\u2019s degree in Computer Science or four or more years of work experience.,Four or more years of relevant work experience.,Four or more years of software engineering experience, including Java, Python, and/or related languages.,Two or more years of experience with relational database technologies (Postgres, MySql, etc) including SQL,A degree.,Four or more years of experience leading large-scale projects.,Four or more years of experience in networking, multi-threaded applications, inter-process communication, and complex software development.,Experience in building high-performance, highly-available and scalable distributed systems.,ETL or Data cleansing & normalizing,Big Data, data analysis, log mining, and automated reporting,PostGIS experience,Experience designing, building and managing Internet-scale APIs.,Experience mentoring and growing software engineers.,Ability and willingness to learn new technologies quickly.,Ability to work in a highly collaborative Agile team,Demonstrated aptitude and desire to learn new skills", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Architect a multi-tier data pipeline to feed data into an OLTP application in addition to an analytics environment,Design and build schemas to handle large scale interactive reporting,Design and build ETL/ELT process to move data through the data processing pipeline.,Manage complex data dependencies across datasets and incremental data loading workflows.,Be a fearless leader in championing smart design,Love scaling systems,Must have firm understanding of database systems \u2013 Data modeling, SQL q uery Processing and Transactions Know how to scale systems and make them fast!,Experience debugging and tracing SQL performance issues,Solid understanding of software development from design and architecture to production.,Large scale DW, MPP, Redshift, or similar technologies,Solid math skills.,T he ability to present impromptu and via a whiteboard,Experience with the Hadoop ecosystem (HBase/Hive/map reduce),Experience working in AWS,Knowledge of Tableau,Experience with Redshift or ParAccel/Actian Matrix,Experience with Pentaho Kettle,Linux basics,Big, Big bonus if you know a programming language or two.,Competitive compensation (salary, equity and bonuses) and comprehensive benefits designed to foster work-life balance, care for your health, protect your finances, and help you save and invest for the future.,Generous paid time away from work including vacation, holidays, sick time, and 2 days of paid time off each year to serve and learn through TiVo Community Outreach.", "you will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other big data technologies,you will design and model data structures to help analyzing our business and technical data,you will support existing processes running in production,you will work together with people from other key areas to assist with data-related technical issues and support their data infrastructure needs", "Help relayr build the world's most advanced IoT Cloud Platform.,Design, develop and improve microservices and data processing pipelines in our analytics/machine learning backend using the latest technologies.,Be customer focused and translate business requirements into technological solutions.,Take part in architecture and code reviews to develop solutions that are simple, functional and sustainable.,Collaborate with other teams to define new features and continuously improve internal systems.,Work with QA and DevOps in the development, testing and deployment of services.,Work together with data scientists to design large-scale machine learning systems.,You have 3+ years of experience in backend engineering.,You have strong knowledge of Java and/or Python and are eager to learn new skills.,You have a good understanding of designing and scaling distributed systems.,You have experience working with SQL and/or NoSQL databases, messaging queues (e.g. Kafka) and large scale data processing (e.g. Apache Spark).,You are able to work in a structured manner and care about your contribution end to end.,You have a very good command of the English language.,Flexible office environment and a modern office located in Central Tower of Munich with a relaxation room, standing-desks, and free fruits and drinks,Relayrians come from all over the world, speak 20+ languages (working language is English), and welcome people of all ages and parental statuses. Our customers are as diverse as we are - join us to connect with a network of companies and people from around the globe,An extensive on-boarding period so you can get up to speed with the rest of the team,A learning environment where you can build upon your skills and interests, share knowledge, and attend events and conferences pertaining to your discipline. We also offer free German classes at all levels,Have some fun with regular team lunches and offsites, a yearly company summit, and our branded goodies,Competitive salary,We fully support your move to Munich with a relocation budget and visa assistance. We'll help you settle into your exciting new city!,25 paid vacation days + public holidays,Choose between a Mac, Windows, or a Linux machine", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Diligence, perseverance, and proactiveness are key to succeed in this position.,Lead structure and implementation of metrics tracking systems, working directly with the Sr Director of Growth.,Utilize Amplitude, Python, Tableau, Looker and SQL to pull data and provide key analytical insights.,Report, monitor, track, and analyze traffic, revenue, and other KPIs.,Identify product risks/opportunities and communicate them succinctly and effectively.,Help product team set up A/B tests and perform statistical analysis on results to provide actionable insights.,Build statistical models to predict interactions and segment users based on behaviour.,Provide ad-hoc analytics support as needed, ranging from helping teams develop the question through tracking and implementation to analytics and insights.,Collaborate with the Business Intelligence team on building shareable data tools, maintain documentation, and participate in deep-dives to understand drivers of success.,Collaborate with multiple cross-functional teams and work on solutions which have a larger impact on Xapo business.,7 years\u2019 experience in growth marketing, data analytics, or a related field;,Proven experience of using successfully the analytic platform Amplitude.,In-depth understanding of data structures and algorithms.,Proven experience of having worked for a mobile app with large growth.,Experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data.,Strong analytical and communication skills.,Experience developing, maintaining, automating, visualizing, analyzing, and communicating reporting to management.,Bachelor\u2019s Degree in computer science or a related field is preferred.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Enable data scientists to train and deploy machine learning algorithms at scale, in fault-tolerant, highly-available systems,Use best practices in continuous integration and delivery with Docker and Kubernetes.,Work closely with application engineers to build data products that power Carta\u2019s core applications,Create data pipelines using batch and streaming tools like Airflow, Spark, Kafka, and Google Pub/Sub,Uphold our engineering standards and bring consistency to the many codebases and processes you will encounter", "U.S. Citizenship - Must be able to obtain a security clearance,Bachelors Degree in Computer Science, Computer Engineering, Electrical Engineering or related field,Experience with Java, Kotlin, or Scala,Experience with scripting languages (Python, Bash, etc.),Experience with object-oriented software development,Experience working within a UNIX/Linux environment,Experience working with a message-driven architecture (JMS, Kafka, Kinesis, SNS/SQS, etc.),Ability to determine the right tool or technology for the task at hand,Works well in a team environment,Strong communication skills,Experience with massively parallel processing systems like Spark or Hadoop,Familiarity with data pipeline orchestration tools (Apache Airflow, Apache NiFi, Apache Oozie, etc.),Familiarity in the AWS ecosystem of services (EMR, EKS, RDS, Kinesis, EC2, Lambda, CloudWatch),Experience working with recommendation engines (Spark MLlib, Apache Mahout, etc.),Experience building custom machine learning models with TensorFlow,Experience with natural language processing tools and techniques,Experience with Kubernetes and/or Docker container environment,Ability to identify external data specifications for common data representations,Experience building monitoring and alerting mechanisms for data pipelines", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Interface with software stakeholders to understand infrastructure and user requirements.,Develop and support Etiometry's data-warehouse solution both for clinical personnel and internal research staff.,Build, test and deploy software and database upgrades into a production environment.,Utilize and improve Etiometry\u2019s clinical data cleaning tools and techniques, which include data extraction, de-identification, and clinical measure normalization & cleaning.,Design and implement data requirements for company research.", "Conducts or oversees business-specific projects by applying deep expertise in subject area; promoting adherence to all procedures and policies; developing work plans to meet business priorities and deadlines; determining and carrying out processes and methodologies; coordinating and delegating resources to accomplish organizational goals; partnering internally and externally to make effective business decisions; solving complex problems; escalating issues or risks, as appropriate; monitoring progress and results; recognizing and capitalizing on improvement opportunities; evaluating recommendations made; and influencing the completion of project tasks by others.,Practices self-leadership and promotes learning in others by building relationships with cross-functional stakeholders; communicating information and providing advice to drive projects forward; influencing team members within assigned unit; listening and responding to, seeking, and addressing performance feedback; adapting to competing demands and new responsibilities; providing feedback to others, including upward feedback to leadership and mentoring junior team members; creating and executing plans to capitalize on strengths and improve opportunity areas; and adapting to and learning from change, difficulties, and feedback.,As part of the IT Engineering job family, this position is responsible for leveraging DEVOPS, and both Waterfall and Agile practices, to design, develop, and deliver resilient, secure, multi-channel, high-volume, high-transaction, on/off-premise, cloud-based solutions.,Provides consultation and expert technical advice on IT infrastructure planning, engineering, and architecture for assigned systems by assessing the implications of IT strategies on infrastructure capabilities.,Provides some recommendations and input on options, risks, costs, and benefits for systems designs.,Leverages partnerships with IT teams and key business partners to troubleshoot complex systems.,Serves as a functional expert and collaborates with architects and software engineers to ensure functional specifications are converted into flexible, scalable, and maintainable system designs.,Translates business requirements, and functional and non-functional requirements, into technical specifications that support integrated and sustainable designs for complex or high impact infrastructure systems by partnering with Business Analysts to understand business needs and functional specifications.,Ensures system designs adhere to company architecture standards.,Builds partnerships with counterparts in various IT Teams (e.g., database, operations, technical support) throughout system development and implementation.,Serves as a technical expert for project teams throughout the implementation and maintenance of assigned enterprise infrastructure systems by defining and overseeing the documentation of detailed standards (e.g., guidelines, processes, procedures) for the introduction and maintenance of services.,Mentors other technical resources throughout infrastructure systems development.,Reviews and validates technical specifications and documentation for complex or multi-dimensional solutions.,Leads the development and modification of solutions by identifying technical solutions to business problems.,Collaborates with business leaders, Solutions, and lead enterprise architects to review business drivers, and establish a foundation for enterprise systems planning.,Reviews benchmarking results, and provides information to support current and future infrastructure needs and projects to IT leadership. Provides preliminary conclusions.,Benchmarks and evaluates IT trends and technologies to identify opportunities and considerations that impact ROI.,Makes recommendations on resources required to maintain service levels and meet new demands.,Guides and drives physical architecture design for new initiatives. khgrsr\nMinimum Qualifications:", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL -S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL -S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "Collaboratively architect, build, launch and maintain new Social Graph components that enhance profiles, increase coverage and edge accuracy.,Create, maintain, and scale data pipelines for and between data ingesters, the Social Graph, machine learning predictors, client deliverables, and data warehousing.,Interact cross-functionally with a wide variety of people and teams. Work closely with client services leads and data scientists to identify opportunities and assess improvements to Applecart products and deliverables.,Integrate systems for monitoring of streaming and batch data processing (e.g. DataDog, Nagios). Track data quality and consistency.,Evangelize solid coding practices (e.g. unit & integration testing, code reviews, continuous deployment, automated linting, staging environments), and mentor junior engineers.,Contribute to the architectural designs and decision making around data stores, schemas, data security and cloud storage.,Rapidly prototype proof-of-concept data pipelines for ROI determination, then replace them with modular productionized versions.,Keep abreast of industry trends, best practices, and emerging methodologies.,Support quality assurance as a part of the engineering process and collaboration with product managers such as producing sampled outputs, supporting KPIs, outlining PR limitations and future improvements.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Maintaining ETL program and DB/DW\nAdding new tables/indices\nExperience in Airflow preferred\nMonitoring for CVEs\nUpdating and troubleshooting existing code,Adding new tables/indices,Experience in Airflow preferred,Monitoring for CVEs,Updating and troubleshooting existing code,Applying Dimensional Modeling principals to integrate various data sources with existing data (APIs, flat files, databases, etc),Data warehousing techniques that ensure the architecture will support the requirements of the business,Building data pipelines (Testing, deploying, and validating code),In-depth knowledge of database design principles (SQL and NoSQL),Basic knowledge of analyst and ML principles is required,Ability to recommend and implement ways to improve data reliability, efficiency and quality,Lead development and software engineering efforts of web applications,Coordinating with developers on data projects,Coordinate and assist in deployment of ML models,Attend design/architecture meetings and assist in project projections and hours,Communicate improvement ideas, needs, and/or concerns,Update projects daily on project management software,Update hours for projects daily or hourly on time tracking software,Update progress and project percentage completion,Complete projects on time and within time budgets,Other duties as assigned,Python/SQL (2+ Years of experience)\nMultithreading/parallel processing (experience preferred),Multithreading/parallel processing (experience preferred),Airflow or other ETL tools (experience required),Docker (experience required)", "Design and build data structures on MPP platform like AWS RedShift and or Druid.io.,Design and build highly scalable data pipelines using AWS tools like Glue (Spark based), Data Pipeline, Lambda.,Translate complex business requirements into scalable technical solutions.,Strong understanding of analytics needs.,Collaborate with the team on building dashboards, using Self-Service tools like Apache Superset or Tableau, and data analysis to support business.,Collaborate with multiple cross-functional teams and work on solutions which have a larger impact on Xapo business.,In-depth understanding of data structures and algorithms.,Experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data.,Experience in designing and developing ETL data pipelines.,Proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs.,Programming experience in building high-quality software. Skills with Python or Scala preferred.,Strong analytical and communication skills.,Work/project experience with big data and advanced programming languages.,Experience using Java, Spark, Hive, Oozie, Kafka, and Map Reduce.,Work experience with AWS tools to process data (Glue, Pipeline, Kinesis, Lambda, etc).", "Focus on designing, building, and launching efficient and reliable data infrastructure to scale and compute for our business,Help us build a world class data lake/data warehouse, by building data pipelines,Design and develop new systems and tools to enable folks to consume and understand data faster,Use your expert coding skills across a number of languages from Python, Java, C++, Go etc.,Work across multiple teams in high visibility roles and own the solution end-to-end,Design, build and launch new data extraction, transformation and loading processes in production,Work with data infrastructure to triage infra issues and drive to resolution.,BS or MS degree in Computer Science or a related technical field,Familiarity with Python,Familiarity with Hadoop stack, Spark, AWS Glue, AWS Athena etc,Diverse data storage technologies (RDBMS, Sql Server, Mysql, ElasticSearch, dynamodb, s3 etc.),Deep familiarity with schemas, metadata catalogs etc.,Ability to manage and communicate data warehouse plans to internal clients", "Use an analytical, data-driven approach to drive a deep understanding of our business.,Build data pipelines and data models that will empower engineers and analysts to make data-driven decisions,Build data models to deliver insightful analytics,Deliver the highest standard in data integrity,Strong analytical skills with ability to analyze and project sales, subscriber, and engagement data. Performs competitive analysis, reviews industry information for current trends and opportunities. Works closely with analytics teams to develop comprehensive analytical reports to enable data-driven decisions to increase engagement and conversions of target customer segments.", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL \u2013S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL \u2013S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "Alpharetta, GA,Bachelor's,Hadoop: 3 years", "Build and maintain a data pipeline for machine learning.,Assist with the development of a data warehouse on which reports are derived.,Process 8 billion event transactions each month.,Assure data is captured and stored without loss.,Write code to provide reports to business and data science.,Write a system that will run reports on a configurable schedule.,Respond to ad-hoc requests for information.,4-6 years of experience with Python or Java,Three or more years of experience developing and operating data engineering solutions in the cloud (preferably with AWS),Three or more years working with distributed big data systems (e.g. Hadoop, Redshift),Professional experience building data science systems with experience building out data pipelines and ETL processes for machine learning,Experience working remotely", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Houston, TX", "A fun and collaborative team environment,Autonomy and resources to get the job done,Weekly paid team lunches,Agile development process,Increase the capabilities of Reporting and Advanced Analytics platforms to support business insights for internal and external users,Maintain Data Integrity by identifying the root causes of problems and deploying solutions,Improve accuracy, timeliness, and efficiency in measurement of Healthcare Quality metrics,Develop and Improve infrastructure for novel Fraud, Waste, and Abuse detection and screening methods.,3+ years of experience in data transformation and data management using SQL; including Stored Procedures, ETL processes, and tuning for performance.,Skilled with integrating data from single-tenant data stores, third party data feeds, and APIs,Supporting operational needs with Business Intelligence tools like Tableau,Experience with Salesforce CRM and Salesforce APIs,Experience with data management technologies including data warehouses, data lakes, data marts, and NoSQL data stores,Nice to have: Experience working with cloud technologies (AWS),Nice to have: Experience in Healthcare IT,Flexibility and collaborative work approach to solve complex problems and promote standardization,Collaborate with business owners and application development team to align business objectives and prioritize work.,Excellent verbal and written communication skills,Ability to work in a structured, governed, methodical approach,A self-starter who delivers consistently high quality work,Strong problem-solving and analytical skills", "Excellent communication skills (verbal and written),Empathy for their colleagues and their clients,Signs of initiative and ability to drive things forward,Understanding of the overall problem being solved and what flows into it,Ability to create and implement data engineering solutions using modern software engineering practices,Ability to scale up from \u201claptop-scale\u201d to \u201ccluster scale\u201d problems, in terms of both infrastructure and problem structure and technique,Ability to deliver tangible value very rapidly, working with diverse teams of varying backgrounds,Ability to codify best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases,A pragmatic approach to software and technology decisions as well as prioritization and delivery,Ability to handle multiple workstreams and prioritize accordingly,Commitment to delivering value and helping clients succeed,Ability to tailor your language to a technical or a non-technical audience,Comfort working with both collocated and distributed team members across time zones,Comfort working with and developing coding standards,Ability to codify best practices for future reuse in the form of accessible, reusable patterns, templates, and codebases,Willingness to travel as required for cases (0 up to 40%),A technical background in computer science, data science, machine learning, artificial intelligence, statistics or other quantitative and computational science,A compelling track record of designing and deploying large scale technical solutions, which deliver tangible, ongoing value\nDirect experience having built and deployed complex production systems that implement modern data science methods at scale and do so robustly\nComfort in environments where large projects are time-boxed and therefore consequential design decisions may need to be made and acted upon rapidly\nFluency with cluster computing environments and their associated technologies, and a deep understanding of how to balance computational considerations with theoretical properties of potential solutions\nAbility to context-switch, to provide support to dispersed teams which may need an \u201cexpert hacker\u201d to unblock an especially challenging technical obstacle\nDemonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value\nAn \u2018engineering\u2019 mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impact; recognizing that the \u2018good\u2019 is not the enemy of the \u2018perfect\u2019\nComfort with working with distributed teams on code-based deliverables, using version control systems and code reviews,Direct experience having built and deployed complex production systems that implement modern data science methods at scale and do so robustly,Comfort in environments where large projects are time-boxed and therefore consequential design decisions may need to be made and acted upon rapidly,Fluency with cluster computing environments and their associated technologies, and a deep understanding of how to balance computational considerations with theoretical properties of potential solutions,Ability to context-switch, to provide support to dispersed teams which may need an \u201cexpert hacker\u201d to unblock an especially challenging technical obstacle,Demonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value,An \u2018engineering\u2019 mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impact; recognizing that the \u2018good\u2019 is not the enemy of the \u2018perfect\u2019,Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews,Demonstrated expertise working with and maintaining open source data analysis platforms, including but not limited to:\nPandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools\nSpark (Scala and PySpark), HDFS, Hive, Kafka and other high-volume data tools\nRelational databases such as SQL Server, Oracle, Postgres\nNoSQL storage tools, such as MongoDB, Cassandra, Neo4j and ElasticSearch,Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools,Spark (Scala and PySpark), HDFS, Hive, Kafka and other high-volume data tools,Relational databases such as SQL Server, Oracle, Postgres,NoSQL storage tools, such as MongoDB, Cassandra, Neo4j and ElasticSearch", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "M.S. in Computer Science, Informatics, Mathematics, Electronic/Electrical Engineering or other relevant field with an emphasis on data analytics.,Experience with big data technologies such as Hadoop, Apache Spark, NoSQL databases.,Strong computer science grounding, with knowledge of data structures, algorithms and computer architectures.,Proficiency developing in one or more languages such as C++, Python or Java.,Self-starting, requiring minimal supervision with strong problem-solving skills.,Excellent communication and teamwork skills.,Hands-on experience with Cloud environments, such as AWS, Google Cloud, or Azure", "You will build and operate large scale data infrastructure in production (performance, reliability, monitoring),Designing, implementing and debugging distributed systems,Yu will think through long-term impacts of key design decisions and handling failure scenarios,Building self-service platforms to power WeWork\u2019s Technology,You are focused on team over individual achievements.,You create software incrementally and make consistent progress.,You love to learn. mentor and teach others.,You are empathetic and build long-lasting relationship characteristic of highly efficient teams.,You keep up-to-date with the latest developments in the field.", "Help us create AI / ML ready datasets from Petabytes of raw data and meta-data,Automate integration of different data-sources into a coherent flow/ data pipelines (support also data normalization and result calculation),Develop and build systems and architectures for ETLs,Perform system & data testing,Understand and apply FAIR data principles,Strong adherence to compliance & regulatory environments,Build algorithms to", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build scalable backend data applications to support the growing needs of the business,Build data pipelines that collect, process, and compute business metrics from marketplace activity,Leverage best practices in continuous integration and delivery.,Collaborate with other engineers,Expertise in building data or training pipelines using Spark or Hadoop,Experience in building production grade API to expose results,Extensive programming experience in Python/Scala/Java,A BS or MS in Computer Science or other related technical fields,At least 3 years experience in production environment,Experience and interest in ML,Qualified applicants receive a few questions to answer,Selected candidates will be invited to schedule a 30 minute intro call with our CTO,Next, Candidates will be invited to schedule a behavioral interview with the CEO,Next, candidates will be invited to schedule a technical interview with our CTO. The technical interview can be split between a technical discussion and a technical test.,Next, candidates will be invited to review the technical test with our CTO and other software engineers.,Candidates will then be invited to schedule an additional interview with (CEO, CTO, COO),Successful candidates will subsequently be made an offer via email.", "Design, implement and support robust, scalable solutions to enhance business analysis capabilities, identify gaps and design processes to fill them.,Work with analysts to understand business priorities and translate requirements into data models.,Collaborate with various stakeholders across the company like data developers, analysts, data science, finance, etc, in order to deliver team tasks.,Build complex multi-cloud ETL pipelines in Apache Airflow.,Build Python API integrations with 3rd party vendors.", "Houston, TX", "Collect, analyze and share data to help product teams drive improvement in key business metrics and customer experience,Propose and prioritize changes to reporting and create additional metrics and processes based on program changes and customer requirements,Work closely with Alexa program teams to create ad-hoc reports to support timely business decisions and project work,Identify and implement new capabilities and best practices to develop and improve automated data analysis processes", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "In-depth knowledge of Python, pandas and its open-source ecosystem, with a focus on parallelized processing,,Experience in configuring and working with relational databases in the terabyte scale, e.g. MS SQL Server,,Several years of experience working on and designing data-intensive applications and/or pipelines,,Commitment to ensuring data quality and integrity of complex systems within a time-sensitive environment,,Experience with OS-independent, cross-platform development,,Passion to work effectively in interdisciplinary teams of technical and non-technical individuals with different cultural backgrounds on health-related (business) problems,,Business proficiency in English and German,Passion for using and contributing to the open-source data science ecosystem,,Ability to evaluate the business value of different technical projects and prioritize appropriately,", "Houston, TX", "5+ Years professional experience with Python,Python data structures and best practices,AWS and Linux (Ubuntu/CentOS) , Bash scripting,Professional experience with a SQL-based database, such as MySQL,Writes organized code with appropriate exception handling and logging,Understanding of HTTP network requests and responses,Understanding of HTML and JSON formats,Ability to write technical documentation and comment code,Ability to write test suites and set up automation environments,An undergraduate degree or advanced degree in Statistics, Computer Science or a related field,Experience with scrapy, beautiful soup, requests, selenium,Amazon Redshift/ Hadoop,Prior experience as a front-end, back-end, or full-stack web developer,Javascript / NodeJS", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL \u2013S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL \u2013S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL \u2013S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL \u2013S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Collaborate across development teams to understand customer's business objectives and data requirements.,Apply repeatable processes, established knowledge, and expert judgment to design, build and operate data and analytics solutions across the financial and insurance value chain.,Help customers overcome data related business challenges by modelling, integrating, profiling, and cleaning data to provide enterprise information that is accurate, consistent, managed, trusted and understood.,Leverage technology to automate common maintenance tasks and provide insights into the health and wellness of the infrastructure platform.,6+ years of experience with data solution delivery and implementations,Ability to identify issues and provide real time solutions for them,Experience with engineering best practices which includes analyzing, designing, developing, deploying, and supporting data solutions and/or infrastructure implementations/upgrades.,Advanced skills and knowledge related to Data Modelling, Data Integration/ETL, Data workload utilization tools,Hands-on experience designing, implementing, and supporting ETL data solutions with the Informatica PowerCenter or Power Exchange ETL tools,Strong SQL knowledge including writing and review complex SQL statements and performance tuning to handle scale, agility, and changes,Experience with database technologies including relational databases such as DB2 or Sybase.,Experienced working in an agile developing environment leveraging BDD/TDD and whole team approaches within a fully-dedicated agile team", "Coordination and execution of the needed processes and governance procedures to accurately define and ensure data quality,Main point of contact for data scientists regarding data preparation,Preparation of data for analytical purposes,Support of data owner for meta data management,Responsible for definition, utilization and monitoring of data quality management mechanisms/tools,Measurement and monitoring of data KPIs,Taking data movement and transformation steps from the prototype status into production,Preparation of reports and presentations for senior staff and relevant stakeholders that will give insights for business decision making,Completed academic studies of information systems, mathematics, machine learning or other technical/scientific studies, or comparable several years of experience in this field,Previous work experience in Data Engineering,English and German language fluently,Strong experience in underlying foundational data, data warehouses, and business intelligence systems,Experience in relational and unstructured data environments (e.g. SQL, noSQL, Hadoop, Hive, Spark),Experience with SQL server is a plus,Understanding of advanced analytic techniques and having gathered first experiences to effectively collaborate with data scientists,Analytical thinking,Self-driven and fast learner, eager to work with new technologies", "Build large-scale and real-time systems to provide FindHotel\u2019s analysts, data scientists, and decision makers with high-quality low latency data.,Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.,Design and implement data models to help our analysts tracking user behaviour across our product,Build ETL and Business Intelligence environment and dashboards used widely by the product teams,When needed improve the way that data is delivered & visualised, better way of presenting multi-dimensional data,Use best practices in continuous integration and delivery.,Help drive optimization, testing and tooling to improve data quality.,Collaborate with other Software Engineers, Data Engineers and stakeholders, taking learning and leadership opportunities that will arise.", "3+ years of experience with:\nScala or Python, both preferred\nDistributed systems (e.g. Spark, Hadoop),Scala or Python, both preferred,Distributed systems (e.g. Spark, Hadoop),Database systems (e.g. Postgres, MySQL),Experience with the following is preferred:\nIP (v4/v6) allocation and addressing conventions\nDNS conventions and best practices\nAnti-abuse investigations,IP (v4/v6) allocation and addressing conventions,DNS conventions and best practices,Anti-abuse investigations,Bachelor\u2019s degree (CS, CE/EE, Math, or Statistics preferred),Comfortable working as part of a distributed team,Excellent communication and teamwork skills,Ability to make data driven decisions,Ability to do independent research,Phone conversation with a Talent Acquisition team member to learn more about your experience and career objectives. 30 minutes.,Technical interview with hiring manager via video (preferred). Will include some coding. 30-45 minutes.,1-2 technical interviews with data engineer and data science team members via video or in person. 45 minutes each.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build analytical solutions to enable Data Scientist by manipulating large data sets and integrating diverse data sources.,Work closely with the data scientists, and database and systems administrators to create data solutions.,Bachelor\u2019s degree or four or more years of work experience.,Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Splunk etc.).", "building products to help non-technical users extract insights,building applications that leverage ML to solve problems,data automation and product development,installing database systems and writing queries,Ability to communicate with business stakeholders & translate business needs into data requirements, tables & data visualization,Experience with statistical, scripting or programming packages (SQL, Python, R, Matlab),great benefits and competitive salary,bonus", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL \u2013S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL \u2013S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Support and improve existing experimentation analytics pipelines and systems in production,Design and develop new data pipelines and streaming processes that are highly available, scalable, and reliable,Maintain and evolve dimensional data models and schema designs to improve performance efficiency,Scale the self-serve experimentation platform to drive product innovations in over 190 countries,BS/MS in Computer Science, Computer Engineering or related technical discipline,4+ years experience building and maintaining large scale analytics systems,4+ years programming experience in Scala, Java or Python,Experience working with Hadoop, Spark or ElasticSearch,Experience with NoSQL or RDBMS platforms - DynamoDB, Redis, or Postgres,Experience with streaming platforms - Apache Kafka/Flink/Beam or Aws Kinesis,Strong understanding of architecting, developing, and maintaining cloud technologies, architecting (especially in Amazon Web Services),The hustle of a startup with the impact of a global business,Tremendous opportunity to solve some of the industry's most exciting problems,Working with an extraordinary team of smart, creative, fun and highly motivated people,Comprehensive health coverage, competitive salary, 401(k) match and meaningful equity,Unlimited vacation and flexible working hours,Daily catered lunches, endless supply of refreshments, basketball court, fitness classes and social events", "Analysis, design, coding, performance tuning, and implementation of new BI solutions.,Evaluation, maintenance, and enhancement of existing ETL processes and model/cube structures.,Creation of BI dashboards and workbooks using Power BI, Tableau, and Excel.,Code data ingest solutions (C#, NodeJS) leveraging 3rd party APIs.,Daily triage of bugs and tasks.,Lead a team of data warehousing engineers, acting as liaison to internal and 3rd party clients and providing regular updates to management.,Create documentation for applications and processes.,Enforce best practices in all aspects of work and evaluate new data warehousing platforms and solutions.,5+ years\u2019 experience in a data warehouse team environment.,Must have Tabular data warehouse design experience including: a solid understanding of MDX/DAX, calculated measures, hierarchies, slowly changing dimensions, and date/time dimension analysis.,Advanced level of experience with Microsoft SQL Server, SSAS and SSMS (Management Studio),Hands-on experience with SSIS or other similar ETL tools,Demonstrated strength with T-SQL for database queries, procedures, tables, views, etc.,Proven skills with data visualization tools: Microsoft Excel (PowerPivot, Pivot Tables, Slicers) Power BI and Tableau (dashboard design, configuration and advanced calculations),Ability to work with .Net development tool Visual Studio 2017 to write code, make changes, and deploy updates.,Experience with storage devices and connectivity tools such as AWS development tools: NodeJS and Lambda Functions.,A basic understanding of HTML, CSS, and web page deployment.,Excellent written and verbal skills and the ability to present to upper management.,A good understanding of basketball.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Participating in the development and maintenance of the analytics data platform for the whole company.,Evolving the platform to make it more robust and scalable.,Supporting the data warehouse, data lake and ETL jobs to ensure that the data is available to the consumers in a reliable, trustworthy and predictable manner.,Creating self-serving tools for data ingestion, quality tracking and consumption.,Closely collaborating and supporting BI analysts, data scientists and other consumers of data.,Ability to understand business requirements and translate them into technical solutions.,Hands-on experience in building large-scale, distributed data platforms in the cloud.,Experience with Apache Spark, Hadoop, Hive, AWS Kinesis, Kafka or similar distributed data processing tool chain.,Programming experience in analyzing and manipulating data - preferably in Python, Scala or Java.,Deep knowledge of SQL queries and traditional data warehousing and data modeling fundamentals. Experience in working with data warehouses like Redshift, Snowflake or BigQuery is an advantage.,Familiarity with stream and batch data processing patterns,Experience with investigating and fixing data quality issues,Willingness to learn new technologies,Great communication skills for proactively communicating with stakeholders,Fluent in English, both speaking and writing (English is our office work language),Exciting technical challenges to work on and great everyday opportunities to learn, grow and develop.,A diverse team of over 40 nationalities.,Free lunches, yoga, German lessons and more.", "Help relayr build the world's most advanced IoT Cloud Platform.,Design, develop and improve microservices and data processing pipelines in our analytics/machine learning backend using the latest technologies.,Be customer focused and translate business requirements into technological solutions.,Take part in architecture and code reviews to develop solutions that are simple, functional and sustainable.,Collaborate with other teams to define new features and continuously improve internal systems.,Work with QA and DevOps in the development, testing and deployment of services.,Work together with data scientists to design large-scale machine learning systems.,You have 3+ years of experience in backend engineering,You have strong knowledge of Python or Java,You have experience developing microservice architecture,You have a good understanding of designing and scaling distributed systems,You have experience working with SQL and/or NoSQL databases, messaging queues (e.g. Kafka) and large scale data processing (e.g. Apache Spark),You are able to work in a structured manner and care about your contribution end to end,You have a very good command of the English language,Flexibility and safety are important to us! As a company we were quick to react when the Corona pandemic began, sending all our employees to work from home. If you start out as a new employee at relayr, you will be working from home, enjoy a structured digital onboarding programme and flexible working hours.,Modern office located in Central Tower of Munich with a relaxation room, standing-desks, and free fruits and drinks,Relayrians come from all over the world, speak 20+ languages (working language is English), and welcome people of all ages and parental statuses. Our customers are as diverse as we are - join us to connect with a network of companies and people from around the globe,An extensive on-boarding period so you can get up to speed with the rest of the team,A learning environment where you can build upon your skills and interests, share knowledge, and attend events and conferences pertaining to your discipline. We also offer free German classes at all levels,Have some fun with regular team lunches and offsites, a yearly company summit, and our branded goodies,Competitive salary,We fully support your move to Munich with a relocation budget and visa assistance. We'll help you settle into your exciting new city!,25 paid vacation days + public holidays,Discounted Urban Sports Club membership,Choose between a Mac, Windows, or a Linux machine", "In-depth knowledge of Python, pandas and its open-source ecosystem, with a focus on parallelized processing,,Experience in configuring and working with relational databases in the terabyte scale, e.g. MS SQL Server,,Several years of experience working on and designing data-intensive applications and/or pipelines,,Commitment to ensuring data quality and integrity of complex systems within a time-sensitive environment,,Experience with OS-independent, cross-platform development,,Passion to work effectively in interdisciplinary teams of technical and non-technical individuals with different cultural backgrounds on health-related (business) problems,,Business proficiency in English and German,Passion for using and contributing to the open-source data science ecosystem,,Ability to evaluate the business value of different technical projects and prioritize appropriately,", "Create data tools for Data Science team members that assist them in building and optimizing our product.,Assemble large datasets that meet requirements set by the Data Science team, including creating web crawlers.,Be proactive in bringing forth new ideas and solutions to problems,Be a strong team player and share knowledge freely and easily with your co-workers,Write software for post-processing and cleaning of the data, taking part in data analysis if required,Automate manual processes, optimize data delivery, improve architecture for greater scalability,Work on integration of Data Science components into our larger systems,Handle mid-size and large datasets (200GB+)", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Houston, TX", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "not only looking for the next gig for your CV but for a permanent occupation in an interesting and continuously changing environment,a good fit for our culture and also you will add value to it,an expert in Java development and a proficient Linux user. Although you have worked with various technologies in the past, your resume points to the most relevant ones and not all of them.,in open and honest communication,that agile has more to do with values and mindset than with roles and processes,that high-performing teams are usually built from diversity, agile collaboration principles, and application of best practices from software development,work on one of our international and cross-functional agile teams,create data pipelines in Hadoop environments using for example Spark, Java, and AWS services,integrate data science and statistical solutions with huge amounts of data,maintain and enhance our existing code base,a permanent position with a competitive salary, and flexible working conditions,a good work life balance, 32 days of vacation, and parent-friendly regulations,support with relocation, visa sponsorship, and language courses", "Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources,Design and implement data pipelines to prepare, process and organize data in the warehouse,Design and implement high performing BI dashboard integrations to leading BI & Visualization tools working with BI developers,Design and implement high performing data pipelines feeding downstream systems,Troubleshooting, problem solving and performance tuning of data pipelines and queries accessing data warehouse,Creation of best practices and standards for data pipelining and integration with Snowflake data warehouses,Ensure enterprise security and access control policies are adhered to in the solution,Creation of architecture and design artifacts and documents,Conduct design and code reviews,Work with Business Analysts and Users to translate functional specifications into technical requirements and designs.", "Design, develop and maintain New Knowledge\u2019s data pipeline,Support and monitor pipeline performance in production,Take ownership of components of the data pipeline,Work with product managers understand upcoming work and design a system capable of meeting long term product vision,Create and maintain documentation capable of describing how the pipeline work to non-technical audience,Work as part of a team to integrate new services into New Knowledge\u2019s data pipeline using tools like Docker, Kubernetes, Kafka, PostgreSQL and more.,3+ year(s) developing software as part of a team, preferably working on some aspect of a data pipeline,Experience or familiarity with Kafka or similar distributed systems (knowledge of schema registry/data types and serialization options and partition strategies a plus),Experience or familiarity with popular stream processing framework such as Spark Streaming, Kafka Streams, Flink or similar,Experience supporting large scale batch analytics in Hadoop ecosystem (loading and retrieving data),Experience working with or building schedulers, workflow automation/coordination tools,Experience implementing tests and sanity checks on large complex data pipelines,Experience helping other developers write performant SQL queries.,Ability to monitor current solution to understand its limits and stay ahead of business needs,Comfortable representing data engineering function in front of Senior leaders and product management during prioritization and design discussions,Knowledge of how to evaluate tools and ability to document pro/cons of infrastructure decisions,Desire to mentor junior developers,3+ years professional software development,You have experience working with data pipelines,Experience processing social media, text and image data is particularly relevant,Highly motivated to research, prototype and implement state of the art data pipeline,Comfortable with knowing what you do not know and asking for help or finding your own answers when required,Previously held leadership positions technical or otherwise,Competitive salary, 401(k) matching, and Fortune 500 level healthcare,We are a business that trusts and embraces technology and harnesses it for good. We embrace diverse ideas, autonomy and collaboration,Professional development opportunities\u2013\u2013 we host lunch and learns, hold weekly 1-1\u2019s, and have a policy where you can expense professional development books,A diverse leadership team that wants to uphold ethical practices in our software development process,A strong commitment to creating a diverse environment,Free parking in our building in downtown Austin,Free access to a gym in our office building,A $2,000 annual credit that you can spend on the technology and work gear of your choice,Parental leave (plan with your manager) and unlimited vacation (and no, that is not code for you never take a vacation, we encourage and value time off)", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL -S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL -S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Architect and develop a framework to automate ingestion and integration of structured data from a wide variety of enterprise data sources,Architect and develop data pipeline components and integrate them with the Formation Platform,Scale pipelines to meet performance requirements,Provide leadership to senior and junior Data Engineers,Architect and design data quality monitoring and automated data cleaning,Responsible for technical direction of the team,8+ years of software development experience,5+ years of experience with building scalable and reliable data pipelines using Big Data engine technologies like Spark, AWS EMR, Redshift, etc.,3+ years of experience with scalable data integration technologies like ETL or Data Virtualization,Experience with cloud technologies such as AWS or Google Cloud,SaaS experience,Significant industrial experience using Scala, or willingness to learn Scala and demonstrated competence with functional programming", "Develop and implement custom data capturing solutions for advanced video analytics in a cloud environment.,Participate in developing requirements for and the implementation of necessary analytics development in order to properly and accurately report on relevant KPIs,Provide guidance for data integrity issues, ongoing maintenance, and data governance,Maintain detailed documentation of data collection methods and dependencies,Work closely with developers and product management team to validate and maintain analytics and reporting,Work with key stakeholders to integrate analytics requirements, and updates into production cycles,Technical validation of digital analytics implementations, conduct audits and troubleshoot tracking gaps to maintain data confidence and implement data standardization practices,Provide general troubleshooting assistance for analytics tracking and break fix,Performing functional, regression, integration, smoke and acceptance tests,Work with application development team to ensure proper deployment, integration, performance, and compliance with architectural standards and best practices,Successfully implement process improvements impacting own work and work of others.,3-5 years of experience building with modern JS libraries and frameworks: React / Redux, Typescript, ES6,Experience with real-time streaming Big Data analytics implementation,Excellent understanding of the implementation of click stream and video analytics,Very good understanding of the technical details regarding the implementing data collection & analytics solutions like Adobe Analytics, Google analytics, Snowplow, Alooma, Kafka. AWS Kinesis, etc.,Experience building, maintaining, testing and debugging modern technology stacks,Solving large-application/user-level problems, performance, scalability, etc.,Keen attention to detail and thoroughness required,BA or BS or higher degree in math, engineering, computer science, or related field required,Experienced working in a fast-paced, high-tech environment (preferably software development) and comfortable navigating conflicting priorities and ambiguous problems", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Houston, TX", "Create robust data workflows that move and transform data at scale to inform key metrics, recommend changes, and predict future results,Own the architecture of Dots' data infrastructure, including exploring/implementing new big-data solutions and optimizing existing ones,Manage data applications on AWS,Design and implement complex big-data solutions with a focus on collecting, parsing, and surfacing meaningful data using multiple platforms,Evaluate new technologies and products", "Business Intelligence development,Design and maintain a set of automated tools and reports,Scraping software to extract key data,2+ years' experience programming with a statistical computer language (ideally R),Strong understanding of databases and data storage,Knowledge of the SDLC,Experience developing software applications (ideally in Python),Excellent communication and problem solving skills,Collaborative team player and self-starter", "Developing data infrastructure that ingest and transforms data from different sources and customers at scale.,Creating machine/deep learning infrastructure that generalizes across hundreds of thousands of Salesforce customers, but is expressive enough to generate high lift.,Partner end-to-end with Product Managers and Data Scientists to understand customer requirements and design prototypes and bring ideas to production,Working with internal product teams to ingest their data and sprinkle machine/deep learning fairy dust on their products.,Participating in meal conversations with your team members about really important topics, such as: Should the cuteness of panda bears be a factor in their survivability? Is love a decision tree or a regression model? How far ahead would society be today if we had 12 fingers instead of 10?,We develop real products. You need to be an expert in coding, including Java and Object-Oriented Programming. We also use Scala and Functional Programming principles. \u200b,We prioritize professional industry experience; advanced degrees alone do not replace real world experience.,We have massive scale. You need to have experience in distributed, scalable systems. Consistency / availability tradeoffs are made here. You\u2019ve tinkered with modern data storage, messaging, and processing tools (Kafka, Spark, Hadoop, Cassandra, etc.) and demonstrated experience designing and coding in big-data components such as HBase, DynamoDB, or similar.,We\u2019re a growing, diverse team and we work together on projects. We love to collaborate and help each other, and we want someone to share that ideology.,You have to be a very quick learner - we face new challenges every day, anything that ranges between the operating model of a financial services companies, conversation model for chatbots, tinkering with convolutional and recurrent networks, to how to make Spark work with the S3 file system. No school could prepare you for all of these, so you need to be very quick on your feet.,\u200bSelf-starter who can see the big picture, and prioritize their work to make the largest impact on the business\u2019 and customer\u2019s vision and requirements,Excellent communication, leadership, and collaboration skills,We run on AWS. We dockerize applications. You should have some notion of how to build, test, and deploy code to run on cloud infrastructure.,Experience with open source tools for information retrieval (e.g. Solr),Search, Data Scoring/ Ranking expertise,Data visualization,Experience with Deep Learning for NLP,Experience developing in open-source machine-learning libraries such as Apache Mahout or MLLib", "Houston, TX", "Houston, TX", "Create and maintain optimal data pipeline architecture,,Assemble large, complex data sets that meet functional / non-functional business requirements.,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.,Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS \u2018big data\u2019 technologies.,Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.,Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.,Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.,Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "5+ years experience with complex data structures.,Real-world experience developing in .Net Core,You have deep experience in the latest libraries and programming techniques.,You have accomplishments that showcase your capabilities by their success and technical depth.,You own new features from idea to completion.,Work well with a core team to design and execute major new features.,Familiar with SQL/NoSQL databases like MongoDB and their declarative query languages is a plus,Enjoy contributing to a fast moving exciting project,Strong experience using GitHub in a professional environment,Strong communicator and fluent in English with excellent written and verbal communication skills.,Thrive and excel in our diverse, distributed and agile team environment", "Houston, TX", "Own Babylist\u2019s data science and analytics infrastructure from DevOps to development,Architect and implement data pipelines and systems for analytics, data science, and ML use cases,Bring engineering rigor and best practices to the art of data management,Mentor analysts and data scientists on the team,Bachelor's or advanced degree in Computer Science or Engineering,Hands-on experience writing and deploying production grade code,Solid experience with an object-oriented scripting language like Python and advanced SQL,Worked with data orchestration tools like Airflow, Luigi, dbt,Strong understanding of data modeling/ETL principles and modern data warehousing systems like Redshift, Snowflake,DevOps in disguise: well-versed with the AWS ecosystem including managing and deploying cloud data resources (EMR, EC2, RDS, S3, Athena, Lambda, Spark),Experience integrating data from multiple sources including third-party APIs preferred", "Houston, TX", "Welcoming and friendly environment,Unlimited opportunity to develop your skills and expertise,Be a part of multinational and international teams,Work we do here is challenging, interesting, and meaningful,Customers from all over the world,Solid benefits package \u2013 health insurance, clear guidelines on the career growth, paid overtime, strong compensation package,Continuous investments in employee\u2019s future \u2013 career counselling, wide range of trainings, and industry recognized certificates,You will be part of a team working on projects related to Big Data technologies. You will work with world leaders in various industires, like TV&Entertainment, Telecommunication, Finances, Insurance, etc.,As a data engineer you will work with Big Data systems like Hadoop, Spark, Storm, etc., and design data processing applications.,You will get a unique possibility to learn programming languages, e.g. Scala, Python, Java and utilize newest technologies, like noSQL doing data engineering at Accenture.,In a final together we will do visualization of the data and will build visual analytics systems to explain our customers the data in a fancy manner.,Data-related job is profession of future. Accenture brings future to today \u2013 we are searching for our new colleagues in Big Data field.", "Houston, TX", "Consult as part of a team that is in charge of building end-to-end digital transformation capabilities and lead fast moving development teams using Agile methodologies.,Design and build real-time analytics solutions using industry standard technologies and work with data architects to make sure Cloud Data solutions align with technology direction.,Lead by example, role-modeling best practices for unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting and incident response.,Keep everyone from individual contributors to top executives in the loop about progress, communicating across organizations and levels. If critical issues block progress, refer them up the chain of command to be resolved in a timely manner.,Develop Cloud Native architecture, data supply chain, Architect, Prototype and Test end to end data supply chain, use cases that drive business value, and provide architecture support to the data scientists.,Pinpoint and clarify key issues that need action, lead the response and articulate results clearly in actionable form.,Show a strong aptitude for carrying out solutions and translating objectives into a scalable solution that meets end customers\u2019 needs within deadlines.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "You thrive in any data environment:\nCloud \u2013 we use AWS\nSomeone who understands complex data and the challenges of accessing it\nA real bottom-line person, not someone who throws terms like \u201cbig data\u201d around because it\u2019s popular\nHadoop or traditional/relational databases make no difference to you,Cloud \u2013 we use AWS,Someone who understands complex data and the challenges of accessing it\nA real bottom-line person, not someone who throws terms like \u201cbig data\u201d around because it\u2019s popular,A real bottom-line person, not someone who throws terms like \u201cbig data\u201d around because it\u2019s popular,Hadoop or traditional/relational databases make no difference to you,You dream in code:\nSQL (seriously, SQL \u2013 not just SQL queries; impress us). Teach us something new, show us what you\u2019ve got\nPython\nScala\nSpark,SQL (seriously, SQL \u2013 not just SQL queries; impress us). Teach us something new, show us what you\u2019ve got,Python,Scala,Spark,You know databases inside and out:\nDatabase concepts \u2013 indexes, execution engines, etc\nDatabase Administration experience (Redshift, MySQL/PostgreSQL, Oracle)\nYou understand that databases are an integral part of being a Data Engineer,Database concepts \u2013 indexes, execution engines, etc,Database Administration experience (Redshift, MySQL/PostgreSQL, Oracle),You understand that databases are an integral part of being a Data Engineer,You enjoy looking at and solving big picture problems:\nNo micromanaging or hand-holding - you like to ask questions and devise a complete solution\nYou want to understand the data (not only the pipes) and you can definitely perform some analytics and build dashboards because you like it. Yes really, because you do.,No micromanaging or hand-holding - you like to ask questions and devise a complete solution,You want to understand the data (not only the pipes) and you can definitely perform some analytics and build dashboards because you like it. Yes really, because you do.,You love learning new things:\nYou know that you don\u2019t know enough, and it bothers you that there isn\u2019t enough time in the day to learn about the next topic.\nYou\u2019re up-to-date on new trends in data \u2013 you know who\u2019s using what to solve various problems and are excited for the next release of your favorite tool\nIf you like being thrown in the deep end of the pool, this team\u2019s for you.,You know that you don\u2019t know enough, and it bothers you that there isn\u2019t enough time in the day to learn about the next topic.,You\u2019re up-to-date on new trends in data \u2013 you know who\u2019s using what to solve various problems and are excited for the next release of your favorite tool,If you like being thrown in the deep end of the pool, this team\u2019s for you.,You take it personally:\nYou don\u2019t sleep well at night when you leave work with a question unanswered\nYou feel accountable for everything you do and that sense of urgency has been driving you your entire life,You don\u2019t sleep well at night when you leave work with a question unanswered,You feel accountable for everything you do and that sense of urgency has been driving you your entire life,You work hard but play harder:\nYou like to have a good time while getting things done\nWhen we say a \u201cteam player\u201d we mean it - you have a crisp high-five and funny stories to tell\nYou have your team\u2019s back. And the team has yours.\nSense of humor is a must-ash. (Mustache\u2026 Get it? Mustache.),You like to have a good time while getting things done,When we say a \u201cteam player\u201d we mean it - you have a crisp high-five and funny stories to tell,You have your team\u2019s back. And the team has yours.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Hands-on, production experience with the Hadoop family of big data technologies (Hive, Impala, HBase, etc.).,Collaboration with business partners to craft and iterate on solutions that extract value from data.,Experience with Spark, Python, and Java.,Strong analytical skills and a fervor for data integrity and accessibility.", "Competitive base salary,Profit sharing or equity, based on experience,100%-covered Health insurance for employees, 75%-covered dependents,Four weeks PTO,401(k) with employer matching,Personalized monthly perks and matched charitable donations", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Produce clean, standards based, modern code with an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented.,Demonstrate an understanding of technology and digital frameworks in the context of data integration.,Ensure code and design quality through the execution of test plans and assist in development of standards, methodology and repeatable processes, working closely with internal and external design, business, and technical counterparts.,Utilizes existing methods and procedures to create designs within the proposed solution to solve business problems.,Understands the strategic direction set by senior management as it relates to team goals.,Contributes to design of solution, executes development of design, and seeks guidance on complex technical challenges where necessary.,Primary upward interaction is with direct supervisor.,May interact with peers, client counterparts and/or management levels within Accenture.,Understands methods and procedures on new assignments and executes deliverables with guidance as needed.,May interact with peers and/or management levels at a client and/or within Accenture.,Determines methods and procedures on new assignments with guidance.,Decisions often impact the team in which they reside.,Manages small teams and/or work efforts (if in an individual contributor role) at a client or within Accenture.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Houston, TX", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Java,Python,Oracle,Redis,RabbitMQ,Docker,CloverETL,Amazon Web Services (Aurora, Redshift, DynamoDB, S3, Lambda, Kinesis, ElastiCache),Have a BA/BS in Computer Science, Engineering, Information Systems or equivalent experience,Self-starter with 3+ years of experience as a Data Engineer dealing with large complex data scenarios,Proven ability to work with varied data infrastructures - including relational databases, column stores, NoSQL databases and file-based storage solutions,Experience with both compiled and scripting languages,Expert level SQL skills,Excellent communication (verbal and written) and interpersonal skills and an ability to effectively communicate with both business and technical teams,Write great code, view it as a craft, and love cranking out solid work,Competitive compensation and generous employee benefits package including fully paid Medical, Dental & Vision plans for employees & their dependents!,Stock options for all employees!,We bring toys to the office but still think the most fun thing to do is build product,Collaborative work environment that fosters bonds beyond the workplace,We treat our people well with employee recognition programs and referral bonuses,Better than average team building activities such as paintball, go cart racing, shuffleboard, organized sporting teams and off-site retreats.,Provide lunches, drinks and snacks so our team can be hungry for other things,Learn from and teach each other at CrowdTwist U", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Produce clean, standards based, modern code with an emphasis on advocacy toward end-users to produce high quality software designs that are well-documented.,Demonstrate an understanding of technology and digital frameworks in the context of data integration", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Extend our product data architecture foundation (Revise existing ontologies and their usage in existing product),Selecting features, building and optimizing classifiers using machine learning techniques,Processing, cleansing, and verifying the integrity of data used for analysis,Deliver scalable application platform on basis of big data technologies that fit current and future data usage scenarios such as ETL, data mining, anomaly detection processes (foundation for product extensions),Assess and (re)design services to consume data (both high volume traffic challenges and spikes in computation demand challenges),Design a scalable Reporting platform (possibly near-real-time reporting),Be hands on on statistical data analysis and data patterns extraction to drive new business insights,Contribute to data security design & implementation,Safe-guard our enterprise product quality,Contribute to Product Roadmap,Contribute to environment and tools designs for resources efficiency,Bachelor degree or equivalent relevant work experience,Experience with Agile methodologies (Scrum / Kanban),Product development experience - knowing what it takes to build and release versions of a future-proof product...,Have worked before in distributed team is a plus,Proficient understanding of distributed computing principles,Experience with building stream-processing systems, (experience with Spark/Amazon EMR, Kafka/Amazon Kinesis is a big plus),Experience with integration of data from multiple data sources,Experience with NoSQL databases, such as MongoDB, etc,Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, etc. and experience with big data ML tools such as SparkML or pattern (python),Experience with common data science toolkits, such as R, NumPy,Experience with ELK stack,Comfort with Scala is a plus,Excellent communication skills in English,Business-focus,Enterpreneurial spirit,Technical curiocity,Good communicator,Pragmatism,Goal driven personality,Lame jokes (what can I say...) & beer on Fridays ;-),Laptop of your preferences and two extra screens on your desk,Flexible working hours,Time and budget for training,5 weeks paid holiday per year,Good salary and additional incentives", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Performing data engineering, data modeling, and implementation of Big Data platform and analytic applications for clients,Analyzing the latest Big Data and Cloud technologies and their innovative applications in both business intelligence analysis and new service offerings,Developing highly scalable and extensible Big Data platforms that enable the collection, storage, modeling, and analysis of massive data sets.,Constructing big data pipelines, both real-time and batch,Implementing data access and processing frameworks.,Supporting data users, data scientists, and analytic applications,Building Cloud-Native Solutions,Cloud platform technologies such as Microsoft Azure, Amazon Web Services and Google Cloud.,Big Data Analytic frameworks and query tools such as Spark, Storm, Hive, HBase, Impala, Hue,Streaming data tools and techniques such as Kafka, AWS Kinesis, Microsoft Streaming Analytics, StreamSets, StreamAnalytixs,ETL (Extract-Transform-Load) tools such as Talend, Informatica); also experience with ELT, SQL,Infrastructure setup using things like Kubernetes, Docker,Continuous Integration and Continuous Development (CI/CD),Data Warehouse and DataMart design and implementation,NoSQL environments such as MongoDB, Cassandra,Metadata management, data lineage, data governance, especially as related to Big Data,Structured, Unstructured, Semi-Structured Data techniques and processes,Around 5-10+ years of engineering and/or software development experience,Around 2-5+ years of experience in data engineering (ETL and Big Data) pipelines, both real-time and batch,Hands-on experience with Python, SQL, AWS, Redshift, Snowflake type tech,Experience in Consulting or supporting Client Requirements", "Take over responsibility in designing and developing our data processing pipeline,Build up a centralized data warehouse in the long run that acts as the unique source of all relevant data,Work closely together with other engineers and the BI team to understand the data that we are collecting,Zattoo a 360 \u00b0 view on it's users actions and journey's,Be in-house consultant for best track, store and access data,Minimum of 3 years working as a Data Engineer,Bachelor Degree in Computer Science or related studies,Ability to maintain ETL stack,Ability to engineer ETL processing through the whole data pipeline,Fluent verbal and written English skills,Strong communication and presentation skills,Experience in the following technologies:\nJenkins\nVarious database technologies search as MySQL / Postgres / vertica / SAP HANA\nSQL\nHadoop / hdfs (AWS-EMR, Pig, Impala, Apache Parquet)\ntableau,Jenkins,Various database technologies search as MySQL / Postgres / vertica / SAP HANA,SQL,Hadoop / hdfs (AWS-EMR, Pig, Impala, Apache Parquet),tableau,Bonus: Deep knowledge in ETL processing big amounts of data,Bonus: Ability to write hadoop map reduce jobs,Bonus: Dev skills to transform data via scripting language like ruby \u200b\u200b/ python or java / golang,Bonus: Experience with streaming frameworks such as spark / storm / samza / nimble,Bonus: Experience with Kafka,Flexible work schedules,Cool and comfortable offices,Relaxed atmosphere, no dress code,Working with an awesome product,Motivated and international team", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Being a member of the Metareview team, you will be working in a cross-discipline delivery team focused on one of many core data products.,Gather and process raw data at scale using frameworks such as Hadoop MR and Spark.,Maintain and write new data processing pipelines handling hundreds of GB of data.,Optimize and improve existing features or data processes for performance and stability,Apply machine learning algorithms to improve our product and drive decisions,3+ years of experience building data intensive applications.,Very strong programming and architectural experience, ideally in Python, Java or Scala but we are open to other experience if you would like to become a Python hacker.,You find creative solutions to tough problems. You are not only a great developer, you are also an architect, who is not afraid to pave the way for bigger and better things.,Experience and skills to clean and scrub noisy datasets.,Experience building data pipelines and ETLs using MapReduce, Spark or Flink.,Expert-level knowledge in Python. Experience in frameworks such as Pandas, Scikit-learn, Scipy, Luigi / Airflow is a plus.,Love for the command line with optional affinity for Linux scripting,Experience with Big data technologies (Hadoop, Spark, Flink, Hive, Impala, HBase, Pig, Redshift, Kafka),Experience building scalable REST-APIs using Python or similar technologies.,Experience with data mining, machine learning, natural language processing, or information retrieval is a plus.,Experience with AWS or other IaaS/PaaS.", "Build ETL processes from 3rd party API integration to ingestion into our data store.,Research, design, and implement NLP functions such as keyword and named entity extraction.,Develop machine learning driven search ranking and recommendation algorithms.,Help diagnose/fix production issues as they arise.,Minimum of 3 years experience developing in a production environment.,Proficient in Python, Go, JVM based, or other server side languages.,Experience with NLTK, scikit-learn, or similar technologies.,Ability to research and implement solutions on your own.,Good testing habits.", "Explore new ways of transforming and analyzing data and continuously expand and improve the performance of our data pipelines.,Build prototypes, fast, and determine what their worth are in the business and within the infrastructure before iterating and improving them.,Work closely with Data Scientists and Product Managers to decide how best to structure and store data in order to make it easily accessible to business users.,Evaluate and develop highly distributed Big Data solutions; You will advance our software architecture and tool set to meet growing business requirements regarding performance and data-quality.,A Data Engineer who likes to experiment with and explore new tools and technologies. You will be familiar with tools in the Hadoop ecosystem including Spark, Kafka, Hive or similar.,You are a Software Engineer with experience in modern backend web technologies.,You know how to design and build low-maintenance, high performing ETL processes and Data pipelines.,You can communicate an idea clearly on various levels of abstraction, depending on the audience.,Professional experience with relational databases: reading, writing and optimizing complex statements.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "SQL. Teach us something new, show us what you\u2019ve got,Python/Scala/C# You\u2019ve coded before, you know the importance of efficient, clean code,Cloud \u2013 we use Azure,Someone who understands complex data and the challenges of accessing it,A real bottom-line person, not someone who throws terms like \u201cbig data\u201d around because it\u2019s popular,Traditional/relational databases, Lakes, or Pub/Subs make no difference to you,Database concepts \u2013 indexes, execution engines, etc,Database Administration experience (Azure DWH, SqlServer, Postgresql),You understand that databases are an integral part of being a Data Engineer,You like to ask questions and devise a complete solution,You want to understand the data (not only the pipes) and you can definitely perform some analytics and build dashboards because you like it.,You know that you don\u2019t know enough, and it bothers you that there isn\u2019t enough time in the day to learn about the next topic.,You\u2019re up-to-date on new trends in data \u2013 you know who\u2019s using what to solve various problems and are excited for the next release of your favorite tool,If you like being thrown in the deep end of the pool, this team\u2019s for you.,You believe and want to participate in a blameless culture which focuses on process and technology,You don\u2019t sleep well at night when you leave work with a question unanswered,You feel accountable for everything you do and that sense of urgency has been driving you your entire life,You like to have a good time while getting things done,When we say a \u201cteam player\u201d we mean it - you have a crisp high-five and funny stories to tell,You have your team\u2019s back. And the team has yours.,Sense of humor is hugely preferred", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL -S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL -S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Provide technical leadership and training to all staff.,Work closely with scientific teams to streamline scientific computing workflows for cloud computing environments.,Design, develop, and deliver scalable data management and processing architectures.,Optimize and support our existing private cloud agreement.,Manage our public cloud billing and resource usage at the project level.,Identify technology gaps/risks and establish mitigation strategies.,Generate metrics to facilitate an understanding of computational strategies and efficiency.,Maintain and optimize custom software stack.,Setup, training, and orientation for new users.,Write tutorials and lead training sessions on programming and data management best practices.,Attend workshops, conferences, and scientific field expeditions to build further expertise as well as rapport with scientific staff.,Remain up to date with cutting edge methods and proactively work to implement them at WHRC.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Houston, TX", "Experience with Event Sourcing concepts and stream processing (Kafka / Flink etc).,An excellent understanding of database systems, relational and otherwise, including sharding for big data applications.,Build data pipeline based on Kafka Streams applications and Kafka Connect to create a real time feed to Snowflake.,Model domain events for a complex business domain.,Stream both domain events and materialized state to snowflake.", "Houston, TX", "Partner with various NBCU Technology teams in the design and execution of an overall Corporate Data Syndication Strategy for Nielsen and Alternative Measurement Data,Process structured and unstructured data into a form suitable for analysis and reporting, empowering state-of-the-art analytics and machine learning environments for business analysts, data scientists and engineers,Operationalize data science models and products in a cluster-computing environment,Evangelize a very high standard of quality, reliability and performance for data models and algorithms that can be streamlined into the engineering and sciences workflow,Manage multiple priorities across a mix of ad-hoc and operational projects", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Architecture design, and development experience around SaaS and platform software.,Routine involvement in high level architectural and design discussions providing authoritative technical guidance.,Experience delivering technical collateral including architecture and design documents, technical case studies, conference papers and whitepapers.,Demonstrated track record of successful customer and external engagement driving influence through deep technical product and industry knowledge.,Seasoned working with fellow senior engineers, architects, product management, senior management, and other partners to define the technical roadmap for the product in response to requirements.", "Design systems that reliably and efficiently provide interactive query performance on large amounts of multi-modal data,Build systems that handle scale,Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL and AWS \u2018big data\u2019 technologies,Collect, parse, analyze, and visualize large sets of data,Turn data into insights,Create data tools for analytics and data scientist team members that assist them in building and optimizing our product", "As a Data Engineer (f/m/d) at real.digital, you act as an important interface between our DevOps, Data Science and software development divisions,You develop scalable applications and systems for processing structured and unstructured data,You deal comprehensively with many different data-related topics and have the right ideas for every situation: Whether it's data exchange between microservices, processing huge amounts of data for recommender models, or lightning-fast, interactive business intelligence applications, we have the right solution,You have a university degree in (business) informatics or a comparable field of study,You have a very high capacity for abstraction, a sound algorithmic understanding and affinity for working with data,Ideally, you have experience in some of the following technologies and, as an autodidact, you are prepared to quickly familiarise yourself with the other technological forms:\nwith containers (docker and kubernetes)\nDatabase systems (MySQL, MongoDB, BigQuery, etc.)\nManagement and automation of infrastructure (Linux, Networking, Cloud Services, Terraform)\nSoftware development (Python, Java)\nCI/CD (Drone, Gitlab CI),with containers (docker and kubernetes),Database systems (MySQL, MongoDB, BigQuery, etc.),Management and automation of infrastructure (Linux, Networking, Cloud Services, Terraform),Software development (Python, Java)", "Houston, TX", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work with a team of engineers to create products that will directly affect the mission of Healthgrades,Develop data pipeline features to process incoming healthcare information quickly and reliably,Review other team members' code for correctness and quality,Write automated test scripts that power a continuous delivery pipeline,Refactor and improve the existing code base for simplicity and clarity,Remove roadblocks to development through collaboration, communication, and creative solution recommendations,Recommend and drive development best practices and continuous integration and delivery as part of a forward-thinking, agile organization,3+ years of experience developing data pipelines or ETLs,3+ years of experience in Python, Scala or Java,Knowledge of HL7 a plus,Proficiency with Apache Spark, Databricks, and Alteryx is required,Strong understanding of SQL, relational databases, columnar data warehouses, and data modeling,Knowledge of TDD, automated testing principles, and testing best practices,Ability to instrument basic automation and CI/CD including a familiarity with Jenkins/Git,Strong familiarity with cloud based services (AWS) and container technologies (Docker/Kubernetes),Client facing experience a plus,Previous experience with micro services architecture and API gateways is a plus,Knowledge (and experience) designing and building distributed systems for scalability and security,A bias towards self-education of new technologies, techniques and methods,Test-and-learn mentality \u2013 you pivot quickly when an approach is not successful,Keen attention to detail, eye for design and understanding the value of collaboration with UX/creative and product teams,Purpose-Driven Business \u2013 we help people make more confident healthcare decisions,Changing the Game \u2013 dynamic, employee-focused culture with career advancement opportunities,Community Builders \u2013 partners of local charity organizations, matching gifts program, Go-Green efforts, and wellness initiatives,Salary: $101K - $128K annually*,Bonus: up to 5% annually,401(k) plan options,Medical, dental, and vision insurance, with HSA contributions for qualifying plans,Company-funded basic Life, AD&D, and disability coverage,Family planning resources,Subsidized wellness benefits,PTO plus paid holiday and volunteer time", "Work with an architect to design and own how data flows through our business, soup to nuts, working collaboratively across the company to partner with all departments.,Manage an efficient data platform to support our products as well as a BI function using modern, low maintenance cloud technologies (AWS or GCP), as well as cleanly interfacing with our existing data science research technologies.,Evolve a sophisticated data model to consume structured data from a growing set of source systems (scanners, VI feeds, etc).,Author and maintain entity-relationship diagrams, data dictionaries, API specs, and data translation documentation at multiple levels of abstraction (conceptual, logical, physical) and across multiple data store technologies (relational, NoSQL).,Implement solutions to proactively monitor data quality with traceability to source systems.,Work with cross functional product development teams on data modeling rules, standards, and best practices, embedding when needed.,BS in Computer Science, Computer Engineering or related industry experience.,You have a product oriented, cloud first perspective on data.,You have strong experience with distributed compute along with ETL/ELT and BI architectures using public cloud technologies, concepts and frameworks, especially around streaming of big data.,Strong, proven data modeling experience at large scale.,5+ years experience with large-scale, distributed data pipelines, as well as data management, modeling, and storage,3+ years building large scale data processing and analytical systems in GCP or AWS with Dataproc, Airflow, BigQuery, Dataflow, Kafka, etc.,Competitive compensation package,Medical, Dental, Vision & Disability Insurance,Wellness Programs,Pet Insurance,Retirement Planning,Company Equity,Generous Paid Parental Leave,Work From Home Setup,Employee Referral Program,Discretionary Time Off,Friendly & Casual Environment,Work/Life Balance", "Gain exposure to investment research content acquisition. You'll learn how we get it, how we selectively distribute it, and how our customers discover it.,Learn about our customers, product offerings, and operational best practices.,Proactively partner with internal customers and utilize business intelligence to help spot trends, identify gaps and opportunities.,Ensure content is delivered to Bloomberg in the optimized fashion.,Own implementation of new content projects from start to finish.,Collaborate with content providers to suggest areas of improvements as it relates to headline composition, tagging and content positioning to our mutual clients.,Build influential and meaningful working relationships.,Develop a complete understanding of our problems by being deeply embedded with our business teams, meeting clients and data contributors and learning how the world's largest financial data operation works.,Be empowered to solve business problems through quantifiable objectives, with the freedom to design workflows and select the right technologies for the job.,Write code to develop robust and scalable technology solutions for data management problems using modern micro services, databases and user interface technology through test-driven and iterative processes.,A BA/BS degree or higher in Engineering, Information Systems, Mathematics, or relevant data technology field, or equivalent experience.,Up to two years of professional work experience in information technology, engineering, or data analysis.,Minimum 2 years of production level coding, with advanced Python programming proficiency outside of academia.,Aptitude for problem solving, particularly to modify and enhance processes and workflows.,Communication, communication, communication! (Especially when explaining technical processes and solutions to customers internally and externally).,System design skills with competency both in back-end and UI.,Passion for data, and the know-how to wrangle large amounts of data.,Knowledge of tests, linting and version control.,Willingness to be outside of your comfort zone, and adaptability.,A can-do attitude with intellectual curiosity.,Ability to legally work in the US without visa sponsorship.,Understanding of the financial markets, in particular sell-side content distribution.", "Distributed file systems and storage technologies (HDFS, HBase, Accumulo, Hive).,Large-scale distributed data analytic platforms and compute environments (Spark, Map/Reduce).,A hands-on engineering position responsible for supporting client engagements for Big Data engineering and planning.,A solid platform for you to drive the engineering/design decisions needed to achieve cost-effective and high performance result.,Thinking out of the box on improvements to current processes & enhancing existing platform.,You have a formal background and proven experience in engineering, mathematics and computer science, particularly within the financial services sector.,You have hands on Programming/Scripting Experience (Python, Java, Scala, Bash).,DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins).,Linux/Windows (Command line). An understanding of Unix/Linux including system administration and shell scripting.,You gave proficiency with Hadoop v2, MapReduce, HDFS, Spark.,Management of Hadoop cluster, with all included services.,You have good knowledge of Big Data querying tools, such as Pig, Hive, Impala and Spark.,Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management).,You have the ability to function within a multidisciplinary, global team. Be a self-starter with a strong curiosity for extracting knowledge from data and the ability to elicit technical requirements from a non-technical audience.,Collaboration with team members, business stakeholders and data SMEs to elicit, translate, and prescribe requirements. Cultivate sustained innovation to deliver exceptional products to customers.,Do you have experience with integration of data from multiple data sources?", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Live by and champion our values: #day-one, #ownership, #empathy, #humility.,Hands-on leadership, influence, and development of all things data services.,Develop modern data architectural approaches for business intelligence reporting and analytics, including that for machine learning models and data science, ensuring effectiveness, scalability, and reliability.,Design, develop, implement, and optimize existing ETL processes that merge data from disparate sources for consumption by data analysts and scientists, business owners, and decisions makers.,Complete current evaluation of new ETL software options, propose recommendations, and implement the solution.,Facilitate data transformation, normalization, cleansing, aggregation, workflow management, and business rule application.,Detect data quality issues, identify their root causes, implement fixes, and design data audits to capture issues.,Distill technical requirements into the product development and operational process via continuous collaboration with product, engineering, and analytics team members.,Influence and communicate with all levels of stakeholders including analysts, developers, business users, and executives.,Use analytics to influence product development, surfacing data around product usage and customer behavior.,ETL tool evaluation and implementation to prepare for scaling and efficiency.,Typically, 6+ years experience in a data engineering related role (Data Warehouse Developer, ETL Developer, Business Intelligence Analytics, Software Engineer) with a track record of manipulating, processing and extracting value from datasets,Experience working with a variety of ETL platforms (Matillion {preferred}, CloverETL, FiveTran, Stitch, DBT, Spark, AWS Glue, DataFlow),3+ years of hands-on experience designing and building ETL pipelines for ingesting, transforming and delivery of large amounts of data, from multiple sources into a Data Warehouse/Data Lake.,Experience with a variety of data storage platforms (Snowflake {preferred}, Redshift, MySQL, Postgres, Oracle, RDS),Expert proficiency in SQL,Deep understanding and application of modern data processing technology and real-time/low-latency data pipeline and ETL architectures,Strong stakeholder interaction and influence experience at executive, business stakeholder, and engineering team levels", "Enable data scientists to train and deploy machine learning algorithms at scale, in fault-tolerant, highly-available systems,Use best practices in continuous integration and delivery with Docker and Kubernetes.,Work closely with application engineers to build data products that power Carta\u2019s core applications,Create data pipelines using batch and streaming tools like Airflow, Spark, Kafka, and Google Pub/Sub,Uphold our engineering standards and bring consistency to the many codebases and processes you will encounter", "Design, implement, and maintain scalable data pipelines,Collaborate with domain experts and analysts to solve data challenges,Develop advanced data reporting and visualizations,Apply data modelling methodologies and contribute to a robust data platform,Master\u2019s degree in Computer Science or equivalent,Experience with SQL and relational databases,Experience with one or more programming languages (Python or Java preferred),Strong understanding of data models (Data Vault and Kimball) and data warehouses in general,Fluent in English, Dutch not required,Python, Pentaho Data Integration (with custom components developed in Java),Snowflake, MongoDB, PostgreSQL, Tableau,Spark, Elastic MapReduce, Snowplow, Kinesis", "Own technical design and project execution for the team, delegating work to other engineers on the team while also making individual engineering contributions,Lead technical development of distributed data infrastructure and search application in collaboration with ML+NLP collaborators,Push the boundaries of search with the latest technologies and methods,Scale our platform to billions of documents and drive millions of searches,Make hands-on engineering contributions to Grata with a focus on overcoming technical challenges and modeling practices that improve quality and velocity,Mentor engineers on the team through activities like pairing and code reviews to promote a culture of technical excellence,Collaborate with other teams across Grata to steward a coordinated strategy,8+ years of experience as a software engineer, developing web applications, large scale data pipelines, and scalable infrastructure,1+ years of experience in a team lead role,You know how to work with data-centric distributed systems, and are comfortable supporting microservice and serverless architectures,Experience collaborating with ML practitioners to build data and analytical systems,Experience with Python, Django, React, Postgres, NoSQL, Elasticsearch, AWS, Kubernetes, Spark, and other big data technologies,Experience planning projects and managing team-wide efforts to completion,Appreciate productivity and care deeply about helping teams collaborate more effectively and efficiently, including your own,Excited to be a part of an inclusive culture,Competitive salary,Meaningful equity,Fully paid healthcare, dental, and vision benefits,2 company trips / yr (starting back up at some point),Happy hours, team events and lunches", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build, scale, and operate a streaming data pipeline.You\u2019ll be on a team responsible for moving, processing, and analyzing about 20 terabytes of images each week.,Create a world-class research platform.You\u2019ll work with Data Scientists and Biologists to create a platform that allows them to generate and access petabytes of data, gives them tools to quickly iterate on novel analysis and research, and deploys new deep learning models into the production data pipeline.,Provide visibility into operations.You\u2019ll create tools, dashboards, and metrics that will help everyone keep track of the work they care about, and alert them when they need to take corrective actions.,Act as a mentor to peers. You will share your technical knowledge and experiences, resulting in an increase in their productivity and effectiveness.,Experimentation- We want Software Engineers who think critically and use data to measure results.Rigorous use of the scientific method allows our Software Engineers to quickly understand the critical aspects of the problems we\u2019re trying to solve and whether or not we\u2019re moving in the right direction.,Collaboration- We want Software Engineers who play well with others. The role will require close collaboration with our Biological, High Throughput Screening, and Data Science teams to help us achieve our mission to discover transformative new treatments.,Curiosity- We want Software Engineers who aren\u2019t satisfied with the status quo. Our Software Engineers openly discuss the tradeoffs inherent in how we build software, and go beyond the traditional boundaries of engineering teams to enable us to get things done faster, cheaper, and more reliably than in traditional drug discovery.,An ability to be resourceful and collaborative in order to complete large projects. We don't have much in the way of project managers.,A track record of learning new technologies as needed to get things done. Our current tech stack uses Python and the pydata libraries, Clojure, Kafka, Kubernetes + Docker, PostgreSQL, Big Query, and other cloud services provided by Google Cloud Platform. Experience with Python or the JVM will be helpful.,An ability to get things done using various tools from the nooks and crannies of software engineering: composing command line tools such as kubectl, jq, and xargs; creating SQL triggers and managing migrations; and operational support.,An ability to write well tested and instrumented code that can be continuously deployed into a production environment with confidence.,An interest in learning from and teaching peers in areas of performance, scalability, and system architecture.,Biology background is not necessary, but intellectual curiosity is a must!,Coverage of health, vision, and dental insurance premiums (in most cases 100%),401(k) with generous matching (immediate vesting),Stock option grants,Two one-week paid company closures (summer and winter) in addition to flexible, generous vacation/sick leave,Commuter benefit and vehicle parking to ease your commute,Complimentary chef-prepared lunches and well-stocked snack bars,Generous paid parental leave for birth, non-birth and adoptive parents,Fully-paid gym membership to Metro Fitness, located just feet away from our new headquarters", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Work closely with our Data Scientists, Data Analysts, and Performance Marketers to identify the data-related tooling and infrastructural needs of the company, and participate in defining the plan to fulfil them.,Help define the software architecture of new data tools or that of new features for existing tools, and then develop them. The work spans broadly from the most infrastructure- or backend-oriented to the implementation of data-science algorithms and the development of complex front-ends.,Constantly improve the quality of our tools and infrastructure by fixing bugs and refactoring the code base where necessary.,Expand our set of libraries as needed to make yourself as well as other engineers more efficient and effective.,Stay informed on new, relevant technologies and test the promising ones to make sure we don\u2019t miss out on game-changing opportunities to boost our productivity and the enjoyment of our work.,Huge impact. Your software will help us store, retrieve, visualize, analyze, and ultimately make sense of enormous amounts of data and ultimately make a real difference on how well we serve our tens of millions of users and whether we win or we lose in the market.,Talented, knowledgeable colleagues. You\u2019ll get the chance to learn (and teach to!) some of the brightest and most skilled people you\u2019ll ever meet. Your talent is going to blossom here.,Cool tech stack. We strive to use the best, most modern tools and technologies, and when they fall short of our expectations we invent our own. It\u2019s a geek\u2019s paradise.,Own products. We don\u2019t work for clients, but only develop our own apps. Freed from politics, we can move fast and be daring.,Fun, young environment. We have near-zero hierarchy and a very relaxed workplace. Also, we\u2019re 28 years old on average and often hang out together, so you might actually end up making new friends!,Top-notch office. Our office is amazing! We designed it ourselves and it goes beyond excellent functionality by offering all sorts of amenities, such as foosball tables and gaming consoles.,International reach. Our audience is wildly international. Our company language is English. We\u2019re a global reality.,Passion for the topic. You have a long-standing, proven passion for writing software. It\u2019s a big plus if you have a demonstrated interest for data science, data analytics, or building tools.,Reasoning and depth of learning. You\u2019re structured and creative enough to solve most challenging problems independently, given the necessary knowledge. You learn new concepts and skills rapidly, and look to understand stuff truly in depth.,Drive. You\u2019re energetic, hard-working, and persevere through adversity until the job is done, and done well. You get turned on by getting results, and always aim for excellence in what you do.,Pragmatism. Far from being too academic or obsessive in your perfectionism, you understand that, when getting things done in a competitive world, speed is often as important as quality.,Curiosity and initiative. You love exploring and you\u2019re entrepreneurial in seeking out new opportunities and testing new ideas of your own accord. You don\u2019t just wait to be told what to do all the time.,Attention to detail. You care to get the small things right as well as the big ones. You\u2019re meticulous in checking that your code will work exactly as expected.,Diligence, organization. You can blindly be entrusted with big responsibilities as well as small, more menial tasks.,Humbleness. You\u2019re down to earth, eager to listen to people\u2019s feedback and constructive criticism, and ready to get your hands dirty with whatever the team needs to succeed.", "Accountable for supporting projects by preparing data for data exploration and research modeling.,Create visualizations of data for purposes of data discovery and data exploration,Provide subject matter expertise to claim's business intelligence data environment,Create and operationalize data products including transformation logic as well as business requirements and specifications.,Serve as data expert of significant projects with broad impact to business and enterprise performance.,Serve as a business intelligence data environment subject matter expert to support the claim research community and claim business partners,Understanding of data warehousing, information delivery, and use of big data,Develops and prepares data using Hive, Pig, SQL, and SAS.,Develops data pipelines utilizing appropriate technologies and frameworks (Java, Python, Scala, NiFi).,Capable of building data visualizations to help support data discovery and data exploration.,Create and operationalize data products. Incorporate core data management competencies - data governance, data security, data quality.,Builds, tests, and implements analytic processes into the business workstream, including pilots and proof of concept.,Provide subject matter expertise to claim's business intelligence data environment.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Experience running and supporting production of enterprise data platforms,Experience creating internal tools that combine content and audience data,Experience in building infrastructure required for optimal extraction, transformation and loading of data from various resources,Knowledge of JavaScript, Python, Bash and SQL,Build data pipelines with tools and cloud-based data services like Google\u2019s BigQuery, AWS, Dataproc and Pub/Sub,2+ years of data engineering experience,Strong statistics skills,LI-JA1-WSJ", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL -S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL -S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "As Data Engineer, you will be responsible for fast, accurate, robust and scalable data-processing in our company,You develop and deploy new features in an agile, test-driven environment in tandem with our team of data engineers, data scientists, software developers, and operations managers,You build fault tolerant, self-healing, adaptive, and highly accurate data computational pipelines,You optimize data transfer processes at the code, memory and architecture level,You develop in an environment of micro-services and event-based architecture,You have either a bachelor's degree with 3+ years of work experience or a master\u2019s degree and 2+ year of work experience,You have prior experience with a high-level programming language like Ruby, Scala or Go,You have the ability to quickly understand business requirements and transform them into data models,You are experienced with Big Data related technologies like HDFS, Hive, Pig and Spark,You have an aptitude to independently learn new technologies and you are passionate about data,Prior experience with online marketing is a plus, e.g. Google AdWords, BingAds, Social Media Advertising or Universal Analytics,We trust in data \u2014 data drive our business decisions and product development roadmap.,Our employees are our best investment \u2014 develop yourself through knowledge sharing sessions, workshops, and personalized training.,We keep innovating \u2014 stay close to users, challenge the status quo, dedicate 20% of your work time to new ideas.,We get great work done \u2014 we enjoy working face-to-face, but value results over office presence.,Our team is diverse \u2014 we unite smart and passionate people from over 30 nationalities.", "Move smart: we are data driven, and employ tools and best practices to ship code quickly and safely (continuous integration, code review, automated testing, etc).,Distribute knowledge: we want to scale our engineering team to a point where our contributions do not stop at the company code base. We believe in the Open Source culture and communication with the outside world.,Leave code better than you found it: because we constantly raise the bar.,Unity makes strength: moving people from A to B is not as easy as it sounds but, we always keep calm and support each other.", "Houston, TX", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Design and develop business-critical data pipelines and related back-end services,Identification of and participation in simplifying and addressing scalability issues for enterprise level data pipeline,Design and build big data infrastructure to support our data lake,2+ years of extensive experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, HBase, Parquet),Experience with building, breaking, and fixing production data pipelines,Hands-on SQL skills and background in other data stores like SQL-Server, Postgres, and MongoDB,Experience with continuous delivery and automated deployments (Terraform),ETL experience,Able to identify and participate in addressing scalability issues for enterprise level data,Python programming experience,Experience with machine learning libraries like scikit-learn, Tensorflow, etc., or an interest in picking it up,Experience with R to mine structured and unstructured data and/or building statistical models,Experience with Elasticsearch,Experience with AWS services like Glue, S3, SQS, Lambda, Fargate, EC2, Athena, Kinesis, Step Functions, DynamoDB, CloudFormation and CloudWatch will be a huge plus,You can work remotely in the continental US with occasional travel to Bend, Oregon,You can be based at a shared office space in the heart of downtown Portland, Oregon,You can be based at our offices in Bend, Oregon (relocation assistance package available),An inclusive, fun, values-driven company culture \u2013 we\u2019ve won awards for it,A growing tech company in Bend, Oregon,Work / Life balance - what a concept!,Excellent benefits package with a Medical Expense Reimbursement Program that helps keep our medical deductibles LOW for our Team Members,401(k) with generous matching component,Generous time off plus a VTO day to use working at your favorite charity,Competitive pay + annual bonus program,FREE TURKEYS (or pies) for every Team Member for Thanksgiving (hey, it's a tradition around here),Your work makes a difference here, and we make a huge impact to our clients\u2019 profits", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Build, scale, and maintain data pipelines to process billions of daily events into our data warehouses,Write and tune complex Java, MapReduce, Spark, and Hive jobs,Explore available technologies and design solutions to continuously improve our data quality, workflow reliability, scalability while reporting performance and capabilities,Troubleshoot data issues and build customized reports to investigate key business questions,Work closely with the Unity Engine, Ads, Analytics and Game Services teams worldwide,Drive key business initiatives with multiple teams and stakeholders across the organization,Mentor and help engineers grow,Work across teams to instill engineering best practices and patterns", "Houston, TX", "Ingesting 3rd party and internal data and creating clean data models to empower business and operational teams.,Developing standardized metrics to drive the behavior of our commercial organization and measure the outcome of that behavior.,Providing visibility to commercial leaders through the creation of dashboards and reports.,Analyzing the impact of commercial programs using our sales and field data.,This role offers tremendous upwards exposure towards senior business leaders and the chance to truly impact the decision-making at JUUL.,Excellent Python and SQL skills.,Experience with data visualization, dash-boarding and analytical report building.,World-class ability to extract and communicate insights from real-world datasets.,4+ years applicable experience or a relevant advanced degree.,Experience with Pandas, GCP/BigQuery, Tableau and Mode is valuable but not required.,You get excited about taking ownership of problems and solving them in a fast-paced and scrappy environment working cross-functionally with both technical and non-technical people,You are output focused and see data science as a powerful tool to get things done rather than as an end in itself.,You are a strong critical thinker with a passion for understanding complex issues and are comfortable working on ambiguous problems.,A place to grow your career. We\u2019ll help you set big goals - and exceed them,People. Work with talented, committed and supportive teammates,Equity and performance bonuses. Every employee is a stakeholder in our success,Boundless snacks and drinks,Cell phone subsidy, commuter benefits, and discounts on JUUL products", "Work in interdisciplinary teams that combine technical, business and data science competencies.,Design and implement solutions around data warehouse implementation ranging from architecture, ETL processes, multidimensional modelling, data marts implementation.,Integrate datasets and dataflows using a variety of best in class software as well as profile and analyze large and complex datasets from disparate sources.,Be involved in the full project lifecycle (gathering business requirements, system design, development, testing and deployment).,We are looking for both experienced specialists as well and non-experienced talents.", "Gather requirements, assess gaps, and build roadmaps and architectures to help the analytics driven organization achieve its goals.,Work closely with Data Analysts to ensure data quality and availability for analytical modelling.,Explore suitable options and designs for specific analytical solutions.,Define extract, load, and transform (ELT) based on jointly defined requirements.,Prepare, clean, and massage data for use in modeling and prototypes,Identify gaps and implement solutions for data security, quality, and automation of processes.,Bachelor\u2019s degree or four or more years of work experience.,Four or more years of experience as a data engineer,Four or more years of experience finding, cleaning, and preparing data for use by Data Scientists,Experience knitting disperate data sources together,Four or more years of experience building data pipelines,Experience using SQL (i.e., PL/SQL or T-SQL with RDBMSs like Teradata, MS SQL Server, Oracle, etc.),Experience in data engineering, databases, and data warehouses.,Strong experience with data engineering in Python.,Master\u2019s degree in Computer Science, Engineering, Statistics, IT, or related field.,Experience with Scala, Julia, R, Python or other machine learning programming language,Experience on Big Data platforms (i.e., Hadoop, Map/Reduce, Spark, HBase, CouchDB, Hive, etc.),Strong analytical and problem-solving skills.,Experience working in a network operations center environment.", "Design, build, and launch efficient and reliable data pipelines into production,Design and implement warehousing solutions to scale with the needs of the business,Develop new systems and tools to enable the team to consume and understand data more intuitively,Partner with engineers, project managers, and analysts to deliver insights to the business,4+ years experience in data engineering with data warehouse technology,4+ years experience in custom ETL design, implementation and maintenance,4+ years experience with schema design and dimensional data modeling,Strong SQL skills on multiple platform,Skilled in programming languages Python, R, and/or Java,Strong computer science fundamentals including data structures and algorithms,Strong software engineering skills in any server side language, preferable Python,Kafka, Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus,NoSQL experience a plus,Experience with Continuous integration and deployment,Experienced in working collaboratively across different teams and departments", "Working in an interdisciplinary field, together with computer scientists, business specs, and telecom network engineers, and will require excellent interpersonal and communication skills.,Integrating multiple data sources, models, and software tools with business line specific decision support and data analysis.,Developing new automation methods to analyze and evaluate business strategies across various business geographies, technologies, drivers and scales.,Modeling complex systems by integrating large varied datasets of economic, demographic and telecom related information.,Creating and walking through executive presentations that explain the complex data and algorithms used in simple easy-to-understand visually striking layman terms that leave a residual impact on an executive audience.,Solving complex geospatial problems utilizing robust and repeatable solutions.,Challenging existing constructs and business paradigms and creatively solving problems that result in a business transformation of a process or a program or a current approach.,Bachelor\u2019s degree or four or more years of work experience.,Masters of Science in Geographic Information Systems.,Experience in one or more programming languages (e.g., Python, JavaScript).,Experience in analysis of large spatial and non-spatial datasets.,Experience with a multitude of databases in general (Oracle, Postgres).,Knowledge of R, SPSS, or SAS for statistical analysis is preferred.,Experience with large-scale parallel computing in distributed environments and familiarity with GIS and spatial databases (e.g., Postgres / PostGIS, Oracle/ESRI sde).,Experience using Python in automation of analytics tasks.,Demonstrated use of / development of spatially enabled Web Front Ends (ArcGIS JavaScript API, React, etc.),Experience managing data in SQL and/or NoSQL databases (Postgres, Oracle, Teradata, Hadoop, Casandra, MongoDB).,Knowledge in application of technical procedures, principles, theories and concepts in the Telecommunications field. General knowledge of other related disciplines.,Experience with leading one or more areas of team, task or project lead responsibilities.,Demonstrated experience managing short and long term projects from start to finish.,Experience in Telco, Cable or other network related field (Electric, Wastewater, Gas/Oil, Urban planning).,Interpersonal and communication skills.,Ability to convey complex information to upper level execs.", "Houston, TX", "Develop understanding of key business, product and user questions.,Collaborate with other Product Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.,Evolve data-driven feature prototypes into production features that scale,Streamline feature engineering, so that the underlying data is efficiently extracted.,Build flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.,Develop and enhance our data warehouse in AWS S3.,You have at least 3 years of relevant experience in a comparable data engineering role,You have expert-level knowledge of SQL/Spark SQL,You have experience in pursuing and launching data-backed decisions, such as recommendations, to make end-user-facing products better,You like to dive-deep on data questions to come up with effective solutions,You believe in writing code that is easy to understand, test and maintain,You thrive in a workplace that values autonomy, applauds ideas and a enjoys a sense of humor,https://www.showtime.com/", "Design, implement and support robust, scalable solutions to enhance business analysis capabilities, identify gaps and design processes to fill them.,Work with analysts to understand business priorities and translate requirements into data models.,Collaborate with various stakeholders across the company like data developers, analysts, data science, finance, etc, in order to deliver team tasks.,Build complex multi-cloud ETL pipelines in Apache Airflow.,Build Python API integrations with 3rd party vendors.", "https://www.showtime.com/", "Leads and influences technical direction for large scale, highly complex technical initiatives and/or projects requiring integration of cross-functional systems.,Provides technical guidance in evaluating applications systems or evaluating requests for proposals.,Collaborates with the business to prioritize key business/technical initiatives.,Utilizes expert knowledge of the customers business to recommend solutions, and ensures business and technology objectives are met and maintained.,Understands user and process requirements and ensures those requirements can be achieved through high quality deliverables.,Creates system documentation/play book(s) and serves as a lead technical reviewer and contributor in requirements, design and code reviews.,Typically serves as a resource to the business and/or as a technical resource to cross functional third party and internal team members on highly complex design/code reviews.,May troubleshoot complex problems and recommend solutions or practices relative to root cause analyses and identification of solutions for improving system performance and availability.,On behalf of the manager; manages the consistent delegation of work packages to cross functional and third-party team members for execution through the full development life cycle.,Appropriately advises management of issues.,Assists team leads and management with delegation of technical work packages to cross functional and third-party team members for execution through the full development life cycle.,Keeps management appropriately informed of progress and issues.,Performs design and analysis, coding and unit/integration testing of highly complex system functionality and/or defect correction across multiple platforms.,Displays advanced knowledge and understanding of functional and technical domains of specific products and appropriately evaluates the impact of changes or additions.,Develops accurate estimates on work packages.,Analyzes and designs specifications for less experienced internal and third-party team members to execute.,Actively mentors and contributes to the technical and soft skills development of internal and third-party teams.,Actively participates in cross departmental staffing and/or technical decisions.,Bachelor's degree or 4 additional years of related experience beyond the minimum required may be substituted in lieu of a degree,8 or more years of software development experience demonstrating depth of technical and functional understanding within specific I/T discipline(s)/technology(s) i.e., Business Intelligence, Mobile, Web, Java, etc,8+ years experience developing, deploying and supporting high-quality, fault-tolerant data pipelines (leveraging distributed, big data movement technologies and approaches, including but not limited to ETL and streaming ingestion and processing),8+ years experience in a software engineer role, leveraging Java, Python, Scala or C++,4+ years advanced distributed schema and SQL development skills including partitioning for performance of ingestion and consumption patterns,4+ years experience with distributed NoSQL databases and event brokers (Apache Cassandra, Kafka, Graph databases, Document Store databases),Curious and excited by new ideas,Energized by a fast-paced environment,Able to understand and translate business needs into leading-edge technology,Comfortable working as part of a connected team, but self-motivated,Community-focused, dependable and committee", "Data munging with emphasis on ability to deal with imperfections in data.,Developing, refining and scaling data management and analytics procedures, systems, workflows, best practices and other issues.,Developing of data-driven products.,Visualizing and communicating data clearly for use both internally and externally.,Building data pipelines that clean, transform and aggregate data from many different sources.,Developing models that can be used to make predictions.,Building complex functions that answer questions for the business.,Modeling data at rest and enable powerful data analysis.,Providing solutions that help share data with the enterprise.,Bachelor\u2019s degree in Mathematics, Statistics, Engineering, Computer Science or related discipline and 3+ years of experience in analytics or business intelligence.,Proficiency in languages such as Java and/or Scala.,Experience with web technologies such HTML, JavaScript, XPath, JSON, or Drupal,Exposure to Linux operating systems. Some exposure to Unix is helpful but not required.,Experience in working with big data technologies Spark, Hadoop, MapReduce, MySQL, NoSQL databases,Experience with AWS Data Tools such as S3, Redshift, RDS and Kenesis,Experience producing and consuming event driven data,Experience with scraping social media, using social media APIs,Basic understanding of statistics", "You will be part of the data science team and work closely with our data scientists to operationalize machine learning pipelines,You will develop and implement effective data processing architectures,You will also collaborate a lot with the data warehouse and data platform team,You will participate at meetups, conferences and the research community and apply what you\u2019ve learned back at your daily work", "Independently installs, customizes and integrates commercial software packages.,Facilitates root cause analysis of system issues.,Works with experienced team members to conduct root cause analysis of issues, review new and existing code and/or perform unit testing.,Learns to create system documentation/play books and attends requirements, design and code reviews.,Receives work packages from manager and/or delegates.,Identifies ideas to improve system performance and impact availability.,Resolves complex technical design issues.,Creates system documentation/play book(s) and participates as a reviewer and contributor in requirements, design and code reviews.,May serve as the subject matter expert on development techniques.,Partners with experienced team members to develop accurate work estimates on work packages.,May serve as a mentor on procedural matters to less experienced internal and third-party team members.,Bachelor\u2019s degree OR 4 additional years of related experience beyond the minimum required may be substituted in lieu of a degree,3+ years of operational experience in Linux environments (configuration, health checks, monitoring, etc.),3+ years of coding/scripting experience (Bash, Python, Perl, etc.),3+ years of experience working with relational databases. Advanced SQL knowledge, strong understanding of key database management concepts. Familiarity with a variety of databases (Oracle, UDB, DB2, SQL Server, etc.),3+ years of experience working with APIs to collect and/or ingest data.,3+ years of infrastructure automation and orchestration experience.,Proven ability to design, build, and operate a technology stack.,Strong analytic and troubleshooting skills.", "Create electrical and optical data visualization tool for various projects.,Develop web based tool to generate and present report for production.,Pursuing a degree in computer science, electrical engineering or a related technical field.,Strong in Python, SQL and web based programming.,Strong in statistical analysis is a plus.,Able to work well in a team environment.", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Design, develop, and support our Data infrastructure utilizing various technologies to process terabytes of data, including SQL, Python, Microsoft Azure, and AWS.,Create solutions to enable diagnostic and predictive analytics capabilities.,Partner with the Analytics, Marketing, and Finance organizations to get feedback and iterate upon the Data Ecosystem development.,Develop components and distributed ETL systems for our suite of large data platforms,Be curious about trends and emerging technologies in the Data space, participate in user communities, and share what you learn with your teammates,Familiarity with developing data processing solutions and data applications using technologies like Python, C#, Java, SQL, Spark, or No SQL DB,Experience working with all kinds of data-- clean, dirty, unstructured, semi-structured and relational,Problem solving and multi-tasking in a fast-paced, globally distributed environment,Strong communication skills, good interpersonal skills,Collaborate with business partners to understand and refine requirements,Experience with developing end-to-end data pipelines in large cloud-compute infrastructure solutions such as Azure, AWS or Google is a plus,Five years working directly on Big Data technologies preferred", "Build analytical solutions to enable Data Scientist by manipulating large data sets and integrating diverse data sources.,Work closely with the data scientists, and database and systems administrators to create data solutions.,Bachelor\u2019s degree or four or more years of work experience.,Experience of designing, building, and deploying production-level data pipelines using tools from Hadoop stack (HDFS, Hive, Spark, HBase, Kafka, NiFi, Oozie, Splunk etc.).", "Working with a variety of different data formats and platforms, including SQL and NoSQL databases, Big Data (MapReduce/Hadoop, Spark, etc.), JSON, XML, and cloud data warehouse technology such as Amazon Redshift or Snowflake.,Working with Business Analytics and IT team members to clarify and refine functional data requirement specifications.,Working with our Business Analytics team on normalizing and aggregating large data sets based on business needs and requirements,Developing processes and techniques for practicing good \u201cdata hygiene\u201d and to ensure data is always up-to-date, accurate, and stored efficiently,Testing new technologies and architectures to help find the best ways to work with unique data sets,Being able to benchmark and troubleshoot data-related issues, analyze system bottlenecks and propose solutions to eliminate them,Designing ETL, ELT processes and data warehouse structures to enhance existing on-premise architectures,Utilizing cloud solutions and cloud data warehouse technologies.,Interacting with various on-premise and cloud data sources as well as RESTful APIs to achieve optimal data footprint.,Following Agile principles to prioritize design/architecture/POC backlog using Agile principles,Writing and optimizing scripts to transform, aggregate and optimize data", "We helped the second largest personal lines insurer in the United States design and build a \u201cVirtual Assistant\u201d to speed up the productivity of new customer contact center employees by utilizing a Cognitive Computing/Artificial Intelligence system.,The world\u2019s largest brewer was experiencing too much downtime during production. We identified 140 causal factors across seven different dimensions and 43 categories \u2014 a pool of more than $100 million in potential cost savings.,When our client, the second-largest cable operator in the United States, acquired another organization, we took the lead, assessing product catalogues for both organizations and providing recommendations on how to transition to a single product catalogue for current customers and the additional 1.4 million subscribers our client gained from the acquisition.,Proven experience analyzing existing tools and databases and providing software solution recommendations.,Ability to translate business requirements into non-technical, lay terms.,High-level experience in methodologies and processes for managing large scale databases.,Demonstrated experience handling large data sets and relational databases.,Understanding of addressing and metadata standards.,Ability to work with stakeholders to assess potential risks.,Unlimited Training & Career Growth, and we mean it: Pluralsight, Safari Library, Conferences, Certifications, Lunch & Learns, etc.,Weekly Coding Challenges,Sponsored Hackathons,Full time Employment - not only do we support our clients, but we grow our company.,Excellent health, dental, vision, maternity and paternity leave,Revenue sharing and a 401(k) retirement savings,Life, disability and long-term care", "We work in small teams, fast-paced, we all get a lot done by everyone wearing many hats.,We are serious about optimizing our time and staying focused on the most important goals and outcomes.,We are a remote team and are completely on board with 100% remote work, meaning we focus on overcommunication to ensure we can stay in sync despite our physical distance.,We coordinate using a kanban board, hold a daily standup, and mostly communicate via ad hoc video calls and Slack.,We\u2019re building lots of new things, but also maintaining a significant business. We are mindful of the balance and need to monitor and pay down tech debt and also innovate with exciting greenfield projects.,GrowFlow\u2019s data pipelines consume data from all of our products (4+ and counting) into a central data warehouse in BigQuery.,We process & transform the data with DBT, visualize it with PowerBI and Amplitude.,Segment.com is our central product analytics customer data platform, collecting behavior/event data from all our sources and sending it to both our warehouse and various partner destinations.,Ensure the availability and timely delivery of data, analytics & reporting, company-wide.,Ingest and aggregate structured & unstructured data from internal and external data sources to our data warehouse.,Model new data sets Build, maintain & improve performant, efficient & reliable ELT workflows and data pipelines,Document new & existing data models, ELT workflows, pipelines, data dictionaries and tracking events.,Build, deploy and monitor robust data tests to monitor & validate production pipelines.,Improve data-driven decisions by providing guidance & assistance to internal business stakeholders (Customer Experience, Sales, Marketing, Growth, etc) who need to unlock insights about our markets, customers and business processes.,Work w/ Product & Engineering to evolve and expand GrowFlow\u2019s Insights product, our customer-facing business intelligence tool.,Work with business data analysts to develop, extend & support data-driven A/B experiments.,7+ years total engineering experience,3+ years experience dealing with \u201cdata\u201d, including things like SQL databases, No-SQL databases, columnar databases and data pipelines.,Familiarity with Embedded Reporting tools/platforms (PowerBI, Domo, Looker, Sisense etc.),Bonuses:,Experience with Google Big Query, DBT (getdbt.com) or PowerBI,Experience working with data using Python (e.g. Pandas, Apache Beam, scikit, tensorflow, etc),Experience using ETL tools (SAS, Informatica, Talend, MSSQL SSIS, etc),Ability to acutely focus on company objectives and mission at hand.,Ability to pass a background check; have appropriate work authorization,Be Customer Obsessed: As a support engineer, our ideal candidate should have a heartbeat on customer satisfaction, and strive to make sure customer issues are resolved quickly and effectively.,Over Communicate: As a 100% remote company, over communication is key to delivering continued productivity across all teams. Our ideal candidate goes above and beyond to ensure important messages are received by the correct party.,Challenge Respectfully: GrowFlow is far more likely to succeed by examining problems and situations through several lenses. Our ideal candidate should be able to engage and work with both the support team and the engineering team to ensure we are delivering the best solution for our customers.,Extreme Ownership: At GrowFlow, we pride ourselves on each member practicing extreme ownership and accountability. Our ideal candidate should be able and willing to take ownership of customer reported issues and see them all the way to their resolution.,Stay Curious, Stay Scrappy: Most of us dork out over non-work related topics at a ridiculous level of detail, because that\u2019s how we\u2019re wired. We\u2019re naturally inquisitive, ask tough questions and aren\u2019t afraid to ruffle feathers to find better answers. Our Ideal candidate should be intuitive and eager to learn new things.,Do Less Better: At GrowFlow, we believe that the path to becoming the market leader looks like focusing on doing fewer things, but doing them better than anyone else in the industry. Our Ideal candidate is able to create a \u201cpath of least resistance\u201d when resolving customer reported issues.,Results Get Rewarded: At GrowFlow, we recognize that the quickest path to becoming the top company in the industry is by forming the top team in the industry. Our ideal candidate should have a heartbeat on KPIs related to the T2 support engineer position and should be focused on measuring and improving metrics related to his or her position.,We are a fully remote company and this position will be remote.,We are looking for someone who is ready to join us full-time after a brief trial period (all our employees do this).,We offer health benefits, 401k, unlimited time off, charity matching, and other cool perks.", "Houston, TX", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Enable data scientists to train and deploy machine learning algorithms at scale, in fault-tolerant, highly-available systems,Use best practices in continuous integration and delivery with Docker and Kubernetes.,Work closely with application engineers to build data products that power Carta\u2019s core applications,Create data pipelines using batch and streaming tools like Airflow, Spark, Kafka, and Google Pub/Sub,Uphold our engineering standards and bring consistency to the many codebases and processes you will encounter", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Experience with any framework for data processing workflows,Experience visualizing and documenting architectures, e.g. a flow diagram describing how data flows from an input to ETL pipelines to a data warehouse,Experience with Google Dataflow or other cloud data processing services (e.g. on AWS),3+ years experience with SQL and Python (or Java or Node.js),Experience with unit and behavioral testing,Great understanding of CI/CD flows, containerization, and related best practices,Understanding of horizontal scalability and throughput of databases,Proactive and \u201ccan-do\u201d mentality,Expereince with Node.js,Experience with BI visualization tools such as Superset", "Experience using LookML (Looker),Hands-on coding experience and expertise in back end related technologies, like Node or Python,B.S. in Computer Science or equivalent experience followed by 5+ years work experience in using SQL and databases in a business environment,Expertise in Data Visualization,Deep experience in the latest libraries and programming techniques,Familiar with SQL/NoSQL databases like MongoDB and their declarative query languages,Knowledge in using BI Analytics and related technologies,You have accomplishments that showcase your capabilities by their success and technical depth.,You own new features from idea to completion.,Work well with a core team to design and execute major new features.,Enjoy contributing to a fast moving exciting project,Strong communicator and fluent in English with excellent written and verbal communication skills.,Thrive and excel in our diverse, distributed and agile team environment,Competitive Vacation Package,Annual Financial Allowance for YOUR development,Flexible Family Leave,Clevertech Gives Back Program,Clevertech U (Leadership Program, Habit Building, New Skills Training),Clevertech Swag", "On-premise and cloud-based deployment patterns,Streaming, micro-batching and right-time use cases,Provide innovative data engineering design and deployment approaches that leverage innovations in in-memory processing, agile delivery, automation, storage architectures etc. to design modern data pipelines at speed and scale,Working closely with technology partners, Accenture Technology Labs and Accenture Innovation centres to incubate emerging technologies and build prototypes/demos to enhance our data engineering codebases and frameworks,Mentor and upskill other data engineers", "Working with the Senior Data Engineer in designing, building, future-proofing and operating a scalable data infrastructure,Working with the Senior Data Engineer in delivering data platform infrastructure, tooling and reporting on a regular basis,Working with the Senior Data Engineer in understanding the needs of our product teams and delivering valued data models,Enabling better testing and development workflows,Supporting the creation of data products that promote customer self-serve,Work with engineering to ensure the collection of high-quality data,Previous software engineering experience working in environments dealing with large datasets,Understanding of modern code-driven data engineering frameworks like Airflow,Understanding of functional data engineering principles,Familiarity with testing data pipelines,An understanding of distributed data processing methodologies, frameworks, and best practices,Solid understanding of databases and a working knowledge of SQL,Experience working with Python,Experience working with cloud products (e.g. S3, Kinesis, SQS, SNS),Experience working with Agile methodologies and a cross-functional environment,Experience with machine learning libraries", "Analyzes processes and data by extracting data from various data warehouse environments.,Performs data mapping for systems integration, data provisioning, and\u2026", "Houston, TX", "Drive design and implementation leveraging modern design patterns,Ability to partner effectively with UX, PM, DevOps, QE and other developers to design and implement meeting the spirit of requirements,Experience in modern front end frameworks,Technically curious to keep present on advances in technology,Experience with segregation of model, presentation, and business logic.,TDD experience and strong desire to build in test from the start.,Write code (Test or Product) to deliver against project timescales, quality and requirements in various languages including Java or other selected languages for Sophos Cloud products.,Provide guidance and mentorship to junior developers in daily Scrum meetings.,Possesses a passion for solving complex Big Data problems,Plan for, design and implement our next-generation cloud security products.,Be involved from inception through implementation in a real hands on fashion.,BS in Computer Science, Engineering, or equivalent with 7+ years of development and data modelling background,Building highly scalable SaaS solutions using Big Data technologies,Experience with CI/CD,Experience with Agile Software Development methodologies (scrum/ kanban),Excellent attention to detail,Excellent verbal and written communication skills,Experience with the following technologies (recommended) and strong desire to learn (required),Programming language -- Java (must)\nBatch processing -- Hadoop ,MapReduce\nStream processing -- Kafka and Amazon Kinesis\nNoSQL -S3 , MongoDB.\nColumnar stores - HBASE, Amazon Redshift\nRestful web services\nCode/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...\nAmazon Web Services\nData Warehousing/ built ETL,Programming language -- Java (must),Batch processing -- Hadoop ,MapReduce,Stream processing -- Kafka and Amazon Kinesis,NoSQL -S3 , MongoDB.,Columnar stores - HBASE, Amazon Redshift,Restful web services,Code/Build/Deployment -- git, hg, svn, maven, sbt, jenkins, bamboo, ...,Amazon Web Services,Data Warehousing/ built ETL,Medical insurance,Dental insurance,Vision insurance,Life insurance,Long-term disability insurance,401k plan,Vacation time", "The specific accommodation requested to complete the employment application.,The name of the organization being represented,The location of the organization", "Optimize and execute on requests to pull, analyze, interpret and visualize data,Partner with team leaders across the organization to build out and iterate on team, and individual performance metrics,Optimize our data release processes, and partner with team leads to iterate on and improve existing data pipelines.,Design and develop systems that ingest and transform our data streams using the latest tools.,Design, build, and integrate new cutting edge databases and data warehouses, develop new data schemas and figure out new innovative ways of storing and representing our data.,Research, architect, build, and test robust, highly available and massively scalable systems, software, and services.,5+ years of data engineering experience,Robust experience with Python, Spark, and Airflow,Experience writing and executing complex SQL queries,Experience building data pipelines and ETL design (implementation and maintenance),Experience with AWS or other IAAS or PAAS provider,Scrum/Agile software development process.,Real estate experience,Experience with Periscope, Looker, Tableau and other BI tools,Experience with building data pipelines,Experience with machine learning,Medical, vision, dental, and paternity/maternity benefits.,401(k),Commuter benefits,Flexible time off policy,Catered lunches and snacks,Corporate gym membership", "Collaborate with Data Engineers and Business SME's to develop data products and services.,Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management.,Implement standardized, automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs.,Work in product teams in support of operations data platform backlog", "Scaling our existing data processes, focusing on quality every step of the way,Provide guidance on technology, library and tooling choices to the team,Wary of things like scope creep, over-engineering and \u2018skipping unit tests for now\u2019,learning our business domain,good software engineering principles and practice,solving business problems whilst avoiding technical complexity,5 years or more of software development experience,,Working together, be that pairing, or wider team collaboration,A belief in the agile principles, not just a single \"Agile Process\",Willingness to use the appropriate tool for the job, not just the latest \"hot\" technology,Sharing your knowledge and experience with others,Good (current) knowledge of SQL and at least two other programming languages,Experience in collaborative design of software systems including contracts, integration points and database schemas,Google Cloud Platform,PostgreSQL,Bash,Python,Flyway,Oracle,Groovy,Build and design large scale real-time and batch data pipelines.,Distributed microservice based applications at scale,Docker and Kubernetes to run your amazing code,Wearing a Scrum Master hat from time to time", "Design data warehouse solutions using dimensional methodologies to support ETL processes and data analytics applications,Develop, implement and tune ETL processes,Write and tune SQL including database queries, ddl and dml, stored procedures, triggers, user defined functions, analytic functions, etc.,Create code that meets design specifications, follows standards, and is easy to maintain,Own features that you develop end to end. Work with end users on requirements gathering, develop and test your code, implement new processes in production, then maintain and support them over time,Drive our data platform and help evolve our technology stack and development best practices\nDevelop and unit test assigned features to meet product requirements,Work with Analytics and Digital Marketing teams to provide them the data they need to make efficient decisions,Work with Quality Assurance team to ensure that the processes are fully tested,Support and maintain dev/test/prod environments to meet business delivery specifications and needs,Assist with adhoc report generation and data analysis for customers,Be part of monthly on call rotation", "Deep technical experts and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends and practices,Design and develop cutting-edge solutions, using existing and emerging technology platforms", "You have experience in SQL: You\u2019ve used written complex SQL queries that join across data from multiple systems, matching them up even when there was not a straightforward way to join the tables. You've designed tables with an eye towards ease of use and high performance. You've documented schemas and created data dictionaries.,You are a skilled written communicator. Zapier is a 100% remote team and writing is our primary means of communication.,You appreciate our team\u2019s values of eagerness to collaborate with teammates from any function of the organization or with any level of data knowledge, iterating over your deliverables, and being curious.,You understand that the perfect is the enemy of the good and default to action by shipping MVP code and iterating as needed to get towards better solutions.,You have experience with APIs: You've ingested large quantities of data from RESTful APIs.,You have experience and a comfort level with programming. You can read and write code in Python, Go, Rust, Java, or C#. You're familiar with distributed source control using Git.,You have experience running infrastructure needed to orchestrate data pipelines, store data with different retention and performance requirements, and perform compute for multiple loads. Experience with tools like Ansible, Terraform, and/or Vagrant is a plus.,You understand columnar-store file formats like Orc or Parquet and are also familiar with Avro and Avro schemas.,Develop ETL to ingest and transform data from upstream databases and APIs into a data warehouse. The tools used include AWS Redshift, NiFi, Kafka, Matillion ETL, and custom Python.,Build, deploy, and continuously improve the infrastructure used by our data scientists and data and business analysts. The tools we have been using here include Docker, Terraform, Ansible, Kubernetes, and AWS EC2.,As a part of Zapier's all-hands philosophy, help customers via support to ensure they have the best experience possible.,Competitive salary (we pay based on the norms of your country),Great healthcare + dental + vision coverage*,Retirement plan with 4% company match*,Profit-sharing,2-3 annual company retreats to awesome places,14 weeks paid leave for new parents of biological or adopted children,Pick your own equipment. We'll set you up with whatever Apple laptop + monitor combo you want plus any software you need.,Unlimited vacation policy. Plus we require you to take at least 2 weeks off each year. We see most employees take 4-5 weeks off per year. This isn't a vague policy where unlimited vacation means no vacation.", "Collaborate with engineers to extract, transform, and load (ETL) data from a wide variety of in-house and 3rd party data sources,Ensure we have data consistency on both production and analytical databases. You will own the integrity of our data from end-to-end and the company will make high impact decisions based on this data.,Build a data warehouse to provide timely data to multiple third party applications (Salesforce, Marketo, etc),Design and build tools that make our data pipelines and surfacing more reliable and easier to use,Work closely with backend engineers to roll out new tools and features,Triage, identify, and fix scaling challenges,Collaborate with internal data customers to gather requirements,You have at least 2 years of experience with at least one relational database\u2014MySQL, Postgres, Oracle or other.,You have experience with SQL and Data Warehousing using a relational database.,You are experienced with large-scale data pipelines and ETL tooling.,You have previous coding experience. Our ETL process is in Ruby and we use Python for some data analysis.,You are experienced with EDA (Exploratory Data Analysis) and Data Visualization (we use Tableau).,You have used Amazon Redshift.,You can manipulate data using Python (pandas, numpy, scikit-learn, etc).", "Design and develop scientific databases, create methods to process and analyze omics data or other biological information.,Lead application development efforts and establish data engineering platforms to enable the storage, organization, dissemination, and analytics of dynamic data generated from innovative research, exploratory and clinical studies.,Contribute to strategic planning and implementation of our data engineering platforms for multiple functions to ensure data accessibility, quality, and integrity.,Bachelor\u2019s with 7+ years relevant experience; or Master\u2019s/PhD with 4+ years relevant experience; or equivalent experience. Degree in computer sciences, bioinformatics or related field.,Track record of successfully delivering large scale informatics solutions to address complex scientific data challenges, and applying modern software engineering practices to deliver applications and/or scientific data analyses.,Experience with omics data analysis and method development,Familiarity with relational and non- relational databases. Working knowledge of scientific applications development cycles, data management techniques and infrastructure requirements.,Working knowledge of scripting languages, Python and R strongly preferred,Demonstrated adherence to best practices in software engineering, particularly iterative development, version control, testing and modular design.,Experience in operating on large data, such as data stored in relational and non-relational databases, HDF5 files or parquet files.", "Trust & confidence \u2013 Making our intent and actions be transparent and honest while fostering healthy, inclusive relationships, actively listening and maintaining open communications, delivering on our promises, investing in other's success, and engaging each other and the community,Creative work environment \u2013 Exploring meaningful new ideas and relationships to foster innovation and encourage collaboration and creativity. Challenging the standard method of doing business in a positive environment.,Now & later perspective \u2013 Looking beyond day to day challenges to better anticipate the future and adjust to change.,Respectful & inclusive work environment \u2013 Seeking out a wide range of voices and making each other feel respected and included.,Self-awareness & emotional intelligence \u2013 Learning to recognize and understand our own emotions and the emotions of others and recognizing our individual strengths and weakness to raise self-awareness so we can perform better.,Wellbeing - We take care of ourselves and each other.,Excellent communication skills & a drive to understand customers,The ability to bring an understanding of Data Governance; Shows an understanding and adherence to established industry standard policies (such as DGI) and related experience,Performance optimization of databases in a multi-tier architectural environment,Obsessive attention to detail with a strong interest in automating any duplicate systems,The ability to provide direct support for the integration of data from, and interfaces and data flows between and among multiple applications from multiple sources, both in-house and vendor-provided.,Oversee all aspects of database security, business continuity, database usage standards and practices, database infrastructure design, implementation, integration, troubleshooting, and administration.,Support database architecture strategy, data modeling, and database design,Provide Tier-2 and Tier-3 support to development teams and business users with database operations, including the development of complex SQL, tuning of DML and the creation of stored procedures.,Develop, install, maintain and monitor globally-distributed hybrid-cloud corporate databases in a high performance/high availability environment supporting existing and new enterprise products,Manage backups and synchronization of multiple databases between data centers.,Support design and development of SQL code and SSRS reports or other reporting platforms,Transact-SQL experience and SQL Server clustering/failover qualifications required,Performs other technical tasks relative to the administration of City databases.", "Carry out data acquisition activities ensuring the accuracy and integrity of data through data analysis, coding, and documentation of ETL processes,Develop, test, and implement ETL program logic using a variety of SQL dialects and the SAS programming language,Analyze and translate business requirements and functional specifications into technical specifications as part of the development process,Collaborate with IT in client organizations to design and test the data acquisition and ETL processes,Perform unit testing, system integration testing, and user acceptance testing and fix defects found by testing,Provide expert guidance and assistance to colleagues in other functional areas,Advanced SQL programming with Oracle, DB2, Teradata, and/or other RDBMS,SAS programming skills, including SAS PROC SQL, SAS Data Step, and SAS Macro programming skills,Understanding and usage of SAS Enterprise Guide and SAS Stored Processes,Ability to convey to management and the client what has been built and why,Ability to contribute in a highly collaborative team environment,Ability to complete analytical tasks independently with minimal guidance,Ability to take initiative,An intrinsic dedication to quality,Proficiency in Microsoft Office (Excel, Access),UNIX shell/Perl/Python scripting,Working knowledge of SAS BI Web Services,Experience programming with .NET or Java,Experience working with Informatica,Experience data modeling in ERwin,Experience with data in financial services, insurance, retail, and energy business,Experience with Hadoop/No-SQL data stores", "Develop data warehouse/data lake (following defined architecture and only as applicable for M&E data) for ease of use, organization, accessibility, security, compliance, performance, scalability, monitoring, and availability;,Under M&E guidance, clean, transform, aggregate and integrate M&E data into data warehouse;,Develop ETL system and data pipelines (following defined standards) that move data from a variety of sources into warehouse, monitor data quality, check for errors and conform data to standards; use the WFP corporate ETL/data-lineage tools;,Build Database objects (tables, views, stored procedures, functions, etc.) and data services that provide data in format most useful for analysis;,Maintain data pipelines, architecture and schemas to maximize the (re-)usability, accuracy, robustness, performance, and scalability;,Collaborate with KECO and IT HQ teams to ensure standards are maintained,Document technical specifications and work in appropriate tools.,Build the Data Lake for M&E Data based on the RMT defined Architecture and using the WFP Corporate ETL/data lineage tools.,Migrate data from old M&E database to Data Lake.,Migrate data from ONA (online M to Data Lake,Develop API (where required) to facilitate an easier process to maintain.,Manage project deliverables and timeline,B.S. in CS, Engineering or other relevant field with additional years of related work experience or trainings/course. Advanced university degree in Computer Science, Engineering,2+ years of ETL/data engineering work (e.g. ETL (Extract Transform Load (ETL), data lakes, data marts, data warehousing),4+ years of professional software engineering,Knowledge and experience working with Hadoop stack (Horton Works),Experience working with/constructing Relational and noSQL/MongoDB databases,Experience working with engineers and business analysts to solve data needs,Experience working with/building a data pipeline,Experience building flexible data APIs,Experience working with Tableau,Ability to perform testing and regression testing and ensure high quality product work (e.g. ETL/ELT to the Data Lake, using the Data Lake for data analysis),Skilled at documenting data warehouses and hierarchies, process flows,Experience in metadata management and data quality processes and tools,Knowledge of Hadoop tools (e.g. Hive, Impala, Pig, Sqoop, Hue, Kafka, etc.", "Design, develop, document, and test advanced data systems that bring together data from disparate sources, making it available to data scientists, analysts, and other users using scripting and/or programming languages (Python, Java, Scala, etc),Evaluate structured and unstructured datasets utilizing statistics, data mining, and predictive analytics to gain additional business insights,Design, develop, and implement data processing pipelines at scale,Present programming documentation and design to team members and convey complex information in a clear and concise manner.,Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient programming processes.,Write and refine code to ensure performance and reliability of data extraction and processing.,Communicate with all levels of stakeholders as appropriate, including executives, data modelers, application developers, business users, and customers,Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.,Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.,Collaborate with Quality Assurance resources to debug code and ensure the timely delivery of products.", "Analytic & systematic.,Service minded problem solver,You have a bachelor/master in IT/Computer Science/Mathematics/Statistics,Working experience as a Data Engineer/ETL developer for 3+ years,You have developed solid SQL and database skills,Good understanding of DWH development concepts and data processing principles,Experience with visual ETL tools like Informatica PowerCenter, Pentaho or similar,Experience with programming/scripting and/or interest to learn python/pyspark", "An analytical, quantitative or IT orientated degree at BSc or MSc level [HBO/WO],3+ years of work experience in commercial environment,Strong analytic SQL skills,Java backend experience,Knowledge of Database architectures,Good social and communication skills,Experience with big data solutions,Knowledge of cloud solutions,Common with data visualization tools,Flexible work mentality,Familiar with Oracle, JSON/REST API and/or Python etc.,Competitive compensation package,Annual company performance-based bonuses,A non-hierarchical workplace,Young and highly motivated teammates,High responsibility", "Executes complex functional work tracks for the team.,Partners with ATSV teams on Big Data efforts.,Partners closely with team members on Big Data solutions for our data science community and analytic users.,Leverages and uses Big Data best practices / lessons learned to develop technical solutions used for descriptive analytics, ETL, predictive modeling, and prescriptive \u201creal time decisions\u201d analytics,Influence within the team on the effectiveness of Big Data systems to solve their business problems.,Participates in the development of complex technical solutions using Big Data techniques in data & analytics processes.,Supports Innovation; regularly provides new ideas to help people, process, and technology that interact with analytic ecosystem.,Participates in the development of complex prototypes and department applications that integrate Big Data and advanced analytics to make business decisions.,Uses new areas of Big Data technologies, (ingestion, processing, distribution) and research delivery methods that can solve business problems.,Understands the Big Data related problems and requirements to identify the correct technical approach.,Works with key team members to ensure efforts within owned tracks of work will meet their needs.,Drives multiple tracks of work within the research group.,Identifies and develops Big Data sources & techniques to solve business problems.,Co-mingles data sources to lead work on data and problems across departments to drive improved business & technical results through designing, building, and partnering to implement models.,Manages various Big Data analytic tool development projects with midsize teams.,Executes on Big Data requests to improve the accuracy, quality, completeness, speed of data, and decisions made from Big Data analysis.,Uses, learns, teaches, and supports a wide variety of Big Data and Data Science tools to achieve results (i.e. Hadoop, SQL, no-SQL [MongoDB] and others).,Uses, learns, teaches, and supports a wide variety of programming languages on Big Data and Data Science work (i.e. Java, Python, and Perl).", "You will be responsible for maintaining and evolving our existing data warehouse, develop data models and ETL and ensuring data consistency;,You will be working closely with the Business Intelligence and Big Data teams in the delivery of application and expanding our platform (using Big Data technology) to support the data requirements of the different departments and brands;,You will be involved, in data Integration which includes internal data, 3rd parties, and migrations;,Involved with Data Modelling, OLAP Cube Development and verifying data consistency and accuracy;,Keeping up to date on research and development of new technologies and techniques to enhance our data platform;,Collaborating with the Data DevOps, Business Intelligence, Big Data and Analytics teams.,You are an enthusiastic developer with experience in Data Warehousing and Business Intelligence;,You will ideally have a Bachelor or Master's degree in a relevant technical area;,Possess excellent English communication skills and enjoy working in a large team;,Think of SQL while having breakfast;,Have an advanced understanding of data warehouse and ETL concepts;,Have strong knowledge of data modelling and database architecture;,Have strong analytical and problem-solving skills;,Have the ability to effectively prioritize and handle multiple tasks and projects;,Have development experience in a Microsoft environment (SQL Server, SSIS, SSAS);,Have experience with data Integration tools (preferably SSIS), Data Warehouse modelling (Kimball);,Be familiar with at least one Object Oriented Language (.NET, Java, Python);,Be experienced and knowledgeable in core database development tuning techniques;,Be experienced with or have an understanding of big data technologies (Hadoop, HBase, Hive, Pig etc.);", "Lead downstream purification process development for exosome therapeutic development programs, currently in the preclinical stage.,Develop FPLC-based scalable column chromatographic methods for the purification and isolation of high-quality exosome products, including, but not limited to, preparatory scale affinity purification, ion exchange (IEX) chromatography, multimodal, hydrophobic interaction, and size exclusion (SEC) chromatography.,Execute and improve current Tangential Flow Filtration (TFF) procedure for isolation and formulation of exosome products.,As a subject matter expert, maintain knowledge and implement state-of-the-art technologies and processes for exosome purification and isolation.,Author process descriptions, SOPs, protocols, and tech transfer documents necessary for seamless process transfer to internal or external collaborators.,Meticulously maintain complete and accurate records of all work performed in LIMS and/or electronic lab notebook (ELN).,Play a central role in selection and interaction with contract manufacturers/research organizations, key suppliers, and other external vendors. Prior experience working with and managing CMO/CDMO/CRO desired.,Manage the downstream process development function, including operational considerations, lab buildout and equipment purchases, team recruitment, and training, setting key metrics and objectives, and providing guidance and mentorship to junior staff.,Coordinate tasks across multiple projects, demonstrating prioritization and planning. We are a start-up and are looking for applicants who are ready and willing to 'roll their sleeves up' and be the Key Driver to meet project timelines and milestones.", "Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools,Implementing data flows to connect operational systems, data for analytics and BI systems. Re-engineer manual data flows to enable scaling and repeatable use,Working closely with data architects (to determine what data management systems are appropriate) and data scientists (to determine what data is needed for analysis).,Tackling problems associated with database integration and unstructured data sets,Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently,Strong technical process understanding regardless of technology,Wide range of strong technical skills (i.e. Azure Devops, Azure Data Factory, Data Bricks, SQL, python),Core SQL Competencies \u2013 SSMS, SSIS, T-SQL, Stored Procedures,ADF Pipelines to build and populate SQL databases,Background in migrating traditional MS products to Azure,Ability to create efficient DW or DL structures to minimise cost of orchestration / processing and ingestion of data,Very high attention to detail,Strong communication skills,Efficient in building ETL and ELT processes for enterprise solutions,Strong software delivery methods and knowledge,Digital delivery \u2013 has a track record of working on DevOps delivery,Exposure in Climate Change data legislation, practices and stakeholders,Experience in Environmental related industries i.e Water, Energy, Forestry related,Presentation skills,Understanding of architecting solutions taking into account wider considerations", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results,Connections to recruiters and industry experts through online and live Devex events", "Create and improve systems to enable end to end solutions and products in support of Commercialization activities,Build data micro services which perform data transformation, metadata extraction, workload management and error processing management,Query, manipulate, and visualize data using R, JavaScript / CSS, and Tableau,Evaluate and utilize state of the art technologies in the industry to meet business needs,Contribute to maturing the Planisware system capabilities,Contribute to continuous development of the data analytics and insights strategy for Commercialization,Integrate the operations data platform with analytical tools such as Tableau, the Data Scientist Workbench, the Data Marketplace, etc.,Develop data processing pipelines for large datasets in the cloud (AWS); integrate with other data sources where applicable;,Collaborate with the other engineering team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms,Adhere to best practices for testing and designing reusable code,Ensure effective communication between key partners, including business clients, technical staff and vendors to analyze scientific needs and implement informatics solutions in data acquisition, integration and analysis,Own and run product backlog delivery", "Working on data cleansing, batch processing, data transformations, and other data manipulations to enable our data science efforts.,Advanced image manipulation,Geometry/geographic calculations,Serving as part of the core team for the technology stack,Partnering closely with the Founders to bring a disruptive AI based technology platform to the insurance and real estate markets", "Planning, building and running enterprise class information management solutions across a variety of technologies (e.g. big data, master data, data profiling, batch processing, and data indexing technologies, ).,Establishing advance search solutions that include synonym, inference and faceted searching.,Ensuring appropriate security and compliance policies are followed for information access and dissemination.,Defining and applying information quality and consistency business rules throughout the data processing lifecycle.,Collaborating with information providers to ensure quality data updates are processed in a timely fashion.,Enforcing and expanding use of AbbVie Common Data Model and industry standard information descriptions (ontologies, taxonomies, vocabularies, lexicons, dictionaries, thesaurasus, glossaries etc...),Managing the information portal and its customer-facing resources (data catalog, data portal, etc...),Bachelor's Degree with 10+ years of related work experience and a strong understanding of specified functional area. Degree in Computer Science or related discipline preferred. Advanced degree preferred.,At least 10 years experience in a several data processing roles such as database developer/administrator, ETL developer, data analyst, BI analytics developer, and/or solution developer of contextual search applications.", "The ability to absorb the nuances of the Bio-Tech operations value chain, including supply chain, logistics, and manufacturing source systems;,High personal standards of productivity and quality,Able to function as scrum master for the Data Engineering Team,The ability to contribute in a collaborative and fast paced environment.,Defines and approves data engineering design patterns to be used for general re-use on multiple implementations,Collaborate with Data Architects, Business SME's, and Data Scientists to architect data products and services.,Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management.,Implement standardized, automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs.,Drive the exploration and adoption of new tools, and techniques and propose improvements to the data pipeline,Integrate the operations data platform with the Data Scientist workbench, the Data Marketplace, and Analytic Tools such as Tableau, Spotfire, R, etc.,Act as a product manager for the operations data platform backlog,Act as a run manager, provide Run/DevOps support", "Work with our CTO to create and maintain optimal data pipeline architecture and documentation.,Assemble large, complex data sets that meet functional / non-functional business requirements.,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.,Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using big data' technologies.,Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.,Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results", "Design local modifications to our global data architecture, including new tools and technologies where necessary to meet regional use-cases,Provide direction to the development of bespoke, client-specific data sandboxes,Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack,Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions,Develop custom-built packages and \u201cglue code\u201d to support the needs of Data Scientists across the region,Work with broader business stakeholders to assist clients and consultants with their data and infrastructure needs", "Intern-Data Engineer,Attend meetings to learn about business practices, and internal processes and procedures.,Assist in developing and maintaining data solutions that support short and long-term information & analysis goals.,Develop and maintain ETL procedures for optimal processing of data from a wide variety of data sources. Ensure data is verified and quality is checked,Create and maintain SSRS reports to ensure business partners have information to make informed decisions.,Build analytic data sets to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.,Work with various Midco stakeholders to assist with data-related technical issues and support their data infrastructure needs.,Ensure data integrity through regular communications with IT developers, Analysts and SMEs,Write database documentation, including data standards, procedures and definitions for the data dictionary ('metadata').,Communicate effectively and professionally in all forms of communication with internal and external customers.,Adhere to Midco privacy guidelines to ensure each customer's privacy.,Maintain regular attendance as required by your position.,Function as an effective team member while supporting the efforts and concepts of other departments.,Support the mission, vision, and values of Midco.,Apply personal ethics, honesty, initiative, flexibility, responsibility and confidentiality in all areas of responsibility.,Possess an enthusiastic, energetic, self-motivated, and detail-oriented approach towards work and all work projects.,Possess strong problem solving and decision making skills while using good judgment.,Multi-task and change from one task to another without loss of efficiency or composure.,Maintain a positive work atmosphere by acting and communicating in a manner so that you get along with customers, clients, co-workers and management.,Identify opportunities for improvement while creating and implementing viable solutions,Actively follow Midco policies and procedures.,Perform other duties as assigned.,High School Diploma or GED equivalent and enrollment in a college or university pursuing an Associate's or Bachelor's degree in MIS or related field. Junior or Senior level student status, or recent graduate is highly preferred.,Experience or knowledge in dimensional modelling is preferred.,Knowledge of SQL server scripts and SSIS preferred.,Employees may be required to work in excess of 40 hours per week and other than normal business hours, such as holidays, evenings and weekends as business demands.,The employee is occasionally required to reach with hands and arms, stoop, kneel, or crouch. The employee must occasionally lift and/or carry loads of up to 30 lbs.,The noise level in the work environment is moderate.,Free and discounted Midco internet / cable,Tuition reimbursement,Support of employee involvement in the communities we serve,Employee referral program,Wellness programs,Never accept a check or other funds from a company to purchase materials necessary for your position.,Avoid and report situations where employers require payment or work without compensation as part of the application process.,Avoid corresponding with anyone who reaches out via text or email or outside of the Chegg Internships platform that you don't recognize.", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results,Connections to recruiters and industry experts through online and live Devex events", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results", "Collaborate with Data Architects, Business SME's, and Data Scientists to architect data products and services.,Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management.,Implement standardized, automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs.,Contribute to the Exploration and understanding of new tools, and techniques and propose improvements to the data pipeline,Integrate the operations data platform with the Data Scientist workbench, the Data Marketplace, and Analytic Tools such as Tableau, Spotfire, R, etc.,Act as a product manager for the operations data platform backlog", "You\u2019re passionate about getting at data and creating high quality, easy to consume views into that data,You continually improve by learning from others, and you jump in when a teammate could use your help,You care about your customers, and understand how your data contributes to the goals of the business,You have an agile mindset, and are comfortable refining vague requirements,You can sense miscommunications among team members, and do your part to improve understanding,You thrive in and contribute to a positive work environment, where everyone shares constructive thoughts and suggestions", "A Data Engineer, BI or database developer who is passionate about having a meaningful, global impact,Experience in data integration, cleaning, validation and analysis,Strong SQL skills, proficiency with R and Git,DevOps, System or Database Administration experience,History of working with Docker and Azure, or familiar services,A pro-active problem-solver with strong analytical skills and a good sense for code and data quality,Further technical expertise, such as web development or data visualisation is very welcome,Expertise in finance is a big plus,Fluent English,Design and implement systems to make our data analysis reliable, transparent and reproducible,Maintain, extend and improve our existing data infrastructure and related services,Support and train colleagues for continuous improvement of skills,Handle technical communication with stakeholders and external partners,Evaluate and develop technical solutions for new projects and use cases,A meaningful impact on one of the planet\u2019s most pressing issues,Engaging working environment in a young, international and rapidly expanding team and the chance to work at a cutting-edge think tank,Flexible working hours,Chance to take on responsibility and shape a project from your first day onwards,Opportunity for significant career development", "Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools,Implementing data flows to connect operational systems, data for analytics and BI systems. Re-engineer manual data flows to enable scaling and repeatable use,Working closely with data architects (to determine what data management systems are appropriate) and data scientists (to determine what data is needed for analysis).,Tackling problems associated with database integration and unstructured data sets,Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently,Strong technical process understanding regardless of technology,Wide range of strong technical skills (i.e. Azure Devops, Azure Data Factory, Data Bricks, SQL, python),Core SQL Competencies \u2013 SSMS, SSIS, T-SQL, Stored Procedures,ADF Pipelines to build and populate SQL databases,Background in migrating traditional MS products to Azure,Ability to create efficient DW or DL structures to minimise cost of orchestration / processing and ingestion of data,Very high attention to detail,Strong communication skills,Efficient in building ETL and ELT processes for enterprise solutions,Strong software delivery methods and knowledge,Digital delivery \u2013 has a track record of working on DevOps delivery,Exposure in Climate Change data legislation, practices and stakeholders,Experience in Environmental related industries i.e Water, Energy, Forestry related,Presentation skills,Understanding of architecting solutions taking into account wider considerations", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results,Connections to recruiters and industry experts through online and live Devex events", "Experienced data pipeline developers with a proven track-record of developing, deploying, and optimizing data systems to support analytics from the ground up,Data and tech savvy,Innately curious and fast learners,Self-motivated team players with a drive to get things done,Design, develop, scale, and maintain data pipelines that extract, load, transform, and integrate data from wide variety of data sources to provide uniform view.,Automate and optimize the data pipelines to improve productivity, processing performance and reliability, and minimize error-prone processes.,Monitor data consumption patterns and ensure responsible use of provisioned data by the data consumers.,Collaborate with data stewards and data governance teams to implement the data governance and compliance best practices.,Support data scientists and data analysts by optimizing data management and delivery processes.", "Build data pipeline frameworks to automate high-volume and real-time data delivery to our cloud platform", "Participate in software design meetings and analyze user needs to determine technical requirements.,Write technical specifications based on conceptual design and stated business requirements.,Knowledge of standards relevant to the software industry , e.g., ISO, CMM, Six Sigma,You are very organized and detail-oriented; able to work with shifting priorities,You love analyzing issues and devising efficiencies to better the client experience,You are looking to join our team and build a long-term career with FIS,Knowledge of financial services industry,Health coverage offered for you and your family through Health/Vision/Dental/Insurance plans,401K with company contribution and Employee Stock Purchase Program with company match,FIS Gives Back Program - charitable events and activities to help support our local community", "Work with the Director of Institutional Research and UDS analytic team to identify common metrics for both internal and external data requests, gather requirements for datasets, and translate those requirements into actionable projects;,Build, manage and \u201cclean\u201d large datasets, creating curated, focused, fit-for-purpose datasets that optimize efficiency in UDS reporting and analysis;,Train UDS analysts on available datasets, including field/variable definitions;,Closely collaborate with the Director of Institutional Research and UDS analysts to update and refine datasets on an annual or as-needed basis, as specifications evolve;,As needed, prepare data and tabulate metrics to meet internal and/or external reporting requirements from accreditors, state and federal governments, and other agencies and organizations;,Develop and standardize reporting procedures and streamline reporting activities and processes, in order to establish efficient methods of reporting;,Thoroughly document reporting efforts in UDS records for future reference;,Identify, investigate, and work to resolve data quality issues with relevant functional units and data administration and governance colleagues.", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results,Connections to recruiters and industry experts through online and live Devex events", "Translate business requirements into data models that are easy to understand and used by different disciplines across the company,Design, implement and build pipelines that deliver data with measurable quality under the SLA,Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service,Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements,Own and document foundational company metrics with a clear definition and data lineage,Identify, document and promote best practices,5+ years of experience working in data architecture, data modeling, master data management, metadata management,Recent accomplishments working with relational as well as NoSQL data stores, methods and approaches (logging, columnar, star and snowflake, dimensional modeling),Proven track record in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLAP and Data Warehouse environments,Demonstrated skills with either Python or Java programming language,Familiar with data governance frameworks, SDLC, and Agile methodology,Excellent written and verbal communication and interpersonal skills, and ability to effectively collaborate with technical and business partners,Hands-on experience with Big Data technologies (e.g Hadoop, Hive, Spark) is a big plus", "Using programming and scripting to parse/prepare data,Building and maintaining scientific tools and infrastructure,Applying machine learning to uncover novel trends in experimental data,Working effectively with project teams to develop informatics solutions to satisfy business needs", "Strong PT behaviors, with an emphasis on continuous improvement", "Work with Data Scientists and business partners on cross functional teams; developing subject matter expertise in the business as well as advanced analytics.,Provide support on requirement development for analytic data sources, breaking down business problems into solvable components and assist with documenting requirements with minimal supervision.,Execute rapid development of new data and analytic work tracks with fast iteration over quick sprints,Help develop and deliver the data infrastructure required to support needs of predictive modeling and analytics with minimal supervision.,Builds test scripts, executes testing, works with data scientists and business to ensure end user acceptance,Leverage \u201cagile\u201d data analysis with technology fluency in parallel processing/programming, software/programming languages and technologies (i.e. Oracle, MongoDB, SQL, Python, Spark, Kafka, Scala and Hadoop), paired with a high degree of analytic agility to be able to meet fluid and dynamic business needs in this space.,Participate in the development of enterprise data assets, information platforms or data spaces designed for exploring and understanding the data.,Participate in the development of new concepts, proof of concept designs, and prototypes for business or research data solutions so that business users or predictive modelers may visually understand and explore a new feature or functionality before implementation to expose design assumptions and drive ideation.", "Lead the development of Relational Database Service (RDS) structure; overseeing the key technical contributors when assessing and evaluating future reporting solutions,Contribute to the design and development for on-demand reporting solutions to support key stakeholders,Assess and leverage new technologies to ensure that the data solutions are stable, efficient, and responsive to appropriate business needs,Maintain and upkeep the program's vision and ensure interoperability cost effective and authorized technologies (e.g., cloud environments),Maintain awareness of the changing analytics environment by partnering with Amgen's Analytics team and industry (e.g., AI, machine learning, etc.),Take ownership of relevant issues and remediate problems through to completion, to include providing on-call support for data and integration solutions,Work with data engineers to provide clear documentation for delivered solutions and processes, integrating documentation with the appropriate corporate stakeholders,Develop, implement, and sustain operational scripts, data structures, libraries and programming code that optimize security in emergent compute patterns with diverse applications throughout the global environment,Analyze, design, develop and operate programs, shell scripts, tests, and infrastructure automation capabilities in an advanced security context,Collaborate cross-functionally with analysts, engineers, data scientists to identify & prioritize requirements, brainstorm solutions, and clarify business objectives for data-centric solutions to achieve continuous improvement in cyber defense/resilience,Assist with data discovery for enhancing reports and designing efficient data stores,Contribute to the exploration and understanding of new tools, and techniques and propose improvements to the data pipeline,Self-starter with a high degree of initiative,Must be highly motivated and able to work effectively under minimal supervision", "Converts, merges and/or assembles data for analysis. Develops and maintains programs that convert data into analyzable formats and/or that can be utilized for data analysis, with a strong emphasis on generalizability and transparency.,Contributes to the ongoing automation of existing and emerging processes for importing, documenting, integrating, and updating data as well as to coding best practices such as code review and version control.,Develops static and interactive reports and visualizations to facilitate improved data use by agencies and organizations that seek to improve the wellbeing of children and families.,Collaborates with researchers, data scientists, policy and implementation specialists, and IT staff in project and system design activities.", "Hybrid technical role interfacing with process SMEs using state of the art technologies, whilst also being able to communicate complex intractable ideas to non-technical audiences.,Collect clear requirements from SMEs and process experts.,Work with our process experts to model their data landscape, obtain data extracts and define secure data exchange approaches,Acquire, ingest, and process data from multiple sources and systems into Big Data platforms,Understanding, assessing and mapping the data landscape,Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models,Maintaining our Information Security standards on the engagement,Defining the technology stack to be provisioned by our infrastructure team", "Never accept a check or other funds from a company to purchase materials necessary for your position.,Avoid and report situations where employers require payment or work without compensation as part of the application process.,Avoid corresponding with anyone who reaches out via text or email or outside of the Chegg Internships platform that you don't recognize.", "Interface with end users to gather, understand and translate requirements into the Agile delivery process,Work closely with other Data Platform engineers to provide data requirements,Work closely with product owners, designers, other developers, architects, quality engineers, and DevOps to deliver innovative solutions that solve complex healthcare problems,Design and implement highly performing ETL used internally by Operation, Product teams,Building generic ingestion engine, data processing through AWS EMR, synchronizing data between up and down streams; AWS RDS, Dynamo, ElasticSearch, RedShift, s3, and dockerizing anything running in production,Manage individual sprint priorities, deadlines and deliverables.,Ensure that we are continuously raising our standard of data engineering excellence by implementing best practices for coding, testing and deploying,In addition to being able to carry out the above responsibilities, we\u2019re looking for someone comfortable working in a fast paced, ever changing environment who has a good deal of experience with SaaS applications, as indicated by the following attributes,8+ years software development experience,Strong experience in database technologies (SQL / MySQL) and data integration,Strong experience with scripting languages; Python and Shell Script,Experience with Amazon Web Services,Hands-on experience with Hive and Pig,Experience in real stream processing with Spark/Kafka/Kinesis (nice to have),Experience in building and working in a heavy agile, collaborative, innovative, flexible and team-oriented environment,Self-driven and ability to lead complex projects,Ability to influence others and direction of projects,An excellent leader with a \u201ccan-do\u201d attitude,Hands-on, detail-oriented, methodical & inquisitive,A motivated self-starter with a solid level of experience that quickly grasps complex challenges,A skillful communicator with experience working with technical management teams,A strong customer focus and metrics driven,A quick learner with a passion to challenge him/herself outside of the comfort zone.,Fantastic collaborator, team player, negotiator, and influencer,Fast fail entrepreneurial and innovative spirit,Thrives in a fast-paced environment where continuous improvement is the norm and the bar for quality is extremely high,Excited by the challenges of working in a product team undergoing rapid growth serving millions of customers,Locally-based applicants highly preferred (Seattle),Making a difference is what we do. We do the right thing for the right reasons \u2013 and we do it well, even when it\u2019s hard.,You operate from a perspective of truly caring about our employees, clients, and customers and creating value for them.\nWe are strong individually and together, we\u2019re powerful.,We roll up our sleeves and get stuff done.,We\u2019re boldly and relentlessly reinventing healthcare.", "Work with business and client IS to develop analytic applications using graph technologies.,Participate in requirements and design workshops with internal business and client IS partners.,Develop information models,Design and develop data pipelines to extract data from varous sources using APIs, perform data transformation, and load triples into graph database/RDF triple store.,Participate in all aspects of the software development process using Agile development methodologies.,Maintain awareness and knowledge of industry trends and proactively identify and drive identification of opportunities to be leveraged.,Learn, evaluate and conduct proof of concepts on new technologies as they emerge and that are pertaining to Amgen environment.,Develop training, roadshow materials, and outreach events to increase the adoption of various solutions developed and of the platforms used.", "Contribute to the design and development for ETL solutions to support key partners,Collaborate with Application Architects and Business SMEs to design and develop end-to-end data pipelines and supporting infrastructure,Build and operationally support new infrastructure and analytics tools in a DevOps model using Python, SQL and AWS,Proactively identify & implement opportunities to automate tasks and develop reusable frameworks,Participate in efforts to design, build, and develop rapid Proof-of-Concept (POC) solutions and services\nBasic Qualifications\nBachelor's degree\nOr\nAssociate's degree and 4 years of Information Systems experience\nOr\nHigh school diploma / GED and 6 years of Information Systems experience\nPreferred Qualifications", "Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools,Implementing data flows to connect operational systems, data for analytics and BI systems. Re-engineer manual data flows to enable scaling and repeatable use,Working closely with data architects (to determine what data management systems are appropriate) and data scientists (to determine what data is needed for analysis).,Tackling problems associated with database integration and unstructured data sets,Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently,Strong technical process understanding regardless of technology,Wide range of strong technical skills (i.e. Azure Devops, Azure Data Factory, Data Bricks, SQL, python),Core SQL Competencies \u2013 SSMS, SSIS, T-SQL, Stored Procedures,ADF Pipelines to build and populate SQL databases,Background in migrating traditional MS products to Azure,Ability to create efficient DW or DL structures to minimise cost of orchestration / processing and ingestion of data,Very high attention to detail,Strong communication skills,Efficient in building ETL and ELT processes for enterprise solutions,Strong software delivery methods and knowledge,Digital delivery \u2013 has a track record of working on DevOps delivery,Exposure in Climate Change data legislation, practices and stakeholders,Experience in Environmental related industries i.e Water, Energy, Forestry related,Presentation skills,Understanding of architecting solutions taking into account wider considerations", "Work with teams to build and continue to evolve data models and data flows to enable data driven decision-making,Design alerting and testing to ensure the accuracy and timeliness of these pipelines. (e.g., improve instrumentation, optimize logging, etc),Identify the shared data needs across Stripe, understand their specific requirements, and build efficient and scalable data pipelines to meet the various needs to enable data-driven decisions across Stripe,Work with our data platform team to identify and integrate new tools into our data stack. For example we\u2019re currently evaluating Presto for use as an ad-hoc query tool.,Have a strong engineering background and are interested in data. You\u2019ll be writing production Scala and Python code along with occasional ad-hoc SQL queries,Have experience in writing and debugging ETL jobs using a distributed data framework (Hadoop/Spark etc\u2026),Have experience managing and designing data pipelines,Can follow the flow of data through various pipelines to debug data issues,Have experience with Scalding or Spark,Have experience with Airflow or other similar scheduling tools,Write a unified user data model that gives a complete view of our users across a varied set of products like Stripe Connect and Stripe Atlas,Continuing to lower the latency and bridge the gap between our production systems and our data warehouse,Working on our customer support data pipeline to help us track our time to response for our users and our total support ticket volume to help us staff our support team appropriately", "Participate in strategic planning discussions with technical and non-technical partners.,Effective and efficient utilization of programming tools and techniques.,Responsible for design, prototyping, and delivery of software solutions within a big data eco-system.,Responsible for development of advanced analytics solutions and integrating new tools to improve.,Inquisitive and continues to seek development opportunities.,Works on geographically dispersed team embracing Agile and DevOps principles.,Bachelor\u2019s Degree towards Computer Science or Computer Engineering.,Solid foundational understanding of object-oriented programing principles.,Familiarity with source control solutions (ex git, GitHub, Jenkins, Artifactory).,Strong Python, Java, or other OOP language skills.,Strong communication, collaboration, and problem-solving skills.", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results,Connections to recruiters and industry experts through online and live Devex events", "Collaborate with Data Architects, Business SME's, and Data Scientists to architect data products and services.,Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management.,Implement standardized, automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs.,Contribute to the Exploration and understanding of new tools, and techniques and propose improvements to the data pipeline,Integrate the operations data platform with the Data Scientist workbench, the Data Marketplace, and Analytic Tools such as Tableau, Spotfire, R, etc.,Act as a product manager for the operations data platform backlog,Act as a run manager, provide Run/DevOps support", "Gain broad experience and grow into the role of Data Solution Architect and build Data (Lake) solutions for the biggest and most complex international organizations.,Be expected as a technology and innovation driven person who is keen to work with new technologies and platforms and take them to the next level in Data Science.,Work with a diverse and global team of highly skilled people.,Alternate between working for clients and creating our own products.,Be a guru but behave as a consultant.,Be responsible for providing technical leadership to enterprise scale projects and solutions,Develop, maintain, and improve architectures, including reference and target architectures, principles, roadmaps, patterns, etc.,You have a Masters in Computer Science, Software Engineering or a comparable discipline.,You have hands on experience with Hadoop, Spark, Kubernetes.,You have strong OO and programming skills in any of Java, Scala, Python, R, C/C++.,You have experience with (Cloud based) Data warehouse, Data lake, ETL Pipelines, and Data Modelling tools,You have experience with cloud native, micro-services oriented, and big data/non-relational data architectures.,Flourish working with like-minded people, yet you can function autonomously.,Are excited about new technology and innovation.,Like to learn outside of your core activities in order to broaden your field of expertise.,Value great quality in your work and thinking ahead come natural to you while being pragmatic.,Are willing to work on client location and travel abroad.,Have or are willing to gain managerial skills and in time apply them,Affinity with Data and Architecture,Experience to articulate the trade-offs, benefits and risks of all architecture and design solutions", "Glovo is looking for a world-class data engineer to work in our Barcelona office. You are someone who loves working in a high-paced startup environment and solving difficult problems. You are passionate about data and inventing new and elegant solutions to support our internal customers\u2019 needs and believe in empowering the entire company by facilitating efficient access to data.,You will develop new data pipelines that leverage cloud architecture and perform transformations on existing data to support new use cases. You will be using Redshift as our primary data warehouse solution to create the curated data model for the enterprise to leverage.,Design, implement, and support an analytical platform providing ad-hoc and automated access to large datasets,Interface with internal teams to extract, transform, and load data from a wide variety of data sources using SQL and big data technologies,Continually improve ongoing ETL, reporting and analysis processes, automating or simplifying self-service support for customers", "Design and implement scalable data workflows and pipelines, and integrate diverse data sources and sinks,Design appropriate database schemas and optimize database deployment architectures for analytics query loads,Implement data transforms and organization for various data stores (data lakes and warehouses),Design and implement new platform architectures for building and serving machine learning models,Work with the platform operations team to monitor and maintain live production systems,Provide tooling and automation for infrastructure, continuous testing, and continuous deploy of data systems", "Work closely with other team members to optimize the organization\u2019s data systems and IT architecture,Design and build the infrastructure for data extraction, preparation, and loading of data from a variety of sources using technology such as SQL and AWS,Build data and analytics tools that will offer deeper insight into the organization\u2019s pipeline, allowing for critical discoveries surrounding key performance indicators and stakeholders\u2019 activity,Determine database structural requirements by analyzing client operations, applications and programming; review objectives with clients and evaluate current systems,Develop database solutions by designing proposed system; define database physical structure and functional capabilities, security, back-up and recovery specifications,Install database systems by developing flowcharts; apply optimum access techniques, coordinate installation actions and document actions,Maintain database performance by identifying and resolving production and application development problems, calculating optimum values for parameters; evaluating, integrating and installing new releases, completing maintenance and answering user questions,Provide database support, responding to user questions and resolving problems", "Full access to our jobs board, including over 1,000 exclusive jobs,Your Devex profile highlighted in recruiter search results", "Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools,Implementing data flows to connect operational systems, data for analytics and BI systems. Re-engineer manual data flows to enable scaling and repeatable use,Working closely with data architects (to determine what data management systems are appropriate) and data scientists (to determine what data is needed for analysis).,Tackling problems associated with database integration and unstructured data sets,Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently,Strong technical process understanding regardless of technology,Wide range of strong technical skills (i.e. Azure Devops, Azure Data Factory, Data Bricks, SQL, python),Core SQL Competencies \u2013 SSMS, SSIS, T-SQL, Stored Procedures,ADF Pipelines to build and populate SQL databases,Background in migrating traditional MS products to Azure,Ability to create efficient DW or DL structures to minimise cost of orchestration / processing and ingestion of data,Very high attention to detail,Strong communication skills,Efficient in building ETL and ELT processes for enterprise solutions,Strong software delivery methods and knowledge,Digital delivery \u2013 has a track record of working on DevOps delivery,Exposure in Climate Change data legislation, practices and stakeholders,Experience in Environmental related industries i.e Water, Energy, Forestry related,Presentation skills,Understanding of architecting solutions taking into account wider considerations", "Curate, design and catalogue high quality data models to ensure that data is accessible and reliable,Build highly scalable data processing frameworks for use across a wide range of datasets and applications,Provide data-driven insight and decision-making critical to GS's business processes, in order to expose data in a scalable and effective manner,Understanding existing and potential data sets in both an engineering and business context,Deploy modern data management tools to curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies,Evaluate, select and acquire new internal & external data sets that contribute to business decision making,Engineer streaming data processing pipelines,Drive adoption of Cloud technology for data processing and warehousing,Engage with data consumers and producers in order to design appropriate models to suit all needs,2-3 years of relevant work experience in a team-focused environment,A Bachelor's degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline),Extensive knowledge and proven experience applying domain driven design to build complex business applications,Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes,In-depth knowledge of relational and columnar SQL databases, including database design,General knowledge of business processes, data flows and the quantitative models that generate or consume data,Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts,Independent thinker, willing to engage, challenge or learn,Ability to stay commercially focused and to always push for quantifiable commercial impact,Strong work ethic, a sense of ownership and urgency,Strong analytical and problem solving skills,Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner,Financial Services industry experience,Working knowledge of more than one programming language (Python, Java, C++, C#, etc.)", "Exploring new ways of building, processing, and analysing data in order to deliver insights to our business partners,Lead and drive the creation of a Data Engineering center of excellence in PD Informatics", "Exploring new ways of building, processing, and analysing data in order to deliver insights to our business partners,Lead and drive the creation of a Data Engineering center of excellence in PD Informatics", "ISE,Professional Services offers clients a complete spectrum of software engineering services, including IoT / Telematics, Big Data, Cloud, Mobile App Development and Agile Consulting,Innovation Services provides the capability to achieve and sustain short and long-term growth throughout the entire organization. This approach allows ISE to remain focused on enhancing the customer\u2019s experience as they look for technology solutions", "Industrial Engineering - IT,Computer Science", "Professional Services offers clients a complete spectrum of software engineering services, including IoT / Telematics, Big Data, Cloud, Mobile App Development and Agile Consulting,Innovation Services provides the capability to achieve and sustain short and long-term growth throughout the entire organization. This approach allows ISE to remain focused on enhancing the customer s experience as they look for technology solutions", "A Data Engineer, BI or database developer who is passionate about having a meaningful, global impact,Experience in data integration, cleaning, validation and analysis,Strong SQL skills, proficiency with R, and familiarity with Git,DevOps, System or Database Administration experience,A pro-active problem-solver with strong analytical skills and a good sense for code quality,Genuine interest in improving our data infrastructure and related services,Fluent English,In addition, experience with finance is a big plus,Further technical expertise, such as web development or data visualisation, is very welcome as well,Somebody who has the courage to apply, even if they don\u2019t tick all the boxes above!,Support implementing systems to make our data analysis reliable, transparent and reproducible,Maintain, extend and improve our existing data infrastructure and related services,Support colleagues with data queries, SQL trainings for continuous improvement of skills,Evaluate and develop technical solutions for new projects and use cases,A meaningful impact on one of the planet\u2019s most pressing issues,Engaging working environment in a young, international and rapidly expanding team and the chance to work at a cutting-edge think tank,Flexible working hours,Chance to take on responsibility and shape a project from your first day onwards,Opportunity for significant career development", "Managing the investigation of corporate data requirements, documenting them according to the required standards utilising the prescribed methods and tools,Implementing data flows to connect operational systems, data for analytics and BI systems. Re-engineer manual data flows to enable scaling and repeatable use,Working closely with data architects (to determine what data management systems are appropriate) and data scientists (to determine what data is needed for analysis).,Tackling problems associated with database integration and unstructured data sets,Ensuring that those using the data structures and associated components have a good understanding and that any queries are dealt with promptly and efficiently,Strong technical process understanding regardless of technology,Wide range of strong technical skills (i.e. Azure Devops, Azure Data Factory, Data Bricks, SQL, python),Core SQL Competencies \u2013 SSMS, SSIS, T-SQL, Stored Procedures,ADF Pipelines to build and populate SQL databases,Background in migrating traditional MS products to Azure,Ability to create efficient DW or DL structures to minimise cost of orchestration / processing and ingestion of data,Very high attention to detail,Strong communication skills,Efficient in building ETL and ELT processes for enterprise solutions,Strong software delivery methods and knowledge,Digital delivery \u2013 has a track record of working on DevOps delivery,Exposure in Climate Change data legislation, practices and stakeholders,Experience in Environmental related industries i.e Water, Energy, Forestry related,Presentation skills,Understanding of architecting solutions taking into account wider considerations", "Build and own multi PB-scale data platform.,Design, code, and develop new features/fix bugs/enhancements to systems and data pipelines (ETLs) while adhering to the SLA.,Follow engineering best methodologies towards ensuring performance, reliability, scalability, and measurability.,Collaborate with other Software Engineers, ML Engineers, Data Scientists, and other stakeholders, taking on learning and leadership opportunities that will arise every single day.,Mentor junior engineers in the team to level them up.,Raise the bar on sustainable engineering by improving best practices, producing best in class of code, documentation, testing and monitoring.,Bachelor\u2019s degree in Computer Science, or a related technical discipline (or equivalent).,6+ years of strong data engineering design/development experience in building massively large scale distributed data platforms/products.,Advanced coding expertise in SQL & Python/JVM-based language.,Expert in heterogeneous data storage systems (relational, NoSQL, in-memory etc).,Deep knowledge of data modeling, lineage, access and its governance.,Excellent skills in AWS services like Redshift, Kinesis, Lambda, EMR, EKS/ECS etc.,Wide exposure to open source software, frameworks and broader cutting edge technologies (Airflow, Spark, Druid etc).,Familiar with infrastructure provisioning tools (e.g Terraform, Chef),Consistent proven ability to deliver work on time with attention to quality.,Excellent written and spoken communication skills and ability to work effectively with others in a team environment.,Work in a studio that has complete P&L ownership of games,Competitive salary, discretionary annual bonus scheme and Zynga RSUs,Full medical, accident as well as life insurance benefits,Catered breakfast, lunch and evening snacks,Child care facilities for women employees and discounted facilities for male employees,Well stocked pantry,Generous Paid Maternity/Paternity leave,Employee Assistance Programs,Active Employee Resource Groups \u2013 Women at Zynga,Frequent employee events,Additional leave options for most employees,Flexible working hours on many teams,Casual dress every single day", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Performing data discovery on old, out of use legacy systems.,Overseeing the ETL development with Azure Data Factory, writing optimal processes to populate the data warehouse,Strong hands-on experience working in an Azure Cloud system,A history of architecting extensive features of ETL processes,Strong commercial experience developing and optimising ETL processes with Azure Data Factory", "Providing efficient ways for data scientists and other team members to access and manipulate data in a timely manner,You will be involved in the decisions about data storage and data models,Highlighting areas of low data quality and technical debt and opportunities for improvement across data quality, technology and process,Vast experience with a variety of databases (SQL, NoSQL etc),Cloud Platforms (AWS or GCP),Golang,Python,Python Libraries,data", "Location: NY - New York,Job Type: Full Time,Lead architecture design and implementation of next generation cloud BI solution.,Build robust and scalable data integration (ETL) pipelines using SQL, EMR, Python and Spark.,Build and deliver high quality data architecture to support business analysis, data scientists, and customer reporting needs.,Interface with other technology teams to extract, transform, and load data from a wide variety of data sources,Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for business constituents.,Support, maintain, optimize, upgrade, enhance our current and future state Data Infrastructure (SQL server, AWS, Snowflake, Tableau, Alteryx).,Bachelors Degree in related field,Demonstrated strength in data modeling, ETL development, and Data warehousing,4-6 years of data engineering experience,Experience in working and delivering end-to-end projects independently,Experience with AWS services including S3, Redshift, EMR, Kinesis and RDS preferred,Experience with cloud Data Warehouses such as Snowflake are a plus,Experience with Big Data Technologies (Hadoop, Hive, Hbase, Pig, Spark, etc.),Medical, Dental, and Vision Insurance,Transit Discount Program,401K Plan,Paid Time Off Program,Flexible Spending Accounts,Employee Dining Program,Referral Bonus,Online Training Program,Career Development,Corporate Fitness Discount Programs,Choice of Global Cash Card or Direct Deposit", "Design, develop and deploy a big data stack and data processing infrastructure platform,Architect and rearchitect multi-tenant databases to meet the needs of our customer base,Improve data validation and data quality monitoring,Work with client and backend teams to guide events driven designs,Optimize and tune the databases to improve performance and reduce cost,4+ years of experience building large scale, robust data processing pipeline,2+ years of experience coding in Python,Background in API development, with a focus data transformations and data streams,Strong background with building and maintaining automation,Advanced experience with data streaming, ingest, ETL and data warehousing technologies,Strong experience in database schema design, data governance and data modeling,Experience with Spark, Beam, Redshift, Tableau, MySQL, etc.,Experience with AWS, GCP, and/or Azure cloud,Good understanding of data security and encryption,Experience with tools like Airflow, Dagster or DBT", "Hand-screened leads,No ads, scams, junk,Great job search support", "Hand-screened leads,No ads, scams, junk,Great job search support", "Several years' experience working in SAP Data Services/Sybase IQ with at least two BI projects.,Experience designing ETL solutions,The ability to liaise with the business to gather requirements and create technical documentation.,Experience in data warehouse design and build.", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Hand-screened leads,No ads, scams, junk,Great job search support", "Develop and maintain developer tooling to support pyspark data pipelines,Implement processes and systems to monitor data quality, ensuring production data is accurate and available for key stakeholders and business processes that depend on it,Develop and maintain data platform components including: data producers and consumers, pipeline architecture, data lake, data warehouse, and Business Intelligence tooling,Collaborate closely with fellow data team members as well as tech and product teams and company leaders,Support continuing increases in data velocity, volume, and complexity,Write unit/integration tests and document work,Experience with or knowledge of Agile Software Development methodologies,Excellent problem solving and troubleshooting skills,Strong SQL and Python development experience,Proven experience with schema design and dimensional data modeling,Practical experience with SQL and NoSQL databases,Practical experience supporting Business Intelligence tooling and third-party systems,Experience designing, building, and maintaining data processing systems,Experience working with MapReduce and Spark clusters,Experience detecting and reporting data quality issues,Familiarity with Docker, CI/CD (such as Jenkins/Circle), AWS,Competitive salary,Health insurance (100% paid for individuals, 75% for families),Primary caregiver 12-week paid leave,401K,Generous vacation policy, plus company holidays,Company equity,Commuter and cell phone benefit,A commitment to an open, inclusive, and diverse work culture,One mental health day per quarter,$100 monthly work-from-home stipend,Tele-mental health services,OneMedical membership, including tele-health services,Increased work flexibility for parents and caretakers,Access to the Axios \u201cFamily Fund\u201d, which was created to allow employees to request financial support when facing financial hardship or emergencies,Weekly company-sponsored exercise and meditation classes", "Implement and maintain new services and dashboards leveraging multiple technologies,Build performance reporting for new LivePerson product offerings,Directly deliver reporting enhancement requests within the LivePerson product,Create dynamic API-based tools and reports to drive insight into client performance,Bring a client-facing perspective to an R&D role \u2013 transform client feedback into actionable projects and ensure all technical stakeholders are driving toward clients\u2019 business needs.,Proven software development experience within the SaaS industry,Proficiency in Javascript, Node.js, Angular 2+ and HTML/CSS,Experience building large scale web applications in the cloud,Experience with data visualization tools, including MicroStrategy Visual Insights,Advanced Excel user, with proficiency developing in VBA,Familiarity with extracting/transforming data from REST API's,Working knowledge of SQL,Working knowledge of database and table relationships,Working knowledge of Hadoop and Impala,Basic understanding and interest in learning Spark, Python, R or any data science language,Passionate about software development, technology, and data analysis,Strong analysis and design skills,Self-starter with strong motivation and execution capabilities", "Design, develop, document, and test ETL solutions using industry standard tools.,Participate in requirements gathering sessions with business and technical staff to distill technical requirements from business requests.,Partner with clients to fully understand business philosophy and IT Strategy; recommend process improvements to increase efficiency and reliability in ETL development.,Present ETL documentation and designs to team members and convey complex information in a clear and concise manner.,Extract data from multiple sources, integrate disparate data into a common data model, and integrate data into a target database, application, or file using efficient ETL processes.", "Strong software development skills,Solid PHP frameworks knowledge (Symfony is nice to have),Solid Python experience,Ability to work fast, quickly get up to speed with existing code, and learn new concepts easily,Experience and passion for Big Data,Great problem solving skills,Ability to work in a team environment.,Facility to learn new technologies,Background in SQL/PostgreSQL,Experience with Big Data architectures and technologies (more than 1TB of data) and BI solutions", "Creation and maintenance of reporting and analytics infrastructure,Full lifecycle of Performance Hub architectures, for the creation of data set processes used in modelling, mining, acquisition, and verification,Solid command of common scripting languages and tools,Skills to constantly improve data quality and quantity by leveraging and improving data analytics systems,Provide a link between the business and the solution providers within the context of functional alignment,Identify and communicate risks and issues as they arise,Travel to Site, on average 1 trips per month of 2 to 3 days,In-depth knowledge of SQL, Data warehousing, ETL tools, Hadoop-Based analytics,Coding skills across Python and Tibco Spotfire,Data modelling across OT and IT datasets,A solid background in business requirements gathering (use cases) and agile delivery,Excellent interpersonal skills and the ability to work well and effectively with a range of business and project stakeholders.,Ability to question, challenge and collaboratively find better ways of working,Excellent communication and presentation skills,Motivated individual with the ability to make decisions and take direction.,Mining or resource industry experience or ability to transfer skill set into mining,Business analyst skills,Esri Insight and Akumen", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Every few months, we have a \u201chack week\u201d that gives our developers the opportunity to explore ideas that might not otherwise make it on the product roadmap.,We are committed to career development. We offer a formal mentoring program as well as tuition reimbursement. We have frequent panel discussions and talks by industry leaders (Sheryl Sandberg, Melinda Gates and Ta-Nahesi Coates are a few recent examples).,We believe diversity fuels innovation and creativity, and we have a variety of employee groups dedicated to fostering a diverse and inclusive workplace.,We offer a generous parental leave policy, which was recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid, adoptive parents and birth fathers receive 10 weeks fully paid. Similarly, we offer competitive health and dental insurance, as well as 401k matching.,Implement complex data projects with a focus on collecting, parsing, analyzing and visualizing large sets of data to turn information into insights across multiple platforms.,Build fault tolerant, self-healing, adaptive and highly accurate ETL platforms.,Design and develop the data model(s) for the Data Warehouse, Data Marts or NoSQL solutions which will be vetted by the team and or guided by the Senior Data Solutions Architect.,Responsible for data administration of warehouse solutions.,Take ownership of the warehouse solutions, troubleshoot issues, and provide production support.,Document processes and standard operating procedures for processes.,Generate reports using a variety of reporting tools such as Business Objects, Tableau, Pentaho.,Work with team of developers who are transitioning current in-house data warehouse solution to Google Cloud platform.", "Apply strong technical skills in a data engineering team building industry-leading technology", "Use technology such as Spark, Kafka and SQS to build large scale real time and batch data pipelines.,Help implement cloud technologies such as AWS, Azure or GCP.,Program in at least one of the following languages - Java, Scala or Python.,Proven experience in data development in a big data/ Hadoop and cloud data warehouse using SQL or SQL based ETL capabilities.,Have a deep understanding of the processes, skills and technologies which are needed to deliver complex development solutions for a business.,Understand and have experience working with a variety of delivery methods, such as Agile, Waterfall and Scrum.,Experience developing in the cloud with AWS or Snowflake.,Work in one of the most data rich businesses in the UK.,Competitive bonus.,Discount across brands.", "Hand-screened leads,No ads, scams, junk,Great job search support", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Conduct advanced statistical analysis to provide actionable insights, identify trends, and measure performance,Use data-mining techniques to collect and compile data from a wide variety of data repositories.,Build learning systems that monitor data flows and react to changes in customer preferences and business objectives,Build high-performance predictive and prescriptive algorithms using cutting-edge statistical techniques (e.g. neural networks).,Research new machine learning solutions to complex business problems,Collaborate with engineers to implement and deploy scalable solutions,Provide thought leadership by researching best practices, conducting experiments, and collaborating with industry leader,Use data visualization tools to develop visuals that can be used to effectively communicate technical findings to a non-technical audience,Teach data science concepts to more junior members of the team,PhD with 1-2 years of experience or Masters with 3-4 years of experience in Data Science, Computer Science, Engineering, Statistics, or related field.,Strong background in machine learning and statistics (Deep Learning and Bayesian statistics is a plus),Prototyping Expertise: quick to build proofs-of-concept involving data munging, scripting and analysis.,Solid foundation in data structures and algorithms,Hands on experience building models with deep learning frameworks like MXNet, Tensorflow, Caffe, Torch, Theano or similar.,Experience processing massive amounts of structured and unstructured data using Spark/SQL/Hive/Impala/HBase.,Proficiency in Python for numerical/statistical programming (including Numpy, Pandas, and Scikit-learn),Experience with natural language processing,Experience using Cloud computing (AWS/Azure/GCP),Experience with Java, Scala, Python etc.,Experience working with large data sets and distributed computing tools a plus (Map/Reduce, Hadoop, Hive, Spark)", "Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business's operational and analytics databases.,Identification and discovery of complex data sets that align to defined use cases,Contribute to the development of data aggregation and metrics calculations,Selecting and integrating any Big Data tools and frameworks,Implementing ETL process,Monitoring performance and advising any necessary infrastructure changes,Defining data retention policies,Define and builds the data pipelines that will enable faster, better, data-informed decision-making within the business.,At least 3 years of working experience as a database engineering support personnel or a database engineering administrator within a fast-paced a complex business setting.,You will also have experience in defining and building data roadmaps, Spark, NoSQL databases, Big Data ML toolkits and good knowledge of Big Data querying tools,Demonstrate experience working with large and complex data sets as well as experience analyzing volumes of data,Experience in the creation and debugging of databases critical to the business's mission. You will have a strong working and conceptual knowledge of building and maintaining physical and logical data models and experience with Tableau, Qlik or other toolsets,Preferable experience with monitoring, disaster recovery, backup, automated testing, automated schema migration, and continuous deployment.,High level of business and commercial acumen with a demonstrated ability to interpret business requirements and deliver outputs that align with business improvement objectives.,High quality written and oral communication skills including presentation skills,Telecommunications Industry experience", "Build and own multi PB-scale data platform.,Design, code, and develop new features/fix bugs/enhancements to systems and data pipelines (ETLs) while adhering to the SLA.,Follow engineering best methodologies towards ensuring performance, reliability, scalability, and measurability.,Collaborate with other Software Engineers, ML Engineers, Data Scientists, and other stakeholders, taking on learning and leadership opportunities that will arise every single day.,Mentor junior engineers in the team to level them up.,Raise the bar on sustainable engineering by improving best practices, producing best in class of code, documentation, testing and monitoring.,Bachelor\u2019s degree in Computer Science, or a related technical discipline (or equivalent).,7+ years of strong data engineering design/development experience in building massively large scale distributed data platforms/products.,Advanced coding expertise in SQL & Python/JVM-based language.,Expert in heterogeneous data storage systems (relational, NoSQL, in-memory etc).,Deep knowledge of data modeling, lineage, access and its governance.,Excellent skills in AWS services like Redshift, Kinesis, Lambda, EMR, EKS/ECS etc.,Wide exposure to open source software, frameworks and broader cutting edge technologies (Airflow, Spark, Druid etc).,Familiar with infrastructure provisioning tools (e.g Terraform, Chef),Consistent proven ability to deliver work on time with attention to quality.,Excellent written and spoken communication skills and ability to work effectively with others in a team environment.,Work in a studio that has complete P&L ownership of games,Competitive salary, discretionary annual bonus scheme and Zynga RSUs,Full medical, accident as well as life insurance benefits,Catered breakfast, lunch and evening snacks,Child care facilities for women employees and discounted facilities for male employees,Well stocked pantry,Generous Paid Maternity/Paternity leave,Employee Assistance Programs,Active Employee Resource Groups \u2013 Women at Zynga,Frequent employee events,Additional leave options for most employees,Flexible working hours on many teams,Casual dress every single day", "Engineer,Data Engineer", "DevOps and Agile Delivery and SLC methodologies adjusted for infrastructure service delivery and integration with Abbott DevOps organizations.,Uses Infrastructure as Code tools (e.g. Terraform and Packer) and techniques to deliver re-usable storage services for public and private cloud.,Implements Continuous Integration and Continuous Delivery pipelines for the deployment of application and infrastructure stacks.,Establishes data governance and security,Optimize cloud storage for security, cost and availability.,Automate cloud operations tasks including backups and restores,Responsible for compliance to applicable Corporate and Divisional Policies and procedures.,Serve as a technical expert for public cloud storage offerings and uses within Abbott,Evaluate and recommend new software, utilities and tools.,Sets the strategy for implementing new technologies.,Develops service implementation scripts, and provides top level support of IT solutions aligned with customer's business objectives and other infrastructure technology disciplines within BTS-IS.,Nurtures relationships with key suppliers of cloud technologies to keep abreast of emerging technologies, ensure solution stability, optimal pricing, and influences their product direction to align with Abbott.,This position reports directly to the Director of Hosting in BTS.,The position is accountable for internal cost recovery mechanisms,This position is a technical contact representing the infrastructure hosting / cloud area to partner with all other Abbott IT and business organizations and key external service providers.,Accountable for awareness of technology trends and the determination/execution of global strategies that satisfy the business-critical reliance that Abbott is increasingly placing on its infrastructure to host applications.,3+ years Cloud Experience,Experience implementing PostgreSQL and MySQL,Experience managing data storage solutions (blob, managed disk, NFS) in AWS and Azure,Bachelor's Degree preferred in a technology or management discipline", "At least 2 years relevant experience,Minimum 1 years of hands-on experience in Big Data Eco System (Hadoop, Hive, and MapReduce),Minimum 1 year of hands-on experience in Spark core,Minimum 1 years of hands-on programming experience in core Python or Java,Minimum 1 year of hands-on experience in HBase or Cassandra", "Develop and administer standards for architecting and working with data management systems, as well as for deploying, monitoring, and maintaining machine learning models,Ensure analytics systems meet business requirements and implement industry best practices,Develop processes and tools to monitor and analyze model performance and data quality,Build the infrastructure required for optimal ETL from a wide variety of data sources using SQL databases and AWS 'big data' services,Extract, integrate, and analyze data from heterogeneous mix of data sources to support create insights and recommendations to internal stakeholders,Collaborate with data scientists and stakeholders from across the business (Sales, Strategy, Finance, Technology, and Business Development) to develop new information services to optimise business operations,Create and maintain optimal data pipeline architecture,Work with data scientists and business stakeholders to develop, refine, and maintain statistical and machine learning models,Productionize machine learning models and other data science products (e.g., A/B tests) to optimise business operations,Recommend and implement different ways to constantly improve data reliability, quality, and disaster recovery procedures,Identify, design, and implement internal process improvements: automating manual process, optimizing data delivery, redesigning infrastructure for greater scalability, etc.,Create tools to improve access and confidence in data so that internal users can make data-driven decisions,Mentor data science and tech team members", "Evaluate cognitive application requirements and make architectural recommendations for implementation, deployment and provisioning of Watson services and applications on IBM Bluemix,Provide expertise in identification of cognitive use cases that bring significant business value, development of use cases from proof of concept to enterprise-wide implementation and integration of cognitive computing services within an existing application environment,Work alongside the engineering teams and align with all aspects of day to day delivery in an Agile or DevOps environment,You will work with clients to identify Cognitive Computing strategies, options and roadmaps,Internally, you will help identify improved ways of working, mentor junior architects and lead on business development opportunities,Experience in a software engineering/development background (in Java or C# or other COTS Products).,Designing and deploying scalable, highly available, and fault tolerant systems on IBM Bluemix or other cloud platforms (AWS, Azure),Significant experience designing the architecture for Big Data and data science platforms including Hadoop, Spark, data warehouses, NoSQL or graph databases,Knowledge, experience and practical application of Natural Language Processing, Information Retrieval, Semantic Technologies, Big Data or Machine Learning,Development experience and expertise with the Watson Data Platform, Data Science Experience (DSX) and Watson APIs and services including Knowledge Studio, Natural Language Understanding, Speech-to-text and Visual Recognition,Designing analytical data stores for enterprise-wide performance management and data visualisation applications,Architecting optimal cognitive computing solutions based on Watson services from user requirements,Experience in data architecture and designing logical data models,Workshop facilitation, client communication and presentation of cognitive computing concepts to non-technical audiences,Understand, implement, and automate security controls, governance processes, and compliance validation,Understanding of how cognitive computing technology can automate and enhance operational business processes,Experience of DevOps type operations \u2013 use of source control (GitHub, Stash), Build tools (Puppet, Jenkins, etc), continuous integration, test driven development (i.e. Cucumber, Lettuce etc),Identifying and defining requirements for a Watson application,Deploying hybrid systems with on-premise and Bluemix components,Providing best practices for building secure and reliable applications on the Bluemix platform,Hadoop administration, set up , security and tuning,Data ingestion,Kafka, Flume, Spark", "Advance the development of the optical read/write drive for Folio Photonics\u2019 DataFilm DiscTM,Hands-on engineering of the data storage system consisting of Folio\u2019s DataFilm DiscTM and associated read/write drive as well as integration into customers\u2019 multidisc library,Signal processing and data engineering for optical data.,Integration of drive with data storage system,Display strong interpersonal skills to work with our cross-functional engineering and executive teams,Management opportunity for appropriately qualified candidates", "Project Type: One-time project", "Assist with creation of data schemas, stored procedures, data pipelines, and views,Help build and maintain technical solutions required for optimal ingestion, transformation, and loading of data from a wide variety of data sources and large, complex data sets,Collaborate across roles to embrace best practices in reporting and analysis, including data integrity, test design, validation, and documentation,Ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy,Build and automate actionable reports,Collaborate with data analysts, data scientists, and stakeholders during design discussions to uncover more detailed business requirements related to data engineering,Develop strong hypotheses, independently solve problems, and share actionable insights with engineering", "Use technology such as Spark, Kafka and SQS to build large scale real time and batch data pipelines.,Help implement cloud technologies such as AWS, Azure or GCP.,Program in at least one of the following languages - Java, Scala or Python.,Proven experience in data development in a big data/ Hadoop and cloud data warehouse using SQL or SQL based ETL capabilities.,Have a deep understanding of the processes, skills and technologies which are needed to deliver complex development solutions for a business.,Understand and have experience working with a variety of delivery methods, such as Agile, Waterfall and Scrum.,Experience developing in the cloud with AWS or Snowflake.,Work in one of the most data rich businesses in the UK.,Competitive bonus.,Discount across brands.", "DataStage", "Leverage company data to drive business outcomes.,Play a role in their analytics team developing data-driven solutions, taking ownership for driving client Growth.,Develop and maintain global data models by analysing use cases and applications and across all other brands.,Design and develop scalable data ingestion frameworks to transform a variety of\nlarge data sets.,Own end-to-end delivery of raw and trained data sets to and from brand cloud environments.,Integrate with data platform architecture by building applications using open-source frameworks such as Apache Spark, containerized applications (i.e. Kubernetes), and Apache Airflow.,Build and maintain data integration utilities, data scheduling and monitoring capabilities, source-to-target mappings and data lineage trees.,Implement and manage production support processes around data lifecycle, data quality, coding utilities, storage, reporting and other data integration points.,Maintain system performance by identifying and resolving production and application development problems; calculating optimum values for parameters; evaluating, integrating and installing new releases; performing routine maintenance; and answering user questions.", "Working in a cross-functional team \u2013 alongside talented Engineers and Data Scientists,Building scalable and high-performant code,Mentoring less experienced colleagues within the team,Implementing ETL process \u2013 including cohorts building and ETL routines customisation,Monitoring cluster (Spark/Hadoop) performance,Working in an Agile Environment,Refactoring and moving our current libraries and scripts to Scala/Java,Enforcing coding standards and best practices,Working in a geographically dispersed team,Working in an environment with a significant number of unknowns \u2013 both technically and functionally", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Engineer", "Working knowledge of data warehousing concepts (change data capture, ETL, data marts etc.),Experience building data marts/BI applications for specific business areas from multiple data sources like files, API's, messaging streams (Kafka, Kinesis etc),Experience with using data and analytics technologies/services eg. Redshift, S3, RDS, BigQuery etc. or comparable technologies.,Programming languages - Java, Python, node.js,Creating ETL pipelines for Data processing workloads for change data capture, set based data transformations, data movement between AWS services,Implementing,optimising and administering cloud based parallel processing/in-memory/columnar relational databases eg. BigQuery, Redshift etc.,Ability to setup and configure ETL frameworks like Airflow using Java/Python,Events/Stream parallel data processing solutions like Spark on AWS or similar technology,Implemented data processing using Hadoop/Hbase/Hive,Implemented AWS Security rules/policies for communication between AWS Services and securing PII data stored, transferred or processed on them.,Working with NoSQL databases eg. Cassandra, DynamoDB", "Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.,Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.,Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.,Writes unit/integration tests, contributes to engineering wiki, and documents work.,Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.,Works closely with a team of frontend and backend engineers, product managers, and analysts.,Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.,Designs data integrations and data quality framework.,Designs and evaluates open source and vendor tools for data lineage.,Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.", "Assist with creation of data schemas, stored procedures, data pipelines, and views,Help build and maintain technical solutions required for optimal ingestion, transformation, and loading of data from a wide variety of data sources and large, complex data sets,Collaborate across roles to embrace best practices in reporting and analysis, including data integrity, test design, validation, and documentation,Ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy,Build and automate actionable reports,Collaborate with data analysts, data scientists, and stakeholders during design discussions to uncover more detailed business requirements related to data engineering,Develop strong hypotheses, independently solve problems, and share actionable insights with engineering", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support,50+ career categories", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support,50+ career categories", "Use a combination of financial market data, social media data, the AuCoDe controversy score engine, and customer supplied data to infer insights into stock price movements.", "Maintaining, improving, and developing databases,Developing data pipelines from sensor input to shared output,Supporting the development of dashboards and automated reports,Supporting the deployment and development of sensors,Master\u2019s degree in computer sciences, information technology, or similar; or a Master\u2019s degree in geosciences with strong programming skills,At least 3 years of relevant work experience in data engineering,Experienced user of databases and dashboard software,Hands-on understanding of machine learning applications and algorithms,Excellent proficiency in an open-source programming language (e.g. Python),Bonuspoints for experience with Azure, PostGIS, Grafana and/or DataIKU,Proficiency in geographic information systems (GIS),Experience with remote sensing and telemetry is a plus,Structured worker with a curious and critical mindset,Comfortable with a high level of autonomy,Enjoys structuring work for yourself and others,Excited about working on back-end systems and being the hero behind the scenes,Enjoys sharing knowledge with team members (and building capacity in the team and with partners),Able to deal with uncertainty, changes in scope, and ambiguity,Enjoys learning and developing new skills and tools", "Partner with data teams to implement pipeline designs to support R&D strategy and conceptual data flows,Partner with the metadata leads to translate conceptual data models into physical database/tables optimized for data analytics in RDIP using established environments and tools,Assist the design, build, test and maintenance of data acquisition and processing pipelines including but not limited to the creation/maintenance of appropriate artifacts,Ensure the preservation of data integrity from source to target state including but not limited to the acquisition of appropriate metadata and the incorporation of appropriate QC checks into the pipelines,Support the use and growth of the Data Engineering DataOps environment, influence strategy and roadmap for the curation toolset, work with R&D and Tech to prioritize enhancements,Provide Tier 3 support for production pipelines,Support DCS and broader R&D in self-service/exploratory efforts,Influence vendor roadmaps, work with R&D and Tech to prioritize DataOps enhancements, and onboard these tools or enhancements,Ensure the quality consistency and availability of guidance documentation of end users of the tools to support high quality outputs,Extend current pipelines to support clinical biomarkers,Assess GxP readiness as it related to the upstream data pipelines and develop a plan for addressing any gaps", "Engage in super interesting high profile projects,Join a fun team full of flair and personality, contribute and be recognized for your value add!,Zero Bureaucracy environment - we are about embracing creative talent,Passionate about working on complex Data problems,You will ideally have a solid demonstrated Big Data experience,Java/ Data/ Big Data Development", "Provide analytical and statistical expertise to company.,Become familiar with a range of data sets, looking at raw customer data, financial data and marketing data.,Experiment with algorithms, machine learning and data exploration in order to build solutions and tools for data-driven purposes.,Perform clustering analysis on customer data, along with segmentation and clustering.,Present and tell the story of the data to clients.,Support in the creation of real time reporting using Kibana.,Own recommendation engines and machine learning systems.,Own reporting data flows include ETL processes,Perform updates to production databases", "Provide Big Data Engineering support to the Media Analytics and User Experience team and Media (Network and Engineering) data customers,Lead the Media Analytics program of work to integrate video streaming data into the development of apps and websites - particularly sports and entertainment.,Drive tangible business values through actionable insights that lead to measurable improvements of the received customer experience and customers engagements,Setting benchmarks and targets for continual improvements of the customers video streaming experience,Engaging and influencing Senior Managers, Product managers, Technology managers, Operations managers, Program managers to promote Media's capabilities and aligning with corporate strategic programs;,Be a key influence in the development and production of automated reporting, recommendations on data sources, data centric platforms and tools, and contribute to the implementation of data centric applications.,Recommending and supporting best of bread Media data source identification and documentation.,Data validation, data access, integration and aggregation to support work of analysts, data scientist and the deployment of data centric applications and solutions,Having the ability to query databases, make data available to experimental and/or production environments and assess data suitability for performing statistical analysis,Experience working in established diverse digital ecosystems with a variety of systems, tools, limitations and stakeholders,Understanding the technologies for the delivery of online streaming video (live and on-demand). Technologies including video compression (eg: H.264, HEVC etc), adaptive bit rate streaming (eg: Microsoft Smooth streaming (MSS), Apple HTTP Live Streaming (HLS), Adobe HTTP's Dynamic Streaming (HDS), Widevine, MPEG-DASH, etc) and Content Delivery Network (CDN).,Experience in the development of OTT Video streaming services and applications.,Understanding metrics for video quality and the measurements of received customer experience for media services,Experience with dashboard and analytical tools supporting data,A working knowledge of protocols related to Internet (TCP/IP),Knowledge of Cloud, NFV and SDN.,Knowledge of video streaming software and analytics tools e.g. Conviva (highly desirable),Practical experience with SQL and/or non SQL storage methods and APIs,Experience in Unix / Linux, administration and scripting - Mandatory,Object orientated and procedural programming languages,R, SAS,,Python", "Propose, conceptualize, design, and implement solutions for difficult and complex applications independently, including both infrastructure and user-facing components.,Adapt and adopt technologies and methods from open source projects; participate in such efforts through code, process and community contributions, especially in the Cultural Heritage Linked Data environment.,Oversee testing, debugging, change control, and documentation for major projects, comprising both new development and refactoring of existing mission critical products.,Engage in strategic technical and architectural planning with a variety of stakeholders.,Define and promote complex application development administration and programming standards.,Oversee the support, maintenance, operation, and upgrades of applications.,Lead projects, as necessary, for special systems and application development in areas of complex problems.,Appropriately discuss, troubleshoot and resolve complex technical problems in a team environment.,Work with other technical professionals\u2014both within and outside Stanford\u2014to develop standards and implement best practices.,Promote the adoption of Linked Data technologies within the institution and broadly within the community,Previous work related to libraries, digital preservation and/or digital repositories,,Participation in large, long-running and successful open source projects,Standards, architecture and/or engineering on linked-data based systems, within or outside the library domain,Expertise in designing, developing, testing, and deploying applications, such as those based on Linked Data.,Proficiency with application design and data modeling, such as for cultural heritage domains, common library-based XML-based schema, relational database and graph-based models.,Ability to define and solve logical problems for highly technical applications.,Strong communication skills with both technical and non-technical partners.,Ability to lead activities on structured team development projects.,Ability to select, adapt, and effectively use a variety of programming methods, such as Java, Ruby / Ruby-on-Rails, Python, and X-query/XSLT.,Knowledge of library and linked data domains.,demonstrated success in engineering linked data systems at scale,demonstrated success in engineering for performance with linked data,successful leadership in open source or collaborative projects in the library or linked data domain,Ability to quickly learn and adapt to new technologies and programming tools.,Demonstrated experience in designing, developing, testing, and deploying applications, such as those based on Linked Data.,Strong understanding of data design; architecture; graph-based, XML and relational data structures; and data modeling.,Ability to define and solve logical problems for highly technical applications.,Thorough understanding of all aspects of software development life cycle and quality control practices, such as automated testing and test driven development practices.,Demonstrated experience leading technical activities on structured team development projects.,Ability to recognize and recommend needed changes in user and/or operations procesures.,Strong understanding of Ruby and Ruby on Rails.,Demonstrated experience with Linked Data patterns, data modeling and architecture,Successful participation and leadership in open source software development.,Constantly perform desk-based computer tasks.,Frequently sit, grasp lightly/fine manipulation.,Occasionally stand/walk, writing by hand.,Rarely use a telephone, lift/carry/push/pull objects that weigh up to 10 pounds.,May work extended hours, evening and weekends.,Interpersonal Skills: Demonstrates the ability to work well with Stanford colleagues and with external partner organizations.,Promote Culture of Safety: Demonstrates commitment to personal responsibility and value for safety; communicates safety concerns; uses and promotes safe behaviors based on training and lessons learned.", "Principals only. Recruiters, please don't contact this job poster.", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Responsible for the maintenance, improvement, cleaning, and manipulation of data in the business's operational and analytics databases.,Identification and discovery of complex data sets that align to defined use cases,Contribute to the development of data aggregation and metrics calculations,Selecting and integrating any Big Data tools and frameworks,Implementing ETL process,Monitoring performance and advising any necessary infrastructure changes,Defining data retention policies,Define and builds the data pipelines that will enable faster, better, data-informed decision-making within the business.,At least 3 years of working experience as a database engineering support personnel or a database engineering administrator within a fast-paced a complex business setting.,You will also have experience in defining and building data roadmaps, Spark, NoSQL databases, Big Data ML toolkits and good knowledge of Big Data querying tools,Demonstrate experience working with large and complex data sets as well as experience analyzing volumes of data,Experience in the creation and debugging of databases critical to the business's mission. You will have a strong working and conceptual knowledge of building and maintaining physical and logical data models and experience with Tableau, Qlik or other toolsets,Preferable experience with monitoring, disaster recovery, backup, automated testing, automated schema migration, and continuous deployment.,High level of business and commercial acumen with a demonstrated ability to interpret business requirements and deliver outputs that align with business improvement objectives.,High quality written and oral communication skills including presentation skills,Telecommunications Industry experience", "design, develop or interface into new or existing data feeds, in order to manipulate and transform data into desired target architectures / applications,manage all aspects of day to day delivery in an Agile or DevOps environment,You will work with clients to identify a data transformation roadmap,Internally, you will help identify improved ways of working mentor junior and share knowledge,Strong software engineering/development background (in Java or C# or COTS Products).,Data manipulation (XML processing, customised ETL solutions) and data merging experience,Data Transformation using ETL COTS tooling.,Experience of engineering solutions to cope with Big (volume) and Fast (real/near real time) requirements,Experience developing, interfacing or designing data centric applications.,Experience of middleware integration applications, either commercial (i.e. Mulesoft) or Open Source,Experience of DevOps type operations \u2013 use of source control (GitHub, Stash), Build tools (Puppet, Jenkins, etc), continuous integration, test driven development (i.e. Cucumber, Lettuce etc),Experience of Cloud Infrastructure usage and set up,Hadoop administration, set up , security and tuning,Data ingestion,Kafka, Flume, Spark,SQL on Hadoop tools,Hawq and Greenplum,ETL tools like Pentaho, Informatica BDE, Talend,Elastic Search& SOLR", "Analyze and resolve complex challenges around data and tools. Optimize analytical workflows by identifying opportunities and automating them,Implement solutions to bring together application data generated by distributed systems, third-party data, and real-time user data needed to make key business decisions,Work within the Data Science team to serve machine learning solutions at scale,Work on projects of growing responsibility, both individually and as part of a team, to build experience and skills at a pace matched to your shown ability", "Scala OR Python | Spark, Cassandra, Kafka, AWS,Data Engineering - the development of a petabyte scale Big Data platform and data pipelines/ETL, primarily using Spark, Cassandra and Kafka, programming in either Scala or Python,Back-end Software Development - building the back end of the Data Science product,Excellent programming skills with either Scala or Python (functional programming being a big plus) and associated libraries, e.g. Cats, Shapeless, Pandas, Numpy,A background developing large scale, production level Big Data platforms, including e.g. Hadoop, Spark, Cassandra,Knowledge of how to build highly scalable, highly tolerable software,An awesome opportunity to join one of the best teams in the industry and build highly complex software, unrivalled by any other competitor,The chance to innovate and gain exposure to a massive scale Big Data platform & production level Machine Learning", "Analyze and resolve complex challenges around data and tools. Optimize analytical workflows by identifying opportunities and automating them,Implement solutions to bring together application data generated by distributed systems, third-party data, and real-time user data needed to make key business decisions,Work within the Data Science team to serve machine learning solutions at scale,Work on projects of growing responsibility, both individually and as part of a team, to build experience and skills at a pace matched to your shown ability", "Conduct advanced statistical analysis to provide actionable insights, identify trends, and measure performance,Use data-mining techniques to collect and compile data from a wide variety of data repositories.,Build learning systems that monitor data flows and react to changes in customer preferences and business objectives,Build high-performance predictive and prescriptive algorithms using cutting-edge statistical techniques (e.g. neural networks).,Research new machine learning solutions to complex business problems,Collaborate with engineers to implement and deploy scalable solutions,Provide thought leadership by researching best practices, conducting experiments, and collaborating with industry leader,Use data visualization tools to develop visuals that can be used to effectively communicate technical findings to a non-technical audience,Teach data science concepts to more junior members of the team,PhD with 1-2 years of experience or Masters with 3-4 years of experience in Data Science, Computer Science, Engineering, Statistics, or related field.,Strong background in machine learning and statistics (Deep Learning and Bayesian statistics is a plus),Prototyping Expertise: quick to build proofs-of-concept involving data munging, scripting and analysis.,Solid foundation in data structures and algorithms,Hands on experience building models with deep learning frameworks like MXNet, Tensorflow, Caffe, Torch, Theano or similar.,Experience processing massive amounts of structured and unstructured data using Spark/SQL/Hive/Impala/HBase.,Proficiency in Python for numerical/statistical programming (including Numpy, Pandas, and Scikit-learn),Experience with natural language processing,Experience using Cloud computing (AWS/Azure/GCP),Experience with Java, Scala, Python etc.,Experience working with large data sets and distributed computing tools a plus (Map/Reduce, Hadoop, Hive, Spark)", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Drive technical design and development around real-time data analytics.,Lead data analytics initiative in a multi-functional matrixed environment.,Design cloud data analytics platform & enable self-service tools to democratize data.,Audience focused (C-team, Engineers, Leaders etc.) communications with influence and clarity.", "Lead technical direction and architectural evolution of our data pipelines and related digital farming platform capabilities.,Partner with other architects and technical leads to collaborate, design and validate appropriate solutions and institute appropriate data governance mechanisms.,Partner with Digital Farming Platform leads to establish and steward a multi-year Data Pipelines technical capabilities roadmap.,Develop cloud-based pipeline architectures that enable massively scalable data processing, while maintaining less than linear operating cost profiles.,Continuously assess next-generation geospatial data and imagery processing techniques to support use cases unique to the agriculture industry.,Work with key leaders and subject matter experts in our Data Science and Product organizations to integrate and enable the latest agronomic data innovations and data quality algorithms.", "Provide analytical and statistical expertise to company.,Become familiar with a range of data sets, looking at raw customer data, financial data and marketing data.,Experiment with algorithms, machine learning and data exploration in order to build solutions and tools for data-driven purposes.,Perform clustering analysis on customer data, along with segmentation and clustering.,Present and tell the story of the data to clients.,Support in the creation of real time reporting using Kibana.,Own recommendation engines and machine learning systems.,Own reporting data flows include ETL processes,Perform updates to production databases", "Apache Spark,Databricks,Data,Spark", "Develop, test and maintain data architecture.,Design and implement secure data pipelines to prepare, process, ingest and organize data into data data lake / data warehouse from disparate on-premise and cloud data sources.,QA and troubleshoot performance of data pipelines and queries accessing data warehouse,Clean, transform and model data to power our analytics and user facing products,Ensure proper data governance and privacy practices,Partner with Analytics team on buildout of advanced data products,Capable coder with Python, Scala, and R,Familiar with modern, cloud-native, scalable ETL solutions/tools, ex: Informatica, Stitch/Talend, Mulesoft/Salesforce, etc\u2026,Experience with workflow orchestration principles & platforms (DAGs, Airflow, DBT, Luigi, Dagster, Prefect, etc\u2026),Prefer experience with Google Cloud Platform(Bigquery, Dataproc, Dataflow, Pub/Sub, etc\u2026). Experience with AWS (DynamoDb, Kinesis Stream, etc\u2026) is a plus,Building an analytic engine, segmentation and grouping data,Experience writing scripts to automate the provisioning and maintenance of systems in a distributed, virtualized infrastructure,Familiarity with managed cloud-based options for building machine learning models,RDBMS database development using SQL queries and stored procedures", "Experience working on cloud-based platforms such as AWS,Expertise with Python,Have worked with streaming technologies such as Spark and Kafka,Exposure to liaising with senior stakeholders", "Design, construct, install, test and maintain highly scalable data management systems,Ensure systems meet business requirements and industry best practices,Create and maintain data management documentation,Design and build scalable data repositories enabling high-performance of consumer/member transaction volume within the ICE products,Build APIs for data consumption and integration of external datasets as necessary,Build high-performance algorithms, prototypes, predictive models and proof of concepts,Problem-solve issues around data integration, unusable data elements, unstructured data sets, and other data management incidents,Research opportunities for data acquisition and new uses for existing data,Develop data set processes for data modeling, mining and production,Integrate new data management technologies and software engineering tools into existing structures,Create custom software components (e.g. specialized UDFs) and analytics applications,Employ a variety of languages and tools (e.g. scripting languages) to marry systems together,Install, implement and update disaster recovery procedures,Recommend ways to ensure continuous improvement around data reliability, efficiency and quality,Collaborate with data architects, modelers and IT team members on project requirements and goals,Collaborate with QA Engineers and Automation to build test plans, scripts and automation to verify and validate data management systems and production implementation,Able to identify and implement a data solution strategy,Skilled in working directly with senior business management to determine requirements and present options,Statistical analysis and modeling,Database architectures,SQL-based and NoSQL-based technologies,Data modeling tools (e.g. UML, ERWin, Enterprise Architect and Visio),Data warehousing solutions,Data mining,Demonstrates initiative with or without direct authority. Able to influence and advocate for approaches to problem solving,Demonstrates intellectual curiosity in exploring new territories and finding creative ways to solve data management problems,Supports and maintains positive attitude and vision with peers, associates, and management,Demonstrates effective teamwork and collaboration,Ability to prioritize competing or conflicting requests and execute tasks in a high-pressure environment,Utilizes good judgment,Must demonstrate ability to handle diversity amongst people and environments,Must be detail oriented and able to follow-up and follow-through on project actions and tasks,Ability to maintain confidentiality of sensitive information", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Build and own multi PB-scale data platform.,Design, code, and develop new features/fix bugs/enhancements to systems and data pipelines (ETLs) while adhering to the SLA.,Follow engineering best methodologies towards ensuring performance, reliability, scalability, and measurability.,Collaborate with other Software Engineers, ML Engineers, Data Scientists, and other stakeholders, taking on learning and leadership opportunities that will arise every single day.,Mentor junior engineers in the team to level them up.,Raise the bar on sustainable engineering by improving best practices, producing best in class of code, documentation, testing and monitoring.,Bachelor\u2019s degree in Computer Science, or a related technical discipline (or equivalent).,7+ years of strong data engineering design/development experience in building massively large scale distributed data platforms/products.,Advanced coding expertise in SQL & Python/JVM-based language.,Expert in heterogeneous data storage systems (relational, NoSQL, in-memory etc).,Deep knowledge of data modeling, lineage, access and its governance.,Excellent skills in AWS services like Redshift, Kinesis, Lambda, EMR, EKS/ECS etc.,Wide exposure to open source software, frameworks and broader cutting edge technologies (Airflow, Spark, Druid etc).,Familiar with infrastructure provisioning tools (e.g Terraform, Chef),Consistent proven ability to deliver work on time with attention to quality.,Excellent written and spoken communication skills and ability to work effectively with others in a team environment.,Work in a studio that has complete P&L ownership of games,Competitive salary, discretionary annual bonus scheme and Zynga RSUs,Full medical, accident as well as life insurance benefits,Catered breakfast, lunch and evening snacks,Child care facilities for women employees and discounted facilities for male employees,Well stocked pantry,Generous Paid Maternity/Paternity leave,Employee Assistance Programs,Active Employee Resource Groups \u2013 Women at Zynga,Frequent employee events,Additional leave options for most employees,Flexible working hours on many teams,Casual dress every single day", "Craft and own the optimal data processing architecture and systems for new data and ETL pipelines,Build canonical datasets as well as scalable and fault-tolerant pipelines,Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage,Define and own the data engineering roadmap for Ecosystems,Collaborate with Software Engineers and Data Scientists to design technical specification for logging and add logging to production code to generate metrics both online as well as offline,Work with different cross functional partners - Data Scientists, Infra Engineering, Logging Framework Infra Teams, Product Managers,Build visualizations to provide insights into the data & metrics generated,Work with data infrastructure teams to suggest improvements and influence their roadmap,Immerse yourself in all aspects of the product, understand the problems, and tie them back to data engineering solutions,Recommend improvements and modifications to existing data and ETL pipelines,Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership,Drive internal process improvements and automating manual processes for data quality and SLA management", "Support the technical data foundation of Henkel\u2019s digital transformation,Support and Implement Henkel wide data integration strategy for the central data foundation covering all internal & external, structured & unstructured data requirements,Support and Implement Henkel BI data model with focus on integrating data from operational systems and other data sources up to reporting and analytical applications for the region,Support and Implement an enterprise wide valid business logic and a data quality concept,Work in a global and regional, multinational BI and analytics teams in a dynamic and challenging environment,Four year degree in Computer Sciences/Information Systems or related,At least 2-3 years of experience in the area of Data Warehousing and Business Intelligence,Knowledge of modern data warehousing concepts, data modelling and data management.,Experience in relational data bases (MS SQL, Teradata, Oracle, DB2) and good knowledge of SQL,Experience in Azure Cloud technology and ETL solution,Knowledge of BI tools e.g. Analysis Server, Cognos, Oracle BI, SAP BW beneficial,Agile and DevOps Experience a plus", "Shape the portfolio of business problems to solve by building detailed knowledge of data sources (internal and external),Model data landscape, obtain data extracts and define secure data exchange approaches,Acquire, ingest, and process data from multiple sources and systems into Cloud Data Lake,Operate in fast-paced, iterative environment while remaining compliant with Information Sec policies/standards,Collaborate with data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models,Help architect the strategic advanced analytics technology landscape,Become expert in claims data sources,Framework set up across the company to define best practice in data engineering space,Robust data sources in the data lake with increasing proportion of data held in the lake,No unexpected issues arise,Meaningful experience (2+ years) with at least two of the following technologies: Python, Scala, SQL, Java,Experience and interest in Cloud platforms such as:, Azure, AWS or Databricks,The ability to work across structured, semi-structured, and unstructured data, extracting information and identifying linkages across disparate data sets,Understanding of Information Security principles to ensure compliant handling and management of data", "Translate business requirements into technical requirements, and develop data pipelines from many industry and proprietary systems into a unified data warehouse.,Collaborate with business operations, software engineers, data analysts, and data scientists.,Build scalable, maintainable data pipelines that extract, transform, and load (ETL \u2013 DMS/Stitch) high-performance data warehouses (Redshift/Snowflake).", "Excellent understanding of Data Architecture, including both on-premise and via the AWS Cloud.,Strong experience with Data Modelling and Data Analysis, including the ability to query, prepare and analyse Data sets.,Proven commercial experience with both SQL and NoSQL databases.,Skilled in software engineering, with experience in some (not necessarily all) of the following programming languages: Python, R, Java, Scala.,Familiarity with Linux/UNIX and tools including Git and SVN.,Good understanding of Data Science models and Data Modelling in general (e.g. Star Schema, Kimball).,Mobile:,Office Line:,Email:,LinkedIn: 'Liam Haghighat',SQL,Data Science,NoSQL,Architect,Data Modelling", "Work with our clients to model their data landscape, obtain data extracts and define secure data exchange approaches,Plan and execute secure, good practice data integration strategies and approaches,Acquire, ingest, and process data from multiple sources and systems into Big Data platforms,Create and manage data environments in the Cloud,Collaborate with our data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models,Have a strong understanding of Information Security principles to ensure compliant handling and management of client data,This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science,Use new and innovative techniques to deliver impact for our clients as well as internal R&D projects,A proven ability to lead a project from an engineering perspective, owning the workflow and setting expectations,Mentoring junior engineers allowing them to develop and be challenged", "Data querying and processing in SQL,Data processing and task management in Python,Communication skills, and ability to translate between the domains of business problems and technical implementations,Refactor operating model code into scalable, transparent processes leveraging Airflow and DBT as core frameworks,Expand the capabilities of Inspire\u2019s core data platform to support incremental product lines and product features,Partner with Analytics to systematize and scale high-integrity value-oriented analysis,Partner with Sales, Operations, and other business stakeholders to design and deliver new data-driven integrations,Cultivated familiarity with Inspire\u2019s frameworks & operating model,Delivery of high-quality pull requests in DBT and Airflow, evidencing strong code standards & testing practices,Comfort with self-directed project management: requires minimal oversight to assess a problem, formulate a solution, deliver code, and document changes.,Technical competency \u2013 comfort on a command line, a good grasp on the fundamentals of programming, a general understanding of Git/source control, and a willingness to read the docs, search stack overflow, and test it until it works,Problem-Solving Mentality \u2013 gets excited about digging into complexity, wants to ask questions and learn more, and isn\u2019t put off by problems they\u2019ve never been explicitly told how to solve. Especially troubleshooting: ability to break down a chain of steps to narrow and locate a problem.,Number Sense \u2013 Strong background in mathematics or physics, comfort with quantitative measurement and estimation. Ability to work in establishing boundaries and orders-of-magnitude to make informed judgements without fussing over exactitude.,1 or more years in a data analytics, engineering or science role,Strong SQL experience working with large datasets, ideally in cloud-based data warehouses,Software development in Python3,Experience automating data processing, cleaning and/or preparation,Experience with key frameworks: Snowflake, Apache Airflow, dbt, AWS services, Docker, Kubernetes,Experience at a similar scale of data processing (Multi-TB/billions of rows),Work with real-time event stream data,Contextual work in the energy industry,Data consultancy experience a plus,Proven ability to break down a chain of steps to narrow and locate a problem", "Several years' experience working in SAP Data Services/Sybase IQ with at least two BI projects.,Experience designing ETL solutions,The ability to liaise with the business to gather requirements and create technical documentation.,Experience in data warehouse design and build.,Data,IQ,SAP,SQL", "Create and promote a technical design and architectural vision for security operations data systems and tooling,Work with engineers on the team to manage infrastructure for moving and processing large-scale data,Improve log flows efficiency through data processing to reduce the cost of analysis and storage,Convert log flows from Logstash to Elastic Common Schema,Work with Analysts to determine and implement the best practices for data retention and storage,Design and promote standards for security operations data telemetry and processing,Help engineers across the team select data technologies for their development needs", "Collaborate with Software Engineers on centralized and standardized data aggregations to pipeline metrics for UI consumption,Defining canonical data schemas and tables for ads, sales and revenue that provide a complete view of a customer,Work with data infrastructure teams to suggest improvements and influence their roadmap,Collaborate with Software Engineers to design technical specification for logging and add logging to production code to generate metrics,Recommend improvements and modifications to existing data and ETL pipelines,Communicate and influence strategies and processes around data modeling and architecture to multi-functional groups and leadership,Evangelize high quality software engineering practices in developing data infrastructure and pipelines at scale,Transform data aggregations from daily batch processing to incremental near-real time,Drive internal process improvements and automating manual processes for data quality and SLA management,Create solutions and tooling to decrease data fragmentation and promote reusability through logical data modeling and rich representations (e.g. structs) of common data objects,Data consistency and accuracy measurement and alerting,Working on better metadata management and connecting write-side/aggregations with query-side metadata,Build data anomaly detection, data quality checks, and optimize pipelines for ideal compute and storage", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs.,Writes ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.,Collaborates with data science team to transform data and integrate algorithms and models into automated processes.,Uses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelines. Uses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systems. Builds data marts and data models to support Data Science and other internal customers.,Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.,Strong problem solving skills and critical thinking ability.,Strong collaboration and communication skills within and across teams.,5 or more years of progressively complex related experience.,Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.,Ability to understand complex systems and solve challenging analytical problems.Experience with bash shell scripts, UNIX utilities & UNIX Commands.,Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.,Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.,Experience building data transformation and processing solutions.Has strong knowledge of large scale search applications and building high volume data pipelines.,Master's degree preferred.", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Hand-screened leads,No ads, scams, junk,Great job search support", "Proven experience and expertise building ETL solutions for data warehouses; preferably for customer insight or e-commerce projects,Experience with low latency technologies (e.g. Spark, Storm) or similar AWS managed services,Solid experience of one or more enterprise data tools (Preferably Talend, SAP Data Services; optional Informatica, DataStage etc.),Solid experience developing on SQL Server (or similar) and an excellent appreciation of data warehouse methodologies (Kimball), design and tuning techniques,Programming experience in Python, Scala, Java,AWS experience preferred though not essential as training will be provided, with particular reference to Redshift, Kinesis and EMR/Hadoop,Familiarity with web, FTP and API technologies,Documentation skills for SDLC e.g. source-target mappings and data flow diagrams", "Collect internal data from different departments and eventually enrich data from external sources,Consolidate various data sources, transform and clean data to build a customer level data hub,Create new variables/features to enrich customer level data,Industrialize and maintain the centralized customer hub,Be able to quickly extract, prepare and provide data according to business requirement, and eventually automate the data providing process by script/batch if needed.,Settle a data providing procedure for business to follow and recommend ongoing improvement.,Work closely with Data Scientist/Analyst to retrieve and transform high dimensional data into a proper shape for further modeling.,Process, clean and control the data used in innovation projects or for business targeting purpose.,Min S1 with background Mathematics / Statistics / Computer Science,Minimum 3 years\u2019 experience in MIS, Data Engineering and Advanced Analytics,Excellent knowledge of Python and SQL,Good understanding of relational and non-relational database, experience with MongoDB or Elastic Search will be a plus,Experience with common data science toolkits (Jupyter notebook, Scikit-learn, Pandas, Numpy\u2026) and linux system will be a big plus,Good understanding of big data and its infrastructure (Hadoop, Spark, HDFS, Hive\u2026),Dynamic working environment,Learning and development,Opportunity for international working assignment,Competitive remuneration package", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Use technology such as Spark, Kafka and SQS to build large scale real time and batch data pipelines.,Help implement cloud technologies such as AWS, Azure or GCP.,Program in at least one of the following languages - Java, Scala or Python.,Proven experience in data development in a big data/ Hadoop and cloud data warehouse using SQL or SQL based ETL capabilities.,Have a deep understanding of the processes, skills and technologies which are needed to deliver complex development solutions for a business.,Understand and have experience working with a variety of delivery methods, such as Agile, Waterfall and Scrum.,Experience developing in the cloud with AWS or Snowflake.,Work in one of the most data rich businesses in the UK.,Competitive bonus.,Discount across brands.", "DataStage", "Home,Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Responsible for developing and optimising data and optimisation platforms and pipelines,Writing data pipelines as code and transforming data using appropriate and efficient methods,Working closely in a team of data scientists, machine learning engineers and operations researchers to build a fully integrated product,Data manipulation skills (SQL and NoSQL),Experience with AWS, and ideally AWS Lambda,Experience with Python", "Create smart solutions around complex data problems.,Work directly with our senior engineers to come up with unique solutions to import as much data as possible.,Own the importing and processing of unstructured web content,Parse content using NLP and ML to import it into our system on fly,Integrate with third party APIs,Build intelligent solutions for matching place names, addresses and geo-data", "Data/Business Analysis,SAS", "Find a better job, faster!,Hand-screened leads,No ads, scams, junk,Great job search support", "Engage in super interesting high profile projects,Join a fun team full of flair and personality, contribute and be recognized for your value add!,Zero Bureaucracy environment - we are about embracing creative talent,Passionate about working on complex Data problems,You will ideally have a solid demonstrated Big Data experience,Java/ Data/ Big Data Development", "Want to work in a business where you have a voice and can be a part of the big picture?,Excited by making and breaking things?,Love working and playing with massive amounts of data?,Excited by greenfield projects?,Handle server-side development of a distributed Google applications,Develop data loading/ETL processes for Google Big Data Platform,Write clear, efficient, tested code.,Develop code as part of a wider team, contributing to code review and solution design.,Contribute to evolution of standards and design patterns.,Deploy and maintain applications in production environment.,Solve challenging technical problems within distributed environments,Work in a team using the Scrum/Kanban methodology ensuring that your team meets your backlog commitments.", "Defining, building and delivering a data architecture to enable the vision of the business.,Building and implementing a data framework, that provides easy to access, clear and scalable databases/datawarehouses that will support data pipelines implemented by you in order to power machine learning capability and deliver Real Time business intelligence.,Integrating multiple, disparate data sources and unlocking data silos in the business thus i mproving the automation, frequency and quality of data collection from business assets and external sources via traditional ETL pipelines and/or APIs,Working in collaboration with the Chief Product Office r and business stakeholders to ensure timely delivery of the product roadmap, creating solutions that will perform at scale.,Providing information, feedback and mentorship to support technology-related decision making as required.,Keeping abreast of industry and technological developments in your field to ensure the architecture remains fit for purpose.,Proven experience of delivering data engineering projects, working with complex, unstructured, semi-structured and structured data. Any experience of spatial data and imagery would be a a plus but is not essential..,A track record in building and delivering data infrastructure & architecture for commercial, software solutions,A good understanding/experience of scaling eg the ability to devise, execute and maintain automated script testing, the ability to build and deploy on cloud infrastructure (or devise a scalable approach to Dev) and the ability to annotate and document to a level where others can work off your scripts, etc.,Experience of working with multiple, disparate sources of data (specific experience of working with Real Time and dynamic data from the likes of IoT sensors, vision technology and satellite data sets would be an advantage but is not essential).,The ability to implement best in class data integration routines including data validation and qualitative checks,Strong coding skills with Python, Java, Scala, C/C++, JavaScript SQL or similar.,Experience in building data driven applications for BI and intuitive consumer facing solutions/apps would be an advantage,Experience of owning the software development life cycle would be be an advantage,Competitive salary,Equity/Share options,Healthcare,Pension", "Hand-screened leads,No ads, scams, junk,Great job search support,50+ career categories", "Work as an integral member of our Research & Development team.,Collaborate with our cross-functional team to define, architect, and implement highly scalable solutions.,Participate in an innovative Lean product development process, where you get to influence what you work on every week and flex into new skill sets.,Heavily influence technical product roadmap and implementation direction.,Build, deploy and support data-intensive applications in a multi-tier, high availability, distributed cloud-computing environment.,A minimum of 3 years relevant experience.,A proven track record of technical team leadership.,Polyglot with an attitude of flexibility and a pragmatic approach towards business solutions.,Solid experience dealing with large quantities of data across relational databases, cloud storage, and distributed compute systems.,Existing familiarity or interest in developing Machine Learning Engineering skills,A solid understanding of TDD approaches and automated testing concepts,Experience with concurrency and RESTful web services,Excitement about influencing the direction of the product,Ability to consider the big picture as just as important as your technical skills,Data stack,Postgresql,S3 + Parquet,Spark,Airflow,Redshift,Looker,Spectrum / Athena,AWS (EC2, RDS, Lambdas),Docker,Microservices,AWS ECS, Fargate, EKS,Jenkins,Programming languages,Python,Scala,Java,JavaScript / Node,Important libraries,React,PySpark,Health, dental, and vision insurance with a generous employer contribution;,An innovative and flexible paid time off policy;,A generous stock options plan and a 401(k) plan;,A kitchen stocked with breakfast and lunch food, coffee, sodas, snacks, and adult beverages;", "Responsibility for the handling, processing and integration of data into the ChEMBL database.,Facilitating the deposition of datasets directly into ChEMBL by working closely with external collaborators.,Applying text- & data-mining techniques for the development of effective large-scale curation strategies.,Developing methods for the application and maintenance of ontologies in ChEMBL.,Working with other teams to facilitate the integration of data between different EBI resources.,A BSc (or equivalent) in a life-science subject (e.g. biological or biomedical sciences).,3+ years of postgraduate experience in scientific data integration, database development or text- & data-mining, with a demonstrable track record of achievement.,Proficient in at least one programming/scripting language (Python knowledge is highly desirable).,Good knowledge of relational databases, data modelling, SQL and PL/SQL, and RESTful web-services.,Good understanding of a range of bioinformatics tools and resources (e.g., BLAST, Pfam, PDB, UniProt).,Experience in integrating diverse data sets.,Knowledge of good practice in software engineering and good code documentation.,Good knowledge of UNIX systems.,Team player, ability to work both as part of a team and independently.,Self-motivated with a driver for quality.,Good communication (verbal and presentational).,Higher degree (e.g., MSc/PhD) or equivalent in life-sciences, computer science, or related discipline.,Formal training in programming, data/entity-relationship modeling, text- & data-mining.,Familiarity with Python, Java and Perl.,Knowledge of drug discovery and development.,Experience working with chemogenomic and pharmaceutical data.,Knowledge of cheminformatics methods (e.g., chemical structure representations, substructure & similarity searching).,Apply now", "Analyse complex business-rules and data to maximise simplification and repeatability for end-users.,Use relevant techniques and expertise in data modelling and database design to integrate structured and unstructured data from internal and external data sources to maximise usability and discoverability for self-service analytics and data science activities.,Ensure that best practise is adopted to maximise agility and responsiveness whilst maintaining appropriate levels of governance and adherence to GDPR.,Provide guidance and consultancy to projects that use internal data, working with both specialists and senior stakeholders, as required.,Keenness to learn new things and apply them in a practical way.,An enjoyment working independently but comfortable asking for support when appropriate.,An analytical and creative approach to problem solving.,Strong data integration skills focussing on business outcomes, ideally using Alteryx.,Logical and methodical approach to solving problems.,Experience of working with complex data where stakeholders have differing requirements from the same data.,An understanding of GDPR when working with data.,Business/data analysis.,Maintenance of documentation & processes.,Data integration using Alteryx.,SQL server.,Data modelling for self-service analytics.,Data governance/data quality/master data management.,Excellent analytical skills.,Excellent attention to detail.,Proven ability to interpret and translate source data alongside business requirements to create data models that are optimised for self-service analytics.,Flexibility and a proven customer focus.,Excellent communication skills and an ability to work effectively both as part of a team and within a wider matrix structure.,Ability to manage competing priorities whilst working to deadlines.", "Lead the creation and maintenance optimal data pipeline architecture.,Assemble complex data sets that meet functional / non-functional business requirements.,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.,Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.,Work with stakeholders including the Executive, Product, and Business Intelligence to assist with data-related technical issues and support their data infrastructure needs.", "Participate in the collaborative design and development of specialized data structures for the purpose of data consumption by a public facing website,Develop a data management framework that is effective, scalable and reliable,Be the subject matter expert for consumption layer data, providing guidance to teams on best practices, capabilities and limitations,Research and prototype new technologies,Oversee data transformation, normalization, cleansing, aggregation, workflow management and business rule application,Load, process and manage incoming data feeds,Contribute to the development of architectural roadmaps and perform periodic reviews to identify improvement opportunities.,Enable integration and deployment tools and methods for managing reliable development life cycle routines,Experience developing and deploying data applications using Open Source frameworks like Spark, Kafka, AWS S3, StreamSets and Redis,Data modeling and corporate-level data management experience,Familiarity of Relational Database Management Systems,Fluency in several programming languages such as Python, Scala or Java,SQL Programming / ETL and data architecture management experience,Define and manage critical data using Master Data Management solutions,Ability to perform Unit Tests and internal QA checks,Good collaboration and idea sharing in a team environment,A Bachelor\u2019s Degree in Computer Science or related field preferred,Previous experience with Microsoft SQL Server is a plus,Meaningful Work \u2013 Connecting Americans with their healthcare providers,Changing the Game \u2013 evolving culture with career advancement opportunities,Community Builders \u2013 partnering with local charity and wellness initiatives", "Build and automate reliable data pipelines using batch and streaming technologies,Design and test ETL systems using the latest technologies,Collaborate with other software engineers and data scientists to develop data driven applications,Excellent communication and collaboration skills,Coding proficiency in at least one of Java, Python, C++, Go, Scala,Strong computer science skills with a focus on algorithms and data structures,Experience querying data using SQL and/or NoSQL,Familiar with MapReduce and other big data concepts,Understand trade offs among data formats such as CSV, JSON, Avro, Parquet,Experience with stream processing technologies such as Apache Beam, Spark Streaming, Flink, Kafka Streams,ETL experience on AWS using EMR, Firehose, Lambda,ETL experience on Google Cloud using Dataproc, Cloud Functions, Dataflow,Experience using a data warehouse such as Redshift or BigQuery,Familiar with messaging systems such as Kinesis, Kafka, PubSub,Competitive base salary plus meaningful equity,Comprehensive benefits (Medical, Dental, Vision, 401k),Daily catered lunches,Dog friendly office", "Develop and maintain tools that bridge disparate data sources across the organization to serve data models, dashboards, decision aids, and business case analysis with up-to-date and accurate information.,Collaborate with our data scientists on engineering machine learning and predictive features from our proprietary data assets.,Prototype, implement, and optimize data new pipelines and architectures that can transform our data to impactful insights.,Create daily, weekly, or monthly automated processes built on proven and completed work.,Create innovative and efficient techniques that better solve the problems we are working on,Assess risks to proposed analytics solutions based on quality of existing data sources and actively develop QA procedures to mitigate those risks.,Tailor our syndicated data to retail and CPG business needs in a robust, repeatable, and scalable way.,Facilitate knowledge transfer across the business between Product Delivery and Commercial teams.,Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations.,Have a professional quantitative background with an education that stresses analytical thinking and quantitative methods (STEM),Experience using python in a data context (iterators, pandas, scikit-learn, etc.),Are passionate about innovating new ways to answer novel problems,Are comfortable with being given the self-autonomy to work independently and experiment with new ideas and new ways to design and implement data science solutions to advance business goals,Can maintain sharp focus amidst multiple priorities and a keen aptitude to prioritize and manage time/projects,Have confidence in being the expert on SPINS data capabilities and best practices based on independent synthesis of a broad range of data,Have knowledge of working in a cloud computing environment,Familiarity with Docker or similar container framework and container orchestration tools such as Kubernetes is a plus,Experience working with retail POS or other transactional data is preferred,Vibrant \u2013 You\u2019re passionate about doing meaningful, impactful work.,A Disruptor \u2013 You\u2019re not afraid to do things differently,Connected \u2013 You work well as part of a team, and you build strong relationships with colleagues,Be Yourself \u2013 We are open & honest with each other; we take responsibility for our actions, and for our work,Health, dental and vision insurance,401k (Traditional and Roth) plus company match,FSA for medical and dependent care expenses,Pre-tax commuter benefit,Life insurance,Short- and long-term disability,Paid maternity and paternity leave,Bike storage,Fresh and healthy snacks", "Identify, verify and score inbound consumers, on-demand, with as little as a single identifier.,Link customer data, update/add missing identifiers and enhanced attributes.,Enable improved digital marketing performance through higher match rates and complete insights.,Working with data scientist to maintain code base for analytics (Git for source control, Unit/Integration Testing, code reviews),Create and maintain optimal data pipeline architecture,,Assemble large, complex data sets that meet functional / non-functional business requirements.,Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.,Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS \u2018big data\u2019 technologies.,Keep our data separated and secure across multiple data centers and AWS regions.,Work with data and analytics experts to strive for greater functionality in our data systems.", "Pair up with teammates, learn about our real time streaming data pipeline and ship both a bug fix and new feature to production; demoing your work at at our end-of-sprint review session,Meet cross-functional peers on our devops, front-end, customer success, marketing and sales teams, experiencing hands-on demos of our fantastic products,The ability to work with loosely defined requirements and exercise your analytical skills to clarify questions, share your approach and build/test elegant solutions in weekly sprint/release cycles,A drive to get results and not let anything get in your way,Proficiency in Scala and/or other functional programming languages and pride in producing clean, maintainable code,A passion for developing, configuring and testing highly scalable applications/data pipelines running on Storm, Spark or similar distributed systems,Competency writing applications that interact with Kafka, Zookeeper, Elasticsearch, Redis and/or similar open source technologies,Experience working with devops teams and tools to ensure your work makes a smooth, automated, repeatable transition from your Mac to our staging and production environments,Competency locating, troubleshooting and fixing bugs and performance issues in distributed systems running on the JVM,Knowledge of best practice around continuous integration, test driven development, code review, local containerized development/testing and everything it takes to ensure your high quality code works both for you and our customers,A desire to keep abreast of the latest industry trends and technologies, a commitment to continuous learning and an open mind to others - no matter how senior or junior they are,Awesome market leading product in an expanding space,Innovative, customer centric culture & bright, passionate teammates,Competitive compensation, including equity,100% paid healthcare", "Grow our existing cloud and data infrastructure, democratize access to data,,Champion the use of data and analytics at the company; be the pioneer of a data-driven culture.,Work with tech, marketing, sales, operations, finance and other business functions to scope, design and implement their data needs.,Help identify significant data sources and key variables for various business functions.,Lead the design and development of the data pipelines to productionize and ingest meaningful business data into a unified data model.,Develop machine learning and predictive analysis tools to identify new growth opportunities and personalize our services,Bachelor's degree in Computer Science/Computer Engineering or relevant industry experience,Proficiency in multiple programming languages, particularly Python, R or Javascript, with minimum two years full time professional experience,You are proficient with SQL and are rarely satisfied with the current query performance,You have developed data ingestion, data warehouse and integration pipelines,You are comfortable with both batch and stream data processing.,You know what it takes to build scalable and performant data models.,You understand the differences between relational and columnar databases and when to use them.,You are familiar with web development.,You value writing tests and people who write tests.,You have worked with one of the major public clouds (preferably AWS).,Ideally, you have experience with data visualization.,Traditional Benefits -- Health, Dental, Vision, Life, Short Term Disability, LTD,10 paid holidays,Generous PTO policy,Daily Fridge Credits,Monthly cell phone credit,Spontaneous company events and community service activities", "2+ years of experience in the field,Deep experience writing in Python,Experience working with Jupyter,Experience with SQL,Experience with data acquisition (writing crawlers, interfacing with various API\u2019s, fetching and transforming data, etc.),Experience with web services architecture,Experience implementing Spark or Hadoop,Experience with distributed computing / concurrent data pipelining,Experience with or interest in learning presentation softwares, such as D3, Tableau, Plotly, and/or MS PowerPoint", "Build statistical and machine learning algorithms that drive business decisions,Measure and predict KPIs related to user acquisition and retention,Improve upon our existing reporting strategies to communicate ongoing performance,Provide various teams with ad-hoc analysis to aide day-to-day operations,Influence product decisions through quantifiable goals,Lead projects for customer segmentation, personalization, and pricing,Identify opportunities for developing new data sources,Fluent English,Bachelor\u2019s degree or higher in statistics, computer science, or other quantitative discipline,Understand how to use Git,Advanced SQL skills,Experience with a language suitable for data analysis such as Python, Java, or R,Fluency in data analysis and communication about data, including time series and event data analysis, and data visualization,Knowledge of recommender systems,Experience with customer retention modeling and/or survival analysis,A/B testing experience,Experience with Amazon Web Services or Google Cloud Platform,Experience processing large datasets using platforms such as Hadoop and Spark", "Working with academic faculty to monitor and optimize current forms of data collection, and develop and integrate new forms of data collection,Working with academic faculty to optimize the transformation of collected data into formats appropriate for analysis,Working with academic faculty and IT developers to apply state-of-the-art approaches in cloud service, containerization and other techniques for scaling the remote staging, storage, computation and analysis of data sets, including at scale by hundreds or thousands of students in online courses,Providing expertise in data set hygiene to the institution, facilitating the cleanup and integration of potentially multiple incomplete, irregular, and partial data sources into interesting and useful data sources for analysis,Working with various offices to ensure that information is used complies with the regulatory and security policies in place,Teaching a Database Systems core course for the MIDS program and possibly other programs,Helping develop advanced courses in Data Engineering,Developing and running workshops for the campus community in different aspects of data management and wrangling,Advising student capstone projects in the MIDS and other programs on data engineering needs,Helping evaluate capstone projects with program faculty", "See technology as a passion, not something you just do between 9-5,Possess the ability to create new solutions; we operate on a web based platform and constantly facing unchartered waters,Possess strong fundamentals within coding technologies and a willingness to wear several hats when called upon,Do not wait for something to break; find a problem before it becomes one and constantly aiming to improve,Having a willingness to vocalize these ideas and pick yourself up if you get knocked down,Value passionate technologists, go-getters, and people who never stop seeking ways to improve existing technology,Have a high focus on career development and the runway to get you there,Work hard, period,Offer competitive compensation, benefits, 401k, challenging projects, company wide events, coworkers and leaders who will push you to get better, a sense of community not found anywhere else,Serve as a member of a data team that solves complex challenges and builds working database solutions using SQL Server, T-SQL, SSIS, stored procedures, views, user-defined functions, and table functions,Develop solutions and contributing to development, leveraging Object-Oriented programming techniques (.Net), Software Development Lifecycles, Unit Test Techniques, and Debugging/Analytical Techniques.,Collaborate with the team to develop database structures that fit into the overall architecture of the systems under development.,Code, install, optimize, and debug database queries and stored procedures using appropriate tools and editors.,Perform code reviews and provide feedback in a timely manner.,Promote collective code ownership for everyone to have visibility into the feature codebase.,Present technical ideas and concepts in business-friendly language.,Provide recommendations, analysis, and evaluation of systems improvements, optimization, development, and maintenance efforts, including capacity planning.,Identify and correct performance bottlenecks related to SQL code.,Support timely production releases and adherence to release activities.,Contribute to data retention strategy.", "Design and build infrastructure to host machine learning models as microservices using modern conventions and coding practices,Help design and implement data models and database layers that support our machine learning and business intelligence activities,Use creativity and independent thinking to solve technical problems,Mentor more junior team members,Communicate clearly and effectively with technical and non-technical colleagues about our data engineering projects,Work closely with data scientists to understand their needs and processes,Work closely with our whole technology team to successfully maintain our data platform alongside the broader technology stack,Implement strong and consistent internal API conventions and documentation,Implement with an emphasis on tests, maintainability, and clean coding practices to produce simple solutions and reduce technical debt,3+ years building and maintaining back-end services in production, preferably using container-based architectures,Experience using AWS tools and services,Experience with relational databases and data pipelines,Proficient in SQL, *nix CLI tools (grep/sed/awk/BASH, etc), and Python,Experience deploying and maintaining code using git-based tools and operating in a continuous deployment/integration environment,Experience writing thorough tests and documentation for maintainable code-bases,Ability to develop creative technical solutions given a set of business requirements and a strong understanding of modern data architectures,Ability to work productively on small teams and lead workstreams independently if needed,Experience mentoring less experienced colleagues,Ability to communicate technical ideas to non-technical colleagues Preferred Skills and Experience,3+ years building and maintaining data science pipelines that incorporate machine learning models in a production environment behind an API,Experience working with data scientists in production roles,Experience with container management solutions like Kubernetes, Marathon, etc.,Experience storing and using large amounts of text data and text transformations,Competitive compensation,Medical, dental, vision, mental health insurance \u2013 with premiums fully paid by Ascent,401K offered,Unlimited PTO and Bank Holidays,Flexible work schedule,As much RAM as you can fit in a Macbook Pro,Professional development stipend,Top floor office in Prudential Plaza with an amazing view of the park/lake and Pedway access", "Designs and develops data-ingestion frameworks, real-time processing solutions, and data processing and transformation frameworks.,Deploys application codes and analytical models. Provides support for deployed data applications and analytical models.", "Provide design and support of the data warehouse,Maintain the acquisition of data,Conduct data migration tasks to transform and join data,Pertform montoring and testing of ETL jobs,Provide documentation for designed warehouses", "Strong academic background,Excellent experience in Big Data Engineering,Demonstrable programming experience e.g. Python, Scala, MapReduce, Java, C++,Knowledge of SQL,Familiarity with Hadoop, Spark, NoSQL.,Good understanding of visualisation tools such as Tableau, D3, Qlik,Interest in Machine Learning,Excellent understanding of manipulation and analysis of large, complex data sets.", "Data Validation - Develop automated procedures to ensure data accuracy and integrity of complex data exports and reports,Test the ETL process and business logic that drives our in-app reporting features,Define, develop, and implement quality assurance practices and procedures, test plans and perform other QA assessments for all data related code changes,Develop automated tests using open source tools,Configure and maintain test automation environments,Create scripts, test sequences, and implement manual procedures to ensure proper test coverage,Work closely with other QA team members to understand and validate upstream application changes and impacts on data, exports, and reports,Work closely with Development, Product team and other organizations in the company to promote software quality standards,Work closely with Customer Support to replicate customer issues and product field use cases,Participate in test team activities including requirements analysis, test planning, tracking, reporting, and support of test cycles.,Engage in test case execution including defect documentation and tracking, resolution support, and fix verification", "Strong academic background,Excellent experience in Big Data Engineering,Demonstrable programming experience e.g. Python, Scala, MapReduce, Java, C++,Knowledge of SQL,Familiarity with Hadoop, Spark, NoSQL.,Good understanding of visualisation tools such as Tableau, D3, Qlik,Interest in Machine Learning", "strong knowledge of SQL,,above average knowledge about statistical methods and predictive analysis,,experience in working with Big Data driven projects,,familiarity with data and text mining,,very meticulous, diligent and creative attitude to your work,,fluency in English is a must,,degree in a field related to data science (IT, mathematics, statistics, etc.),,high degree of individual initiative, strong analytical skills for understanding complex issues and, last but not least, the ability to challenge the status quo and proactively tackle problem areas.,you will translate your knowledge on topics such as data extraction and transformation, data modeling, data mining, crawling, parsing and predictive analytics into modern, customer-oriented solutions,,you develop the solution portfolio and the data universe for the kantwert-BusinessGraph with currently more than 100 million relationships between 5 million people and institutions,,you will be responsible for and implement Big Data projects for demanding customers from various industries within the framework of agreed quality, time and budget targets,,in coordination with Sales, Product, IT and Data Science, you play a decisive role in the data acquisition and maintenance processes and the international expansion of the data pool.,comfortable employment contract (you choose the type of agreement),,healthy work \u2013 life balance: home office, flexible working hours, 30 minutes lunch break included in 8 - hours working day,,medical care, gym card,,modern office in the center of Poznan,,integration events, fresh fruits, cookies and breakfast cereals, dining discount card,,time for self \u2013 development, German lessons, skilled and helpful team,,startup culture atmosphere with mature business approach,", "Design and implement complex Big Data Platform based on requirements with possible technical solutions.,Willing to learn and understand business logic, and use that to continue to optimize/improve Big Data Platform to better support data processing requirements.,Discover any feasible new technologies lied in Big Data ecosystem, share them to team with your professional perspectives, apply them to production piece by piece for continue improvement, make it happen.,Be comfortable conducting detailed discussions with Data Analyst, other Big Data Engineers regarding specific questions related to specific new platform requirement.,Communicate well and clearly with teams oversea in both oral and written English.,A proficient engineer with minimal 1 year Linux or Big Data operation experience,Experience and knowledge of Cloud (AWS/Azure/Google Cloud/Ali), Virtualization or Containerization,Experience and knowledge of Big Data ecosystem (Hadoop/Hive/Pig/Spark/Presto/Storm/Heron/Flink and so on),Experience and knowledge of Operation Automation (Any of Salt/Puppet/Chef/Ansible),Experience and knowledge of Docker and K8S (is plus),Experience and knowledge of Monitoring & Logging system (is plus),Experience and knowledge of Security (is plus),Experience and knowledge of Database (MySQL/PostgreSQL/Redis/\u2026 is plus),Experience and knowledge of Distributed Storage (is plus),Experience and knowledge of CI/CD (is plus),Good English spoken and written skills (is plus),Good ability of communication.,Energy and creativity are key characteristics that describe you and the projects you are involved. You make it happen. Boom!,You\u2019re an app fanatic, positively curious and a technology enthusiast.,Competitive compensation.,All the tech tools you need to succeed,Free breakfast & lunch, snacks, fruit, good coffee and other delicious treats to keep you well fed.,16 days of paid leave, so long as you promise to come back!,Great social & labor insurance packages to fit your needs to ensure you\u2019re happy and healthy.,Commuter benefits that make getting to and from work a breeze.,Working a little later? We\u2019ll cover your dinner and ride home.", "Competitive salary,Company equity,Health insurance (100% paid for individuals, 70% for families),401K,Generous vacation policy, plus company holidays,Flexible schedules", "\u0412 \u043f\u0435\u0440\u0432\u0443\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c - Team leader \u2013 Team leader \u0433\u0440\u0443\u043f\u043f\u044b \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043e\u0432/\u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u043e\u0432 big data, data-scientist\u2019\u043e\u0432, \u0441\u043e\u0437\u0434\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438. \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u043d\u0430\u043c \u043d\u0443\u0436\u0435\u043d \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440-\u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0442\u043e\u0440, \u0430 \u0447\u0435\u043b\u043e\u0432\u0435\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043c\u043e\u0436\u0435\u0442 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u0440\u0435\u0448\u0430\u0442\u044c \u0441\u0430\u043c\u044b\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438, \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 big data, \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u043e\u0439, data mining\u2019\u043e\u043c;,\u0415\u0432\u0430\u043d\u0433\u0435\u043b\u0438\u0441\u0442\u0430 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0439, \u043f\u0440\u043e\u0441\u0442\u043e \u0438 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0435\u0433\u043e \u043e \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0432\u0435\u0449\u0430\u0445;,Start up - manager, \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u044e\u0449\u0435\u0433\u043e \u043f\u0438\u043b\u043e\u0442\u043d\u044b\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u044b \u0434\u043b\u044f \u0430\u043f\u0440\u043e\u0431\u0430\u0446\u0438\u0438 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0438\u0434\u0435\u0439 (proof of concept);,\u041a\u043b\u044e\u0447\u0435\u0432\u043e\u0433\u043e \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0430 \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0432 \u043c\u0438\u0440\u0435 \u043e\u0431\u043b\u0430\u043a\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 (AppStore) \u0434\u043b\u044f \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043d\u0435\u0444\u0442\u0435\u043f\u0435\u0440\u0435\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u00ab\u043f\u043e\u0434 \u043a\u043b\u044e\u0447\u00bb \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438, \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0430 (AI) \u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f (ML);", "Read Codes and Clinical Terms Version 3,SNOMED CT,ICD10 and OPCS4,MedDRA,LOINC,Information Resources \u2013 performing upgrades to code editions or cross-maps.,Communications and Leadership \u2013 ability to provide and receive complex technical information instruction and advice relevant to the role, be courteous and provide effective service to support NWEH, clinical trial sponsors and other stakeholders in various projects.,Work with the Technical Architect to define the scope and content of data coding services and work with Database Administrators to produce automated ETLs, coding improvements and business intelligence reports to deliver the required outputs.,Assist IT support staff in troubleshooting data and coding issues in NWEH applications.,Partnership working \u2013 liaise with other members of NWEH technical, clinical and operational teams as necessary to ensure the data coding services are fit for purpose and available as required.,Analysis and data management \u2013 ensure the data coding updates and ETL processes are properly documented and understood, that systems are designed and in place to monitor data quality and categorisation against agreed targets and that the Data Dictionary and Mapper is properly documented to enable any problems to be quickly identified and corrected.,Research, Development and Audits \u2013 participate in audits of performance, quality, regulatory and operational effectiveness.,Degree in IT related field or equivalent experience,Experience of mapping between different clinical coding systems.,Experience of working with NHS medical data.,Experience of any relational database system.,Basic knowledge DML and SQL programming.,Basic knowledge of database modelling including referential integrity and DDL.,May be required to handle heavy objects infrequently,Required to use a VDU for long periods during the working day,Required to undertake prolonged concentration,Experience creating ETL (Extract, Transform and Load) processes.,Experience with data presentation models used on clinical trials e.g. CDISC SDTM,Experience with data standardisation models used in clinical data analytics e.g. OMOP Common Data Model.,Experience using R,Understanding of clinical governance/data handling/safety monitoring and reporting,Experience working on Computer System Validation.,Modern offices in Central Manchester,Competitive Salary,Flexible working hours,The ability to work from home,Child care vouchers,Excellent pension benefits,Access to training resources,27 day\u2019s annual leave", "Previous experience as a Data Engineer working in a fast-paced agile development environment, ideally within a digital organisation.,Be an expert in deploying the appropriate data design techniques to solve a wide range of complicated problems and will understand industry best practices around web traffic data.,Significant database design and data modelling experience on multiple platforms (Oracle, MySQL, Teradata, Vertica, and Redshift) and possess the ability to work well with technical partners and business end users.,Strong interpersonal, leadership and customer service skills and will be a proven team player with an enthusiasm for new technologies and keeping up to date within your field.", "Advises management and customers on scalable enterprise analytics solutions that provide the business with a competitive advantage.,Works on unusually complex technical problems and provides solutions which are highly innovative and ingenious.,Design and build a modern data warehouse in the cloud,Design and build a customer 360,Implement a flexible and audible data pipeline,Enhance data collection procedures to build analytic systems.,Process, cleanse, and verify the integrity of data used for analysis.,Perform ad-hoc analysis and present results in a clear and user friendly manner,Perform testing, resolve issues and automate unit tests.,Develop proof-of-concept (POC) solutions to help business units better visualize their business needs and to clarify requirements for development.", "Assembling large, complex data sets that meet business requirements,Identifying, designing, and implementing internal process improvements: including process automation, optimizing data delivery, etc.,Designing optimal ETL infrastructures from variety of data sources,Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.,Big Data, including the Hadoop Ecosystem, NoSQL approaches and Cloud Data Management.,Data warehousing and BI (incl. SQL / Data Modelling, Data Integration, Data Governance),Thorough understanding of the capabilities of commercial Apache Hadoop distributions such as e.g. Hortonworks, Cloudera, or MapR,Experience in estimating, planning and managing data integration aspects of implementation projects.,At least 2 years of overall relevant IT experience.,At least 1 years experience with Big Data projects (using Hadoop, SQL, ETL, and/or similar technologies),At least 1 year experience with data engineering / data integration in the Big Data, Analytics, Business Intelligence and/or Data Warehousing domain,Experience with custom application development/enhancements using relevant technologies (i.e. Python, JSON, SQL, NoSQL, Unix/Linux scripting, Hadoop, HortonWorks, Cloudera, etc),Experience of a data warehouse environment and knowledge of existing and emerging data integration approaches,Experience with Watson Discovery/Conversation/Knowledge Studio,Experience with SSIS,Custom application development using technologies such as Python, JSON, SQL, NoSQL, Unix/Linux scripting, Hadoop, HortonWorks, Cloudera, Hibench, etc,Data architecture experience of designing and developing data models- Understanding of -Distributed computing design patterns and algorithms and data structures and security protocols,Involved primarily in coding, debugging and unit testing of applications,API experience, multithreading is a plus,May be involved in conceptual technology phase", "Understand our current data sets and models and help us discovering new ways to enrich the data,Creatively extracting real-world behaviour and trends out of the location data,Monitor and build processes for cleaning up inbound data,Dream up a solution, perform the R&D and deploy to production within our fluid work environment", "Hands-on leadership, influence, and development of all things data services.,Develop modern data architectural approaches for business intelligence reporting and analytics, including that for machine learning models and data science, ensuring effectiveness, scalability, and reliability.,Design, develop, implement, and optimize existing ETL processes that merge data from disparate sources for consumption by data analysts and scientists, business owners, and decisions makers.,Complete current evaluation of new ETL software options, propose recommendations, and implement the solution.,Facilitate data transformation, normalization, cleansing, aggregation, workflow management, and business rule application.,Detect data quality issues, identify their root causes, implement fixes, and design data audits to capture issues.,Distill technical requirements into the product development and operational process via continuous collaboration with product, engineering, and analytics team members.,Influence and communicate with all levels of stakeholders including analysts, developers, business users, and executives.,Use analytics to influence product development, surfacing data around product usage and customer behavior.,ETL tool evaluation and implementation to prepare for scaling and efficiency.,2+ years of hands-on experience in collection, mining, reporting, and analysis of large amounts of data,Experience designing and implementing database tables, as well as performance profiling and tuning of database processes and table usage,Experience in a variety of data storage platforms (MySQL, Postgres, Oracle, Redshift, RDS),Experience as a Data Engineer or related role (Data Warehouse Developer, ETL Developer, Business Intelligence Analytics, Software Engineer) with a track record of manipulating, processing and extracting value from datasets,Advanced proficiency in SQL,Experience designing and developing ETL processes in a variety of platforms (Pentaho, Microstrategy, Talend, CloverETL, FiveTran, Stitch),Experience with analytics platforms (Google Analytics, Mixpanel, etc.),Experience with log aggregation and extraction (Elasticsearch) and Business Intelligence reporting tools (Tableau, etc.) a plus,Proven track record of innovation and expertise in data engineering,Tenure in architecting and delivering complex projects,Deep understanding and application of modern data processing technology and real-time/low-latency data pipeline and ETL architectures,Strong stakeholder interaction and influence experience at executive, business stakeholder, and engineering team levels,Blinker Inc. does not currently provide Visa sponsorship,Competitive pay + equity,Paid/flexible time off,Comprehensive medical, dental, vision, disability, life insurance, and parental leave benefits,401K Plan,Paid covered parking in Tabor Center garage or RTD transportation,Fully stocked kitchen with free snacks, beverages, & cold brew coffee on tap,Regularly recurring, company-side social activities (e.g. BBQ lunches on patio and afternoon happy hours)", "develop data processing software, including designing, coding and testing,work collaboratively in a cross-functional team to deliver quality software,mentor others, sharing technical knowledge and providing guidance and support,communicate effectively with managers, peer developers, testers, business analysts, product owners and scrum masters,experience with distributed data-processing technologies such as Apache Hadoop, Apache Spark and key-value stores,experience in open-source technologies including JVM languages and Python,experience of cloud architecture,a good understanding of ETL/ELT data-processing pipelines,experience of developing multiple large-scale data-processing solutions,a good understanding of NoSQL data stores,experience of data modelling with RDBMS and NoSQL data stores", "Develop and deliver data sources and infrastructure to support the needs of predictive modeling and analytics as part of a cross-functional team,Connect raw data to business meaning and understand data generation and flows in the context of business impact. Communicate anomalies and work towards resolution,Execute rapid development of new data and analytic work tracks with fast iteration over quick sprints,Oversee the development of new concepts and proof of concept designs of moderate complexity,Ensure prototypes meet the organization standards to allow transfer to production, if applicable,Solve problems with a multidisciplinary approach, combining technical expertise with business knowledge", "Bachelors degree in IT, or similar experience,Extensive Data integration and data orchestration experience with Microsoft Azure,In-depth experience of designing and implementing data flows and pipelines,Deep experience as a Data Engineer is highly desirable,Experience of working in an Agile (ideally SCRUM) team,Comfortable being hands on with data, data modelling, query techniques,Background in the Data management Space,Experience of the FMCG/CPG industry,A good understanding of and adherence to data security standards", "Maintain the development infrastructure supporting the marketing organization,Development and maintenance of data processing on our new Marketing data platform,Leading contributor in the team in terms of working knowledge of big data technologies,Work with the technology and business community to turn business requirements into technical solutions.,Work autonomously from specifications and produce high quality, accurate, efficient and well documented code. Ensure accurate estimating for work based on detailed requirements.,Undertake maintenance and \u2018bug fix\u2019 development activities for existing applications.,Construct and execute unit and system testing.,Undertake peer-to-peer code reviews of colleagues\u2019 development tasks.,Ensure clear and early communication, in particular ensuring that that line manager and/or relevant parties are kept informed of progress, issues and difficulties.,Proactively work to mitigate risks, improve quality in an efficient manner and resolve problems that arise.,Lead adherence to best practice solutions across the team,Ability to manage ongoing project work alongside business as usual support and maintenance.,Provide estimates of duration and effort required to complete development tasks from high-level or loosely-defined requirements.,Create robust system-level design for assigned development activities with a solid grasp of business and commercial drivers to produce designs that clearly meet customer/user needs and feedback is positive,Undertake a thorough impact analysis of all assigned development activities, understanding the impact of own work on other tasks and areas of the system,Share process expertise across the team in order to enhance team effectiveness,Contribute to the marketing systems roadmap bringing technical leadership and oversight,Solid working knowledge of the following:,One or more of Python, Java,Performance tuning of both MapReduce queries and relational databases,Working knowledge of the following technologies:,AWS technologies,Big data technologies such as Spark,Analytical platforms such as Databricks, Blue Insight,Interpersonal Skills:,Excellent verbal and written communication skills,Team player able to work under own initiative,Customer focused and service-oriented,Confident in establishing good working relationships other IT teams (internal and external),Knowledge of the following technologies advantageous:,Marketing automation tools, including campaign management and workflow (Adobe Campaign preferred, other examples Eloqua, Aprimo),Web analytics: Adobe Analytics, SiteCatalyst/Google Analytics/Webtrends,Experience using a recognized development methodology, e.g. Agile Experience and understanding of the role of the developer in the full SDLC,Proven track record of feature level design,Knowledge of service management and ITIL framework,Previous experience with marketing, publishing and analytics preferred.,Able to create imaginative solutions that move the platform forward in features, maintainability and cost-effectiveness", "IoT data development and management.,Data Engineering or BI Development experience,At least 1 years\u2019 experience of modern programming language (Python, Java, C#, Scala),ETL experience,Experience in NoSQL/Big Data technologies (Redshift, Cassandra, MongoDB, BigQuery, Hadoop or similar),Experience in Cloud platforms (AWS, GCP, Azure, Oracle),Agile experience nice to have", "Work with Data Engineering Lead and key stakeholders to design and develop Reify's next-generation Kappa-style data architecture in a functional programming environment using Kafka, Kubernetes, PostgreSQL and potentially additional tooling from the AWS/Confluent Ecosystems (e.g. EKS, Athena, Confluent Operator, etc.),Take responsibility for the day-to-day maintenance, upgrades, orchestration, and troubleshooting of our data architecture and tooling.,Use a combination of Clojure (functional programmers welcome!), Python, and SQL to support analytics work (statistical modeling, machine learning) and develop/integrate analytics insights into our data products,Become intimately familiar with HIPAA, GDPR, and other applicable regulatory frameworks and how they influence our architecture and development decisions,Rapidly learn new tools, techniques, and languages as we scale and encounter additional data challenges,Frequently communicate your results to Data Engineering Lead and other technical/non-technical stakeholders in clear written, verbal, or presentation form,Live our data philosophy, which focuses on ethical decision making, being aware of how biased data (and assumptions) can affect results (and people), and being laser-focused on business needs,Extensive experience with DevOps, systems engineering/orchestration, and strong ability to understand and work with distributed systems,While this is primarily a data engineering role (70+%), the ideal candidate will also have the ability to handle analytical challenges as well (30+%),Understanding the nuances of testing in distributed/probabilistic systems,Experience with applied statistics (particularly of the Bayesian flavor), supervised/unsupervised learning techniques,Masters degree or greater in a relevant field,Relevant published work (academic, blog posts, open-source contributions),Previous experience with functional programming languages/philosophy (or existing Clojure chops!),Experience in a startup environment (as a remote employee, if you\u2019d like to work remotely),Competitive Salary and Stock Options: Compensation varies from mid-level to very senior and is commensurate with your experience.,Comprehensive Health and Wellness Coverage: 100% premium coverage for you (and >50% for your dependents) for: a top-tier health plan covering you in all 50 states (with option of HSA for medical expenses and as investment vehicle) dental, vision, disability (short-term and long-term), and basic term life insurance (for your entire tenure at Reify). We enable 24/7 access to doctor by phone or online via telemedicine coverage.,Company-provided Workstation: You have the option of getting a brand new Macbook Pro (or similar, if desired) laptop if you\u2019d like to use a separate computer for work.", "Implement ETL jobs for various functions,Support and maintain daily ETL jobs,Support the development teams by optimizing data access,Work with data science teams to deliver metrics to consumers,Implement ETL jobs for various functions,Support and maintain daily ETL jobs,Support the development teams by optimizing data access", "Develop, enhance, and automate processes for queuing and prioritizing data management and curation requests,Implement a quality control (QC) process to evaluate compliance with data models and taxonomies to ensure cohesion of curated data across time as data models and configurations evolve,Create a rapid means to remediate data that fail to meet curation specifications,Empower scientists with tools, processes and data structures needed to support project objectives,Ensure accurate, complete and timely collection, delivery and tracking of analytical information from translational, CRO or collaborating laboratories for curation, ingestion and delivery to computational scientists,Help define, deliver and implement R/ED, collaborator and partner laboratory analytical data management systems, processes and procedures,Work with R/ED study teams to develop R/ED information management plans that outline data capture, data flow, data queries, manual checks, and data listings needed to ensure data integrity,Participate in comprehensive data review activities in coordination with project and study teams,Work with computational biologists, computational scientists, biostatisticians and study scientists to resolve data quality issues,Make data, including raw/interim data, available to R/ED department personnel as required,Collaborate with users to enable data access and ingestion,Acquire user feedback to inform business requirements for future development.", "Partnering with cross-functional stakeholders to identify and plan for new data related business requirements, ensuring the details of those requirements are adequately fleshed out before development begins,Accountability for the consistent and effective flow of work for the Data Engineering team,Positioning yourself as the go-to expert with internal stakeholders and seen as deeply aware of, and able to address, their technical needs,Working with our devops and product teams to ensure our data engineers are positioned to satisfy their sprint commitments,Executing against the \u2018technical vision\u2019 for data, ensuring that all data engineering efforts are building toward long-term success,Reviewing pull requests, troubleshooting tough performance issues, and fostering our pair-programming and team-wide knowledge sharing efforts,Helping to develop and maintain code standards and accountability to them", "Develop, enhance, and automate processes for queuing and prioritizing data management and curation requests,Implement a quality control (QC) process to evaluate compliance with data models and taxonomies to ensure cohesion of curated data across time as data models and configurations evolve,Create a rapid means to remediate data that fail to meet curation specifications,Empower scientists with tools, processes and data structures needed to support project objectives,Ensure accurate, complete and timely collection, delivery and tracking of analytical information from translational, CRO or collaborating laboratories for curation, ingestion and delivery to computational scientists,Help define, deliver and implement R/ED, collaborator and partner laboratory analytical data management systems, processes and procedures,Work with R/ED study teams to develop R/ED information management plans that outline data capture, data flow, data queries, manual checks, and data listings needed to ensure data integrity,Participate in comprehensive data review activities in coordination with project and study teams,Work with computational biologists, computational scientists, biostatisticians and study scientists to resolve data quality issues,Make data, including raw/interim data, available to R/ED department personnel as required,Collaborate with users to enable data access and ingestion,Acquire user feedback to inform business requirements for future development.", "Is Service-Centric: Someone who desires to make an impact in and outside of our office. We look for service minded people to support our customers, each other and our community.,Has a Growth Mindset: Driven to own your individual learning and development (We\u2019ll help you - we have a team dedicated to training you and providing extra educational resources).,Rocks Impact: Thinks two steps ahead to ensure the work we do will solve problems and make a difference.,Will be a StrataPro: Accountable. Prepared. Positive. Core to who we are and how we treat one another.,Participate in the full life-cycle of development, from definition, design, implementation, and testing,Work with our Data Science team on transitioning a proof-of-concept to a real product,Selecting and integrating any Data tools and frameworks required to provide requested capabilities,Be an advocate for developing best practices in the organization, and bring in knowledge of new technologies to the team,Monitoring performance and advising any necessary infrastructure changes,Regularly contribute to ongoing improvements in engineering process and product development ecosystem,Give technical presentations to both development, product, and leadership teams,Work on building proof of concept architectures that have an eye towards production.,Participates in architecting and building large distributed systems that scale well,Work closely with our current engineering team to integrate data architectures into our existing data platform.,Support business decisions with ad hoc analysis as needed.,Expected to cross train team members on areas of technical expertise,Develop tools and utilities to maintain high system availability, monitor data quality, and provide statistics,Develop understanding of healthcare & finance terminology and workflows.,5+ Years of experience as a data/software engineer,Experience with SaaS/Cloud based offerings/products,Worked with big data and data warehousing technologies,Strong understanding of ETL processes and data flow architectures and tools,Experience building decoupled infrastructures,Proficient understanding of distributed computing principles,Experience building infrastructures that process large amounts of structured and unstructured data,Experience building tools for technical teams,Advanced knowledge of SQL or other relational databases,Intermediate to advanced knowledge in Unix and Linux command line tools and scripting,Experience programming in a highly regulated industry (healthcare or finance preferred).", "Read Codes and Clinical Terms Version 3,SNOMED CT,ICD10 and OPCS4,MedDRA,LOINC,Information Resources \u2013 performing upgrades to code editions or cross-maps.,Communications and Leadership \u2013 ability to provide and receive complex technical information instruction and advice relevant to the role, be courteous and provide effective service to support NWEH, clinical trial sponsors and other stakeholders in various projects.,Work with the Technical Architect to define the scope and content of data coding services and work with Database Administrators to produce automated ETLs, coding improvements and business intelligence reports to deliver the required outputs.,Assist IT support staff in troubleshooting data and coding issues in NWEH applications.,Partnership working \u2013 liaise with other members of NWEH technical, clinical and operational teams as necessary to ensure the data coding services are fit for purpose and available as required.,Analysis and data management \u2013 ensure the data coding updates and ETL processes are properly documented and understood, that systems are designed and in place to monitor data quality and categorisation against agreed targets and that the Data Dictionary and Mapper is properly documented to enable any problems to be quickly identified and corrected.,Research, Development and Audits \u2013 participate in audits of performance, quality, regulatory and operational effectiveness.,Degree in IT related field or equivalent experience,Experience of mapping between different clinical coding systems.,Experience of working with NHS medical data.,Experience of any relational database system.,Basic knowledge of database modelling including referential integrity and DDL.,May be required to handle heavy objects infrequently,Required to use a VDU for long periods during the working day,Required to undertake prolonged concentration,Experience creating ETL (Extract, Transform and Load) processes.,Experience with data presentation models used on clinical trials e.g. CDISC SDTM,Experience with data standardisation models used in clinical data analytics e.g. OMOP Common Data Model.,Experience using R,Understanding of clinical governance/data handling/safety monitoring and reporting,Experience working on Computer System Validation.,Modern offices in Central Manchester,Competitive Salary,Flexible working hours,The ability to work from home,Child care vouchers,Excellent pension benefits,Access to training resources,27 day\u2019s annual leave", "Entrepreneurial full-stack engineer who can Design, model and architect data and data-automation systems.,Develop and refine parsing, munging, and integrations from existing and future datasets to refine, clean, transform and apply through core systems channels.,Fluency in bash and python scripting languages, with significant automation experience.,Strong understanding Database development/implementation, and relational databases.,Proficiency in Salesforce and/or similar structural CRM and relational database systems.,The understanding and/or proficiencies in the following are a plus, but not required:,Define and apply best practices for building scalable and secure systems internationally,Self-manage, team lead and be able to manage partner interactions and coordinate on delivery for multiple ongoing projects.,Manage internal and cloud-based systems,Performance tune and optimize data systems,Define specifications for functional and technical deliverables,Personal network of highly skilled engineers and experience hiring.,Extensive experience developing and instituting team processes, including those around testing, revision control, deployment, and release forecasting.,Strong familiarity with data science and data munging.,Eagerness to communicate technical concepts to nontechnical team members, especially in the context of strategy and development prioritization discussions.,Leading edge awareness of new ideas and trends in Silicon Valley and the larger technology industry.,Experience in the financial services industry.,Comfortable in a decisive leadership role.", "Interacts with senior level customers to consult on scalable enterprise analytics solutions that provide the business with a competitive advantage.,Develops technical solutions to complex problems which require the regular use of ingenuity and creativity.,Works on unusually complex technical problems and provides solutions which are highly innovative and ingenious.,Design and build a modern data warehouse in the cloud,Design and build a customer 360,Implement a flexible and audible data pipeline,Enhance data collection procedures to build analytic systems.,Process, cleanse, and verify the integrity of data used for analysis.,Perform ad-hoc analysis and present results in a clear and user friendly manner,Perform testing, resolve issues and automate unit tests.,Develop proof-of-concept (POC) solutions to help business units better visualize their business needs and to clarify requirements for development.", "Develop, enhance, and automate processes for queuing and prioritizing data management and curation requests,Implement a quality control (QC) process to evaluate compliance with data models and taxonomies to ensure cohesion of curated data across time as data models and configurations evolve,Create a rapid means to remediate data that fail to meet curation specifications,Empower scientists with tools, processes and data structures needed to support project objectives,Ensure accurate, complete and timely collection, delivery and tracking of analytical information from translational, CRO or collaborating laboratories for curation, ingestion and delivery to computational scientists,Help define, deliver and implement R/ED, collaborator and partner laboratory analytical data management systems, processes and procedures,Work with R/ED study teams to develop R/ED information management plans that outline data capture, data flow, data queries, manual checks, and data listings needed to ensure data integrity,Participate in comprehensive data review activities in coordination with project and study teams,Work with computational biologists, computational scientists, biostatisticians and study scientists to resolve data quality issues,Make data, including raw/interim data, available to R/ED department personnel as required,Collaborate with users to enable data access and ingestion,Acquire user feedback to inform business requirements for future development.", "Building data infrastructure and back-end services to support user-facing and internal-use applications,Identifying, researching, and analyzing new data sources,Developing and maintaining REST APIs for Amne's back-end services,Managing and developing ETL pipelines to ensure and improve the accuracy, quality and usability of data across Amne's infrastructure.,Improving and maintaining industry-leading home valuation models and methodologies using millions of real-estate transactions and complementary data sets.,Collaborating with Software Engineers, Product Managers, and Product Designers,Maintaining development best practices, including test coverage, continuous integration, A/B testing, and documentation,Strong Python programming skills, including experience with NumPy, SciPy, Pandas, and scikit-learn,Familiarity with common Python web frameworks (Flask, Django, etc.),Experience with the AWS ecosystem (S3, Elastic Beanstalk, EC2, etc.) and application deployment,Ability to write clean, re-usable, and production-ready code.,Ability to translate complex data-oriented challenges into solutions for business objectives, and vice versa.,Experience with databases, statistics, web services, algorithms, and Python,Strong communication skills,Experience building modular, scalable, cloud-based systems,An interest in learning new technologies and taking the lead on the integration of new technologies into Amne's stack.", "Build industry-leading big data architecture,Support real-time analytics,Solid programming skills in Python, Scala or Java,Good experience of big data tools e.g. Spark, Kafka, Hadoop or similar,Experience of building onto Cloud platforms,Experience with SQL and NoSQL technologies and data modelling,Exposure to DevOps environments", "Built and implemented a data profiling tool to reverse engineer data schemas from new data sources facilitating normalization of the data into our data model.,Built the logic to combine real-time messaging and batch query processing so there is a single, accurate source of truth from a source system.,Analyzed and designed the best ways to expand our data model to incorporate more data that\u2019s mission critical.,A real passion for problem solving and learning new technology,Vision to balance speed and maintainability in solution design,Strong analytical and technical skills,The ability to handle multiple, concurrent projects,Excellent ability to craft and implement requirements, keep projects on track, and engage partners,Challenging the status quo to improve our processes and tools,Communicate complex technical details in meaningful business context,A low ego and humility; an ability to gain trust by doing what you say you will do,Own ten projects working multi-functionally with the Physician Success and Analytics teams to design and implement best-in-class data processing enabling clean data flow directly to our data model,Work with an HIE, engineering, analytics, and operations to design and implement an integration that streamlines our transitional care management workflows,Design a new concept within our data model to meet a new operational or analytical need,Build an app to send data anomalies to operations,2+ years of full-time experience,Experience building information pipelines utilizing Python or Java (willingness to expand knowledge of Python is required),High degree of comfort with relational data structures required,Knowledge of, and/or willingness to learn, non-relational data structures and other technologies (eg Postgres, Redshift, Cassandra, MongoDB, Neo4j, S3, etc.),BS/MS in computer science, math, engineering, or other related fields is required.", "Build industry-leading big data architecture,Support real-time analytics,Solid programming skills in Python, Scala or Java,Good experience of big data tools e.g. Spark, Kafka, Hadoop or similar,Experience of building onto Cloud platforms,Experience with SQL and NoSQL technologies and data modelling,Exposure to DevOps environments", "Assembling large, complex data sets that meet business requirements,Identifying, designing, and implementing internal process improvements: including process automation, optimizing data delivery, etc.,Designing optimal ETL infrastructures from variety of data sources,Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.,Big Data, including the Hadoop Ecosystem, NoSQL approaches and Cloud Data Management.,Data warehousing and BI (incl. SQL / Data Modelling, Data Integration, Data Governance),Thorough understanding of the capabilities of commercial Apache Hadoop distributions such as e.g. Hortonworks, Cloudera, or MapR,Experience in estimating, planning and managing data integration aspects of implementation projects.,At least 3 years of overall relevant IT experience.,At least 2 years experience with Big Data projects (using Hadoop, SQL, ETL, and/or similar technologies),At least 1 year experience with data engineering / data integration in the Big Data, Analytics, Business Intelligence and/or Data Warehousing domain,Experience with custom application development/enhancements using relevant technologies (i.e. Python, JSON, SQL, NoSQL, Unix/Linux scripting, Hadoop, HortonWorks, Cloudera, etc),Experience of a data warehouse environment and knowledge of existing and emerging data integration approaches,Experience with Watson Discovery/Conversation/Knowledge Studio,Experience with SSIS,Custom application development using technologies such as Python, JSON, SQL, NoSQL, Unix/Linux scripting, Hadoop, HortonWorks, Cloudera, Hibench, etc,Data architecture experience of designing and developing data models- Understanding of -Distributed computing design patterns and algorithms and data structures and security protocols,Involved primarily in coding, debugging and unit testing of applications,API experience, multithreading is a plus,May be involved in conceptual technology phase", "Work with the most innovative and most scalable data processing technologies,Build innovative state-of-the-art solutions with our customers,Work closely with our tech partners: Google Cloud Platform, Tableau, Looker,Work in an agile and dynamic environment together with a small team with our data scientists, machine learning experts, data analysts and data engineers,Strong programming and architectural experience, ideally in Python and/or Java, and SQL,2+ years of experience building (big) data solutions,Working experience with Google Cloud Platform (GCP) or Amazon Web Services (AWS),Extremely passionate about data and analytics,Experience with ETL tools, Hadoop-based technologies (e.g. Spark) and/or data pipelines (e.g. Beam, Flink),Experience building scalable and high-performant code,Experience in producing tested, resilient and well documented applications,The ability to take ownership, end-to-end and finding creative solutions,Experience in architecting, building, maintaining and troubleshooting cloud infrastructure,Excellent interpersonal skills, verbal and written communication skills; a team player and keen learner who loves building great things together,BSc or MSc degree in Computer Science or a related technical field,Love for the command line with optional affinity for Linux scripting,Experience building scalable REST APIs using Python or similar technologies,Experience with Agile methodologies such as Scrum,Basic knowledge of and ideally some experience with data science topics like machine learning, data mining, statistics, and visualisation,Contributions to open source projects,25 days holiday plus bank holidays,Pension scheme,Situated in the innovation hub of Canary Wharf,Laptop of your choice,Monthly social events and team offsites,Generous desk budget,Free fruit, cookies, tea/coffee throughout the week,Regular networking events, mentoring events and conferences,Exposure to experts from a number of industries,Freedom to explore the latest tools and technologies", "Build data ingestion pipelines for various data sources including Postgres, SQLServer, and REST APIs,Participate in design and architecture planning for our infrastructure and code,Develop features to support dynamic ETL, automated data quality validation, and data delivery,Work with Data Management to build data pipelines in Spark,Automate operational data tasks,Perform periodic on-call duties", "Help develop, enhance, and automate processes for queuing and prioritizing data management and curation requests,Empower scientists with tools, processes and data structures needed to support project objectives, including data integration efforts that may span multiple studies or experiments,Ensure accurate, complete and timely collection, delivery and tracking of analytical information from internal or contract laboratory providers or collaborating laboratories for curation, ingestion and delivery to computational and translational scientists,Help define, deliver and implement R/ED, collaborator and partner laboratory analytical data management systems, processes and procedures,Work with R/ED study teams to develop R/ED information management plans that outline data capture, data flow, data queries, manual checks, and data listings needed to ensure data integrity and interpretability,Participate in comprehensive data review activities in coordination with project and study teams,Work with computational biologists, computational scientists, biostatisticians and study scientists to resolve data quality issues,Make data, including raw/interim data, available to R/ED department personnel as required,Acquire user feedback to inform business requirements for future data systems development.", "Solid Java programming skills, with more than one production release under your belt,Solid experience in building high performance services for backend and mid-tier systems", "Build data pipelines and deploy machine-learning algorithms.,Minimum 3 years\u2019 experience in Data Engineering,Modern programming experience with Python, Java, Scala or similar,Solid ETL experience (Design, implementation and maintenance),Cloud ecosystem experience in Azure, AWS, GCP, Oracle or similar,DevOps experience in Kubernetes, Docker or similar desirable,Application of machine-learning methods Desirable,Degree in Computer Science, Physics, Mathematics or similar,Stakeholder engagement and project experience", "s,s,s,Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities,Implementing ETL process {{if importing data from existing data sources is relevant}},Monitoring performance and advising any necessary infrastructure changes,Defining data retention policies,{{Add any other responsibility that is relevant}}", "Leverage modern analytics stacks and cloud platforms to design, implement and maintain scalable infrastructure for ingesting, processing and persisting large volumes of batched and streaming data.,Contribute to multiple production code bases in a continuous delivery (CI/CD) environment.,Write, debug, maintain and constructively review code on a highly collaborative software engineering team.,Develop production quality software for interacting with distributed data pipelines, data stores and data models.,Provide thought leadership and advocate for best practices regarding big data and scalable processing and storage infrastructure.", "Consult with campus researchers, faculty and staff to understand data questions and needs.,Develop prototypes and proof of concepts for the possible solutions.,Create and maintain optimal data lake and pipeline infrastructure.,Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources using SQL, NoSQL and cloud \u2018big data\u2019 technologies.,Code, test, and document new or modified data systems to create robust and scalable applications.,Assemble large, complex data sets that meet functional / non-functional business requirements.,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.,Work with data and analytics experts to strive for greater functionality in our data systems.,Develop, refine and scale data management and analytics procedures, systems, workflows, and best practices.,Contribute to efforts in creating, refining, managing and enforcing data management policies, procedures, conventions and standards.,Collaborate with other members of formal and informal groups in the pursuit of common missions, vision, values and mutual goals.,Understand the overall processes and procedures of the organization and make recommendations in the continual improvement of those processes and procedures, providing for management analysis and recommendations on continuous improvement.", "Maintain the development infrastructure supporting the marketing organization,Development and maintenance of data processing on our new Marketing data platform,Leading contributor in the team in terms of working knowledge of big data technologies,Work with the technology and business community to turn business requirements into technical solutions.,Work autonomously from specifications and produce high quality, accurate, efficient and well documented code. Ensure accurate estimating for work based on detailed requirements.,Undertake maintenance and \u2018bug fix\u2019 development activities for existing applications.,Construct and execute unit and system testing.,Undertake peer-to-peer code reviews of colleagues\u2019 development tasks.,Ensure clear and early communication, in particular ensuring that that line manager and/or relevant parties are kept informed of progress, issues and difficulties.,Proactively work to mitigate risks, improve quality in an efficient manner and resolve problems that arise.,Lead adherence to best practice solutions across the team,Ability to manage ongoing project work alongside business as usual support and maintenance.,Provide estimates of duration and effort required to complete development tasks from high-level or loosely-defined requirements.,Create robust system-level design for assigned development activities with a solid grasp of business and commercial drivers to produce designs that clearly meet customer/user needs and feedback is positive,Undertake a thorough impact analysis of all assigned development activities, understanding the impact of own work on other tasks and areas of the system,Share process expertise across the team in order to enhance team effectiveness,Contribute to the marketing systems roadmap bringing technical leadership and oversight,One or more of Python, Java,Performance tuning of both MapReduce queries and relational databases,AWS technologies,Big data technologies such as Spark,Analytical platforms such as Databricks, Blue Insight,Excellent verbal and written communication skills,Team player able to work under own initiative,Customer focused and service-oriented,Confident in establishing good working relationships other IT teams (internal and external),Marketing automation tools, including campaign management and workflow (Adobe Campaign preferred, other examples Eloqua, Aprimo),Web analytics: Adobe Analytics, SiteCatalyst/Google Analytics/Webtrends,Experience using a recognized development methodology, e.g. Agile Experience and understanding of the role of the developer in the full SDLC,Proven track record of feature level design,Knowledge of service management and ITIL framework,Previous experience with marketing, publishing and analytics preferred.,Able to create imaginative solutions that move the platform forward in features, maintainability and cost-effectiveness.", "work to architect and develop data pipelines and storage to enable insight from Wellcome's data,work collaboratively in a cross-functional team to deliver quality software,mentor others, sharing technical knowledge and providing guidance and support,communicate effectively with managers, scientists, peer developers, testers, business analysts, product owners and scrum master,experience with distributed data processing technologies such as Apache Hadoop, Apache Spark,experience with language(s) commonly used in data science e.g. Python, JVM languages etc.,experience of Cloud Architecture,an understanding of ETL/ELT data processing pipelines,experience developing data processing solutions,experience data modelling with RDBMS and NoSQL data stores", "Develop, construct, test and maintain architectures, such as databases and large-scale processing systems,Ensure data architecture that supports the requirements of the business,Discover opportunities for data acquisition,Develop data set processes for data modeling, mining, and production,Employ a variety of languages and tools (e.g scripting languages) to marry the systems together,Identity, recommend and implement ways to improve data reliability, quality,4+ years of experience working as a Data Engineer,Experience with building Data Architecture,Experience in PostgreSQL or similar SQL-like database, Redis, Hive, and Sqoop Experience with NoSQL databases such as Cassandra or MongoDB,Development experience with Scala or any other functional or object-oriented language such as Python, Perl, Java, etc.,Have experience building a full stack data pipelines and data infrastructure,Live/work in the San Francisco Bay Area,Have a passion for education,Have previous experience at a growth stage internet/software company", "Database maintenance,Building and analyzing dashboards and reports,Evaluating and defining metrics and perform exploratory analysis,Monitoring key product metrics and understanding root causes of changes in metrics,Empower and assist operation and product teams through building key data sets and data-based recommendations,Automating analyses and authoring pipelines via SQL/python based ETL framework,Superb SQL programming skill.,Understanding of ETL tools and database architecture.,Advanced knowledge of data warehousing.,Demonstrable familiarity with code and programming concepts. Experience with Python is preferred but not required.,A product mindset - you ask and address the most important analytical questions with a view on enhancing product impact.,Passionate and attentive self-starters, great communicators.,1-3 years of experience in quantitative analysis experience.,Bachelor's degree in Computer Science, Statistics, Math or other technical field required. Graduate degrees preferred.", "Responsibility for the handling, processing and integration of data into the ChEMBL database.,Facilitating the deposition of datasets directly into ChEMBL by working closely with external collaborators.,Applying text- & data-mining techniques for the development of effective large-scale curation strategies.,Developing methods for the application and maintenance of ontologies in ChEMBL.,Working with other teams to facilitate the integration of data between different EBI resources.,A BSc (or equivalent) in a life-science subject (e.g. biological or biomedical sciences).,3+ years of postgraduate experience in scientific data integration, database development or text- & data-mining, with a demonstrable track record of achievement.,Proficient in at least one programming/scripting language (Python knowledge is highly desirable).,Good knowledge of relational databases, data modelling, SQL and PL/SQL, and RESTful web-services.,Good understanding of a range of bioinformatics tools and resources (e.g., BLAST, Pfam, PDB, UniProt).,Experience in integrating diverse data sets.,Knowledge of good practice in software engineering and good code documentation.,Good knowledge of UNIX systems.,Team player, ability to work both as part of a team and independently.,Self-motivated with a driver for quality.,Good communication (verbal and presentational).,Higher degree (e.g., MSc/PhD) or equivalent in life-sciences, computer science, or related discipline.,Formal training in programming, data/entity-relationship modeling, text- & data-mining.,Familiarity with Python, Java and Perl.,Knowledge of drug discovery and development.,Experience working with chemogenomic and pharmaceutical data.,Knowledge of cheminformatics methods (e.g., chemical structure representations, substructure & similarity searching).,Apply now", "An environment where it matters to make the right design decisions the first time. \"Move fast and break things\" doesn't really work for the type of system that we build. We take less technical debt than other companies.,At Brex, engineers make the product decisions with input from business people, instead of business / product people making decisions with input from engineers,We'd rather have one strong, well-compensated engineer instead of having 5 mediocre engineers. Our customers are fine with fewer features, but are not ok with broken features.,Small, accountable and autonomous teams of amazing people, eager to learn, teach and constantly improve our way of working.,Exceptional technical background.,Strong sense of ownership and accountability for what you're building. What you build today will be the foundation for dozens of other systems in the future.,Frankness on discussing technical matters. If you disagree with how things are being done, we encourage you to speak up. You can attack an idea without attacking the person behind it.", "Responsibility for the handling, processing and integration of data into the ChEMBL database.,Facilitating the deposition of datasets directly into ChEMBL by working closely with external collaborators.,Applying text- & data-mining techniques for the development of effective large-scale curation strategies.,Developing methods for the application and maintenance of ontologies in ChEMBL.,Working with other teams to facilitate the integration of data between different EBI resources.,A BSc (or equivalent) in a life-science subject (e.g. biological or biomedical sciences).,3+ years of postgraduate experience in scientific data integration, database development or text- & data-mining, with a demonstrable track record of achievement.,Proficient in at least one programming/scripting language (Python knowledge is highly desirable).,Good knowledge of relational databases, data modelling, SQL and PL/SQL, and RESTful web-services.,Good understanding of a range of bioinformatics tools and resources (e.g., BLAST, Pfam, PDB, UniProt).,Experience in integrating diverse data sets.,Knowledge of good practice in software engineering and good code documentation.,Good knowledge of UNIX systems.,Team player, ability to work both as part of a team and independently.,Self-motivated with a driver for quality.,Good communication (verbal and presentational).,Higher degree (e.g., MSc/PhD) or equivalent in life-sciences, computer science, or related discipline.,Formal training in programming, data/entity-relationship modeling, text- & data-mining.,Familiarity with Python, Java and Perl.,Knowledge of drug discovery and development.,Experience working with chemogenomic and pharmaceutical data.,Knowledge of cheminformatics methods (e.g., chemical structure representations, substructure & similarity searching).,Apply now", "Work with data, analytics pros, and product managers to strive for greater functionality in our data systems.,Analyze and translate functional specifications and change requests into technical designs.,Design, develop, and implement streaming and near-real time data pipelines that feed systems that are the operational backbone of our business.,Execute unit tests and validating expected results; iterating until test conditions have passed,Ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution.,Identify and remediate issues impacting data pipelines.,Take care of the tools, techniques and components being used in the industry through research and apply this knowledge to the system(s) being developed.,Mentor and develop more junior engineers,Bachelor\u2019s degree,You have hands-on experience with the full range of designing, developing, testing, and implementing low-latency big data pipelines,You have 5 years\u2019 experience using Object Oriented Languages, such as Scala, Python, Java.,You have 3 years\u2019 experience and working knowledge of Lambda and Kappa architecture in a cloud environment (AWS, Google, Azure),You have a deep understanding of SDLC, Agile methodologies, metadata, data modeling, and designing databases,Working knowledge on Linux/Unix Operating systems,Strong scripting skills - Python (a huge plus), Bash , Shell etc.,You're super comfortable with Gitflow", "Working with academic faculty to monitor and optimize current forms of data collection, and develop and integrate new forms of data collection,Working with academic faculty to optimize the transformation of collected data into formats appropriate for analysis,Working with academic faculty and IT developers to apply state-of-the-art approaches in cloud service, containerization and other techniques for scaling the remote staging, storage, computation and analysis of data sets, including at scale by hundreds or thousands of students in online courses,Providing expertise in data set hygiene to the institution, facilitating the cleanup and integration of potentially multiple incomplete, irregular, and partial data sources into interesting and useful data sources for analysis,Working with various offices to ensure that information is used complies with the regulatory and security policies in place,Teaching a Database Systems core course for the MIDS program and possibly other programs,Helping develop advanced courses in Data Engineering,Developing and running workshops for the campus community in different aspects of data management and wrangling,Advising student capstone projects in the MIDS and other programs on data engineering needs", "Own the entire end-to-end execution from requirements gathering from key stakeholders to building scalable, efficient, and reliable data marts which provide our business partners with clarity into the complexities of our platform,Create maintainable, scalable data processing pipelines in PostgresSQL, Python, and other data processing language in the data platform running in AWS,Define, develop, and operate functional data marts/cubes with common open source and SaaS based data processing and management tools like embulk, airflow, rundeck, Spark, Informatica, etc.,Partner with cross-functional engineering peers as you take ownership of the entire analytics lifecycle including instrumentation, logging, data modeling, data warehousing, data delivery and dashboarding,Provide guidance on development and implementation of data quality rules and validation processes, leading initiatives to improve data quality and issue resolution,Partner with Sales and Enterprise teams in defining and executing their analytics and reporting roadmap,Function as the subject area expert in Sales and Enterprise data domain maintaining business specific metadata and data integration contracts with 3rd party vendors as well as internal data owners,8+ years combined experience in database application development, data management and governance in medium and large size companies,Experience with marketplace businesses, especially as it pertains to working with large datasets inherent to two-sided marketplaces,A proven record of taking large data projects from ideation to implementation,Expert in writing advanced SQL and performance tuning others\u2019 SQL,Expert in designing, implementing, and operating efficient, scalable, and reliable data transformation pipelines,Strong analytical skills; ability to collect, organize, and analyze information,Solid experience in database modeling, architecture, design, and implementation,Familiar with sales process, sales commission attribution methodologies, and Salesforce.com application and data architecture,Fluent in data visualization techniques with tools such as Looker, Domo, or similar,Able to influence, lead, and communicate effectively across engineering teams, business units and other partners to negotiate priority, scope, and design solution,Excellent oral, written, and presentation skills, including the ability to work in person, virtually, and in a globally-staffed environment,Fluent and effective in working with a global distributed team", "Writing scheduled Spark pipelines that perform sophisticated query plans on the entirety of our datasets,Writing real-time pipelines that execute complex operations on incoming data,Synchronizing large amounts of data between unstructured and structured formats on various data sources,Creating testing and alerting for data pipelines,Building out our data infrastructure and managing dependencies between data pipelines", "Bachelors degree in IT, or similar experience,Data integration and data orchestration experience with Microsoft Azure,In-depth experience of designing and implementing data flows and pipelines,Experience as a Data Engineer is highly desirable,Experience of working in an Agile (ideally SCRUM) team,Comfortable being hands on with data, data modelling, query techniques,Background in the Data management Space,Experience of the FMCG/CPG industry,A good understanding of and adherence to data security standards", "Experience building applications and RESTful APIs in production systems (Java or Python),ETL Pipeline and tooling experience,Experience with Big Data/HPC Concepts and Technologies such as Spark, MapReduce, HIVE, Hadoop, or Kafka,Familiarity with BI Tools such as Looker, Spotfire, or Google Analytics,Data Storage experience with MySQL, Redshift, Elasticsearch,Predictive Analytics: Machine Learning, Modeling, or Data Mining,Platform experience using Unix and AWS (ECS, EMR, S3. Route53),Containerization: Docker, Kubernetes,Data specific tooling and libraries: Jupyter Notebook, Zeppelin, numpy, pandas,Data oriented languages such as R and Julia,CI/CD: Jenkins, CircleCI, Travis", "Develop and extend in-house data toolkits based in Python and Java.,Consult and educate internal users on Hadoop technologies and assist them in finding and effectively utilizing the best solutions for their problem space.,Improve the performance of financial analytics platforms built around the Hadoop ecosystem.,IMC is on the cutting edge of financial applications of Hadoop, processing terabytes of data daily for mission critical trading systems.,We operate at the bleeding edge of technology. If something new can potentially bring an advantage we will adopt and incorporate the new technology.,The landscape is always changing creating new and exciting challenges. What we focus on today is very different than what we focused on two years ago.,We really believe in sharing knowledge and technology between the different offices. Much of our technology stack is shared globally between our offices, and we provide opportunities to travel between the regions both for personal growth and to assist where it has the biggest impact.,Working at IMC is a great way to gain exposure to and learn about financial markets and technology. We know from experience that a lot of people really enjoy learning about a field beyond their immediate area of expertise, it\u2019s one of the things that makes this job more interesting than others.,We employ a broad range of people with varying backgrounds. What they have in common is their superior technical expertise, their extraordinary smarts and their collaborative approach.,3+ years of experience working with Hadoop 2 (YARN), cluster management experience preferable,3+ year of experience with Hadoop SQL interfaces including Hive and Impala,2+ years of experience developing solutions using Spark,Experience with common data-science toolkits, Python-based preferred,Strong Java, SQL, and Python development skills,Strong statistical analysis skills,Strong systems background, preferably including Linux administration,Unix scripting experience (bash, tcsh, zsh, python, etc),Experience with DevOps tools such as SALT and Puppet as part of a CI/CD development and deployment process.,Demonstrated ability to troubleshoot and conduct root-cause analysis,Developing with Apache Kafka,Containerization and Docker,OSS scheduling tools, preferably Luigi,Developing solutions in the Machine learning space, with an emphasis on Change/Anomaly detection", "Gathering, documenting, examining and managing data integration and data management requirements in an Agile/Scrum development team,Build extensible data acquisition and integration solutions using various integration tools (Informatica, Pentaho, Ab>Initio, IBM DataStage, Kafka, Flume, etc.) and a variety of data environments (Hadoop, Oracle, Mongo, etc.),Optimize data integration platforms to provide optimal performance under increasing data volumes and complexity,Creating test plans and scripts for data integration/data model testing, ranging from unit to integration testing.,Expertise in different patterns and technologies around enterprise-level data integration, data management and data warehousing,Expertise in the design, development and optimisation/tuning of database technologies,Expertise in Hadoop and related NoSQL technologies,Experience with Agile principles and methodology,An applied knowledge of several data technologies, practices and analytical approaches, including: Data Integration (ETL/ELT, SOA/Middleware, Enterprise Service Bus), Relational Databases (OLTP, Analytical MPP Appliances), Data Preparation and Data Warehousing (SQL, ETL, Warehouse architecture, OLAP), Data Architecture (Logical & Physical Modelling, Policy and Rules Management), Programming (SQL, Unix Shell, Python),A grounding in Database / Data Warehouse / Data Mart design and development and be able to demonstrate structured approaches to problem solving in an Agile/Scrum development environment,Hands-on experience in two of the following disciplines and be an expert in one: Data Integration / Extract, Transform and Load, Data Management (incl. Data Quality Management), Data Warehousing, Data Virtualisation / Federation or Data Modelling,A creative data Technologist, passionate about data/information management and architectures,Innovative, creative, articulate and able to work and collaborate with a broad range of Stakeholders in a multi-site, multi-country global organisation,Able to demonstrate examples of previous successful deliveries and have a track record in an Agile-centric development environment, built on a foundation of hands-on development of Enterprise Data Services,An exceptional communicator (written and verbal) with internal teams and outside Consultants/Contractors/Suppliers,Goal oriented with structured thinking and effective organisational skills,Educated to Bachelor\u2019s degree level, or have the equivalent experience or qualifications,A creative data technologist passionate about data/information management and architectures,Innovative, creative, articulate and able to work and collaborate with a broad range of stakeholders in a multi-site, multi-country global organization.", "Assembling large, complex data sets that meet business requirements,Identifying, designing, and implementing internal process improvements: including process automation, optimizing data delivery, etc.,Designing optimal ETL infrastructures from variety of data sources,Building analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.,Big Data, including the Hadoop Ecosystem, NoSQL approaches and Cloud Data Management.,Data warehousing and BI (incl. SQL / Data Modelling, Data Integration, Data Governance),Thorough understanding of the capabilities of commercial Apache Hadoop distributions such as e.g. Hortonworks, Cloudera, or MapR,Experience in estimating, planning and managing data integration aspects of implementation projects.,Experience with Watson Discovery/Conversation/Knowledge Studio,SSIS experience,Custom application development,Data architecture experience of designing and developing data models,API experience, multithreading is a plus,May be involved in conceptual technology phase", "Build data pipelines and deploy machine-learning algorithms.,Minimum 3 years\u2019 experience in Data Engineering,Modern programming experience with Python, Java, Scala or similar,Solid ETL experience (Design, implementation and maintenance),Cloud ecosystem experience in Azure, AWS, GCP, Oracle or similar,DevOps experience in Kubernetes, Docker or similar desirable,Application of machine-learning methods Desirable,Degree in Computer Science, Physics, Mathematics or similar,Stakeholder engagement and project experience", "Apply technologies to solve big data problems and to develop innovative big data solutions.,Select, optimize and integrate data science tools and frameworks required to provide data science solutions for science teams.,Implement complex big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms.,Collaborate effectively with other technology teams and architects to solve complex problems spanning their respective areas.,Develop, refine and scale data management and analytics procedures, systems, workflows, and best practices.,Develop analysis techniques for unstructured data.,Develop prototypes and proof of concepts for the selected solutions.,Enable big data and batch/real-time analytical solutions that leverage emerging technologies.,Code, test, and document new or modified data systems to create robust and scalable applications.,Lead efforts in creating, refining, managing and enforcing data management policies, procedures, conventions and standards.,Diagnose problems using formal problem-solving tools and techniques from multiple angles and probe underlying issues to generate multiple potential solutions. Proactively anticipate and prevent problems.,Collaborate with other members of formal and informal groups in the pursuit of common missions, vision, values and mutual goals.,Provide general and in-depth support/guidance for Blue Waters science teams in multiple areas of specialization.,Engage the data science community to improve the capability and performance of data science software on HPC systems.,Keep abreast of developments in the high-performance computing field, writing technical reports, conference and journal papers as appropriate, review scientific papers and proposals as appropriate.,Participate in writing joint proposals with Blue Waters staff and/or application teams.,BA/BS degree in engineering, mathematics, science, computer science, or related field. Alternative degree fields will be considered if accompanied by equivalent experience (depending on nature and depth of experience as it relates to current NCSA projects and technologies).,At least 1 year of experience working with real world data science applications.,Strong verbal and written communication skills.,Master's or Ph.D. in engineering, mathematics, science, computer science or related field highly preferred.,Expertise with exploring, understanding, cleaning, and wrangling big data.,Strong software engineering skills and a track record of contributing to software projects and collaborating with other developers.,Parallel programming experience on high-performance computers including development, porting, and evaluating the scalability of one or more parallel libraries or applications written in Fortran, C, and/or C++, and utilizing communication protocols such as MPI and OpenMP,Working knowledge of the Linux operating system, a programming or data analysis language (e.g., Python, R, Stata), and databases (e.g., MySQL, Oracle, NoSQL).,Ability to work both independently and as a team member.,Ability to manage multiple projects with competing priorities and deadlines, and an eagerness to take ownership of challenging and open-ended assignments.,Effective at communicating with audiences whose technical backgrounds vary widely.,Expertise with ETL processes, visualization tools, and web programming.,Experience developing and presenting technical training material and web-based technical documentation.", "Hadoop - Proficiency in designing, running an troubleshooting Hadoop clusters (crucial),Strong understanding of Linux OS core principles, performance and tuning (crucial),Batch and streaming job frameworks - eg Spark, Storm,NoSQL databases - Hbase, Cassandra, MongoBD,Knowledge of Middlewares and messaging systems (eg Kafka, RabbitMQ, FTL),Automation via the use of configuration management and orchestration tools,Data collection and Querying (eg Flume, Sqoop, Hive),SSL certificates,Scalable distributed systems eg Splunk,SQL database administration and querying experience", "You\u2019ll evaluate, benchmark, and improve the scalability, robustness, and performance of our data platform and applications, making significant contributions to the architecture and design of our data processing platform,You\u2019ll implement scalable, fault tolerant, and accurate ETLs,You\u2019ll gather and process raw data at scale from diverse sources,You\u2019ll collaborate with product management, data scientists, analysts, and other engineers on technical vision, design, and planning,You\u2019ll implement and maintain a high level of data quality monitoring in our analytics ecosystem,You\u2019ll train and collaborate with teammates effectively in data engineering best practices,You\u2019ll be involved and supportive of agile sprint model of development, helping to implement the practice and the discipline,5+ years of experience as a software engineer and 3+ years of experience as a data engineer,Excellent communication and collaboration skills,Experience implementing data pipelines and improving the performance of ETL processes and SQL queries,Enthusiasm for working in an agile development environment,Strong database schema design and query optimization skills,Proficiency with relational databases and SQL queries (PostgreSQL preferred),Strong scripting skills in Python or Ruby,Experience in data modeling for OLTP and OLAP applications,Understanding of basic principles of data governance,Familiarity with workflow management tools (Airflow preferred),Familiarity with cloud-based data warehouses (Amazon Redshift preferred),Shown ability to understand automated testing concepts and ability to consistently apply those concepts,Experience with streaming technologies and concepts used with data warehouses,Experience in taking machine learning models from development to production,Experience working with visualization tools like Tableau,Experience working with sensitive data, i.e. PHI / PII,Application development experience,Competitive salary,Stock options + extended post termination option exercise window (for Omadans who are with us 3 years or more),Flexible vacation,Parental leave,Health, dental, and vision,Healthy snacks and meals,Wellness events (e.g. running club),Community volunteering", "Identify and evangelize programming best practices with the Data Engineering and Modeling teams.,Work closely with Analysis to ensure quality & availability of data in the Data Warehouse along with support of our Business Intelligence platform.,Lead and participate in design / architecture reviews, as well as code reviews and walkthroughs.,Design/Implement robust end-2-end ETL pipelines.,Develop solutions with a mind towards quality, scalability and high performance on large data sets.,8+ years experience in a senior developer or data engineer role,Hands-on coding experience in R, Python, Scala or similar,Solid foundation with Data Science workflow, experience with Pandas and Scikit-Learn preferred,Experience with distributed processing frameworks like Amazon EMR and Spark,Experience working with high volume heterogeneous data,Amazon Web Services, e.g. RedShift, EMR, VPC, RDS, S3, and Route53,Docker for provisioning servers and deploying applications/services,Excellent verbal and written communication skills including the ability to explain technical issues to a non-technical audience,B.S. in Computer Science or equivalent experience,People \u2013 the best part of Zest,Robust healthcare plans, matching 401K and unlimited vacation time,Dog friendly office with lounge areas, video games and gigantic jigsaw puzzles,On-site gym with yoga, salsa and other employee run fitness classes,Generous family leave policy (6 month maternity leave/3 month paternity leave),Tuition reimbursement, conference allowance and Zest talks,Complimentary massages, manicures, pedicures and more", "Extract, centralize and collect data from different databases.,Analyze and provide answers as needed by the different operational teams,Set up periodic data flow (data pipeline),maintenance and development of a BI platform,the contribution and deployment of machine learning models,POC of backends for analysis and data processing.,Higher education from Bac +3, with a specialization in data processing, data science or good experience of database systems.,You are familiar with web applications and the e-commerce environment.,You have a good web culture and an analytical mind,You are looking for challenges and want to join an international startup.,You are organized, autonomous and you are force of proposal.,You have good experience with SQL and NoSQL databases (MongoDB, PostgreSQL).,You have development experience in Python,A knowledge of one of the following databases would be a plus: Elasticsearch, Redshift, Kubu, Druid, Hive, etc.,Knowledge of the AWS environment,The opportunity to revolutionize mobility with us,A superb integration with lots of surprises,Career evolution,Atypical premises in the heart of Paris (5min from St Lazare),Latest work tools,Maximum collaboration between the different teams,No room for coffee is old fashioned!,A restaurant card allowing you to have good food delivered,A GymLib gym pass to burn all those good food ..,Foosball duels for lunch,Regular afterworks,Internationally-oriented teams and projects to perfect your languages,Employment type: Internship agreement - Minimum duration of 6 months,Job location: Paris 75 (9th),Immediate availability", "Bachelor\u2019s Degree in Business Administration, Computer Science or related field; AND,Two (2) years progressively more responsible directly related subject matter expertise, quantitative, business intelligence and data management experience.,MSc in Business, Economics, Mathematics, Statistics, or Computer Science,Bachelor\u2019s in Business, Economics, Mathematics, Statistics, or Computer Science (OR the equivalent combination of directly related education and experience in quantitative, business intelligence, and data management analysis), AND,Three (3) years directly related subject matter expertise, quantitative, business intelligence and data management skills.,Data modeling and database design for analytics and business intelligence solutions.,Strong analytical skills and experience with Extract, Transform, and Load (ETL) tools.,Experience in Software Development Cycle (Agile) and has a strong quality ethic.,Work effectively in a collaborative team environment along with the commitment to the overall success of a group.", "You have a hands-on mentality to solve complex issues,Learning new tools and software development skills is your second nature,You have at least 2 years\u2019 experience in software development,You think about data as a massive asset to leverage,You are keen on automation using Continuous Integration/Delivery tooling,You have an Agile mindset and SCRUM way of working,You feel personally committed to the goal setting and delivery of the team (local and international),Able to create data pipelines, using at least one of the following; ADF, Kafka, SSIS, Databricks,Bring data science solutions to life using any of Spark, Python and SQL,Familiar with CI, CD, testing and version control (e.g. VSTS/TFC, ARM, GIT or similar),Basic knowledge in Linux,Experience working with configuration management tools,Proficient in spoken and written English, Dutch is beneficial,A central office location with easy access, by either public transport or car,Healthy work-life balance (smart working),The time for you to develop your profession as Data Engineer,Opportunity to work for a clean energy company that aims for creating fossil free energy within one generation", "Help set up a data lake,Setup Integration points from various back end systems into the data lake,Help with the overall Big data strategy, architectural approach and Hadoop eco system tool selection,Big Data Engineer - Hadoop, SalesForce.com, SQL Server required,SQL Server (2008 & 2012),Web/Call Analytics,Hadoop/HDFS,Oozie,Sqoop,Kafka,Flume,SOAP/ RESTful web services,Java,PIG,Spark,Scala,HBASE,Cloudera,Good to have travel industry experience,Good working knowledge of Spark, Hive and Impala would be very beneficial,A good knowledge of the languages Scala and Java", "Use of SAS Data Integration (DI) Studio to populate data warehouse/lakes,Extracting data from data sources (including databases, XML, flat files, Excel etc.),Writing and optimising SQL queries, functions, procedures (Greenplum, Postgres),Documenting ETL designs and implementations, and operations guide,Data Analysis and Modelling (including Entity Relationship Diagrams, Star Schemas, Relational Data Analysis),Experience with POLE models and data matching,ETL Scheduling (concepts/practice and any use of ActiveEon),Experience with Business As Usual ETL operations,Database partitioning design and strategy,Experience interfacing with data sources/systems via APIs e.g. REST,Data Quality monitoring and reporting,SAS Macro programming,SAS Enterprise Guide,Scripting (DOS, Linux, PowerShell, VBScript),Windows and/or Linux networking fundamentals,XML (XML validation, XSDs),Programmatic text file manipulation (Grep, Sed, Awk, Tr, Findstr)", "Predict item availability in stores for online orders,Entire lifecycle of product development from research support to production deployment, monitoring, evaluation, and documentation.,Apply machine learning and optimization algorithms to maximize the efficiency of our business and minimize risks.,Build, validate, test and deploy predictive models using machine learning techniques to explain or predict behavior and solve a variety of business and engineering problems.", "Design, build and maintain data loading routines using a variety of tools including open source ETL tools, stored procedures, scripting and programming languages such as Java, C, Python, etc.,Improve the performance and scale of our data operations to deliver information on a real-time basis.,Design, build and maintain data QA routines to ensure that our systems deliver quality information to our customers.,Contribute to the design and architecture of our Enterprise Data Infrastructure.,Work closely with end-users and technical team members on data ingestion projects, investigating and solving data and reporting problems.,5+ years development experience using MySQL, Oracle or other relational database software experience. Experience writing and optimizing complex SQL queries.,5+ years developing back end data processing routines for data warehouses or any backend data system using open source tools, and/or high level languages.,5+ years experience developing database related software using Java, C, C++, etc.,Excellent verbal and written communication skills.,B.S. Computer Science, Math, Physics or equivalent experience.,Knowledge of BI Analytical tools a plus.,Los Angeles Business Journal \u2013 Best Places to Work, August 2018, 2017, 2016, 2014 & 2013,The 2016 & 2017 Career Launching Companies, Wealthfront,Inc 500 \u2013 Fastest Growing Private Companies,Deloitte\u2019s Technology Fast 500,Los Angeles Business Journal \u2013 Fastest Growing Companies,Los Angeles Business Journal\u2013 Best Places to Work,President Josef Gorowitz -- Ernst & Young 2014 Entrepreneur of the Year: Los Angeles, Advertising Category,President Josef Gorowitz -- SoCal Tech Top 50 Executives Award 2013,CEO Chuck Davis \u2013 Los Angeles Venture Association (LAVA) Hall of Fame Honoree,CFO Brad Kates -- Los Angeles Business Journal: 2014 CFO of the Year, Winner", "Design, Develop, and unit test new or existing Data Integration solutions to meet business requirements.,Participate in troubleshooting and resolving data integration issues such as data quality.,Conduct exploratory data analysis & model prototyping using Spark/Python/SQL,Perform data sourcing, integrating, mining and modeling to produce insights, predictions, and forecasts, in collaboration with domain experts across a variety of disciplines.,Responsible for selecting and using DevOps tools for continuous integration, builds, and monitoring of solutions.,Deliver increased productivity and effectiveness through rapid delivery of high-quality applications.,Provide work estimates and communicate status of assignments.,Assist in QA efforts on tasks by providing input for test cases and supporting test case execution.,Analyze transaction errors, troubleshoot issues in the software, develop bug-fixes, involved in performance tuning efforts.,Experience in developing Informatica/Mulesoft Mappings and complex Oracle PL/SQL programs for the Data Warehouse.,Makes some independent decisions and recommendations which affect the section, department and/or division.,Manage complex API portfolio.,Participates and provides input to area budget. Works within financial objectives/budget set by management.,Develops alternative solutions for decision-making which support organizational goals/objectives and budget constraints.,Identifies project growth and integration challenges and helps with budgeting and strategic planning.,Makes strategic contributions about designing enterprise integration solutions with Anypoint Platform.,Operates with substantial latitude for independent action in setting objectives and deciding how to proceed.,This is the highest level subject matter/technical expert and may include limited supervisory responsibilities.,Works with minimum supervision, conferring with superior on unusual matters. Incumbents have considerable freedom to decide on work priorities and procedures to be followed.,Provide reporting and analytics functionality to monitor API usage and load (overall hits, completed transactions, number of data objects returned, amount of compute time and other internal resources consumed, volume of data transferred).,Provide leadership for API-based monetization functionality to support charging for access to commercial APIs and/or the services the API enable.", "Develop a highly scalable, reliable, and real-time data processing pipeline.,Partner with the Data Science team to provide data infrastructure for a variety of projects.,Enable data science/analytics team to write new transformations in infrastructure managed by data engineering.,Design, build and launch new ETL processes into production.,Work with data infrastructure to triage issues and drive to resolution.,Work with our entire team to help provide a consistent, fast, and delightful experience to our customers and designers.,BS/MS in Engineering, Computer Science, Math, Physics, or equivalent work experience.,5 or more years in a Data Engineering role.,5+ years experience with Python or Java development.,Experience with modern data platforms (Spark, Hadoop/Map Reduce, Hive, Airflow, Kafka/Kinesis),Experience with Docker and Kubernetes,Experience with machine learning frameworks Tensorflow and Keras,Experience analyzing data to identify deliverables, gaps and inconsistencies.,Experience working cross-functionally to communicate data plans that address business challenges.,Ability to develop and scale ETL pipelines.,Expertise with SQL, databases management, and best practices.,A true team player. You enjoy collaborating, learning from or teaching others so we can all become better developers.,A great communicator - You have excellent communication skills and enjoy working in cross-functional group settings.,Startup Savvy - You want to work hard and build something great, and have the ability to learn and adapt in a rapidly changing environment.,A challenging opportunity, and a great team to work beside,A data driven environment where your insights and expertise will be valued,An environment where you can move fast, work hard, see results, and feel your impact,An opportunity to get in on the ground floor of a successful startup", "Collaborate with product owners and stakeholders to plan and define requirements. Translate business requirements into business value and assist in supporting leadership and project managers to establish priorities and meeting timelines.,Represent the Data Warehouse team at meetings and events, which may include preparing and delivering presentations in front of a large group.,Supports the Data Governance Committee by attending meetings, acting as a subject matter expert, working with workgroups to solve problems, providing input into BCPS policies and rules, and representing the Data Warehouse team.,Responsible for implementing and following data warehouse best practices and development processes, including conceptual/logical/physical dimensional data modeling, data flow diagrams, design scalability for the future, ETL architecture, source to target mappings, automation through ETL and stored procedures, data validation, quality assurance, and monitoring/alerting.,Lead data model and ETL design review sessions with team members and provide feedback. Approve the release of changes to the production data warehouse. Ensure all data models, dictionaries, and metadata are maintained.,Meets regularly with leadership and the project manager to provide input and feedback on project level of effort, feasibility, timelines, issues, and project status. Advises leadership on issues, including resolution. Participate in and contribute to daily stand-up and project planning meetings.,Collaborate daily with developers to ensure data availability and structure meet the needs of the report or data visualization.,Create SQL scripts to extract data from the data warehouse to support data analysis or fulfill an ad hoc data request.,Ensure the stability of the BCPS Enterprise Data Warehouse through monitoring, performance tuning, disaster recovery planning, new installations, patching, upgrading, storage planning, issue resolution, and collaboration with vendors and the Department of Information Technology. Escalate issues and risks as needed.,Oversee the data security architecture to ensure student and employee data privacy guidelines are met. Work closely with Network Administrators to define and maintain Windows AD groups used to establish access to business intelligence systems.,Evaluate and recommend new and emerging technologies that will enhance the ability of the team to produce quality output to include evaluating vendor products, reviewing publications, interviewing vendors, researching reviews and competitors, negotiating pricing, and providing a summary to leadership. May participate in a request for proposal (RFP) process.,Performs other duties as assigned.,Graduation from an accredited college with a bachelor's degree in Computer Science, Information Systems, Engineering, or related field.,Five years' hands-on experience designing, architecting, and implementing enterprise scale data warehouses, data marts, ETL architecture and code, and SQL code/stored procedures.,Experience in K-12 education preferred.,Thorough knowledge of the infrastructure and architecture required to build and maintain an enterprise data warehouse that integrates multiple disparate data sources on a Microsoft SQL Server platform.,Familiarity with data visualization tools like Tableau, Cognos, and Microsoft BI Suite and how the data warehouse supports their solutions.,Understanding of information systems and data life-cycle management best practices and methodologies, and systems development life cycle (SDLC).,Proficient in ETL using Microsoft SSIS and SQL programming including stored procedures/views/functions. VB/C# scripting for enhanced automation is a plus.,Hands-on experience in Kimball dimensional data modeling and master data management. Strong experience with one or more recognized data modeling tools like ER Studio or ERwin.,Ability to work on multiple projects simultaneously and adhere to deadlines while working in a fast past environment using agile project management methodologies.,Ability to work in a team environment and mentor other team members.,Excellent communication skills?.,Applicants are required to have a completed application on file for employment with the Baltimore County Public Schools, and a separate completed application must be submitted for each position and location in which you are interested.,Professional references must be submitted to complete your application. Examples of professional references include current and former principals, supervisors, managers, mentor teachers and university/college supervisors. Personal references from colleagues, friends, community members, etc. will not be accepted.,Be sure to account for all periods of employment and unemployment, including school psychology practicum and school psychology internship experience, and include names, addresses, and telephone numbers of employers.,Be sure to answer all criminal background questions. If you answer \"yes\" to any of the criminal background questions you must provide a written explanation. A criminal offense does not necessarily exclude an applicant from employment with BCPS. Factors such as passage of time since the offense, the nature of the violation, and the extent of rehabilitation will be taken into consideration.", "Shape a solid data infrastructure that collects, stores, processes and serves massive amounts of information,Work with spatial information and build leverageable solutions that our Data Viz team will use to create powerful visualizations of a city\u2019s deliveries,Dive deep into the core of large scale data processing systems and produce high-quality code that can help answer questions from the business, operations and clients side", "Bachelors degree in IT, or similar experience,Extensive experience in??data integration and a year data orchestration experience with Microsoft Azure,In-depth experience of designing and implementing data flows and pipelines,3+??years\u2019 experience as a Data Engineer is highly desirable,Comfortable being hands on with data, data modelling, query techniques,Background in the Data management Space", "Large scale treatment of operative data to develop analytics solutions and high quality systems for the product\u2019s service cloud,Develop and evaluate data to make logical recommendations of data usage, treatment and storage,Implement alternatives to the existing system to improve the capacity, performance and robustness of the solution, detect problems with the application and define new methods for processing data,Analyse internal libraries and tools to enable our teams to work at maximum capacity,Experience in Software Engineering and Big Data solutions,Experience in AWS, services in cloud and other technologies used for Big Data: S3, Storm, Alluxio, NoSql, Drill, Redshift, Zookeeper, Docker, Kubernetes, Tableau, etc,Confident use of usual development environments and languages in these kind of solutions: Node.js, Clojure, SQL,Ability to analyse, design and be pro-active,Availability to travel,Good level of both spoken and written English,Experience in TV or media solutions,Experience in deployment of solutions, network dimensioning, AWS CloudFormation, etc,Fixed salary depending on the experience of the successful candidate,Variable incentives,Ticket Restaurant\u24c7,Health insurance,Short working hours every Friday and throughout July and August,English classes,Flexibility with taking holidays,Fantastic work environment,Regular outdoor and team-building events,In-office perks such as Fruit Friday", "Leverage modern analytics stacks and cloud platforms to design, implement and maintain scalable infrastructure for ingesting, processing and persisting large volumes of batched and streaming data.,Contribute to multiple production code bases in a continuous delivery (CI/CD) environment.,Write, debug, maintain and constructively review code on a highly collaborative software engineering team.,Develop production quality software for interacting with distributed data pipelines, data stores and data models.", "Help set up a data lake,Setup Integration points from various back end systems into the data lake,Help with the overall Big data strategy, architectural approach and Hadoop eco system tool selection,Big Data Engineer - Hadoop, SalesForce.com, SQL Server required,SQL Server (2008 & 2012),Web/Call Analytics,Hadoop/HDFS,Oozie,Sqoop,Kafka,Flume,SOAP/ RESTful web services,Java,PIG,Spark,Scala,HBASE,Cloudera,Good to have travel industry experience,Good working knowledge of Spark, Hive and Impala would be very beneficial,A good knowledge of the languages Scala and Java", "5+ years of experience with data engineering and/or machine learning engineering,Strong proficiency in SQL and Python,Proven ability to devise innovative solutions.,Experience gathering complex business requirements and identifying data needs.,Extensive experience with the design and development of relational databases and data warehouses.,Extensive ETL development experience with large-scale DBS or big data systems.,AWS Redshift experience strongly preferred.,Experience supporting Data Scientists, including code optimization and productionalization of ML models.", "Chance to work in a high priority business role with tangible impact.,Ability to lead and ultimately grow a data team to maximise data potential.,Innovative scaleup atmosphere with flat hierarchy and short lines of communication.,Chance to work with and implement new and innovative technologies.", "Internship", "Collaborate with Product Owners and Team Leads to identify, design, and implement new features to support the growing real time data needs of Intelligent Retail Lab,Assist and mentor Junior Engineers in troubleshooting and tuning of high volume, distributed applications.,Identify and suggest or implement remediation of cases where we diverge from industry best practices,Evangelize and practice an extremely high standard of code quality, system reliability, and performance to ensure SLAs are metfor uptime, data freshness, data correctness, and quality,Display sense of ownership over assigned work, requiring minimal direction and driving to completion in a sometimes fuzzy and uncharted environment,Focus on enabling developers and analysts through self-service and automated tooling, rather than manual requests and acting as a gatekeeper,Participate in on-call rotation, including continuously seeking to reduce noise, improve monitoring coverage, and improve quality-of-life for on-call engineers,3-5 years experience in running, using and troubleshooting industry standard data technologies such as Spark, HDFS, Cassandra, Kafka,Deep development experience, ideally in a typed language but we are open to other experience if you\u2019re willing to learn the languages we use,Experience processing large amounts of structured and unstructured data in streaming and batch.,Experience with integrating with Business Insights tooling, ideally Power BI from Microsoft,Experience with cloud infrastructure. We use Azure, specifically, but any will do,A focus on automation and providing leverage-based solutions to enable sustainable and scalable growth in an ever-changing ecosystem.,Experience building and maintaining a centralized platform or services, to be consumed by other teams, is ideal, but not necessary,A passion for Operational Excellence and SRE/DevOps mindset, including an eye for monitoring, alerting, self-healing, and automation", "Work with our analytics and data science teams to understand our data processing needs,Be a key hands-on contributor to the design and implementation of our data platform from the infrastructure layer up to the API,Model and architect our data in a way that will scale with the increasingly complex ways we\u2019re analyzing it,Build robust pipelines that make sure data is where it needs to be, when it needs to be there,Build frameworks and tools to help our software engineers, data analysts and scientists design and build their own data pipelines in a self-service manner,Performance testing and engineering to ensure that our systems always scale to meet our needs,Incorporate our data science models back into our customer-facing products,Key member of the team focused on pure hands-on contribution to the implementation and operation of our data platform,We value humility, a strong work ethic, flexibility, collaboration, technical curiosity, and constant learning,At least one project that demonstrate prowess in designing, implementing, and operating large scale, high throughput, low latency distributed systems,You can go up and down the stack from deep in the infrastructure layer all the way up to the client libraries,Experience with small teams that move fast -- all members are expected to be able to achieve maximum results with minimal direction,You have at least 7 years of hands on experience as an Engineer across multiple environments on complex distributed polyglot systems. While we primarily work in Python and Java, we welcome others with relevant experience in languages like Scala, Clojure, Go, or C++,Kubernetes and/or Docker experience,Message driven or streaming architectures, such as those with Kafka, Spark, Flink, RabbitMQ, etc.,Postgres, MySql, or other RDBMS experience,AWS, GCP and/or Azure experience,Redshift, Presto, or other MPP database experience,Airflow, Luigi, or other ETL scheduling tool experience,Open source contributions to a few major projects,Career game changer \u2013 A truly unique experience to work for a fast-growing startup in a role with unlimited potential for growth.,Excellent benefits \u2013 We cover 90% of Medical Premiums, 50% of Dental & Vision Premiums, and offer company sponsored Life Insurance.,Flexible PTO policy, generous parental leave, and great work/life balance \u2013 We value and support each individual team member.,Fun perks like snacks, catered lunches, happy hours, wellness programs, and SpotHero swag.,Annual parking stipend (duh \u2013 we help people park!).,The opportunity to collaborate with fun, innovative, and passionate people in a casual, yet highly productive atmosphere.", "Experience in generating custom ETL/ELT Pipelines and translation of business requirements into ETL Designs and mapping.,Kimball Methodologies are very important to this role.,Experience in Azure technologies including; Azure Data Factory, Azure SQL Database, Azure Synapse Analytics,Strong SQL Scripting Skills,SSIS, SSAS Skills", "Collaborate with Compliance & Directory engineering teams and product managers to define the vision and deliver the product,Continuously champion efficiency improvements in our team via automation, tools, and better technologies,Contribute to the architecture, implementation quality, and overall efficiency,Mentor other team members and contribute to growing high performance engineering team,3+ years of Software Systems development in Java, Scala, or other JVM-based language on Linux platform,Strong grasp of data modeling and using relational database technologies (e.g. oracle, DB2, MySQL),Experience designing robust and modular data interface layer and integrating it with multiple downstream systems and applications,Experience implementing scalable data processing (e.g. calculations, data migrations, reports) using caches, schedulers, and highly concurrent frameworks,NoSQL and Hadoop experience (i.e. Hbase, Cassandra, Parquet),Experience defining architecture and implementing backbone systems of enterprise configuration, user permissions, authentication, and auditing,Experience with continuous integration, load testing, canary testing, and test automation", "providing our AI team with access to large amounts of data for model training,creating self-service BI platform for Memsource team members and our customers,Bachelor or master level university degree in the field of computer science (alternatively, an adequate professional experience in this field),3 years experience in (big) data engineering domain,Experience with Hadoop and Hadoop-related technologies,Experience with modern data processing technologies, such as Apache Spark, Apache Kafka, Kafka Streams or others,Good understanding of NoSQL database systems, such as MongoDB, Elasticsearch or others,Proficiency in SQL,Proficiency in Python programming language,Experience with AWS, Terraform, Ansible,Experience with streaming data processing,You\u2019re interested in working at a successful and rapidly growing technology company with a global presence.,You prefer to work independently in a small international team, with no corporate bureaucracy.,We know you have a life outside of the office - enjoy flexible hours and a home office option (up to 2 days a week).,You deserve 5 weeks of vacation and 3 sick days.,We value your work and offer competitive salaries.,You are interested in attending professional development and learning opportunities such as conferences, workshops, meetups etc.,You can improve your language skills with free on-site English and Czech lessons.,You will work in a newly renovated office in Prague\u2019s city center near the N\u00e1rodn\u00ed t\u0159\u00edda metro station, with wrap-around terraces, a chill-out area, complimentary coffee, tea, and snacks, bicycle stands, and a shower.,We hold weekly company breakfasts in the office and monthly all-hands meetings where everyone is updated on the latest company news.", "Implement machine learning models to production,Design, build and operate Honey\u2019s data pipelines with a focus on performance and reliability,Participate in new feature development for recommendations, product catalogs, and mobile applications,Propose and evaluate storage technologies and methodologies with an eye toward scalability and performance,Design and implement data pipelines that handle a thousand messages per second streaming,5+ years programming in at least one modern programming environment. Python, Scala, or Node.js are helpful but not required,5+ years architecting with both SQL and no-SQL data stores. We use Big Table/HBase, Spark, Dataflow, Spanner, BigQuery, and Elasticsearch, but if you have experience using Hive, Hadoop, or Pig that works too!,Experience designing schemas and maintaining representations for low latency, request- cycle queries,Experience with streaming platforms (PubSub, Kafka, Kinesis) and near-real-time data pipelines,Working knowledge of statistics and experimental design", "Strong experience with Hadoop and it\u2019s ecosystem (e.g. Yarn, MapReduce, Spark, HBase, Kafka, etc.) in development and implementation.,Experience in loading from disparate data sets and pre-processing using Hive and Pig.,Experience in writing high-performance, reliable and maintainable code in languages like java, scala and python. Writing of MapReduce jobs.,Track record of working on perform analysis of vast data stores and uncover insights while maintaining the security and data privacy.,Deep knowledge in distributed processing principles and frameworks - Experience in modelling complex big data architectures.,Should have worked on data analytics projects involving feature extraction.,Should have good communication (verbal/written) and presentation skills.,Should have good problem understanding and analytical skills.,Preferred to have played a role in project coordination and knowledge of project management methodologies.", "Design, build, test and maintain AWS platforms architected to support batch and real-time data processing.,Operationalize machine learning solutions that are deployed as either serverless lambda applications, SageMaker managed services or EMR/Spark platforms.,Ensure deployed applications meet architecture guidelines defined by Company.,Design data ingestion processes for acquiring new datasets supporting key business initiatives.,Create continuous integration and code deployment processes regard for versioning and deploying machine learning solutions.,Identify opportunities to improve data reliability, efficiency, and quality.,Provide thought-leadership on AWS functionality to manage and determine the business value of new features.,Work together with the team's data scientist and product manager to ensure machine learning solutions are compatible with the AWS architecture.,Bachelor\u2019s degree in Computer Science or related fields,5+ years of experience engineering data ingestion solutions,3+ years of experience building data-related solutions in AWS,Experience with programming languages such as Python, NodeJS or R,Experience using AWS technologies such as Lambda, S3, Step Functions, Kinesis, Glue, Athena, EMR, Kafka, Hive or Spark,High level understanding of machine learning concepts applicable to recommendation logic and forecasting,Knowledge of AWS Machine Learning resources such as SageMaker is helpful,Experience with GitHub and automated code deployment solutions such as AWS Code Pipeline or Jenkins,Able to effectively communicate complex technical ideas to business stakeholders,Willing to collaborate with colleagues across Technology functions in order to establish architecture guidelines, provide thought-leadership and solve complex problems,Passionate about learning new technologies that will improve functionality, scalability and reliability of relevant solutions,High personal and professional standards; unassailable integrity and ethics,High-energy and self-starter personality,Strong structured coding skills,Competitive compensation and an extensive benefits package,A comprehensive list of medical, dental, and vision coverage plans for you to pick from,Company-paid short-term disability, long-term disability, and life insurance,401K with a 3-year vesting schedule and up to 4% matching of your elected contribution,11 observed, company-paid holidays,21 vacation days to start and an additional day of PTO at your anniversary date each year (up to 28 days total) \u2013 and we expect you to use all of it,Generous paid parental leave, including 4 paid weeks for primary caregivers and 2 paid weeks for secondary caregivers,A light-filled Midtown Atlanta office with both private and collaborative spaces to work, unlimited snacks and beverages, and easy access to MARTA,Company-paid volunteer days to support the community that supports us,Company and team outings because we play just as hard as we work,Employee referral bonuses to encourage the addition of stellar people to the team", "Utilize expertise in data modeling, ETL architecture, and report design for various department initiatives,Work with business partners and management teams to the ensure collection and analysis of appropriate data and metrics to facilitate improvements in processes,Design enterprise data components that support reporting and analytic needs,Generate technical specifications for data flows and ensure the accuracy of data loaded into the enterprise data warehouse,Manipulate process and extract value from various disconnected data sources,Maintain metadata associated with enterprise data tables,5+ years of experience with SQL, NoSQL, Salesforce CRM, Zuora, and software industry,5+ years of hands-on experience in ETL, data mining and using database scale and complex datasets,5+ years of experience in data design and implementation of data warehouse components,Strong knowledge of distributed computing, data warehouse, data mining, business analytics and software development,Strong analytical and quantitative skills with the ability to use data and metrics to back up assumptions, recommendations and drive actions,Proven ability to manage multiple, competing priorities simultaneously,Bachelor's Degree in Computer Science or related field / MBA or Master\u2019s is a bonus", "Create, Document, and Maintain Various ETL processes,Write code to interface with 3rd party platform services,Assist with all parts of our internal analytics system (ETL, Redshift, Airflow),Provide technical support and advice to the analytics and data visualization team,Other duties as assigned", "Implement and maintain IT data and ETL platforms; understand and deliver on the functional support and data needs of data management within Vistra.,Follow data policies, standards, guidelines and procedures in order to ensure data to support reporting and analysis is available, responsive, and achieving business outcomes and objectives,Implement data and ETL solutions or enhancements to improve overall Vistra data architecture; conduct research and make recommendations on new data management processes and innovations.,Work closely with vendors, service providers, the business and internal team to define and understand analytics needs in order to achieve key performance indicators and Service Level Agreements for the benefit of Vistra and its business objectives.,Translate business needs into functional requirements, update/create documentation (Business Process Designs, Functional Designs, Data Architecture),Participate in projects and Agile teams; make recommendations and implement changes to mitigate risks and optimize data platform performance", "Experience in generating custom ETL/ELT Pipelines and translation of business requirements into ETL Designs and mapping.,Kimball Methodologies are very important to this role.,Experience in Azure technologies including; Azure Data Factory, Azure SQL Database, Azure Synapse Analytics,Strong SQL Scripting Skills,SSIS, SSAS Skills", "Design and implement new ETL pipelines based on client business logic.,Optimize and scale existing ETL pipelines.,Develop tools to support ETL platform.,Perform ad-hoc queries to support client teams.,Python, Spark, AWS, Airflow, Bash, Docker, SQL, Scala, Hive.,Strong communication skills relating technical work to non-technical people.,Bachelor\u2019s degree or higher in a relevant technical field, or comparable work experience.,Competence in Python and SQL.,Working knowledge of Unix systems, such as Fedora, CentOS, Redhat, etc.,Understanding of data quality Best Practices.,Experience working with big data technologies such as Hadoop, Hive, Pig, Spark, Presto, etc.,Experience working in a cloud-based infrastructure, such as Amazon AWS or Cloudera.,Competence in at least one OOP language, such as Java, Scala, C++, etc.,Experience working with NoSQL database technologies such as MongoDB, Cassandra, etc.,Disrupt the norm: Cherish the opportunity to find better ways to do things, regardless of how disruptive or initially painful it might be.,Dissent then execute: Raise issues and fixes in the planning stage, but when it\u2019s time to execute, execute diligently and without reservation.,Test and learn: When developing new processes or features, link it to a hypothesis, and evaluate that hypothesis without bias.,Nothing is impossible: Imagine what\u2019s possible and then have the discipline to execute it. One without the other is not valuable.,Defect is treasure: Every mistake is an opportunity for the organization to learn and make improvements.", "Work with engineering, analytics, and product management to implement data-driven features,Use Scala, Java or Python to utilize Hadoop/Spark to collect and analyze large-scale datasets in batch,Help implement real-time processes using event-driven architecture,Build, monitor, and maintain data ETL pipelines,Administer and maintain our data infrastructure,Bachelor\u2019s degree in Computer Science, Engineering or a related field or equivalent work experience,1+ years of experience in software development, preferably with Java and/or Python,Experience with SQL programming, data modeling, query tuning, and optimization,Understanding of fundamental data structures, algorithms, and distributed systems,Working knowledge of ETL processes,Experience working in the Hadoop ecosystem, using tools such as Hive, Spark, or Pig,Data Warehouse, Data Integration/Automation, and Business Intelligence experience,Experience with managed, cloud-based data warehouses; e.g. Snowflake, Vertica, etc,Experience with BI tools; e.g. Looker, Tableau, etc,Experience with data serialization technologies, e.g. Avro, Protobuf, etc,Experience with Qubole,This position is located in Denver, CO and includes competitive pay, benefits package (including medical, dental, vision), 401k, commuter stipend, and equity.,Ibotta provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, and genetics.", "Help set up a data lake,Setup Integration points from various back end systems into the data lake,Help with the overall Big data strategy, architectural approach and Hadoop eco system tool selection,Big Data Engineer - Hadoop, SalesForce.com, SQL Server required,SQL Server (2008 & 2012),Web/Call Analytics,Hadoop/HDFS,Oozie,Sqoop,Kafka,Flume,SOAP/ RESTful web services,Java,PIG,Spark,Scala,HBASE,Cloudera,Good to have travel industry experience,Good working knowledge of Spark, Hive and Impala would be very beneficial,A good knowledge of the languages Scala and Java", "Manage our existing analytics solutions and improve them to satisfy business and product requirements,Architect data pipelines to transform and validate data,Maintain data quality and integrity across our systems,Communicate goals and progress to teams and stakeholders,Assist in data governance processes, planning, security and execution", "Break big data problems down into cases, patterns, component parts and manageable steps,Design and implement scalable database, service and application frameworks to solve big real-world problems,Build efficient and scalable data processing routines, working with files, APIs, and databases,Optimize storage and querying patterns to make the best use of space and time,Analyze large, relational datasets to understand patterns, troubleshoot issues and answer questions,Design, build and deploy new features, functions, structures, processing, analytics or data visualization.,You are a highly analytical thinker, passionate about data, technology, research and the scientific method,You have a strong background in mathematics,You have 2+ years of experience with SQL and Python,You have a working knowledge of software architecture, development lifecycles and agile methodologies,Inspire\u2019s culture balances the serious nature and tone of a professional company with the move-fast-and-execute tone of a rapidly growing emerging tech company. We believe in working hard and producing, but doing it in a positive and friendly environment where collaboration is rampant.Inspire calls its employees Avengers. It\u2019s a team out to even the score on behalf of the common man, to challenge the status quo and confront apathy and old-world bureaucracy.,Every Avenger lives for the mission: To build the world\u2019s most customer-focused clean power platform and inspire a connected moment towards a brighter energy future. Avengers are scrappy, restless, humble and committed to balancing passion with purpose and profit.", "Design new enterprise data models and ETL processes to populate them,Extract and transform data from production databases and 3rd party services to provide consumable data and support functions across the organization,Detect quality issues, track them to their root source, and implement fixes and preventative audits,Manage and optimize Redshift clusters/data lake to ensure current health and performance and future scaling needs,Help maintain the process we use to develop, test, and deploy good code,Become the \u201cgo to\u201d expert of our data. Work closely with staff to understand all data from our core systems, partner services, and any other platforms we rely on,Experience with AWS; expertise in Redshift, Postgres or other RDBSs (preferably column-oriented),Expertise in SQL and ability to write and optimize complex queries,Experience with Docker, Elastic Container Service, Lambda a plus,Ability to write customized software in Python, Bash, Go or other common open source languages. Experience with Airflow or similar scheduling service a plus,Experience with CI/CD tools like Jenkins or Drone,Creativity in approaching data organization challenges with an understanding of the end goal", "Creating reliable pipelines,Combining data sources,Architecting data stores,Collaborating with data science teams and building the right solutions for them", "Plan, design and implement robust data pipelines using technologies such as Hadoop MapReduce and Spark,Deliver near real-time data to our customers using our high-availability data infrastructure,Participate in architectural planning for our current and upcoming data challenges,Create solutions for data analytics and reporting,Enable our team to generate valuable insights,Seasoned Java developer,Experience with the Hadoop ecosystem (MapReduce, Hive, HDFS, Pig, HBase\u2026),Knowledge and experience in writing Spark applications (Spark streaming, SparkSQL),Knowledge of Apache KAFKA,Used to work in Linux based environments,Big plus: knowledge in cluster administration (Cloudera),A permanent contract in full time,An office with excellent transport connections directly at Alexanderplatz,A settled work environment with flat hierarchies in an international team,Free drinks, fresh fruit, lunch vouchers, employee discounts, team events, as well as subsidies sports courses,Flexible working hours and home office for a better work-life balance,Regular feedback and the opportunity to participate in trainings, conferences, meetups and language courses as part of our employee development,Choose your technical equipment by yourself (Mac / Linux / Windows),Plenty of room for your ideas and the chance to have an impact on the company\u2019s development", "Participate in analyzing, designing, coding, testing, configuring and modifying software for the functional delivery of data solutions,Contribute to the automated delivery of data software using source control, infrastructure as code, and continuous integration throughout the entire delivery model,Ensure that implemented data software is successfully monitored, with relevant alerts, logging and tracing that guarantee the relevant durability, availability and performance,Validate that data solutions follow data governance policies, standards and intent,Complete technical documentation that adds value, including but not limited to testing, training, governance and software delivery,Have 2-5 years of experience in a Data Engineering or similar role,Proficiency with at least one programming language (our team uses Python),Have contributed to the development and maintenance of data technology/software; working in the e-commerce and/or retail space a huge plus,Hands on experience working with large volumes of data; e.g, user Clickstream data, event-based log data, etc,Enjoy working on a Scrum Team in an agile delivery environment,Familiar with both batch and streaming data processes a plus,Experience modeling data for enterprise consumption a huge plus,Experience with data governance regulations like CCPA, GDPR and DP a huge plus,Work hard because we love what we're building, but also believe in balance,Are an apparel company that wholeheartedly embraces and is built on technology,Back up our talk with a competitive compensation and benefits package, challenging projects, random acts of team-wide fun and awesome co-workers that feel and operate like a championship team,Are located in the Flatiron District in the heart of Manhattan", "Participate to the management of our database on Microsoft Azure;,Optimize consolidated database for analyzing software usage;", "BigData (Hadoop/ Spark/ Yarn/ Oozie),Scala,Agile, TDD, Scrum/KanBan and XP,NoSQL,Cloud and Grid Computing,Java", "Creating reliable pipelines,Combining data sources,Architecting data stores,Collaborating with data science teams and building the right solutions for them", "Rapidly architect, design, prototype, and implement architectures to tackle the Big Data and Data Science needs for a variety of Fortune 1000 corporations and other major organizations,Work in cross-disciplinary teams with industry experts to understand client needs and ingest rich data sources such as social media, news, internal/external documents, emails, financial data, and operational data,Research, experiment, and utilize leading Big Data methodologies, such as Hadoop, Spark, Redshift, Netezza, SAP HANA, with a particular emphasis on Microsoft Azure,Architect, implement and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as AWS, Azure, client technology stacks, and client\u2019s own clusters,Translate advanced business analytics problems into technical approaches that yield actionable recommendations, in diverse domains such as risk management, product development, marketing research, supply chain, and public policy; communicate results and educate others through insightful visualizations, reports and presentations", "Help set up a data lake,Setup Integration points from various back end systems into the data lake,Help with the overall Big data strategy, architectural approach and Hadoop eco system tool selection,Big Data Engineer - Hadoop, SalesForce.com, SQL Server required,SQL Server (2008 & 2012),Web/Call Analytics,Hadoop/HDFS,Oozie,Sqoop,Kafka,Flume,SOAP/ RESTful web services,Java,PIG,Spark,Scala,HBASE,Cloudera,Good to have travel industry experience,Good working knowledge of Spark, Hive and Impala would be very beneficial,A good knowledge of the languages Scala and Java", "Exhibiting a core level of skills across at least two of the core streams (contact centre, unified communications, next gen networking, security, cloud migration, Analytics or Data Centre) with the ability to be self-sufficient at customer deployments and fault investigations.,Have an appropriate level of vendor certifications to reflect your expert skills, to be agreed with your line manager.,Have a knowledge of competitive solutions, technology and product offerings.,Understand and be conversant about company, solutions and product strengths, weaknesses, opportunities and threats.,Be an active member of the team, sharing knowledge to bring fellow team members up to a core skill level.,Strong customer satisfaction skills: ability to assess a problem and determine an effective course of action.,Constantly wanting to learn new skills, actively driving this yourself.,Effective and persuasive written and verbal technical sales communication skills, including an ability in overcome technical objections and internal or external obstacles.,Be bold, go \"against the grain\" and take informed risks.,Make fact-based decisions in the best interest of the customer and", "Participate in analyzing, designing, coding, testing, configuring and modifying software for the functional delivery of data solutions,Contribute to the automated delivery of data software using source control, infrastructure as code, and continuous integration throughout the entire delivery model,Ensure that implemented data software is successfully monitored, with relevant alerts, logging and tracing that guarantee the relevant durability, availability and performance,Validate that data solutions follow data governance policies, standards and intent,Complete technical documentation that adds value, including but not limited to testing, training, governance and software delivery,Have 5+ years of experience in a Data Engineering or similar role,Proficiency with at least one programming language (our team uses Python),Have contributed to the development and maintenance of data technology/software; working in the e-commerce and/or retail space a huge plus,Hands on experience working with large volumes of data; e.g, user Clickstream data, event-based log data, etc,Enjoy working on a Scrum Team in an agile delivery environment,Familiar with both batch and streaming data processes a plus,Experience modeling data for enterprise consumption a huge plus,Experience with data governance regulations like CCPA, GDPR and DP a huge plus,Work hard because we love what we're building, but also believe in balance,Are an apparel company that wholeheartedly embraces and is built on technology,Back up our talk with a competitive compensation and benefits package, challenging projects, random acts of team-wide fun and awesome co-workers that feel and operate like a championship team,Are located in the Flatiron District in the heart of Manhattan", "Very flat hierarchy with minimal bureaucracy getting in the way of your day to day work. If you want to take something up with the CEO, you can do so.,Tangible impact on a rapidly growing business building solutions for a new, remote-friendly, market.,You will be the owner of data within the company, and will be coming in with buy-in from stakeholders to build the best solution possible.", "Develop complex queries, very large volume data pipelines, and analytics applications,Develop complex queries and software programs to solve analytics and data mining problems,Interact with data analysts, product managers, and engineers to understand business problems, technical requirements to deliver reporting solutions,Prototype new metrics or data systems,Lead data investigations to troubleshoot data issues that arise along the data pipelines,Maintenance and improvement of released systems,Engineering consulting on large and complex warehouse data,BS/MS/PhD in Computer Science, Electrical Engineering or related disciplines,Strong fundamentals: data structures, algorithms, database,5-7+ years of software industry experience in analytics, data mining, or data mart and warehouse,Fluency with at least two of: Python/Java/C++,Proficiency with SQL/Unix/Shell,Self-driven, challeng-loving, detail oriented, teamwork spirit, excellent communication skills, ability to multi-task and manage expectations,Industry experience in Hadoop technologies (Map/Reduce, Pig, Hive, HBase, Storm, Spark, Kafka, Oozie).,Experience with machine learning algorithms and/or statistical methods a plus,Experience with any MPP analytics engines like Teradata, Vertica, or AWS/Redshift a plus", "Work on innovation and early stage product development projects across the company.,Clean and preprocess various large data sets from domains such as media, videogaming, and connected transport.,Collaborate with data scientists during model development.,Design highly available and scalable data pipelines.,Containerize pre-developed machine learning models.,Build CI/CD pipelines to deploy data pipelines, machine learning models and performance monitoring.,At least 3 years experience in a data engineering or similar role.,Experience in manipulating, processing and extracting value from large datasets,Experience supporting and working with cross-functional teams in a dynamic environment,Competitive remuneration and benefits,Career and personal development opportunities,Flexible working spaces in state of the art offices,A focus on collaboration, creativity, health and happiness", "At least 2+ years working in a Data Engineer, or Data Warehousing Engineer role,Strong experience with any ETL tool like Informatica or SSIS or Talend etc.,,Ability to lead projects individually and delivering them on time,Experience in performance tuning techniques,Experience building reports and with data visualization tools like Tableau, Qlikview, PowerBI etc.,,Strong Foundation in SQL coding and experience with ETL,Need to own problems from end-to-end, so that you can best collect, extract and clean the data,Help to implement maintenance/support strategy for all datasets,Work with relevant stakeholders to deliver appropriate BI, data warehousing, reporting, and analytical infrastructure required to support Centerfield\u2019s assets,Experience with any ETL tools like Talend, Informatica, SSIS etc.,,Tableau or any other BI tool like Microstrategy, Power BI, Redash.io etc.,,Experience with Amazon Web Services(S3, Redshift) is a plus,Competitive salary + profit sharing bonus,Unlimited PTO - Take a break when you need it!,401K company match plan \u2013 fully vested day 1,Free lunch,Award winning culture & unprecedented team spirit,Fully stocked break rooms with drinks & snacks,Free onsite gym & weekly in office exercise classes (yoga, kickboxing & circuit training),Generous parental leave,Paid volunteer days", "Atlanta , GA,Full-time,Design, develop, document, and test Business Intelligence solutions using industry standard tools.,Create, own, and present documentation/designs to fellow team members and clients.,Facilitate requirements gathering sessions with business and technical stakeholders to distill data and reporting requirements from business requests.,Coordinate design and development efforts with client stakeholders to ensure the solution delivered meets the business need and is consistent with approved architectural standards.,Performance tuning to ensure a responsive solution."], "meta": ["www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "bit.ly", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.mediabistro.com", "www.mediabistro.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.dlr.de", "indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.yelp.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.airbnb.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "www.indeed.com", "www.llnl.gov", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.mckinsey.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "militaryjobs.homedepot.com", "www.llnl.gov", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.llnl.gov", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.meetup.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "stackoverflow.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.bath.ac.uk", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.sitepoint.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "japan.careers.vmware.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.mediabistro.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "karriere.nzz.ch", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.sitepoint.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.monster.com", "www.sophos.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.wix.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.sophos.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "derstandard.at", "www.sitepoint.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.verizon.com", "www.indeed.com", "www.sophos.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "nbacareers.nba.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.accenture.com", "militaryjobs.homedepot.com", "www.accenture.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.wix.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "japan.careers.vmware.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.sitepoint.com", "www.sitepoint.com", "www.mediabistro.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.sophos.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.sitepoint.com", "www.accenture.com", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.wix.com", "www.indeed.com", "stackoverflow.com", "www.idealist.org", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.verizon.com", "dfwishiring.dallasnews.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "www.accenture.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.biospace.com", "www.gfk.com", "www.geekwire.com", "www.snagajob.com", "zapier.com", "www.themuse.com", "www.gene.com", "www.governmentjobs.com", "www.kdnuggets.com", "www.standardmedia.co.ke", "www.themuse.com", "www.finn.no", "tweakers.net", "www.insurancejournal.com", "www.timesofmalta.com", "www.snagajob.com", "www.cdp.net", "www.devex.com", "www.biospace.com", "www.insurancejournal.com", "www.biospace.com", "www.biospace.com", "www.careercast.com", "www.devex.com", "usa.visa.com", "www.internships.com", "www.devex.com", "www.devex.com", "www.biospace.com", "www.geekwire.com", "feedproxy.google.com", "www.cdp.net", "www.devex.com", "www.kdnuggets.com", "www.careercast.com", "www.careerjet.com", "www.airweb.org", "www.devex.com", "slack.com", "www.biospace.com", "www.gene.com", "www.insurancejournal.com", "www.biospace.com", "feedproxy.google.com", "www.gene.com", "www.internships.com", "www.geekwire.com", "www.biospace.com", "www.biospace.com", "www.cdp.net", "stripe.com", "www.insurancejournal.com", "www.devex.com", "www.biospace.com", "tweakers.net", "hh.ru", "www.insurancejournal.com", "www.devex.com", "www.devex.com", "www.cdp.net", "www.themuse.com", "www.gene.com", "www.gene.com", "careers.trimble.com", "vtk.ugent.be", "www.dice.com", "feedproxy.google.com", "www.cdp.net", "www.zynga.com", "www.flexjobs.com", "www.reed.co.uk", "www.reed.co.uk", "www.shakeshack.com", "technical.ly", "www.flexjobs.com", "www.flexjobs.com", "www.careerjet.co.uk", "www.flexjobs.com", "www.flexjobs.com", "technical.ly", "www.indeed.co.uk", "www.smartrecruiters.com", "www.indeed.co.uk", "www.computerworld.co.nz", "www.flexjobs.com", "www.nytco.com", "www.careerbliss.com", "www.reed.co.uk", "www.flexjobs.com", "www.flexjobs.com", "www.cs.mcgill.ca", "www.computerworld.co.nz", "www.zynga.com", "www.reed.co.uk", "www.gettinghired.com", "www.gettinghired.com", "www.ziprecruiter.com", "www.indeed.co.uk", "optics.org", "www.upwork.com", "www.gettinghired.com", "www.reed.co.uk", "www.cgi.com", "www.reed.co.uk", "www.indeed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.careerjet.co.uk", "hiring.monster.com", "www.gettinghired.com", "www.flexjobs.com", "www.flexjobs.com", "www.ziprecruiter.com", "theoceancleanup.com", "www.gettinghired.com", "www.techworld.com.au", "www.techworld.com.au", "www.computerworld.co.nz", "www.diglib.org", "newyork.craigslist.org", "www.flexjobs.com", "www.techworld.com.au", "www.indeed.co.uk", "technical.ly", "www.careerjet.co.uk", "technical.ly", "www.cs.mcgill.ca", "www.flexjobs.com", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.computerworld.co.nz", "www.reed.co.uk", "technical.ly", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.flexjobs.com", "www.zynga.com", "www.gettinghired.com", "www.henkel.com", "www.reed.co.uk", "technical.ly", "www.reed.co.uk", "www.indeed.co.uk", "technical.ly", "www.careerjet.co.uk", "www.ziprecruiter.com", "www.gettinghired.com", "www.flexjobs.com", "www.gettinghired.com", "www.flexjobs.com", "www.flexjobs.com", "www.indeed.co.uk", "www.axa.com", "www.flexjobs.com", "www.reed.co.uk", "www.cgi.com", "www.flexjobs.com", "www.reed.co.uk", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.computerworld.co.nz", "www.indeed.co.uk", "www.net-temps.com", "www.flexjobs.com", "www.builtincolorado.com", "jobs.newscientist.com", "jobs.theguardian.com", "jobs.lever.co", "www.builtincolorado.com", "jobs.lever.co", "www.builtinchicago.org", "www.builtinchicago.org", "jobs.lever.co", "www.builtinchicago.org", "www.builtinchicago.org", "moikrug.ru", "careers.insidehighered.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "spb.hh.ru", "jobs.theguardian.com", "www.builtincolorado.com", "jobs.theguardian.com", "www.pracuj.pl", "boards.greenhouse.io", "boards.greenhouse.io", "spb.hh.ru", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.newscientist.com", "krb-sjobs.brassring.com", "ca.indeed.com", "www.builtincolorado.com", "jobs.theguardian.com", "www.builtinchicago.org", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.lever.co", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "jobs.theguardian.com", "careers.insidehighered.com", "jobs.newscientist.com", "jobs.newscientist.com", "jobs.lever.co", "jobs.theguardian.com", "www.builtinchicago.org", "jobs.theguardian.com", "krb-sjobs.brassring.com", "www.level39.co", "www.builtinchicago.org", "jobs.newscientist.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.toptal.com", "www.builtincolorado.com", "careers.insidehighered.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.lever.co", "www.builtinchicago.org", "jobs.newscientist.com", "jobs.lever.co", "jobs.newscientist.com", "www.builtinchicago.org", "careers.insidehighered.com", "www.builtinchicago.org", "jobs.lever.co", "jobs.theguardian.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.irishjobs.ie", "krb-sjobs.brassring.com", "jobs.theguardian.com", "careers.insidehighered.com", "www.jobserve.com", "boards.greenhouse.io", "www.builtinla.com", "www.parking-net.com", "jobs.seattletimes.com", "www.nuon.com", "www.careerjet.co.uk", "www.careerjet.co.uk", "careers.walmart.com", "www.builtinla.com", "electricenergyonline.com", "www.builtincolorado.com", "www.topschooljobs.org", "www.indeed.es", "illinoisjoblink.illinois.gov", "www.indeed.es", "www.builtincolorado.com", "www.careerjet.co.uk", "www.builtinla.com", "www.iamexpat.nl", "www.hipo.ro", "careers.walmart.com", "www.parking-net.com", "www.randstad.co.uk", "www.connecticum.de", "www.startupjobs.cz", "www.builtinla.com", "ejob.bz", "www.parking-net.com", "www.builtinla.com", "jobs.gamasutra.com", "electricenergyonline.com", "www.randstad.co.uk", "marketingevolution.theresumator.com", "www.builtincolorado.com", "www.careerjet.co.uk", "www.builtinla.com", "www.builtinla.com", "www.parking-net.com", "www.iamexpat.nl", "dasauge.de", "careers.walmart.com", "www.xlstat.com", "jobs.telegraph.co.uk", "www.iamexpat.nl", "www.classifiedads.com", "www.careerjet.co.uk", "jobs.telegraph.co.uk", "careers.walmart.com", "www.iamexpat.nl", "diversityjobs.com", "join.irdeto.com", "www.builtinla.com", "justjobs.com"]}}; }
plotInterface = buildViz(1000,
600,
null,
null,
false,
false,
false,
false,
false,
true,
false,
false,
true,
0.1,
false,
undefined,
undefined,
getDataAndInfo(),
true,
false,
null,
null,
null,
null,
true,
false,
true,
false,
null,
null,
10,
null,
null,
null,
false,
true,
true,
undefined,
null,
false,
false,
".3f",
".3f",
false,
-1,
true,
false,
true,
false,
false,
false,
true,
null,
null,
null,
false,
null,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
14,
0);


autocomplete(
    document.getElementById('searchInput'),
    plotInterface.data.map(x => x.term).sort(),
    plotInterface
);

</script>
