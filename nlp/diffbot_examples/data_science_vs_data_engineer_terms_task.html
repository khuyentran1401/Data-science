<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}

svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.grid line {
  stroke: lightgrey;
  stroke-opacity: 0.4;
  shape-rendering: crispEdges;
}

.grid path {
  stroke-width: 0;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 50px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}
/*
input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}*/

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}
/*
input:focus {
  background-color: lightyellow;
  outline: none;
}*/

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}

.topic_preview {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;
  font-weight: normal;
  text-decoration: none;
}


#d3-div-1-categoryinfo {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;    

}


#d3-div-1-title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}

.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}

.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  color: darkblue;
}

.not_match {
    background-color: #F0F8FF;
}
    
.contexts {
  width: 45%;
  float: left;
}

.neut_display {
  display: none;
  float: left
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.obscured {
  /*font-size: 14px;
  font-weight: normal;
  color: dimgrey;
  font-family: Helvetica;*/
  text-align: center;
}

.small_label {
  font-size: 10px;
}

#d3-div-1-corpus-stats {
  text-align: center;
}

#d3-div-1-cat {
}

#d3-div-1-notcat {
}

#d3-div-1-neut {
}

#d3-div-1-neutcol {
  display: none;
}
/* Adapted from https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_autocomplete */

.autocomplete {
  position: relative;
  display: inline-block;
}

input {
  border: 1px solid transparent;
  background-color: #f1f1f1;
  padding: 10px;
  font-size: 16px;
}

input[type=text] {
  background-color: #f1f1f1;
  width: 100%;
}

input[type=submit] {
  background-color: DodgerBlue;
  color: #fff;
  cursor: pointer;
}

.autocomplete-items {
  position: absolute;
  border: 2px solid #d4d4d4;
  border-bottom: none;
  border-top: none;
  z-index: 99;
  /*position the autocomplete items to be the same width as the container:*/
  top: 100%;
  left: 0;
  right: 0;
}

.autocomplete-items div {
  padding: 10px;
  cursor: pointer;
  background-color: #fff;
  border-bottom: 2px solid #d4d4d4;
}

/*when hovering an item:*/
.autocomplete-items div:hover {
  background-color: #e9e9e9;
}

/*when navigating through the items using the arrow keys:*/
.autocomplete-active {
  background-color: DodgerBlue !important;
  color: #ffffff;
}
</style>

<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>

<!-- INSERT SEMIOTIC SQUARE -->
<!--<a onclick="maxFreq = Math.log(data.map(d => d.cat + d.ncat).reduce((a,b) => Math.max(a,b))); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, false); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, true)">View Score Plot</a>-->
<span id="d3-div-1-title-div"></span>
<div class="scattertext" id="d3-div-1" style="float: left"></div>
<div style="floag: left;">
    <div autocomplete="off">
        <div class="autocomplete">
            <input id="searchInput" type="text" placeholder="Search the chart">
        </div>
    </div>
</div>
<br/>
<div id="d3-div-1-corpus-stats"></div>
<div id="d3-div-1-overlapped-terms"></div>
<a name="d3-div-1-snippets"></a>
<a name="d3-div-1-snippetsalt"></a>
<div id="d3-div-1-termstats"></div>
<div id="d3-div-1-overlapped-terms-clicked"></div>
<div id="d3-div-1-categoryinfo" style="display: hidden"></div>
<div id="d3-div-2">
  <div class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-cathead"></div>
    <div class="snippet" id="d3-div-1-cat"></div>
  </div>
  <div id="d3-div-1-notcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-notcathead"></div>
    <div class="snippet" id="d3-div-1-notcat"></div>
  </div>
  <div id="d3-div-1-neutcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-neuthead"></div>
    <div class="snippet" id="d3-div-1-neut"></div>
  </div>
</div>
<script charset="utf-8">
    // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 1000,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.1,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null,
                     fullData = null,
                     showTopTerms = true,
                     showNeutral = false,
                     getTooltipContent = null,
                     xAxisValues = null,
                     yAxisValues = null,
                     colorFunc = null,
                     showAxes = true,
                     showExtra = false,
                     doCensorPoints = true,
                     centerLabelsOverPoints = false,
                     xAxisLabels = null,
                     yAxisLabels = null,
                     topic_model_preview_size = 10,
                     verticalLines = null,
                     horizontal_line_y_position = null,
                     vertical_line_x_position = null,
                     unifiedContexts = false,
                     showCategoryHeadings = true,
                     showCrossAxes = true,
                     divName = 'd3-div-1',
                     alternativeTermFunc = null,
                     includeAllContexts = false,
                     showAxesAndCrossHairs = false,
                     x_axis_values_format = '.3f',
                     y_axis_values_format = '.3f',
                     matchFullLine = false,
                     maxOverlapping = -1,
                     showCorpusStats = true,
                     sortDocLabelsByName = false,
                     alwaysJump = true,
                     highlightSelectedCategory = false,
                     showDiagonal = false,
                     useGlobalScale = false,
                     enableTermCategoryDescription = true,
                     getCustomTermHtml = null,
                     headerNames = null,
                     headerSortingAlgos = null,
                     ignoreCategories = false,
                     backgroundLabels = null,
                     labelPriorityColumn = null,
                     textColorColumn = undefined,
                     suppressTextColumn = undefined,
                     backgroundColor = undefined,
                     censorPointColumn = undefined,
                     rightOrderColumn = undefined,
                     subwordEncoding = null,
                     topTermsLength = 14,
                     topTermsLeftBuffer = 0
    ) {
        function formatTermForDisplay(term) {
            if (subwordEncoding === 'RoBERTa' && (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289))
                term = '_' + term.substr(1, term.length - 1);
            return term;
        }

        //var divName = 'd3-div-1';
        // Set the dimensions of the canvas / graph
        var padding = {top: 30, right: 20, bottom: 30, left: 50};
        if (!showAxes) {
            padding = {top: 30, right: 20, bottom: 30, left: 50};
        }
        var margin = padding,
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;
        fullData.data.forEach(function (x, i) {
            x.i = i
        });

        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        if (unifiedContexts) {
            document.querySelectorAll('#' + divName + '-' + 'notcol')
                .forEach(function (x) {
                    x.style.display = 'none'
                });
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '90%'
                });
        } else if (showNeutral) {
            if (showExtra) {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '25%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol', 'extracol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '25%'
                        });
                })

            } else {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '33%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '33%'
                        });
                })


            }
        } else {
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '45%'
                    //x.style.display = 'inline'
                    x.style.float = 'left'
                });

            ['notcol'].forEach(function (columnName) {
                document.querySelectorAll('#' + divName + '-' + columnName)
                    .forEach(function (x) {
                        //x.style.display = 'inline'
                        x.style.float = 'left'
                        x.style.width = '45%'
                    });
            })
        }

        var yAxis = null;
        var xAxis = null;

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }


        function bs(ar, x) {
            function bsa(s, e) {
                var mid = Math.floor((s + e) / 2);
                var midval = ar[mid];
                if (s == e) {
                    return s;
                }
                if (midval == x) {
                    return mid;
                } else if (midval < x) {
                    return bsa(mid + 1, e);
                } else {
                    return bsa(s, mid);
                }
            }

            return bsa(0, ar.length);
        }


        console.log("fullData");
        console.log(fullData);


        var sortedX = fullData.data.map(x => x).sort(function (a, b) {
            return a.x < b.x ? -1 : (a.x == b.x ? 0 : 1);
        }).map(function (x) {
            return x.x
        });

        var sortedOx = fullData.data.map(x => x).sort(function (a, b) {
            return a.ox < b.ox ? -1 : (a.ox == b.ox ? 0 : 1);
        }).map(function (x) {
            return x.ox
        });

        var sortedY = fullData.data.map(x => x).sort(function (a, b) {
            return a.y < b.y ? -1 : (a.y == b.y ? 0 : 1);
        }).map(function (x) {
            return x.y
        });

        var sortedOy = fullData.data.map(x => x).sort(function (a, b) {
            return a.oy < b.oy ? -1 : (a.oy == b.oy ? 0 : 1);
        }).map(function (x) {
            return x.oy
        });
        console.log(fullData.data[0])

        function labelWithZScore(axis, axisName, tickPoints, axis_values_format) {
            var myVals = axisName === 'x' ? sortedOx : sortedOy;
            var myPlotedVals = axisName === 'x' ? sortedX : sortedY;
            var ticks = tickPoints.map(function (x) {
                return myPlotedVals[bs(myVals, x)]
            });
            return axis.tickValues(ticks).tickFormat(
                function (d, i) {
                    return d3.format(axis_values_format)(tickPoints[i]);
                })
        }

        if (xAxisValues) {
            xAxis = labelWithZScore(d3.axisBottom(x), 'x', xAxisValues, x_axis_values_format);
        } else if (xAxisLabels) {
            xAxis = d3.axisBottom(x)
                .ticks(xAxisLabels.length)
                .tickFormat(function (d, i) {
                    return xAxisLabels[i];
                });
        } else {
            xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        }
        if (yAxisValues) {
            yAxis = labelWithZScore(d3.axisLeft(y), 'y', yAxisValues, y_axis_values_format);
        } else if (yAxisLabels) {
            yAxis = d3.axisLeft(y)
                .ticks(yAxisLabels.length)
                .tickFormat(function (d, i) {
                    return yAxisLabels[i];
                });
        } else {
            yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));
        }

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");


        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230),
            d3.rgb(130, 130, 130));
        // setup fill color
        if (color == null) {
            color = d3.interpolateRdYlBu;
        }
        if ((headerNames !== undefined && headerNames !== null)
            && (headerSortingAlgos !== undefined && headerSortingAlgos !== null)) {
            showTopTerms = true;
        }

        var pixelsToAddToWidth = 200;
        if (!showTopTerms && !showCharacteristic) {
            pixelsToAddToWidth = 0;
        }

        if (backgroundColor !== undefined) {
            document.body.style.backgroundColor = backgroundColor;
        }

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + pixelsToAddToWidth)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");


        origSVGLeft = svg.node().getBoundingClientRect().left;
        origSVGTop = svg.node().getBoundingClientRect().top;
        var lastCircleSelected = null;

        function getCorpusWordCounts() {
            var binaryLabels = fullData.docs.labels.map(function (label) {
                return 1 * (fullData.docs.categories[label] != fullData.info.category_internal_name);
            });
            var wordCounts = {}; // word -> [cat counts, not-cat-counts]
            var wordCountSums = [0, 0];
            fullData.docs.texts.forEach(function (text, i) {
                text.toLowerCase().trim().split(/\W+/).forEach(function (word) {
                    if (word.trim() !== '') {
                        if (!(word in wordCounts))
                            wordCounts[word] = [0, 0];
                        wordCounts[word][binaryLabels[i]]++;
                        wordCountSums[binaryLabels[i]]++;
                    }
                })
            });
            return {
                avgDocLen: (wordCountSums[0] + wordCountSums[1]) / fullData.docs.texts.length,
                counts: wordCounts,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)]
                })
            };
        }

        function getContextWordCounts(query) {
            var wordCounts = {};
            var wordCountSums = [0, 0];
            var priorCountSums = [0, 0];
            gatherTermContexts(termDict[query])
                .contexts
                .forEach(function (contextSet, categoryIdx) {
                    contextSet.forEach(function (context) {
                        context.snippets.forEach(function (snippet) {
                            var tokens = snippet.toLowerCase().trim().replace('<b>', '').replace('</b>', '').split(/\W+/);
                            var matchIndices = [];
                            tokens.forEach(function (word, i) {
                                if (word === query) matchIndices.push(i)
                            });
                            tokens.forEach(function (word, i) {
                                if (word.trim() !== '') {
                                    var isValid = false;
                                    for (var matchI in matchIndices) {
                                        if (Math.abs(i - matchI) < 3) {
                                            isValid = true;
                                            break
                                        }
                                    }
                                    if (isValid) {
                                        //console.log([word, i, matchI, isValid]);
                                        if (!(word in wordCounts)) {
                                            var priorCounts = corpusWordCounts.counts[word]
                                            wordCounts[word] = [0, 0].concat(priorCounts);
                                            priorCountSums[0] += priorCounts[0];
                                            priorCountSums[1] += priorCounts[1];
                                        }
                                        wordCounts[word][categoryIdx]++;
                                        wordCountSums[categoryIdx]++;
                                    }
                                }
                            })
                        })
                    })
                });
            return {
                counts: wordCounts,
                priorSums: priorCountSums,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)];
                })
            }

        }

        function denseRank(ar) {
            var markedAr = ar.map((x, i) => [x, i]).sort((a, b) => a[0] - b[0]);
            var curRank = 1
            var rankedAr = markedAr.map(
                function (x, i) {
                    if (i > 0 && x[0] != markedAr[i - 1][0]) {
                        curRank++;
                    }
                    return [curRank, x[0], x[1]];
                }
            )
            return rankedAr.map(x => x).sort((a, b) => (a[2] - b[2])).map(x => x[0]);
        }


        function getDenseRanks(fullData, categoryNum) {
            console.log("GETTING DENSE RANKS")
            console.log("CAT NUM " + categoryNum)
            console.log(fullData)

            var fgFreqs = Array(fullData.data.length).fill(0);
            var bgFreqs = Array(fullData.data.length).fill(0);
            var categoryTermCounts = fullData.termCounts[categoryNum];

            Object.keys(categoryTermCounts).forEach(
                key => fgFreqs[key] = categoryTermCounts[key][0]
            )
            fullData.termCounts.forEach(
                function (categoryTermCounts, otherCategoryNum) {
                    if (otherCategoryNum != categoryNum) {
                        Object.keys(categoryTermCounts).forEach(
                            key => bgFreqs[key] += categoryTermCounts[key][0]
                        )
                    }
                }
            )
            var fgDenseRanks = denseRank(fgFreqs);
            var bgDenseRanks = denseRank(bgFreqs);

            var maxfgDenseRanks = Math.max(...fgDenseRanks);
            var minfgDenseRanks = Math.min(...fgDenseRanks);
            var scalefgDenseRanks = fgDenseRanks.map(
                x => (x - minfgDenseRanks) / (maxfgDenseRanks - minfgDenseRanks)
            )

            var maxbgDenseRanks = Math.max(...bgDenseRanks);
            var minbgDenseRanks = Math.min(...bgDenseRanks);
            var scalebgDenseRanks = bgDenseRanks.map(
                x => (x - minbgDenseRanks) / (maxbgDenseRanks - minbgDenseRanks)
            )

            return {'fg': scalefgDenseRanks,
                'bg': scalebgDenseRanks,
                'bgFreqs': bgFreqs,
                'fgFreqs': fgFreqs,
                'term': fullData.data.map((x)=>x.term)}
        }

        function getCategoryDenseRankScores(fullData, categoryNum) {
            var denseRanks = getDenseRanks(fullData, categoryNum)
            return denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
        }

        function getTermCounts(fullData) {
            var counts = Array(fullData.data.length).fill(0);
            fullData.termCounts.forEach(
                function (categoryTermCounts) {
                    Object.keys(categoryTermCounts).forEach(
                        key => counts[key] = categoryTermCounts[key][0]
                    )
                }
            )
            return counts;
        }

        function getContextWordLORIPs(query) {
            var contextWordCounts = getContextWordCounts(query);
            var ni_k = contextWordCounts.sums[0];
            var nj_k = contextWordCounts.sums[1];
            var n = ni_k + nj_k;
            //var ai_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            //var aj_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            var a0 = 0.00001 //corpusWordCounts.avgDocLen;
            var a_k0 = Object.keys(contextWordCounts.counts)
                .map(function (x) {
                    var counts = contextWordCounts.counts[x];
                    return a0 * (counts[2] + counts[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                })
                .reduce(function (a, b) {
                    return a + b
                });
            var ai_k0 = a_k0 / ni_k;
            var aj_k0 = a_k0 / nj_k;
            var scores = Object.keys(contextWordCounts.counts).map(
                function (word) {
                    var countData = contextWordCounts.counts[word];
                    var yi = countData[0];
                    var yj = countData[1];
                    //var ai = countData[2];
                    //var aj = countData[3];
                    //var ai = countData[2] + countData[3];
                    //var aj = ai;
                    //var ai = (countData[2] + countData[3]) * a0/ni_k;
                    //var aj = (countData[2] + countData[3]) * a0/nj_k;
                    var ai = a0 * (countData[2] + countData[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                    var aj = ai;
                    var deltahat_i_j =
                        +Math.log((yi + ai) * 1. / (ni_k + ai_k0 - yi - ai))
                        - Math.log((yj + aj) * 1. / (nj_k + aj_k0 - yj - aj));
                    var var_deltahat_i_j = 1. / (yi + ai) + 1. / (ni_k + ai_k0 - yi - ai)
                        + 1. / (yj + aj) + 1. / (nj_k + aj_k0 - yj - aj);
                    var zeta_ij = deltahat_i_j / Math.sqrt(var_deltahat_i_j);
                    return [word, yi, yj, ai, aj, ai_k0, zeta_ij];
                }
            ).sort(function (a, b) {
                return b[5] - a[5];
            });
            return scores;
        }

        function getContextWordSFS(query) {
            // from https://stackoverflow.com/questions/14846767/std-normal-cdf-normal-cdf-or-error-function
            function cdf(x, mean, variance) {
                return 0.5 * (1 + erf((x - mean) / (Math.sqrt(2 * variance))));
            }

            function erf(x) {
                // save the sign of x
                var sign = (x >= 0) ? 1 : -1;
                x = Math.abs(x);

                // constants
                var a1 = 0.254829592;
                var a2 = -0.284496736;
                var a3 = 1.421413741;
                var a4 = -1.453152027;
                var a5 = 1.061405429;
                var p = 0.3275911;

                // A&S formula 7.1.26
                var t = 1.0 / (1.0 + p * x);
                var y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);
                return sign * y; // erf(-x) = -erf(x);
            }

            function scale(a) {
                return Math.log(a + 0.0000001);
            }

            var contextWordCounts = getContextWordCounts(query);
            var wordList = Object.keys(contextWordCounts.counts).map(function (word) {
                return contextWordCounts.counts[word].concat([word]);
            });
            var cat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - cat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - cat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            var ncat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - ncat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - ncat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            function scaledFScore(cnt, other, freq_xbar, freq_var, prec_xbar, prec_var) {
                var beta = 1.5;
                var normFreq = cdf(scale(cnt), freq_xbar, freq_var);
                var normPrec = cdf(scale(cnt / (cnt + other)), prec_xbar, prec_var);
                return (1 + Math.pow(beta, 2)) * normFreq * normPrec / (Math.pow(beta, 2) * normFreq + normPrec);
            }

            var sfs = wordList.map(function (x) {
                cat_sfs = scaledFScore(x[0], x[1], cat_freq_xbar,
                    cat_freq_var, cat_prec_xbar, cat_prec_var);
                ncat_sfs = scaledFScore(x[1], x[0], ncat_freq_xbar,
                    ncat_freq_var, ncat_prec_xbar, ncat_prec_var);
                return [cat_sfs > ncat_sfs ? cat_sfs : -ncat_sfs].concat(x);

            }).sort(function (a, b) {
                return b[0] - a[0];
            });
            return sfs;
        }

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
                + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d, includeAll = true) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], [], [], []];
            console.log("searching")

            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d, includeAll);
            } else {
                return searchInExtraFeatures(d, includeAll);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], [], [], []];
            var term = d.term;
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });

            var pattern = null;
            if ('metalists' in fullData && term in fullData.metalists) {
                // from https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(str) {
                    return str.replace(/[\\?\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|\']/g, "\\$&");
                }

                console.log('term');
                console.log(term);
                pattern = new RegExp(
                    '\\W(' + fullData.metalists[term].map(escapeRegExp).join('|') + ')\\W',
                    'gim'
                );
            }

            for (var i in fullData.docs.extra) {
                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }
                    var text = fullData.docs.texts[i];

                    if (fullData.offsets !== undefined) {

                        if (fullData.offsets[term] !== undefined && fullData.offsets[term][i] !== undefined) {
                            var curMatch = {
                                'id': i,
                                'snippets': [],
                                'strength': strength,
                                'docLabel': docLabel,
                                'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                            }
                            for (const offset_i in fullData.offsets[term][i]) {
                                var offset = fullData.offsets[term][i][offset_i];
                                var spanStart = Math.max(offset[0] - 50, 0);
                                var spanEnd = Math.min(50, text.length-offset[1]);
                                var leftContext = text.substr(spanStart, offset[0] - spanStart);
                                var matchStr = text.substr(offset[0], offset[1] - offset[0]);
                                var rightContext = text.substr(offset[1], spanEnd);
                                var snippet = leftContext + '<b style="background-color: lightgoldenrodyellow">' + matchStr + '</b>' + rightContext;
                                if(spanStart > 0)
                                    snippet = '...' + snippet;
                                if(text.length - offset[1] > 50)
                                    snippet = snippet + '...'
                                curMatch.snippets.push(snippet)
                            }
                            matches[numericLabel].push(curMatch);
                        }
                    } else {

                        if (!useFullDoc)
                            text = text.slice(0, 300);
                        if (pattern !== null) {
                            text = text.replace(pattern, '<b>$&</b>');
                        }
                        var curMatch = {
                            'id': i,
                            'snippets': [text],
                            'strength': strength,
                            'docLabel': docLabel,
                            'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                        }

                        matches[numericLabel].push(curMatch);
                    }
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://mathiasbynens.be/notes/es-unicode-property-escapes#emoji
        var emojiRE = (/(?:[\u261D\u26F9\u270A-\u270D]|\uD83C[\uDF85\uDFC2-\uDFC4\uDFC7\uDFCA-\uDFCC]|\uD83D[\uDC42\uDC43\uDC46-\uDC50\uDC66-\uDC69\uDC6E\uDC70-\uDC78\uDC7C\uDC81-\uDC83\uDC85-\uDC87\uDCAA\uDD74\uDD75\uDD7A\uDD90\uDD95\uDD96\uDE45-\uDE47\uDE4B-\uDE4F\uDEA3\uDEB4-\uDEB6\uDEC0\uDECC]|\uD83E[\uDD18-\uDD1C\uDD1E\uDD1F\uDD26\uDD30-\uDD39\uDD3D\uDD3E\uDDD1-\uDDDD])(?:\uD83C[\uDFFB-\uDFFF])?|(?:[\u231A\u231B\u23E9-\u23EC\u23F0\u23F3\u25FD\u25FE\u2614\u2615\u2648-\u2653\u267F\u2693\u26A1\u26AA\u26AB\u26BD\u26BE\u26C4\u26C5\u26CE\u26D4\u26EA\u26F2\u26F3\u26F5\u26FA\u26FD\u2705\u270A\u270B\u2728\u274C\u274E\u2753-\u2755\u2757\u2795-\u2797\u27B0\u27BF\u2B1B\u2B1C\u2B50\u2B55]|\uD83C[\uDC04\uDCCF\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE1A\uDE2F\uDE32-\uDE36\uDE38-\uDE3A\uDE50\uDE51\uDF00-\uDF20\uDF2D-\uDF35\uDF37-\uDF7C\uDF7E-\uDF93\uDFA0-\uDFCA\uDFCF-\uDFD3\uDFE0-\uDFF0\uDFF4\uDFF8-\uDFFF]|\uD83D[\uDC00-\uDC3E\uDC40\uDC42-\uDCFC\uDCFF-\uDD3D\uDD4B-\uDD4E\uDD50-\uDD67\uDD7A\uDD95\uDD96\uDDA4\uDDFB-\uDE4F\uDE80-\uDEC5\uDECC\uDED0-\uDED2\uDEEB\uDEEC\uDEF4-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])|(?:[#\*0-9\xA9\xAE\u203C\u2049\u2122\u2139\u2194-\u2199\u21A9\u21AA\u231A\u231B\u2328\u23CF\u23E9-\u23F3\u23F8-\u23FA\u24C2\u25AA\u25AB\u25B6\u25C0\u25FB-\u25FE\u2600-\u2604\u260E\u2611\u2614\u2615\u2618\u261D\u2620\u2622\u2623\u2626\u262A\u262E\u262F\u2638-\u263A\u2640\u2642\u2648-\u2653\u2660\u2663\u2665\u2666\u2668\u267B\u267F\u2692-\u2697\u2699\u269B\u269C\u26A0\u26A1\u26AA\u26AB\u26B0\u26B1\u26BD\u26BE\u26C4\u26C5\u26C8\u26CE\u26CF\u26D1\u26D3\u26D4\u26E9\u26EA\u26F0-\u26F5\u26F7-\u26FA\u26FD\u2702\u2705\u2708-\u270D\u270F\u2712\u2714\u2716\u271D\u2721\u2728\u2733\u2734\u2744\u2747\u274C\u274E\u2753-\u2755\u2757\u2763\u2764\u2795-\u2797\u27A1\u27B0\u27BF\u2934\u2935\u2B05-\u2B07\u2B1B\u2B1C\u2B50\u2B55\u3030\u303D\u3297\u3299]|\uD83C[\uDC04\uDCCF\uDD70\uDD71\uDD7E\uDD7F\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE02\uDE1A\uDE2F\uDE32-\uDE3A\uDE50\uDE51\uDF00-\uDF21\uDF24-\uDF93\uDF96\uDF97\uDF99-\uDF9B\uDF9E-\uDFF0\uDFF3-\uDFF5\uDFF7-\uDFFF]|\uD83D[\uDC00-\uDCFD\uDCFF-\uDD3D\uDD49-\uDD4E\uDD50-\uDD67\uDD6F\uDD70\uDD73-\uDD7A\uDD87\uDD8A-\uDD8D\uDD90\uDD95\uDD96\uDDA4\uDDA5\uDDA8\uDDB1\uDDB2\uDDBC\uDDC2-\uDDC4\uDDD1-\uDDD3\uDDDC-\uDDDE\uDDE1\uDDE3\uDDE8\uDDEF\uDDF3\uDDFA-\uDE4F\uDE80-\uDEC5\uDECB-\uDED2\uDEE0-\uDEE5\uDEE9\uDEEB\uDEEC\uDEF0\uDEF3-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])\uFE0F/g);

        function isEmoji(str) {
            if (str.match(emojiRE)) return true;
            return false;
        }

        function displayObscuredTerms(obscuredTerms, data, term, termInfo, div = '#' + divName + '-' + 'overlapped-terms') {
            d3.select('#' + divName + '-' + 'overlapped-terms')
                .selectAll('div')
                .remove();
            d3.select(div)
                .selectAll('div')
                .remove();
            if (obscuredTerms.length > 1 && maxOverlapping !== 0) {
                var obscuredDiv = d3.select(div)
                    .append('div')
                    .attr("class", "obscured")
                    .style('align', 'center')
                    .style('text-align', 'center')
                    .html("<b>\"" + term + "\" obstructs</b>: ");
                obscuredTerms.map(
                    function (term, i) {
                        if (maxOverlapping === -1 || i < maxOverlapping) {
                            makeWordInteractive(
                                data,
                                svg,
                                obscuredDiv.append("text").text(term),
                                term,
                                data.filter(t => t.term === term)[0],//termInfo
                                false
                            );
                            if (i < obscuredTerms.length - 1
                                && (maxOverlapping === -1 || i < maxOverlapping - 1)) {
                                obscuredDiv.append("text").text(", ");
                            }
                        } else if (i === maxOverlapping && i !== obscuredTerms.length - 1) {
                            obscuredDiv.append("text").text("...");
                        }
                    }
                )
            }
        }

        function displayTermContexts(data, termInfo, jump = alwaysJump, includeAll = false) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            var notmatches = termInfo.notmatches;
            if (contexts[0].length + contexts[1].length + contexts[2].length + contexts[3].length == 0) {
                //return null;
            }
            //!!! Future feature: context words
            //var contextWords = getContextWordSFS(info.term);
            //var contextWords = getContextWordLORIPs(info.term);
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;


            function addSnippets(contexts, divId, isMatch = true) {
                var meta = contexts.meta ? contexts.meta : '&nbsp;';
                var headClass = 'snippet_meta docLabel' + contexts.docLabel;
                var snippetClass = 'snippet docLabel' + contexts.docLabel;
                if (!isMatch) {
                    headClass = 'snippet_meta not_match docLabel' + contexts.docLabel;
                    snippetClass = 'snippet not_match docLabel' + contexts.docLabel;
                }
                d3.select(divId)
                    .append("div")
                    .attr('class', headClass)
                    .html(meta);
                contexts.snippets.forEach(function (snippet) {
                    d3.select(divId)
                        .append("div")
                        .attr('class', snippetClass)
                        .html(snippet);
                })
            }


            if (ignoreCategories) {
                divId = '#' + divName + '-' + 'cat';

                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                var numDocs = fullData.docs.texts.length.toLocaleString('en');
                var numMatches = allContexts.length;
                d3.select(divId)
                    .append("div")
                    .attr('class', 'topic_preview')
                    .attr('text-align', "center")
                    .html(
                        "Matched " + numMatches + " out of " + numDocs + ' documents: '
                        + (100 * numMatches / numDocs).toFixed(2) + '%'
                    );

                if (allContexts.length > 0) {
                    var headerClassName = 'text_header';
                    allContexts.forEach(function (singleDoc) {
                        addSnippets(singleDoc, divId);
                    });
                    if (includeAll) {
                        allNotMatches.forEach(function (singleDoc) {
                            addSnippets(singleDoc, divId, false);
                        });
                    }
                }

            } else if (unifiedContexts) {
                divId = '#' + divName + '-' + 'cat';
                var docLabelCounts = fullData.docs.labels.reduce(
                    function (map, label) {
                        map[label] = (map[label] || 0) + 1;
                        return map;
                    },
                    Object.create(null)
                );
                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                allContexts.forEach(function (singleDoc) {
                    numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel] || 0) + 1;
                });
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);

                /*contexts.forEach(function(context) {
                     context.forEach(function (singleDoc) {
                         numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel]||0) + 1;
                         addSnippets(singleDoc, divId);
                     });
                 });*/
                console.log("ORDERING !!!!!");
                console.log(fullData.info.category_name);
                console.log(sortDocLabelsByName);
                var docLabelCountsSorted = Object.keys(docLabelCounts).map(key => (
                    {
                        "label": fullData.docs.categories[key],
                        "labelNum": key,
                        "matches": numMatches[key] || 0,
                        "overall": docLabelCounts[key],
                        'percent': (numMatches[key] || 0) * 100. / docLabelCounts[key]
                    }))
                    .sort(function (a, b) {
                        if (highlightSelectedCategory) {
                            if (a['label'] === fullData.info.category_name) {
                                return -1;
                            }
                            if (b['label'] === fullData.info.category_name) {
                                return 1;
                            }
                        }
                        if (sortDocLabelsByName) {
                            return a['label'] < b['label'] ? 1 : a['label'] > b['label'] ? -1 : 0;
                        } else {
                            return b.percent - a.percent;
                        }
                    });
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted);
                console.log(numMatches)
                console.log('#' + divName + '-' + 'categoryinfo')
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                if (showCategoryHeadings) {
                    d3.select('#' + divName + '-' + 'categoryinfo').attr('display', 'inline');
                }

                function getCategoryStatsHTML(counts) {
                    return counts.matches + " document"
                        + (counts.matches == 1 ? "" : "s") + " out of " + counts.overall + ': '
                        + counts['percent'].toFixed(2) + '%';
                }

                function getCategoryInlineHeadingHTML(counts) {
                    return '<a name="' + divName + '-category'
                        + counts.labelNum + '"></a>'
                        + (ignoreCategories ? "" : counts.label + ": ") + "<span class=topic_preview>"
                        + getCategoryStatsHTML(counts)
                        + "</span>";
                }


                docLabelCountsSorted.forEach(function (counts) {

                    var htmlToAdd = "";
                    if (!ignoreCategories) {
                        htmlToAdd += "<b>" + counts.label + "</b>: " + getCategoryStatsHTML(counts);
                        ;
                    }

                    if (counts.matches > 0) {
                        var headerClassName = 'text_header';
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId)
                                .append('div')
                                .attr('class', 'separator')
                                .html("<b>Selected category</b>");
                        }
                        d3.select(divId)
                            .append("div")
                            .attr('class', headerClassName)
                            .html(getCategoryInlineHeadingHTML(counts));

                        allContexts
                            .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                            .forEach(function (singleDoc) {
                                addSnippets(singleDoc, divId);
                            });
                        if (includeAll) {
                            allNotMatches
                                .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                                .forEach(function (singleDoc) {
                                    addSnippets(singleDoc, divId, false);
                                });
                        }
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId).append('div').attr('class', 'separator').html("<b>End selected category</b>");
                            d3.select(divId).append('div').html("<br />");
                        }
                    }


                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'categoryinfo')
                            .attr('display', 'inline')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }

                })


            } else {
                var contextColumns = [
                    fullData.info.category_internal_name,
                    fullData.info.not_category_name
                ];
                if (showNeutral) {
                    if ('neutral_category_name' in fullData.info) {
                        contextColumns.push(fullData.info.neutral_category_name)
                    } else {
                        contextColumns.push("Neutral")
                    }
                    if (showExtra) {
                        if ('extra_category_name' in fullData.info) {
                            contextColumns.push(fullData.info.extra_category_name)
                        } else {
                            contextColumns.push("Extra")
                        }
                    }

                }
                contextColumns.map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        //var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var divId = null
                        if (fullData.info.category_internal_name == catName) {
                            divId = '#' + divName + '-' + 'cat'
                        } else if (fullData.info.not_category_name == catName) {
                            divId = '#' + divName + '-' + 'notcat'
                        } else if (fullData.info.neutral_category_name == catName) {
                            divId = '#' + divName + '-' + 'neut';
                        } else if (fullData.info.extra_category_name == catName) {
                            divId = '#' + divName + '-' + 'extra'
                        } else {
                            return;
                        }

                        var temp = d3.select(divId).selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            addSnippets(context, divId);
                        });
                        if (includeAll) {
                            notmatches[catIndex].forEach(function (context) {
                                addSnippets(context, divId, false);
                            });
                        }
                    }
                );
            }

            var obscuredTerms = getObscuredTerms(data, termInfo.info);
            displayObscuredTerms(obscuredTerms, data, info.term, info, '#' + divName + '-' + 'overlapped-terms-clicked');

            d3.select('#' + divName + '-' + 'termstats')
                .selectAll("div")
                .remove();
            var termHtml = 'Term: <b>' + formatTermForDisplay(info.term) + '</b>';
            if ('metalists' in fullData && info.term in fullData.metalists) {
                termHtml = 'Topic: <b>' + formatTermForDisplay(info.term) + '</b>';
            }
            if (getCustomTermHtml !== null) {
                termHtml = getCustomTermHtml(info);
            }
            d3.select('#' + divName + '-' + 'termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html(termHtml);
            if ('metalists' in fullData && info.term in fullData.metalists && topic_model_preview_size > 0) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Topic preview</b>: "
                        + fullData.metalists[info.term]
                            .slice(0, topic_model_preview_size)
                            .reduce(function (x, y) {
                                return x + ', ' + y
                            }));
            }
            if ('metadescriptions' in fullData && info.term in fullData.metadescriptions) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Description</b>: " + fullData.metadescriptions[info.term]);
            }
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;


            var numCatDocs = fullData.docs.labels
                .map(function (x) {
                    return (x == fullData.docs.categories.indexOf(
                        fullData.info.category_internal_name)) + 0
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });


            var numNCatDocs = fullData.docs.labels
                .map(function (x) {
                    return notCategoryNumList.indexOf(x) > -1
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            function getFrequencyDescription(name, count25k, count, ndocs) {
                var desc = name;
                if (!enableTermCategoryDescription) {
                    return desc + ':';
                }
                desc += ' frequency: <div class=text_subhead>' + count25k + ' per 25,000 terms</div>';
                if (!isNaN(Math.round(ndocs))) {
                    desc += '<div class=text_subhead>' + Math.round(ndocs) + ' per 1,000 docs</div>';
                }
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    if (!isNaN(Math.round(ndocs))) {
                        desc += '<u>Some of the ' + count + ' mentions:</u>';
                    } else {
                        desc += count + ' mentions';
                    }
                }
                /*
                desc += '<br><b>Discriminative:</b> ';

                desc += contextWords
                    .slice(cat_name === name ? 0 : contextWords.length - 3,
                        cat_name === name ? 3 : contextWords.length)
                    .filter(function (x) {
                        //return Math.abs(x[5]) > 1.96;
                        return true;
                    })
                    .map(function (x) {return x.join(', ')}).join('<br>');
                */
                return desc;
            }

            if (!unifiedContexts && !ignoreCategories) {
                console.log("NOT UNIFIED CONTEXTS")
                d3.select('#' + divName + '-' + 'cathead')
                    .style('fill', color(1))
                    .html(
                        getFrequencyDescription(cat_name,
                            info.cat25k,
                            info.cat,
                            termInfo.contexts[0].length * 1000 / numCatDocs
                        )
                    );
                d3.select('#' + divName + '-' + 'notcathead')
                    .style('fill', color(0))
                    .html(
                        getFrequencyDescription(ncat_name,
                            info.ncat25k,
                            info.ncat,
                            termInfo.contexts[1].length * 1000 / numNCatDocs)
                    );
                if (showNeutral) {
                    var numList = fullData.docs.categories.map(function (x, i) {
                        if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                            return i;
                        } else {
                            return -1;
                        }
                    }).filter(function (x) {
                        return x > -1
                    });

                    var numDocs = fullData.docs.labels
                        .map(function (x) {
                            return numList.indexOf(x) > -1
                        })
                        .reduce(function (a, b) {
                            return a + b;
                        }, 0);

                    d3.select("#" + divName + "-neuthead")
                        .style('fill', color(0))
                        .html(
                            getFrequencyDescription(fullData.info.neutral_category_name,
                                info.neut25k,
                                info.neut,
                                termInfo.contexts[2].length * 1000 / numDocs)
                        );

                    if (showExtra) {
                        var numList = fullData.docs.categories.map(function (x, i) {
                            if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                                return i;
                            } else {
                                return -1;
                            }
                        }).filter(function (x) {
                            return x > -1
                        });

                        var numDocs = fullData.docs.labels
                            .map(function (x) {
                                return numList.indexOf(x) > -1
                            })
                            .reduce(function (a, b) {
                                return a + b;
                            }, 0);

                        d3.select("#" + divName + "-extrahead")
                            .style('fill', color(0))
                            .html(
                                getFrequencyDescription(fullData.info.extra_category_name,
                                    info.extra25k,
                                    info.extra,
                                    termInfo.contexts[3].length * 1000 / numDocs)
                            );

                    }
                }
            } else if (unifiedContexts && !ignoreCategories) {
                // extra unified context code goes here
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted)

                docLabelCountsSorted.forEach(function (counts) {
                    var htmlToAdd = (ignoreCategories ? "" : "<b>" + counts.label + "</b>: ") + getCategoryStatsHTML(counts);
                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'contexts')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }
                })
            }
            if (jump) {
                if (window.location.hash === '#' + divName + '-' + 'snippets') {
                    window.location.hash = '#' + divName + '-' + 'snippetsalt';
                } else {
                    window.location.hash = '#' + divName + '-' + 'snippets';
                }
            }
        }

        function searchInText(d, includeAll = true) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function removeUnderScoreJoin(term) {
                /*
                '_ _asjdklf_jaksdlf_jaksdfl skld_Jjskld asdfjkl_sjkdlf'
                  ->
                "_ _asjdklf jaksdlf jaksdfl skld Jjskld asdfjkl_sjkdlf"
                 */
                return term.replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3");
            }

            function buildMatcher(term) {


                var boundary = '(?:\\W|^|$)';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                if (matchFullLine) {
                    boundary = '($|^)';
                }
                var termToRegex = term;


                // https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(string) {
                    return string.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\,\\\^\$\|\'#?]/g, "\\$&");
                    //return string.replace(/[\?#.*+^${}()|[\]\\]'\%/g, '\\$&'); // $& means the whole matched string
                }

                /*
                ['[', ']', '(', ')', '{', '}', '^', '$', '|', '?', '"',
                    '*', '+', '-', '=', '~', '`', '{'].forEach(function (a) {
                    termToRegex = termToRegex.replace(a, '\\\\' + a)
                });
                ['.', '#'].forEach(function(a) {termToRegex = termToRegex.replace(a, '\\' + a)})
                */
                termToRegex = escapeRegExp(termToRegex);
                console.log("termToRegex")
                console.log(termToRegex)

                var regexp = new RegExp(boundary + '('
                    + removeUnderScoreJoin(
                        termToRegex.replace(' ', wordSep, 'gim')
                    ) + ')' + boundary, 'gim');
                console.log(regexp);

                if (subwordEncoding === 'RoBERTa') {
                    if (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289) {
                        // Starts with character  indicating it's a word start
                        console.log("START")
                        regexp = new RegExp(boundary + escapeRegExp(term.substr(1, term.length)), 'gim');
                    } else {
                        regexp = new RegExp("\w" + escapeRegExp(term), 'gim');
                    }
                    console.log("SP")
                    console.log(regexp)
                }


                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], [], [], []];
            var notmatches = [[], [], [], []];
            var pattern = buildMatcher(d.term);
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            console.log('extraCategoryNumList')
            console.log(extraCategoryNumList);
            console.log("categoryNum");
            console.log(categoryNum);
            console.log("categoryNum");
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    //var numericLabel = 1 * (fullData.docs.categories[fullData.docs.labels[i]] != fullData.info.category_internal_name);

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }

                    var text = removeUnderScoreJoin(fullData.docs.texts[i]);
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': [], 'notsnippets': [], 'docLabel': docLabel};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }

                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[numericLabel].push(curMatch);
                    } else {
                        if (includeAll) {
                            curMatch.snippets = [
                                text.replace(/\n$/g, '\n\n')
                            ];
                            notmatches[numericLabel].push(curMatch);
                        }

                    }
                }
            }
            var toRet = {
                'contexts': matches,
                'notmatches': notmatches,
                'info': d,
                'docLabel': docLabel
            };
            return toRet;
        }

        function getDefaultTooltipContent(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            message += '<br/>score: ' + d.os.toFixed(5);
            return message;
        }

        function getDefaultTooltipContentWithoutScore(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            return message;
        }

        function getObscuredTerms(data, d) {
            //data = fullData['data']
            var matches = (data.filter(function (term) {
                    return term.x === d.x && term.y === d.y && (term.display === undefined || term.display === true);
                }).map(function (term) {
                    return formatTermForDisplay(term.term)
                }).sort()
            );
            return matches;
        }

        function showTooltip(data, d, pageX, pageY, showObscured = true) {
            deselectLastCircle();

            var obscuredTerms = getObscuredTerms(data, d);
            var message = '';
            console.log("!!!!! " + obscuredTerms.length)
            console.log(showObscured)
            if (obscuredTerms.length > 1 && showObscured)
                displayObscuredTerms(obscuredTerms, data, d.term, d);
            if (getTooltipContent !== null) {
                message += getTooltipContent(d);
            } else {
                if (sortByDist) {
                    message += getDefaultTooltipContentWithoutScore(d);
                } else {
                    message += getDefaultTooltipContent(d);
                }
            }
            pageX -= (svg.node().getBoundingClientRect().left) - origSVGLeft;
            pageY -= (svg.node().getBoundingClientRect().top) - origSVGTop;
            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX - 40) + "px")
                .style("top", (pageY - 85 > 0 ? pageY - 85 : 0) + "px");
            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            }).on('mouseout', function () {
                tooltip.transition().style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            var searchTerm = document
                .getElementById(this.divName + "-searchTerm")
                .value;
            handleSearchTerm(searchTerm);
            return false;
        };

        function highlightTerm(searchTerm, showObscured) {
            deselectLastCircle();
            var cleanedTerm = searchTerm.toLowerCase()
                .replace("'", " '")
                .trim();
            if (this.termDict[cleanedTerm] === undefined) {
                cleanedTerm = searchTerm.replace("'", " '").trim();
            }
            if (this.termDict[cleanedTerm] !== undefined) {
                showToolTipForTerm(this.data, this.svg, cleanedTerm, this.termDict[cleanedTerm], showObscured);
            }
            return cleanedTerm;
        }

        function handleSearchTerm(searchTerm, jump = false) {
            console.log("Handle search term.");
            console.log(searchTerm);
            console.log("this");
            console.log(this)
            highlighted = highlightTerm.call(this, searchTerm, true);
            console.log("found searchTerm");
            console.log(searchTerm);
            if (this.termDict[searchTerm] != null) {
                var runDisplayTermContexts = true;
                if (alternativeTermFunc != null) {
                    runDisplayTermContexts = this.alternativeTermFunc(this.termDict[searchTerm]);
                }
                if (runDisplayTermContexts) {
                    displayTermContexts(
                        this.data,
                        this.gatherTermContexts(this.termDict[searchTerm], this.includeAllContexts),
                        alwaysJump,
                        this.includeAllContexts
                    );
                }
            }
        }

        function getCircleForSearchTerm(mysvg, searchTermInfo) {
            var circle = mysvg;
            if (circle.tagName !== "circle") { // need to clean this thing up
                circle = mysvg._groups[0][searchTermInfo.ci];
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0].children !== undefined) {
                        circle = mysvg._groups[0].children[searchTermInfo.ci];
                    }
                }
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0][0].children !== undefined) {
                        circle = Array.prototype.filter.call(
                            mysvg._groups[0][0].children,
                            x => (x.tagName == "circle" && x.__data__['term'] == searchTermInfo.term)
                        )[0];
                    }
                }
                if ((circle === undefined || circle.tagName != 'circle') && mysvg._groups[0][0].children !== undefined) {
                    circle = mysvg._groups[0][0].children[searchTermInfo.ci];
                }
            }
            return circle;
        }

        function showToolTipForTerm(data, mysvg, searchTerm, searchTermInfo, showObscured = true) {
            //var searchTermInfo = termDict[searchTerm];
            console.log("showing tool tip")
            console.log(searchTerm)
            console.log(searchTermInfo)
            if (searchTermInfo === undefined) {
                console.log("can't show")
                d3.select("#" + divName + "-alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#" + divName + "-alertMessage").text("");
                var circle = getCircleForSearchTerm(mysvg, searchTermInfo);
                if (circle) {
                    var mySVGMatrix = circle.getScreenCTM().translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;
                    circle.style["stroke"] = "black";
                    //var circlePos = circle.position();
                    //var el = circle.node()
                    //showTooltip(searchTermInfo, pageX, pageY, circle.cx.baseVal.value, circle.cx.baseVal.value);
                    showTooltip(
                        data,
                        searchTermInfo,
                        pageX,
                        pageY,
                        showObscured
                    );

                    lastCircleSelected = circle;
                }

            }
        };


        function makeWordInteractive(data, svg, domObj, term, termInfo, showObscured = true) {
            return domObj
                .on("mouseover", function (d) {
                    showToolTipForTerm(data, svg, term, termInfo, showObscured);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    if (showObscured) {
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    }
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(termInfo);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(termInfo, includeAllContexts), alwaysJump, includeAllContexts);
                    }
                });
        }



        function processData(fullData) {

            modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            var data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                //!!!
                //termDict[x.term].i = i;
            });

            var padding = 0.1;
            if (showAxes || showAxesAndCrossHairs) {
                padding = 0.1;
            }

            // Scale the range of the data.  Add some space on either end.
            if (useGlobalScale) {
                var axisMax = Math.max(
                    d3.max(data, function (d) {
                        return d.x;
                    }),
                    d3.max(data, function (d) {
                        return d.y;
                    }),
                )
                var axisMin = Math.min(
                    d3.min(data, function (d) {
                        return d.x;
                    }),
                    d3.min(data, function (d) {
                        return d.y;
                    }),
                )
                axisMin = axisMin - (axisMax - axisMin) * padding;
                axisMax = axisMax + (axisMax - axisMin) * padding;
                x.domain([axisMin, axisMax]);
                y.domain([axisMin, axisMax]);
            } else {
                var xMax = d3.max(data, function (d) {
                    return d.x;
                });
                var yMax = d3.max(data, function (d) {
                    return d.y;
                })
                x.domain([-1 * padding, xMax + padding]);
                y.domain([-1 * padding, yMax + padding]);
            }

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            var axisRectHolder = new RectangleHolder();
            // Add the scatterplot
            data.forEach(function (d, i) {
                d.ci = i
            });

            //console.log('XXXXX'); console.log(data)


            function getFilter(data) {
                return data.filter(d => d.display === undefined || d.display === true);
            }


            var mysvg = svg
                .selectAll("dot")
                .data(getFilter(data))
                //.filter(function (d) {return d.display === undefined || d.display === true})
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (colorFunc) {
                        return colorFunc(d);
                    } else if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else if (d.p <= minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        if (d.term === "the") {
                            console.log("COLS " + d.s + " " + color(d.s) + " " + d.term)
                            console.log(d)
                            console.log(color)
                        }
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    /*var mySVGMatrix = circle.getScreenCTM()n
                        .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;*/

                    /*showTooltip(
                        d,
                        d3.event.pageX,
                        d3.event.pageY
                    );*/
                    console.log("point MOUSOEVER")
                    console.log(d)
                    showToolTipForTerm(data, this, d.term, d, true);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(d);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                    }
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    d3.select('#' + divName + '-' + 'overlapped-terms')
                        .selectAll('div')
                        .remove();
                })


            coords = Object();

            var pointStore = [];
            var pointRects = [];

            function censorPoints(datum, getX, getY) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(getX(datum)))
                    .attr("y", y(getY(datum)) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            function censorCircle(xCoord, yCoord) {
                var curLabel = svg.append("text")
                    .attr("x", x(xCoord))
                    .attr("y", y(yCoord) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            var configs = [
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': 10, 'alignment-baseline': 'ideographic'},

                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
            ];
            if (centerLabelsOverPoints) {
                configs = [{'anchor': 'middle', 'xoff': 0, 'yoff': 0, 'alignment-baseline': 'middle'}];
            }

            function labelPointsIfPossible(datum, myX, myY) {
                if (suppressTextColumn !== undefined
                    && datum.etc !== undefined
                    && datum.etc[suppressTextColumn] === true) {
                    return false;
                }

                var term = datum.term;
                if (datum.x > datum.y) {
                    configs.sort((a, b) => a.anchor == 'end' && b.anchor == 'end'
                        ? a.group - b.group : (a.anchor == 'end') - (b.anchor == 'end'));
                } else {
                    configs.sort((a, b) => a.anchor == 'start' && b.anchor == 'start'
                        ? a.group - b.group : (a.anchor == 'start') - (b.anchor == 'start'));
                }
                var matchedElement = null;

                var termColor = 'rgb(0,0,0)';
                if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                    termColor = datum.etc[textColorColumn];
                }
                term = formatTermForDisplay(term);

                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = svg.append("text")
                        //.attr("x", x(data[i].x) + config['xoff'])
                        //.attr("y", y(data[i].y) + config['yoff'])
                        .attr("x", x(myX) + config['xoff'])
                        .attr("y", y(myY) + config['yoff'])
                        .attr('class', 'label')
                        .attr('class', 'pointlabel')
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("text-anchor", config['anchor'])
                        .attr("alignment-baseline", config['alignment'])
                        .attr("fill", termColor)
                        .text(term);
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = doCensorPoints ? 0.5 : .25;

                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        curLabel = makeWordInteractive(data, svg, curLabel, term, datum);
                        break;
                    }
                }

                if (!matchedElement) {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    var labelRect = new Rectangle(x1, y1, x2, y2)
                    rectHolder.add(labelRect);
                    pointStore.push([x1, y1]);
                    pointStore.push([x2, y1]);
                    pointStore.push([x1, y2]);
                    pointStore.push([x2, y2]);
                    return {label: curLabel, rect: labelRect};
                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                if (b.bg === a.bg)
                    return (b.cat + b.ncat) - (a.cat + a.ncat);
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return -1;
                    if (!aGood && bGood) return 1;
                }
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return 1;
                    if (!aGood && bGood) return -1;
                }
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            var sortedData = data.map(x => x).sort(sortByDist ? euclideanDistanceSort : scoreSort);
            if (doCensorPoints) {
                for (var i in data) {
                    var d = sortedData[i];

                    if (!(censorPointColumn !== undefined
                        && d.etc !== undefined
                        && d.etc[censorPointColumn] === false)) {

                        censorPoints(
                            d,
                            function (d) {
                                return d.x
                            },
                            function (d) {
                                return d.y
                            }
                        );
                    }

                }
            }


            function registerFigureBBox(curLabel, axis = false) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var rect = new Rectangle(x1, y1, x2, y2)
                if (axis) {
                    axisRectHolder.add(rect)
                } else {
                    rectHolder.add(rect);
                }
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }

            function drawXLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "x label")
                    .attr("text-anchor", "end")
                    .attr("x", width)
                    .attr("y", height - 6)
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            function drawYLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "y label")
                    .attr("text-anchor", "end")
                    .attr("y", 6)
                    .attr("dy", ".75em")
                    .attr("transform", "rotate(-90)")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            d3.selection.prototype.moveToBack = function () {
                return this.each(function () {
                    var firstChild = this.parentNode.firstChild;
                    if (firstChild) {
                        this.parentNode.insertBefore(this, firstChild);
                    }
                });
            };

            if (verticalLines) {
                if (typeof (verticalLines) === "number") {
                    verticalLines = [verticalLines]; // r likes to make single element vectors doubles; this is a hackish workaround
                }
                for (i in verticalLines) {
                    svg.append("g")
                        .attr("transform", "translate(" + x(verticalLines[i]) + ", 1)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#dddddd")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (fullData['line'] !== undefined) {
                var valueline = d3.line()
                    .x(function (d) {
                        return x(d.x);
                    })
                    .y(function (d) {
                        return y(d.y);
                    });
                fullData.line = fullData.line.sort((a, b) => b.x - a.x);
                svg.append("path")
                    .attr("class", "line")
                    .style("stroke-width", "1px")
                    .attr("d", valueline(fullData['line'])).moveToBack();
            }
            if (showAxes || showAxesAndCrossHairs) {

                var myXAxis = svg.append("g")
                    .attr("class", "x axis")
                    .attr("transform", "translate(0," + height + ")")
                    .call(xAxis);

                //rangeTree = registerFigureBBox(myXAxis);


                var xLabel = drawXLabel(svg, getLabelText('x'));

                //console.log('xLabel');
                //console.log(xLabel);

                //rangeTree = registerFigureBBox(xLabel);
                // Add the Y Axis

                if (!yAxisValues) {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr("dx", "30px")
                        .attr("dy", "-13px")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("transform", "rotate(-90)");
                } else {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px');
                }
                registerFigureBBox(myYAxis, true);
                registerFigureBBox(myXAxis, true);

                function getLabelText(axis) {
                    if (axis == 'y') {
                        if (yLabelText == null)
                            return modelInfo['category_name'] + " Frequency";
                        else
                            return yLabelText;
                    } else {
                        if (xLabelText == null)
                            return modelInfo['not_category_name'] + " Frequency";
                        else
                            return xLabelText;
                    }
                }

                var yLabel = drawYLabel(svg, getLabelText('y'))

            }

            if (!showAxes || showAxesAndCrossHairs) {
                horizontal_line_y_position_translated = 0.5;
                if (horizontal_line_y_position !== null) {
                    var loOy = null, hiOy = null, loY = null, hiY = null;
                    for (i in fullData.data) {
                        var curOy = fullData.data[i].oy;
                        if (curOy < horizontal_line_y_position && (curOy > loOy || loOy === null)) {
                            loOy = curOy;
                            loY = fullData.data[i].y
                        }
                        if (curOy > horizontal_line_y_position && (curOy < hiOy || hiOy === null)) {
                            hiOy = curOy;
                            hiY = fullData.data[i].y
                        }
                    }
                    horizontal_line_y_position_translated = loY + (hiY - loY) / 2.
                    if (loY === null) {
                        horizontal_line_y_position_translated = 0;
                    }
                }
                if (vertical_line_x_position === null) {
                    vertical_line_x_position_translated = 0.5;
                } else {
                    if (vertical_line_x_position !== null) {
                        var loOx = null, hiOx = null, loX = null, hiX = null;
                        for (i in fullData.data) {
                            var curOx = fullData.data[i].ox;
                            if (curOx < vertical_line_x_position && (curOx > loOx || loOx === null)) {
                                loOx = curOx;
                                loX = fullData.data[i].x;
                            }
                            if (curOx > vertical_line_x_position && (curOx < hiOx || hiOx === null)) {
                                hiOx = curOx;
                                hiX = fullData.data[i].x
                            }
                        }
                        vertical_line_x_position_translated = loX + (hiX - loX) / 2.
                        if (loX === null) {
                            vertical_line_x_position_translated = 0;
                        }
                    }
                }
                if (showCrossAxes) {
                    var x_line = svg.append("g")
                        .attr("transform", "translate(0, " + y(horizontal_line_y_position_translated) + ")")
                        .append("line")
                        .attr("x2", width)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                    var y_line = svg.append("g")
                        .attr("transform", "translate(" + x(vertical_line_x_position_translated) + ", 0)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (showDiagonal) {
                var diagonal = svg.append("g")
                    .append("line")
                    .attr("x1", 0)
                    .attr("y1", height)
                    .attr("x2", width)
                    .attr("y2", 0)
                    .style("stroke-dasharray", "5,5")
                    .style("stroke", "#cccccc")
                    .style("stroke-width", "1px")
                    .moveToBack();
            }

            function showWordList(word, termDataList, xOffset=null) {
                var maxWidth = word.node().getBBox().width;
                var wordObjList = [];
                for (var i in termDataList) {
                    var datum = termDataList[i];
                    var curTerm = datum.term;
                    word = (function (word, curTerm) {
                        var termColor = 'rgb(0,0,0)';
                        if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                            termColor = datum.etc[textColorColumn];
                        }
                        console.log("Show WORD "); console.log(word.node().getBBox().x)
                        var curWordPrinted = svg.append("text")
                            .attr("text-anchor", "start")
                            .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                            .attr('font-size', '12px')
                            .attr("fill", termColor)
                            .attr("x", xOffset == null ? word.node().getBBox().x : xOffset)
                            .attr("y", word.node().getBBox().y
                                + 2 * word.node().getBBox().height)
                            .text(formatTermForDisplay(curTerm));
                        wordObjList.push(curWordPrinted)
                        return makeWordInteractive(
                            termDataList, //data,
                            svg,
                            curWordPrinted,
                            curTerm,
                            termDataList[i]);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth,
                    'wordObjList': wordObjList
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(isTopPane) {
                console.log("PICK SCORE ALGO")
                console.log(isTopPane)
                if (isTopPane === true) {
                    if (headerSortingAlgos !== null && headerSortingAlgos['upper'] !== undefined)
                        return headerSortingAlgos['upper'];
                    return scoreSortForCategory;
                } else {
                    if (headerSortingAlgos !== null && headerSortingAlgos['lower'] !== undefined)
                        return headerSortingAlgos['lower'];
                    return scoreSortForNotCategory;
                }

            }

            function pickTermSortingAlgorithm(isUpperPane) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(isUpperPane);
                return pickScoreSortAlgo(isUpperPane);
            }

            function showAssociatedWordList(data, word, header, isUpperPane, xOffset, length = topTermsLength) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isUpperPane);
                console.log("showAssociatedWordList");
                console.log(header);
                console.log("WORD");
                console.log(word)
                sortedData = data.filter(term => (term.display === undefined || term.display === true)).sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isUpperPane)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(word, sortedData.slice(0, length), xOffset);

            }

            var characteristicXOffset = width;

            function showCatHeader(startingOffset, catName, registerFigureBBox) {
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset //width
                    )
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(catName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                return catHeader;
            }

            function showNotCatHeader(startingOffset, word, notCatName) {
                console.log("showNotCatHeader")
                return svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("y", word.node().getBBox().y + 3 * word.node().getBBox().height)
                    .text(notCatName);
            }

            function showTopTermsPane(data,
                                      registerFigureBBox,
                                      showAssociatedWordList,
                                      upperHeaderName,
                                      lowerHeaderName,
                                      startingOffset) {
                data = data.filter(term => (term.display === undefined || term.display === true));
                //var catHeader = showCatHeader(startingOffset, catName, registerFigureBBox);
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(upperHeaderName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                var word = catHeader;
                var wordListData = showAssociatedWordList(data, word, catHeader, true, startingOffset);
                word = wordListData.word;
                var maxWidth = wordListData.maxWidth;

                var notCatHeader = showNotCatHeader(startingOffset, word, lowerHeaderName);
                word = notCatHeader;
                characteristicXOffset = Math.max(
                    catHeader.node().getBBox().x + maxWidth + 10,
                    notCatHeader.node().getBBox().x + maxWidth + 10
                )
                console.log("characteristicXOffset", characteristicXOffset)
                console.log(catHeader.node().getBBox().x + maxWidth + 10)
                console.log(notCatHeader.node().getBBox().x + maxWidth + 10)

                var notWordListData = showAssociatedWordList(data, word, notCatHeader, false, startingOffset);
                word = wordListData.word;
                if (wordListData.maxWidth > maxWidth) {
                    maxWidth = wordListData.maxWidth;
                }
                return {
                    wordListData, notWordListData,
                    word, maxWidth, characteristicXOffset, startingOffset,
                    catHeader, notCatHeader, registerFigureBBox
                };
            }

            var payload = Object();
            if (showTopTerms) {
                var upperHeaderName = "Top " + fullData['info']['category_name'];
                var lowerHeaderName = "Top " + fullData['info']['not_category_name'];
                if (headerNames !== null) {
                    if (headerNames.upper !== undefined)
                        upperHeaderName = headerNames.upper;
                    if (headerNames.lower !== undefined)
                        lowerHeaderName = headerNames.lower;
                }
                payload.topTermsPane = showTopTermsPane(
                    data,
                    registerFigureBBox,
                    showAssociatedWordList,
                    upperHeaderName,
                    lowerHeaderName,
                    width + topTermsLeftBuffer
                );
                payload.showTopTermsPane = showTopTermsPane;
                payload.showAssociatedWordList = showAssociatedWordList;
                payload.showWordList = showWordList;

                /*var wordListData = topTermsPane.wordListData;
                var word = topTermsPane.word;
                var maxWidth = topTermsPane.maxWidth;
                var catHeader = topTermsPane.catHeader;
                var notCatHeader = topTermsPane.notCatHeader;
                var startingOffset = topTermsPane.startingOffset;*/
                characteristicXOffset = payload.topTermsPane.characteristicXOffset;
            }


            if ((!nonTextFeaturesMode && !asianMode && showCharacteristic)
                || (headerNames !== null && headerNames.right !== undefined)) {
                var sortMethod = backgroundScoreSort;
                var title = 'Characteristic';
                if (headerNames !== null && headerNames.right !== undefined) {
                    title = headerNames.right;
                }
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                    sortMethod = scoreSortReverse;
                } else if (data.reduce(function (a, b) {
                    return a + b.bg
                }, 0) === 0) {
                    title = 'Most frequent';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", characteristicXOffset)
                    .attr("dy", "6px")
                    .text(title);

                var rightSortMethod = sortMethod;
                if (rightOrderColumn !== undefined && rightOrderColumn !== null) {
                    rightSortMethod = ((a, b) => b.etc[rightOrderColumn] - a.etc[rightOrderColumn]);
                }

                var wordListData = showWordList(
                    word,
                    data.filter(term => (term.display === undefined || term.display === true))
                        .sort(rightSortMethod).slice(0, topTermsLength * 2 + 2),
                    characteristicXOffset
                );

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            function performPartialLabeling(
                data,
                existingLabels,
                getX,
                getY,
                labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            ) {
                for (i in existingLabels) {
                    rectHolder.remove(existingLabels[i].rect);
                    existingLabels[i].label.remove();
                }

                var labeledPoints = [];

                //var filteredData = data.filter(d=>d.display === undefined || d.display === true);
                //for (var i = 0; i < filteredData.length; i++) {
                data.sort(labelPriorityFunction).forEach(function (datum, i) {
                    //console.log(datum.i, datum.ci, i)
                    //var label = labelPointsIfPossible(i, getX(filteredData[i]), getY(filteredData[i]));
                    if (datum.display === undefined || datum.display === true) {
                        var label = labelPointsIfPossible(datum, getX(datum), getY(datum));
                        if (label !== false) {
                            //console.log("labeled")
                            labeledPoints.push(label)
                        }
                    }
                    //if (labelPointsIfPossible(i), true) numPointsLabeled++;
                })
                return labeledPoints;
            }

            //var labeledPoints = performPartialLabeling();
            var labeledPoints = [];
            var labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            if (labelPriorityColumn !== undefined && labelPriorityColumn !== null) {
                labelPriorityFunction = (a, b) => b.etc[labelPriorityColumn] - a.etc[labelPriorityColumn];
            }

            labeledPoints = performPartialLabeling(
                data,
                labeledPoints,
                function (d) {
                    return d.x
                },
                function (d) {
                    return d.y
                },
                labelPriorityFunction
            );

            if (backgroundLabels !== null) {
                backgroundLabels.map(
                    function (label) {
                        svg.append("text")
                            .attr("x", x(label.X))
                            .attr("y", y(label.Y))
                            .attr("text-anchor", "middle")
                            .style("font-size", "30")
                            .style("fill", "rgb(200,200,200)")
                            .text(label.Text)
                            .lower()
                            .on('mouseover', function (d) {
                                d3.select(this).style('stroke', 'black').style('stroke-width', '1px').raise()
                            })
                            .on('mouseout', function (d) {
                                d3.select(this).style('stroke-width', '0px').style('fill', 'rgb(200,200,200)').lower()
                            })
                    }
                )
            }


            /*
            // pointset has to be sorted by X
            function convex(pointset) {
                function _cross(o, a, b) {
                    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0]);
                }

                function _upperTangent(pointset) {
                    var lower = [];
                    for (var l = 0; l < pointset.length; l++) {
                        while (lower.length >= 2 && (_cross(lower[lower.length - 2], lower[lower.length - 1], pointset[l]) <= 0)) {
                            lower.pop();
                        }
                        lower.push(pointset[l]);
                    }
                    lower.pop();
                    return lower;
                }

                function _lowerTangent(pointset) {
                    var reversed = pointset.reverse(),
                        upper = [];
                    for (var u = 0; u < reversed.length; u++) {
                        while (upper.length >= 2 && (_cross(upper[upper.length - 2], upper[upper.length - 1], reversed[u]) <= 0)) {
                            upper.pop();
                        }
                        upper.push(reversed[u]);
                    }
                    upper.pop();
                    return upper;
                }

                var convex,
                    upper = _upperTangent(pointset),
                    lower = _lowerTangent(pointset);
                convex = lower.concat(upper);
                convex.push(pointset[0]);
                return convex;
            }

            console.log("POINTSTORE")
            console.log(pointStore);
            pointStore.sort();
            var convexHull = convex(pointStore);
            var minX = convexHull.sort(function (a,b) {
                return a[0] < b[0] ? -1 : 1;
            })[0][0];
            var minY = convexHull.sort(function (a,b) {
                return a[1] < b[1] ? -1 : 1;
            })[0][0];
            //svg.append("text").text("BLAH BLAH").attr("text-anchor", "middle").attr("cx", x(0)).attr("y", minY);
            console.log("POINTSTORE")
            console.log(pointStore);
            console.log(convexHull);
            for (i in convexHull) {
                var i = parseInt(i);
                if (i + 1 == convexHull.length) {
                    var nextI = 0;
                } else {
                    var nextI = i + 1;
                }
                console.log(i, ',', nextI);
                svg.append("line")
                    .attr("x2", width)
                    .style("stroke", "#cc0000")
                    .style("stroke-width", "1px")
                    .attr("x1", convexHull[i][0])     // x position of the first end of the line
                    .attr("y1", convexHull[i][1])      // y position of the first end of the line
                    .attr("x2", convexHull[nextI][0])     // x position of the second end of the line
                    .attr("y2", convexHull[nextI][1]);    // y position of the second end of the line
            }*/

            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?\-!]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    var name = null;
                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt;
                    } else {
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt
                        }
                    }
                    //!!!

                });
                fullData.docs.labels.forEach(function (x) {

                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                    } else {
                        var name = null;
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                        }
                    }
                });
                console.log("docCounts");
                console.log(docCounts)
                var messages = [];
                if (ignoreCategories) {
                    var wordCount = getCorpusWordCounts();
                    console.log("wordCount")
                    console.log(wordCount)
                    messages.push(
                        '<b>Document count: </b>' + fullData.docs.texts.length.toLocaleString('en') +
                        '; <b>word count: </b>'
                        + wordCount['sums'].reduce((a, b) => a + b, 0).toLocaleString('en')
                    )
                } else if (unifiedContexts) {
                    fullData.docs.categories.forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            var message = '<b>' + x + '</b>: ';
                            message += 'document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en')
                            messages.push(message);
                        }
                    });
                } else {
                    [fullData.info.category_name,
                        fullData.info.not_category_name,
                        fullData.info.neutral_category_name,
                        fullData.info.extra_category_name].forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            messages.push('<b>' + x + '</b> document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en'));
                        }
                    });
                }

                if (showCorpusStats) {
                    d3.select('#' + divName + '-' + 'corpus-stats')
                        .style('width', width + margin.left + margin.right + 200)
                        .append('div')
                        .html(messages.join('<br />'));
                }
            }


            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById(divName);

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }

            function rerender(xCoords, yCoords, color) {
                labeledPoints.forEach(function (p) {
                    p.label.remove();
                    rectHolder.remove(p.rect);
                });
                pointRects.forEach(function (rect) {
                    rectHolder.remove(rect);
                });
                pointRects = []
                /*
                var circles = d3.select('#' + divName).selectAll('circle')
                    .attr("cy", function (d) {return y(yCoords[d.i])})
                    .transition(0)
                    .attr("cx", function (d) {return x(xCoords[d.i])})
                    .transition(0);
                */
                d3.select('#' + divName).selectAll("dot").remove();
                d3.select('#' + divName).selectAll("circle").remove();
                console.log(this.fullData)
                console.log(this)
                console.log("X/Y coords")
                console.log(this.fullData.data.filter(d => d.display === undefined || d.display === true).map(d => [d.x, d.y]))
                var circles = this.svg//.select('#' + divName)
                    .selectAll("dot")
                    .data(this.fullData.data.filter(d => d.display === undefined || d.display === true))
                    //.filter(function (d) {return d.display === undefined || d.display === true})
                    .enter()
                    .append("circle")
                    .attr("cy", d => d.y)
                    .attr("cx", d => d.x)
                    .attr("r", d => 2)
                    .on("mouseover", function (d) {
                        /*var mySVGMatrix = circle.getScreenCTM()n
                            .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                        var pageX = mySVGMatrix.e;
                        var pageY = mySVGMatrix.f;*/

                        /*showTooltip(
                            d,
                            d3.event.pageX,
                            d3.event.pageY
                        );*/
                        console.log("point MOUSOEVER")
                        console.log(d)
                        showToolTipForTerm(data, this, d.term, d, true);
                        d3.select(this).style("stroke", "black");
                    })
                    .on("click", function (d) {
                        var runDisplayTermContexts = true;
                        if (alternativeTermFunc != null) {
                            runDisplayTermContexts = alternativeTermFunc(d);
                        }
                        if (runDisplayTermContexts) {
                            displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                        }
                    })
                    .on("mouseout", function (d) {
                        tooltip.transition()
                            .duration(0)
                            .style("opacity", 0);
                        d3.select(this).style("stroke", null);
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    });

                if (color !== null) {
                    console.log("COLOR")
                    console.log(color)
                    circles.style("fill", d => color(d));
                }
                xCoords.forEach((xCoord, i) => censorCircle(xCoord, yCoords[i]))
                labeledPoints = [];
                labeledPoints = performPartialLabeling(
                    this.fullData.data,
                    labeledPoints,
                    (d => d.ox), //function (d) {return xCoords[d.ci]},
                    (d => d.oy) //function (d) {return yCoords[d.ci]}

                );
            }

            //return [performPartialLabeling, labeledPoints];
            return {
                ...payload,
                ...{
                    'rerender': rerender,
                    'performPartialLabeling': performPartialLabeling,
                    'showToolTipForTerm': showToolTipForTerm,
                    'svg': svg,
                    'data': data,
                    'xLabel': xLabel,
                    'yLabel': yLabel,
                    'drawXLabel': drawXLabel,
                    'drawYLabel': drawYLabel,
                    'populateCorpusStats': populateCorpusStats
                }
            };
        }


        //fullData = getDataAndInfo();
        if (fullData.docs) {
            var corpusWordCounts = getCorpusWordCounts();
        }
        var payload = processData(fullData);

        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            //.attr("class", getTooltipContent == null && sortByDist ? "tooltip" : "tooltipscore")
            .attr("class", "tooltipscore")
            .style("opacity", 0);

        plotInterface = {}
        if (payload.topTermsPane) {
            plotInterface.topTermsPane = payload.topTermsPane;
            plotInterface.showTopTermsPane = payload.showTopTermsPane;
            plotInterface.showAssociatedWordList = payload.showAssociatedWordList;
        }
        plotInterface.includeAllContexts = includeAllContexts;
        plotInterface.divName = divName;
        plotInterface.displayTermContexts = displayTermContexts;
        plotInterface.gatherTermContexts = gatherTermContexts;
        plotInterface.xLabel = payload.xLabel;
        plotInterface.yLabel = payload.yLabel;
        plotInterface.drawXLabel = payload.drawXLabel;
        plotInterface.drawYLabel = payload.drawYLabel;
        plotInterface.svg = payload.svg;
        plotInterface.termDict = termDict;
        plotInterface.showToolTipForTerm = payload.showToolTipForTerm;
        plotInterface.fullData = fullData;
        plotInterface.data = payload.data;
        plotInterface.rerender = payload.rerender;
        plotInterface.populateCorpusStats = payload.populateCorpusStats;
        plotInterface.handleSearch = handleSearch;
        plotInterface.handleSearchTerm = handleSearchTerm;
        plotInterface.highlightTerm = highlightTerm;
        plotInterface.y = y;
        plotInterface.x = x;
        plotInterface.tooltip = tooltip;
        plotInterface.alternativeTermFunc = alternativeTermFunc;

        plotInterface.showTooltipSimple = function (term) {
            plotInterface.showToolTipForTerm(
                plotInterface.data,
                plotInterface.svg,
                term.replace("'", "\\'"),
                plotInterface.termDict[term.replace("'", "\\'")]
            )
        };

        plotInterface.drawCategoryAssociation = function (category, otherCategory = null) {
            console.log("+++++++ Entering drawCategoryAssociation")
            console.log("Category: " + category)
            console.log("Other Category: " + otherCategory)
            var categoryNum = this.fullData.info.categories.indexOf(category);

            var otherCategoryNum = null;
            if(otherCategory !== null)
                otherCategoryNum = this.fullData.info.categories.indexOf(otherCategory);

            console.log("cat/other: " + category + "/" + otherCategory + " ::: " + categoryNum + "/" + otherCategoryNum)

            console.log("Full Data")
            console.log(this.fullData)
            /*
            var rawLogTermCounts = getTermCounts(this.fullData).map(Math.log);
            var maxRawLogTermCounts = Math.max(...rawLogTermCounts);
            var minRawLogTermCounts = Math.min(...rawLogTermCounts);
            var logTermCounts = rawLogTermCounts.map(
                x => (x - minRawLogTermCounts) / maxRawLogTermCounts
            )
            */

            //var rawScores = getCategoryDenseRankScores(this.fullData, categoryNum);
            //console.log("RAW SCORES")
            //console.log(rawScores);
            /*
            function logOddsRatioUninformativeDirichletPrior(fgFreqs, bgFreqs, alpha) {
                var fgVocabSize = fgFreqs.reduce((x,y) => x+y);
                var fgL = fgFreqs.map(x => (x + alpha)/((1+alpha)*fgVocabSize - x - alpha))
                var bgVocabSize = bgFreqs.reduce((x,y) => x+y);
                var bgL = bgFreqs.map(x => (x + alpha)/((1+alpha)*bgVocabSize - x - alpha))
                var pooledVar = fgFreqs.map(function(x, i) {
                    return (
                        1/(x + alpha)
                        + 1/((1+alpha)*fgVocabSize - x - alpha)
                        + 1/(bgFreqs[i] + alpha)
                        + 1/((1+alpha)*bgVocabSize - bgFreqs[i] - alpha))
                })
                return pooledVar.map(function(x, i) {
                    return (Math.log(fgL[i]) - Math.log(bgL[i]))/x;
                })
            }
            var rawScores = logOddsRatioUninformativeDirichletPrior(
                denseRanks.fgFreqs, denseRanks.bgFreqs, 0.01);
            */


            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            if (otherCategoryNum !== null) {
                var otherDenseRanks = getDenseRanks(this.fullData, otherCategoryNum);
                denseRanks.bg = otherDenseRanks.fg;
                denseRanks.bgFreqs = otherDenseRanks.fgFreqs;
            }

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            //!!! OLD and good
            var ox = denseRanks.bg;
            var oy = denseRanks.fg;

            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            var ox = ox.map(x => (x - oxmin) / (oxmax - oxmin))
            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            var oy = oy.map(x => (x - oymin) / (oymax - oymin))
            //var ox = logTermCounts
            //var oy = scores;
            var xf = this.x;
            var yf = this.y;

            this.fullData.data = this.fullData.data.map(function (term, i) {
                //term.ci = i;
                term.s = scores[term.i];
                term.os = rawScores[term.i];
                term.cat = denseRanks.fgFreqs[term.i];
                term.ncat = denseRanks.bgFreqs[term.i];
                term.cat25k = parseInt(denseRanks.fgFreqs[term.i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[term.i] * 25000 / bgFreqSum);
                term.x = xf(ox[term.i]) // logTermCounts[term.i];
                term.y = yf(oy[term.i]) // scores[term.i];
                term.ox = ox[term.i];
                term.oy = oy[term.i];
                term.display = true;
                return term;
            })

            // Feature selection
            var targetTermsToShow = 1500;

            var sortedBg = denseRanks.bg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedFg = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedScores = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]);
            var myFullData = this.fullData

            sortedBg.concat(sortedFg)//.concat(sortedScores.slice(0, parseInt(targetTermsToShow/2))).concat(sortedScores.slice(-parseInt(targetTermsToShow/4)))
                .forEach(function (i) {
                    myFullData.data[i].display = true;
                })

            console.log('newly filtered')
            console.log(myFullData)

            // begin rescaling to ignore hidden terms
            /*
            function scaleDenseRanks(ranks) {
                var max = Math.max(...ranks);
                return ranks.map(x=>x/max)
            }
            var filteredData = myFullData.data.filter(d=>d.display);
            var catRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.cat)))
            var ncatRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.ncat)))
            var rawScores = catRanks.map((x,i) => x - ncatRanks[i]);
            function stretch_0_1(scores) {
                var max = 1.*Math.max(...rawScores);
                var min = -1.*Math.min(...rawScores);
                return scores.map(function(x, i) {
                    if(x == 0) return 0.5;
                    if(x > 0) return (x/max + 1)/2;
                    return (x/min + 1)/2;
                })
            }
            var scores = stretch_0_1(rawScores);
            console.log(scores)
            filteredData.forEach(function(d, i) {
                d.x = xf(catRanks[i]);
                d.y = yf(ncatRanks[i]);
                d.ox = catRanks[i];
                d.oy = ncatRanks[i];
                d.s = scores[i];
                d.os = rawScores[i];
            });
            console.log("rescaled");
            */
            // end rescaling


            this.rerender(//denseRanks.bg,
                fullData.data.map(x => x.ox), //ox
                //denseRanks.fg,
                fullData.data.map(x => x.oy), //oy,
                d => d3.interpolateRdYlBu(d.s));
            if (this.yLabel !== undefined) {
                this.yLabel.remove()
            }
            if (this.xLabel !== undefined) {
                this.xLabel.remove()
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];
            if (otherCategoryNum !== null) {
                bottomName = this.fullData.info.categories[otherCategoryNum];
            }


            this.yLabel = this.drawYLabel(this.svg, leftName + ' Frequncy Rank')
            this.xLabel = this.drawXLabel(this.svg, bottomName + ' Frequency Rank')
            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (
                data,
                word,
                header,
                isUpperPane,
                xOffset=this.topTermsPane.startingOffset,
                length = 14
            ) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            if (otherCategoryNum === null) {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x !== this.fullData.info.categories[categoryNum]);
            } else {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x === this.fullData.info.categories[otherCategoryNum]);

                fullData.info.neutral_category_internal_names = this.fullData.info.categories
                    .filter(x => (x !== this.fullData.info.categories[categoryNum]
                        && x !== this.fullData.info.categories[otherCategoryNum]));
                fullData.info.neutral_category_name = "All Others";

            }
            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();

            console.log(fullData)
        };

        plotInterface.yAxisLogCounts = function (categoryName) {
            var categoryNum = this.fullData.docs.categories.indexOf(categoryName);
            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            console.log("denseRanks")
            console.log(denseRanks);

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            var oy = denseRanks.fgFreqs.map(count => Math.log(count + 1) / Math.log(2))

            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            oy = oy.map(y => (y - oymin) / (oymax - oymin))
            var xf = this.x;
            var yf = this.y;
            var ox = this.fullData.data.map(term => term.ox);
            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            ox = ox.map(y => (y - oxmin) / (oxmax - oxmin))


            this.fullData.data = this.fullData.data.map(function (term, i) {
                term.s = 1;//scores[i];
                term.os = rawScores[i];
                term.cat = denseRanks.fgFreqs[i];
                term.ncat = denseRanks.bgFreqs[i];
                term.cat25k = parseInt(denseRanks.fgFreqs[i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[i] * 25000 / bgFreqSum);
                //term.x = xf(term.ox) // scores[term.i];
                //term.ox = term.ox;
                term.y = yf(oy[i]) // scores[term.i];
                term.oy = oy[i];
                term.x = xf(ox[i]) // scores[term.i];
                term.ox = ox[i];
                term.display = true;
                return term;
            })


            this.rerender(//denseRanks.bg,
                this.fullData.data.map(point => point.ox), //ox
                this.fullData.data.map(point => point.oy), //oy,
                d => d3.interpolateRdYlBu(d.s)
            );

            if (this.yLabel !== undefined) {
                this.yLabel.remove()
                this.yLabel = this.drawYLabel(this.svg, this.fullData.info.categories[categoryNum] + ' log freq.')
            }

            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (data, word, header, isUpperPane, xOffset=this.topTermsPane.startingOffset, length = 14) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];

            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            fullData.info.not_category_internal_names = this.fullData.info.categories
                .filter(x => x !== this.fullData.info.categories[categoryNum]);

            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();
        };

        return plotInterface
    };
}(d3);

; 
 
 // Adapted from https://www.w3schools.com/howto/howto_js_autocomplete.asp
function autocomplete(inputField, autocompleteValues, myPlotInterface) {
    var currentFocus; // current position in autocomplete list.

    inputField.addEventListener("input", function (e) {
        var matchedCandidateListDiv, matchedCandidateDiv, i, userInput = this.value;

        closeAllLists();
        if (!userInput) {
            return false;
        }
        currentFocus = -1;

        matchedCandidateListDiv = document.createElement("div");
        matchedCandidateListDiv.setAttribute("id", this.id + "autocomplete-list");
        matchedCandidateListDiv.setAttribute("class", "autocomplete-items");

        this.parentNode.appendChild(matchedCandidateListDiv);
        autocompleteValues.map(function (candidate) {
            var candidatePrefix = candidate.substr(0, userInput.length);
            if (candidatePrefix.toLowerCase() === userInput.toLowerCase()) {
                matchedCandidateDiv = document.createElement("div");
                matchedCandidateDiv.innerHTML = "<strong>" + candidatePrefix + "</strong>";
                matchedCandidateDiv.innerHTML += candidate.substr(userInput.length);
                matchedCandidateDiv.innerHTML += '<input type=hidden value="' + encodeURIComponent(candidate) + '">';
                matchedCandidateDiv.addEventListener("click", function (e) {
                    console.log("CLICK")
                    console.log(this.getElementsByTagName("input")[0].value)
                    inputField.value = decodeURIComponent(this.getElementsByTagName("input")[0].value);
                    console.log(inputField.value)
                    closeAllLists();
                    myPlotInterface.handleSearchTerm(inputField.value);
                });
                matchedCandidateListDiv.appendChild(matchedCandidateDiv);
            }
        });
    });

    inputField.addEventListener("keydown", function (keyboardEvent) {

        var candidateDivList = document.getElementById(this.id + "autocomplete-list");

        if (!candidateDivList)
            return true;

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList.children,
            x => x.className !== ""
        );

        if (keyboardEvent.keyCode === 40 || keyboardEvent.keyCode === 9) { // down or tab
            keyboardEvent.preventDefault();
            currentFocus++;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 38) { //up
            currentFocus--;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 13) { // enter
            keyboardEvent.preventDefault();
            var selectedTerm = inputField.value;
            console.log("selected term");console.log(selectedTerm);
            console.log(myPlotInterface);
            //if (selectedCandidate)
            //    selectedTerm = selectedCandidate.children[1].value;
            myPlotInterface.handleSearchTerm(selectedTerm);
            closeAllLists(null);
        } else if (keyboardEvent.keyCode === 27) { // esc
            closeAllLists(null);
        }
    });

    function addActive(candidateDivList) {
        if (!candidateDivList) return false;

        removeActive(candidateDivList);

        if (currentFocus >= candidateDivList.length)
            currentFocus = 0;
        if (currentFocus < 0)
            currentFocus = (candidateDivList.length - 1);

        candidateDivList[currentFocus].classList.add("autocomplete-active");

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList,
            x => x.className !== ""
        );

        if (selectedCandidate) {
            var candidateValue = decodeURIComponent(selectedCandidate.children[1].value);

            myPlotInterface.highlightTerm(candidateValue);
            inputField.value = candidateValue;
        }

    }

    function removeActive(candidateDivList) {
        Array.prototype.find.call(
            candidateDivList,
            x => x.classList.remove("autocomplete-active")
        );
    }

    function closeAllLists(elmnt) {
        /*close all autocomplete lists in the document,
        except the one passed as an argument:*/
        var x = document.getElementsByClassName("autocomplete-items");
        for (var i = 0; i < x.length; i++) {
            if (elmnt != x[i] && elmnt != inputField) {
                x[i].parentNode.removeChild(x[i]);
            }
        }
    }

    /*execute a function when someone clicks in the document:*/
    document.addEventListener("click", function (e) {
        closeAllLists(e.target);
    });
}

function getDataAndInfo() { return{"info": {"category_name": "Data scientist", "not_category_name": "Data Engineer", "category_terms": ["statistical", "research", "statistics", "ml", "ai", "algorithms", "predictive", "techniques", "analyze", "results"], "not_category_terms": ["etl", "pipelines", "cloud", "aws", "services", "kafka", "infrastructure", "integration", "warehouse", "platform"], "category_internal_name": "data scientist", "not_category_internal_names": ["data engineer"], "categories": ["data scientist", "data engineer"], "neutral_category_internal_names": [], "extra_category_internal_names": [], "neutral_category_name": "Neutral", "extra_category_name": "Extra"}, "data": [{"x": 0.3071895424836601, "y": 0.46853146853146843, "ox": 0.3071895424836601, "oy": 0.46853146853146843, "term": "lead", "cat25k": 48, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 47, "s": 0.9314516129032259, "os": 0.16053391053391053, "bg": 3.3622777695198937e-06}, {"x": 0.11764705882352941, "y": 0.11888111888111888, "ox": 0.11764705882352941, "oy": 0.11888111888111888, "term": "determine", "cat25k": 12, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 18, "s": 0.6706989247311829, "os": 0.001623376623376624, "bg": 1.6359743084724914e-06}, {"x": 0.5620915032679737, "y": 0.5594405594405594, "ox": 0.5620915032679737, "oy": 0.5594405594405594, "term": "sources", "cat25k": 57, "ncat25k": 54, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 81, "ncat": 93, "s": 0.6599462365591399, "os": -0.0024350649350649567, "bg": 5.555694891390077e-06}, {"x": 0.47712418300653586, "y": 0.39860139860139854, "ox": 0.47712418300653586, "oy": 0.39860139860139854, "term": "information", "cat25k": 41, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 74, "s": 0.2036290322580645, "os": -0.07774170274170272, "bg": 2.8305680718635514e-07}, {"x": 0.14379084967320263, "y": 0.11888111888111888, "ox": 0.14379084967320263, "oy": 0.11888111888111888, "term": "automate", "cat25k": 12, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 22, "s": 0.5456989247311828, "os": -0.024350649350649345, "bg": 3.2817896019441655e-05}, {"x": 0.3594771241830065, "y": 0.13286713286713286, "ox": 0.3594771241830065, "oy": 0.13286713286713286, "term": "procedures", "cat25k": 13, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 55, "s": 0.04032258064516129, "os": -0.22474747474747475, "bg": 3.0741293899398535e-06}, {"x": 0.7581699346405228, "y": 0.6363636363636364, "ox": 0.7581699346405228, "oy": 0.6363636363636364, "term": "understanding", "cat25k": 69, "ncat25k": 88, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 153, "s": 0.12365591397849464, "os": -0.12085137085137088, "bg": 1.0147139382945986e-05}, {"x": 0.4901960784313725, "y": 0.4895104895104895, "ox": 0.4901960784313725, "oy": 0.4895104895104895, "term": "improve", "cat25k": 50, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 76, "s": 0.6646505376344086, "os": -0.0004509379509379241, "bg": 4.789348879060226e-06}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "pricing", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 5.894735887492165e-07}, {"x": 0.1503267973856209, "y": 0.06293706293706293, "ox": 0.1503267973856209, "oy": 0.06293706293706293, "term": "available", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 23, "s": 0.18548387096774194, "os": -0.08639971139971139, "bg": 1.6854305873412475e-07}, {"x": 0.823529411764706, "y": 0.7272727272727272, "ox": 0.823529411764706, "oy": 0.7272727272727272, "term": "support", "cat25k": 88, "ncat25k": 110, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 191, "s": 0.16935483870967744, "os": -0.09550865800865804, "bg": 1.691678983602007e-06}, {"x": 0.07189542483660129, "y": 0.1608391608391608, "ox": 0.07189542483660129, "oy": 0.1608391608391608, "term": "initiatives", "cat25k": 16, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 11, "s": 0.8528225806451613, "os": 0.08874458874458874, "bg": 4.190257921466926e-06}, {"x": 1.0, "y": 1.0, "ox": 1.0, "oy": 1.0, "term": "data", "cat25k": 1135, "ncat25k": 1590, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1617, "ncat": 2759, "s": 0.6666666666666666, "os": 0.0, "bg": 2.1504047004151357e-05}, {"x": 0.5751633986928104, "y": 0.4335664335664335, "ox": 0.5751633986928104, "oy": 0.4335664335664335, "term": "process", "cat25k": 44, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 95, "s": 0.09543010752688173, "os": -0.14042207792207795, "bg": 1.7862211118232696e-06}, {"x": 0.7973856209150327, "y": 0.6503496503496503, "ox": 0.7973856209150327, "oy": 0.6503496503496503, "term": "quality", "cat25k": 74, "ncat25k": 96, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 105, "ncat": 166, "s": 0.09005376344086023, "os": -0.1459235209235209, "bg": 2.8587966793103893e-06}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "interface", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 4.3779614699315456e-07}, {"x": 0.1764705882352941, "y": 0.1678321678321678, "ox": 0.1764705882352941, "oy": 0.1678321678321678, "term": "others", "cat25k": 17, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 27, "s": 0.6538978494623656, "os": -0.008207070707070718, "bg": 9.14976942715599e-07}, {"x": 0.7908496732026143, "y": 0.944055944055944, "ox": 0.7908496732026143, "oy": 0.944055944055944, "term": "science", "cat25k": 215, "ncat25k": 94, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 306, "ncat": 163, "s": 0.9247311827956989, "os": 0.15223665223665217, "bg": 5.381102161245678e-06}, {"x": 0.8366013071895424, "y": 0.846153846153846, "ox": 0.8366013071895424, "oy": 0.846153846153846, "term": "analytics", "cat25k": 121, "ncat25k": 111, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 173, "ncat": 193, "s": 0.6774193548387097, "os": 0.009559884559884546, "bg": 0.0001859422573085214}, {"x": 0.738562091503268, "y": 0.7902097902097902, "ox": 0.738562091503268, "oy": 0.7902097902097902, "term": "teams", "cat25k": 98, "ncat25k": 86, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 140, "ncat": 150, "s": 0.7681451612903226, "os": 0.051406926406926345, "bg": 1.6829139434410925e-05}, {"x": 0.039215686274509796, "y": 0.11888111888111888, "ox": 0.039215686274509796, "oy": 0.11888111888111888, "term": "explain", "cat25k": 12, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.8400537634408602, "os": 0.07954545454545454, "bg": 1.8352729242345075e-06}, {"x": 0.3202614379084967, "y": 0.6923076923076923, "ox": 0.3202614379084967, "oy": 0.6923076923076923, "term": "insights", "cat25k": 81, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 115, "ncat": 49, "s": 0.9858870967741936, "os": 0.36976911976911975, "bg": 4.281812167343663e-05}, {"x": 0.9738562091503268, "y": 0.9160839160839161, "ox": 0.9738562091503268, "oy": 0.9160839160839161, "term": "team", "cat25k": 151, "ncat25k": 179, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 215, "ncat": 311, "s": 0.28763440860215056, "os": -0.05735930735930739, "bg": 6.124511374460573e-06}, {"x": 0.3594771241830065, "y": 0.20979020979020974, "ox": 0.3594771241830065, "oy": 0.20979020979020974, "term": "members", "cat25k": 21, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 55, "s": 0.08803763440860217, "os": -0.14835858585858586, "bg": 7.497683700880668e-07}, {"x": 0.14379084967320263, "y": 0.048951048951048945, "ox": 0.14379084967320263, "oy": 0.048951048951048945, "term": "employees", "cat25k": 5, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 22, "s": 0.17204301075268819, "os": -0.09379509379509379, "bg": 1.0430821523770017e-06}, {"x": 0.6470588235294118, "y": 0.5174825174825174, "ox": 0.6470588235294118, "oy": 0.5174825174825174, "term": "provide", "cat25k": 53, "ncat25k": 65, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 112, "s": 0.1088709677419355, "os": -0.12851731601731597, "bg": 2.0649106883338374e-06}, {"x": 0.9281045751633986, "y": 0.8111888111888111, "ox": 0.9281045751633986, "oy": 0.8111888111888111, "term": "technical", "cat25k": 104, "ncat25k": 139, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 148, "ncat": 241, "s": 0.135752688172043, "os": -0.1160714285714286, "bg": 7.562671214793628e-06}, {"x": 0.326797385620915, "y": 0.28671328671328666, "ox": 0.326797385620915, "oy": 0.28671328671328666, "term": "expertise", "cat25k": 29, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 50, "s": 0.3978494623655914, "os": -0.03950216450216448, "bg": 1.0189457282072e-05}, {"x": 0.08496732026143791, "y": 0.048951048951048945, "ox": 0.08496732026143791, "oy": 0.048951048951048945, "term": "aspects", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 13, "s": 0.4543010752688172, "os": -0.03535353535353536, "bg": 1.40557111464214e-06}, {"x": 0.2483660130718954, "y": 0.30769230769230765, "ox": 0.2483660130718954, "oy": 0.30769230769230765, "term": "needed", "cat25k": 31, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 38, "s": 0.7963709677419355, "os": 0.05925324675324678, "bg": 2.140751402436919e-06}, {"x": 0.2679738562091503, "y": 0.3006993006993007, "ox": 0.2679738562091503, "oy": 0.3006993006993007, "term": "bachelor", "cat25k": 30, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 41, "s": 0.726478494623656, "os": 0.03282828282828287, "bg": 1.3531044444406859e-05}, {"x": 0.522875816993464, "y": 0.6993006993006994, "ox": 0.522875816993464, "oy": 0.6993006993006994, "term": "degree", "cat25k": 82, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 117, "ncat": 83, "s": 0.9408602150537635, "os": 0.17541486291486286, "bg": 5.960333385287573e-06}, {"x": 0.09803921568627451, "y": 0.46853146853146843, "ox": 0.09803921568627451, "oy": 0.46853146853146843, "term": "quantitative", "cat25k": 48, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 15, "s": 0.9845430107526882, "os": 0.3683261183261183, "bg": 2.3930190732269605e-05}, {"x": 0.33986928104575165, "y": 0.6293706293706293, "ox": 0.33986928104575165, "oy": 0.6293706293706293, "term": "field", "cat25k": 68, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 52, "s": 0.9751344086021506, "os": 0.28778860028860026, "bg": 2.2013781912845916e-06}, {"x": 0.8562091503267975, "y": 0.7902097902097902, "ox": 0.8562091503267975, "oy": 0.7902097902097902, "term": "strong", "cat25k": 98, "ncat25k": 116, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 140, "ncat": 201, "s": 0.24731182795698925, "os": -0.06547619047619047, "bg": 1.1171170805203249e-05}, {"x": 0.0457516339869281, "y": 0.18881118881118877, "ox": 0.0457516339869281, "oy": 0.18881118881118877, "term": "natural", "cat25k": 19, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 7, "s": 0.9099462365591399, "os": 0.1424963924963925, "bg": 6.46496858372285e-07}, {"x": 0.4379084967320261, "y": 0.3496503496503496, "ox": 0.4379084967320261, "oy": 0.3496503496503496, "term": "language", "cat25k": 35, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 68, "s": 0.18346774193548387, "os": -0.08739177489177485, "bg": 1.7027593446061966e-06}, {"x": 0.8366013071895424, "y": 0.5384615384615384, "ox": 0.8366013071895424, "oy": 0.5384615384615384, "term": "processing", "cat25k": 55, "ncat25k": 111, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 193, "s": 0.025537634408602152, "os": -0.29599567099567103, "bg": 9.753886586377782e-06}, {"x": 0.9934640522875817, "y": 0.9930069930069929, "ox": 0.9934640522875817, "oy": 0.9930069930069929, "term": "experience", "cat25k": 385, "ncat25k": 593, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 548, "ncat": 1029, "s": 0.663978494623656, "os": -0.0004509379509379796, "bg": 2.2985541562352036e-05}, {"x": 0.8954248366013071, "y": 0.8391608391608392, "ox": 0.8954248366013071, "oy": 0.8391608391608392, "term": "python", "cat25k": 121, "ncat25k": 123, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 172, "ncat": 213, "s": 0.29435483870967744, "os": -0.05582611832611828, "bg": 4.352393027647248e-05}, {"x": 0.33986928104575165, "y": 0.7832167832167832, "ox": 0.33986928104575165, "oy": 0.7832167832167832, "term": "r", "cat25k": 98, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 139, "ncat": 52, "s": 0.9919354838709677, "os": 0.44056637806637805, "bg": 1.1804153786542631e-06}, {"x": 0.2091503267973856, "y": 0.2937062937062937, "ox": 0.2091503267973856, "oy": 0.2937062937062937, "term": "proficiency", "cat25k": 29, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 32, "s": 0.8467741935483871, "os": 0.08432539682539683, "bg": 4.164141452508811e-05}, {"x": 0.09150326797385622, "y": 0.1048951048951049, "ox": 0.09150326797385622, "oy": 0.1048951048951049, "term": "basic", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 14, "s": 0.6801075268817205, "os": 0.013708513708513698, "bg": 6.829900584084856e-07}, {"x": 0.9411764705882353, "y": 0.6923076923076923, "ox": 0.9411764705882353, "oy": 0.6923076923076923, "term": "sql", "cat25k": 81, "ncat25k": 145, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 115, "ncat": 251, "s": 0.03091397849462366, "os": -0.24711399711399717, "bg": 2.8015906605071478e-05}, {"x": 0.2483660130718954, "y": 0.08391608391608392, "ox": 0.2483660130718954, "oy": 0.08391608391608392, "term": "queries", "cat25k": 8, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 38, "s": 0.07661290322580647, "os": -0.16296897546897543, "bg": 6.76932391512784e-06}, {"x": 0.4836601307189542, "y": 0.6643356643356643, "ox": 0.4836601307189542, "oy": 0.6643356643356643, "term": "analytical", "cat25k": 75, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 75, "s": 0.9428763440860216, "os": 0.17965367965367962, "bg": 4.361013307799841e-05}, {"x": 0.8758169934640523, "y": 0.895104895104895, "ox": 0.8758169934640523, "oy": 0.895104895104895, "term": "skills", "cat25k": 140, "ncat25k": 118, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 199, "ncat": 204, "s": 0.6948924731182796, "os": 0.019209956709956733, "bg": 1.1278334103296528e-05}, {"x": 0.3333333333333333, "y": 0.22377622377622375, "ox": 0.3333333333333333, "oy": 0.22377622377622375, "term": "self", "cat25k": 22, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 51, "s": 0.1471774193548387, "os": -0.108495670995671, "bg": 1.4692513181197906e-06}, {"x": 0.4183006535947712, "y": 0.31468531468531463, "ox": 0.4183006535947712, "oy": 0.31468531468531463, "term": "learn", "cat25k": 32, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 65, "s": 0.1592741935483871, "os": -0.10263347763347763, "bg": 1.2255663160897082e-06}, {"x": 0.5032679738562091, "y": 0.4895104895104895, "ox": 0.5032679738562091, "oy": 0.4895104895104895, "term": "excellent", "cat25k": 50, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 79, "s": 0.6357526881720431, "os": -0.01343795093795086, "bg": 4.967139803842019e-06}, {"x": 0.5424836601307189, "y": 0.5314685314685315, "ox": 0.5424836601307189, "oy": 0.5314685314685315, "term": "communication", "cat25k": 54, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 87, "s": 0.6391129032258065, "os": -0.010732323232323204, "bg": 5.088492294005886e-06}, {"x": 0.2810457516339869, "y": 0.2727272727272727, "ox": 0.2810457516339869, "oy": 0.2727272727272727, "term": "written", "cat25k": 27, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 43, "s": 0.6545698924731184, "os": -0.007936507936507908, "bg": 1.671126638730728e-06}, {"x": 0.2026143790849673, "y": 0.14685314685314685, "ox": 0.2026143790849673, "oy": 0.14685314685314685, "term": "verbal", "cat25k": 15, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 31, "s": 0.2977150537634409, "os": -0.055014430014430016, "bg": 1.9475556958794405e-05}, {"x": 0.16993464052287582, "y": 0.18881118881118877, "ox": 0.16993464052287582, "oy": 0.18881118881118877, "term": "demonstrated", "cat25k": 19, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 26, "s": 0.6942204301075269, "os": 0.01911976911976912, "bg": 8.981880919409056e-06}, {"x": 0.8366013071895424, "y": 0.7692307692307693, "ox": 0.8366013071895424, "oy": 0.7692307692307693, "term": "ability", "cat25k": 94, "ncat25k": 111, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 134, "ncat": 193, "s": 0.2439516129032258, "os": -0.0668290043290043, "bg": 1.2556381320021264e-05}, {"x": 0.9150326797385621, "y": 0.8531468531468531, "ox": 0.9150326797385621, "oy": 0.8531468531468531, "term": "build", "cat25k": 124, "ncat25k": 136, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 177, "ncat": 236, "s": 0.2674731182795699, "os": -0.06141774891774898, "bg": 1.068786061663366e-05}, {"x": 0.07189542483660129, "y": 0.1678321678321678, "ox": 0.07189542483660129, "oy": 0.1678321678321678, "term": "relationships", "cat25k": 17, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 11, "s": 0.8669354838709679, "os": 0.09568903318903318, "bg": 2.3387271397357057e-06}, {"x": 0.9803921568627452, "y": 0.972027972027972, "ox": 0.9803921568627452, "oy": 0.972027972027972, "term": "work", "cat25k": 281, "ncat25k": 231, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 401, "ncat": 401, "s": 0.653225806451613, "os": -0.008297258297258359, "bg": 3.823005303366387e-06}, {"x": 0.261437908496732, "y": 0.20979020979020974, "ox": 0.261437908496732, "oy": 0.20979020979020974, "term": "part", "cat25k": 21, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 40, "s": 0.3178763440860215, "os": -0.05095598845598842, "bg": 4.623363824979167e-07}, {"x": 0.6601307189542484, "y": 0.951048951048951, "ox": 0.6601307189542484, "oy": 0.951048951048951, "term": "analysis", "cat25k": 227, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 324, "ncat": 120, "s": 0.9758064516129032, "os": 0.2890512265512265, "bg": 7.102270568707438e-06}, {"x": 0.2745098039215686, "y": 0.5594405594405594, "ox": 0.2745098039215686, "oy": 0.5594405594405594, "term": "deep", "cat25k": 57, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 81, "ncat": 42, "s": 0.9744623655913979, "os": 0.2832792207792208, "bg": 4.5957267176888234e-06}, {"x": 0.2483660130718954, "y": 0.6083916083916083, "ox": 0.2483660130718954, "oy": 0.6083916083916083, "term": "opportunities", "cat25k": 65, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 38, "s": 0.9838709677419355, "os": 0.35786435786435794, "bg": 3.7367604580230015e-06}, {"x": 0.261437908496732, "y": 0.2937062937062937, "ox": 0.261437908496732, "oy": 0.2937062937062937, "term": "partner", "cat25k": 29, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 40, "s": 0.7237903225806451, "os": 0.032377344877344894, "bg": 3.0908204028673315e-06}, {"x": 0.5424836601307189, "y": 0.30769230769230765, "ox": 0.5424836601307189, "oy": 0.30769230769230765, "term": "scientists", "cat25k": 31, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 87, "s": 0.035618279569892476, "os": -0.23295454545454541, "bg": 1.537943171238819e-05}, {"x": 0.2810457516339869, "y": 0.0769230769230769, "ox": 0.2810457516339869, "oy": 0.0769230769230769, "term": "analysts", "cat25k": 8, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 43, "s": 0.055779569892473124, "os": -0.20238095238095238, "bg": 1.443630441835127e-05}, {"x": 0.4967320261437908, "y": 0.6643356643356643, "ox": 0.4967320261437908, "oy": 0.6643356643356643, "term": "help", "cat25k": 75, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 78, "s": 0.9368279569892474, "os": 0.16666666666666663, "bg": 6.054312025591781e-07}, {"x": 0.7450980392156864, "y": 0.8181818181818182, "ox": 0.7450980392156864, "oy": 0.8181818181818182, "term": "product", "cat25k": 108, "ncat25k": 87, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 154, "ncat": 151, "s": 0.8239247311827957, "os": 0.0726911976911977, "bg": 1.528067159323201e-06}, {"x": 0.1503267973856209, "y": 0.1818181818181818, "ox": 0.1503267973856209, "oy": 0.1818181818181818, "term": "strategy", "cat25k": 18, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 23, "s": 0.7224462365591398, "os": 0.03165584415584416, "bg": 1.7117707188623057e-06}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "partnering", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 1.1591747097584595e-05}, {"x": 0.261437908496732, "y": 0.3006993006993007, "ox": 0.261437908496732, "oy": 0.3006993006993007, "term": "leadership", "cat25k": 30, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 40, "s": 0.7419354838709677, "os": 0.03932178932178937, "bg": 4.587010828799631e-06}, {"x": 0.1045751633986928, "y": 0.08391608391608392, "ox": 0.1045751633986928, "oy": 0.08391608391608392, "term": "prioritize", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 16, "s": 0.5759408602150538, "os": -0.02011183261183261, "bg": 5.450646826312218e-05}, {"x": 0.12418300653594772, "y": 0.3636363636363636, "ox": 0.12418300653594772, "oy": 0.3636363636363636, "term": "areas", "cat25k": 37, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 19, "s": 0.9650537634408602, "os": 0.23818542568542572, "bg": 1.179681432436044e-06}, {"x": 0.11764705882352941, "y": 0.12587412587412586, "ox": 0.11764705882352941, "oy": 0.12587412587412586, "term": "opportunity", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 18, "s": 0.676747311827957, "os": 0.008567821067821071, "bg": 1.1977294642546125e-06}, {"x": 0.33986928104575165, "y": 0.6083916083916083, "ox": 0.33986928104575165, "oy": 0.6083916083916083, "term": "drive", "cat25k": 65, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 52, "s": 0.9690860215053764, "os": 0.266955266955267, "bg": 2.6881727905560226e-06}, {"x": 0.1633986928104575, "y": 0.2517482517482517, "ox": 0.1633986928104575, "oy": 0.2517482517482517, "term": "growth", "cat25k": 25, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 25, "s": 0.8521505376344086, "os": 0.08811327561327559, "bg": 1.5234956432707478e-06}, {"x": 0.1633986928104575, "y": 0.4615384615384615, "ox": 0.1633986928104575, "oy": 0.4615384615384615, "term": "evaluate", "cat25k": 47, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 25, "s": 0.9771505376344086, "os": 0.29644660894660896, "bg": 1.1472823033388222e-05}, {"x": 0.22875816993464052, "y": 0.6083916083916083, "ox": 0.22875816993464052, "oy": 0.6083916083916083, "term": "define", "cat25k": 65, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 35, "s": 0.9879032258064517, "os": 0.3773448773448774, "bg": 7.078160700127418e-06}, {"x": 0.9869281045751633, "y": 0.986013986013986, "ox": 0.9869281045751633, "oy": 0.986013986013986, "term": "business", "cat25k": 366, "ncat25k": 236, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 522, "ncat": 409, "s": 0.6606182795698925, "os": -0.0009018759018758482, "bg": 2.922088082964929e-06}, {"x": 0.261437908496732, "y": 0.37762237762237755, "ox": 0.261437908496732, "oy": 0.37762237762237755, "term": "metrics", "cat25k": 39, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 40, "s": 0.8870967741935485, "os": 0.11571067821067821, "bg": 4.79345143893105e-05}, {"x": 0.09803921568627451, "y": 0.0769230769230769, "ox": 0.09803921568627451, "oy": 0.0769230769230769, "term": "execution", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 15, "s": 0.5745967741935484, "os": -0.020562770562770574, "bg": 2.375499174605402e-06}, {"x": 0.261437908496732, "y": 0.5874125874125874, "ox": 0.261437908496732, "oy": 0.5874125874125874, "term": "communicate", "cat25k": 62, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 88, "ncat": 40, "s": 0.9811827956989247, "os": 0.3240440115440116, "bg": 1.9581737154036218e-05}, {"x": 0.08496732026143791, "y": 0.14685314685314685, "ox": 0.08496732026143791, "oy": 0.14685314685314685, "term": "state", "cat25k": 15, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 13, "s": 0.7970430107526881, "os": 0.06186868686868688, "bg": 1.5004922243378596e-07}, {"x": 0.45751633986928103, "y": 0.5104895104895104, "ox": 0.45751633986928103, "oy": 0.5104895104895104, "term": "stakeholders", "cat25k": 52, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 71, "s": 0.7735215053763441, "os": 0.052849927849927814, "bg": 4.3250214871541635e-05}, {"x": 0.0261437908496732, "y": 0.13286713286713286, "ox": 0.0261437908496732, "oy": 0.13286713286713286, "term": "interpret", "cat25k": 13, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 4, "s": 0.8797043010752689, "os": 0.10642135642135643, "bg": 8.992352199939868e-06}, {"x": 0.0261437908496732, "y": 0.3286713286713286, "ox": 0.0261437908496732, "oy": 0.3286713286713286, "term": "experiments", "cat25k": 33, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 4, "s": 0.9778225806451614, "os": 0.30086580086580084, "bg": 7.406847994051285e-06}, {"x": 0.11764705882352941, "y": 0.4265734265734265, "ox": 0.11764705882352941, "oy": 0.4265734265734265, "term": "decision", "cat25k": 44, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 18, "s": 0.9791666666666667, "os": 0.3071789321789322, "bg": 2.2507629910698872e-06}, {"x": 0.9215686274509804, "y": 0.8671328671328672, "ox": 0.9215686274509804, "oy": 0.8671328671328672, "term": "tools", "cat25k": 128, "ncat25k": 138, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 182, "ncat": 239, "s": 0.29973118279569894, "os": -0.05402236652236647, "bg": 4.5357398496566e-06}, {"x": 0.7777777777777778, "y": 0.49650349650349646, "ox": 0.7777777777777778, "oy": 0.49650349650349646, "term": "building", "cat25k": 51, "ncat25k": 91, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 158, "s": 0.02620967741935484, "os": -0.27922077922077926, "bg": 3.5844514568609436e-06}, {"x": 0.08496732026143791, "y": 0.1608391608391608, "ox": 0.08496732026143791, "oy": 0.1608391608391608, "term": "dashboards", "cat25k": 16, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 13, "s": 0.8272849462365591, "os": 0.07575757575757575, "bg": 0.0001852852621014437}, {"x": 0.196078431372549, "y": 0.28671328671328666, "ox": 0.196078431372549, "oy": 0.28671328671328666, "term": "reports", "cat25k": 29, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 30, "s": 0.8561827956989247, "os": 0.09036796536796537, "bg": 1.2391305129396465e-06}, {"x": 0.3529411764705882, "y": 0.6293706293706293, "ox": 0.3529411764705882, "oy": 0.6293706293706293, "term": "key", "cat25k": 68, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 54, "s": 0.9717741935483871, "os": 0.27480158730158727, "bg": 2.205290008780267e-06}, {"x": 0.3790849673202614, "y": 0.6223776223776223, "ox": 0.3790849673202614, "oy": 0.6223776223776223, "term": "sets", "cat25k": 67, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 59, "s": 0.9663978494623656, "os": 0.24188311688311687, "bg": 6.05033539448743e-06}, {"x": 0.869281045751634, "y": 0.22377622377622375, "ox": 0.869281045751634, "oy": 0.22377622377622375, "term": "pipelines", "cat25k": 22, "ncat25k": 117, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 203, "s": 0.0006720430107526882, "os": -0.6409632034632035, "bg": 0.0002241865842932017}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "empower", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 1.1830790124247946e-05}, {"x": 0.2549019607843137, "y": 0.18881118881118877, "ox": 0.2549019607843137, "oy": 0.18881118881118877, "term": "operational", "cat25k": 19, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 39, "s": 0.24798387096774194, "os": -0.06529581529581527, "bg": 7.5973356834425044e-06}, {"x": 0.0326797385620915, "y": 0.11188811188811187, "ox": 0.0326797385620915, "oy": 0.11188811188811187, "term": "exploratory", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.8387096774193549, "os": 0.07909451659451659, "bg": 3.375861146008407e-05}, {"x": 0.5816993464052287, "y": 0.8041958041958042, "ox": 0.5816993464052287, "oy": 0.8041958041958042, "term": "projects", "cat25k": 103, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 147, "ncat": 97, "s": 0.9603494623655915, "os": 0.2211399711399712, "bg": 5.26519230780435e-06}, {"x": 0.3725490196078431, "y": 0.6643356643356643, "ox": 0.3725490196078431, "oy": 0.6643356643356643, "term": "identify", "cat25k": 75, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 58, "s": 0.976478494623656, "os": 0.29004329004329, "bg": 9.183314485276057e-06}, {"x": 0.058823529411764705, "y": 0.048951048951048945, "ox": 0.058823529411764705, "oy": 0.048951048951048945, "term": "candidate", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.6505376344086021, "os": -0.009379509379509376, "bg": 1.4735671976573965e-06}, {"x": 0.6274509803921569, "y": 0.4475524475524475, "ox": 0.6274509803921569, "oy": 0.4475524475524475, "term": "high", "cat25k": 46, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 107, "s": 0.06787634408602151, "os": -0.17848124098124102, "bg": 9.956763824025166e-07}, {"x": 0.47058823529411764, "y": 0.3706293706293706, "ox": 0.47058823529411764, "oy": 0.3706293706293706, "term": "real", "cat25k": 38, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 73, "s": 0.16599462365591397, "os": -0.09902597402597402, "bg": 8.530501146052677e-07}, {"x": 0.15686274509803919, "y": 0.20279720279720279, "ox": 0.15686274509803919, "oy": 0.20279720279720279, "term": "world", "cat25k": 20, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 24, "s": 0.7526881720430108, "os": 0.045995670995671006, "bg": 2.4536198944093474e-07}, {"x": 0.12418300653594772, "y": 0.20279720279720279, "ox": 0.12418300653594772, "oy": 0.20279720279720279, "term": "clinical", "cat25k": 20, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 19, "s": 0.8373655913978495, "os": 0.07846320346320348, "bg": 2.230901928319587e-06}, {"x": 0.0457516339869281, "y": 0.15384615384615385, "ox": 0.0457516339869281, "oy": 0.15384615384615385, "term": "explore", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 7, "s": 0.8817204301075269, "os": 0.10777417027417026, "bg": 1.822713861924334e-06}, {"x": 0.09803921568627451, "y": 0.11188811188811187, "ox": 0.09803921568627451, "oy": 0.11188811188811187, "term": "position", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 15, "s": 0.6834677419354839, "os": 0.01415945165945165, "bg": 7.108488094291388e-07}, {"x": 0.3986928104575163, "y": 0.26573426573426573, "ox": 0.3986928104575163, "oy": 0.26573426573426573, "term": "existing", "cat25k": 27, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 62, "s": 0.10618279569892475, "os": -0.1317640692640693, "bg": 3.396529673925849e-06}, {"x": 0.0261437908496732, "y": 0.11888111888111888, "ox": 0.0261437908496732, "oy": 0.11888111888111888, "term": "novel", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.8608870967741935, "os": 0.09253246753246754, "bg": 1.917662511852981e-06}, {"x": 0.5163398692810457, "y": 0.5804195804195804, "ox": 0.5163398692810457, "oy": 0.5804195804195804, "term": "collaborate", "cat25k": 59, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 82, "s": 0.8064516129032259, "os": 0.06385281385281394, "bg": 0.00012551970640486986}, {"x": 0.11764705882352941, "y": 0.0979020979020979, "ox": 0.11764705882352941, "oy": 0.0979020979020979, "term": "computational", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 18, "s": 0.5772849462365591, "os": -0.019209956709956705, "bg": 9.76225849850363e-06}, {"x": 0.11111111111111112, "y": 0.24475524475524474, "ox": 0.11111111111111112, "oy": 0.24475524475524474, "term": "scientific", "cat25k": 25, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 17, "s": 0.9005376344086021, "os": 0.13311688311688313, "bg": 2.44729163294287e-06}, {"x": 0.0261437908496732, "y": 0.11188811188811187, "ox": 0.0261437908496732, "oy": 0.11188811188811187, "term": "champion", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 4, "s": 0.8474462365591399, "os": 0.08558802308802309, "bg": 2.764454260100037e-06}, {"x": 0.08496732026143791, "y": 0.08391608391608392, "ox": 0.08496732026143791, "oy": 0.08391608391608392, "term": "discovery", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.6612903225806452, "os": -0.0006313131313131354, "bg": 2.193915832612998e-06}, {"x": 0.9673202614379085, "y": 0.8391608391608392, "ox": 0.9673202614379085, "oy": 0.8391608391608392, "term": "development", "cat25k": 121, "ncat25k": 178, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 172, "ncat": 308, "s": 0.11155913978494625, "os": -0.1272546897546898, "bg": 3.35227900585163e-06}, {"x": 0.8169934640522876, "y": 0.923076923076923, "ox": 0.8169934640522876, "oy": 0.923076923076923, "term": "develop", "cat25k": 152, "ncat25k": 107, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 217, "ncat": 185, "s": 0.8776881720430109, "os": 0.10542929292929293, "bg": 1.4436571844246196e-05}, {"x": 0.1633986928104575, "y": 0.19580419580419578, "ox": 0.1633986928104575, "oy": 0.19580419580419578, "term": "innovative", "cat25k": 20, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 25, "s": 0.7244623655913979, "os": 0.03255772005772006, "bg": 5.9651624384075884e-06}, {"x": 0.12418300653594772, "y": 0.11888111888111888, "ox": 0.12418300653594772, "oy": 0.11888111888111888, "term": "creative", "cat25k": 12, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 19, "s": 0.6586021505376344, "os": -0.004870129870129858, "bg": 1.5057102807639406e-06}, {"x": 0.14379084967320263, "y": 0.3426573426573426, "ox": 0.14379084967320263, "oy": 0.3426573426573426, "term": "approaches", "cat25k": 34, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 22, "s": 0.9516129032258065, "os": 0.19787157287157287, "bg": 8.280042223550526e-06}, {"x": 0.4640522875816993, "y": 0.7412587412587412, "ox": 0.4640522875816993, "oy": 0.7412587412587412, "term": "understand", "cat25k": 90, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 128, "ncat": 72, "s": 0.9724462365591398, "os": 0.27552308802308806, "bg": 6.472426661459383e-06}, {"x": 0.6993464052287582, "y": 0.7132867132867132, "ox": 0.6993464052287582, "oy": 0.7132867132867132, "term": "complex", "cat25k": 84, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 120, "ncat": 134, "s": 0.6821236559139786, "os": 0.01397907647907648, "bg": 1.08207848617731e-05}, {"x": 0.8300653594771242, "y": 0.7342657342657343, "ox": 0.8300653594771242, "oy": 0.7342657342657343, "term": "using", "cat25k": 88, "ncat25k": 111, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 126, "ncat": 192, "s": 0.17069892473118278, "os": -0.09505772005772006, "bg": 2.3596661639291547e-06}, {"x": 0.196078431372549, "y": 0.08391608391608392, "ox": 0.196078431372549, "oy": 0.08391608391608392, "term": "multi", "cat25k": 8, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 30, "s": 0.14314516129032256, "os": -0.11102092352092353, "bg": 1.5214095192673747e-06}, {"x": 0.09803921568627451, "y": 0.04195804195804195, "ox": 0.09803921568627451, "oy": 0.04195804195804195, "term": "dimensional", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.2956989247311828, "os": -0.05528499278499279, "bg": 3.961317171913808e-06}, {"x": 0.3333333333333333, "y": 0.3496503496503496, "ox": 0.3333333333333333, "oy": 0.3496503496503496, "term": "within", "cat25k": 35, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 51, "s": 0.6895161290322581, "os": 0.01650432900432902, "bg": 7.725508462084063e-07}, {"x": 0.08496732026143791, "y": 0.04195804195804195, "ox": 0.08496732026143791, "oy": 0.04195804195804195, "term": "outside", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.38642473118279574, "os": -0.0422979797979798, "bg": 5.629936648841546e-07}, {"x": 0.22875816993464052, "y": 0.8741258741258741, "ox": 0.22875816993464052, "oy": 0.8741258741258741, "term": "research", "cat25k": 133, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 190, "ncat": 35, "s": 0.9993279569892473, "os": 0.6412337662337663, "bg": 1.4440708426369336e-06}, {"x": 0.065359477124183, "y": 0.0979020979020979, "ox": 0.065359477124183, "oy": 0.0979020979020979, "term": "area", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 10, "s": 0.7251344086021506, "os": 0.03273809523809525, "bg": 1.8464944888775346e-07}, {"x": 0.08496732026143791, "y": 0.1048951048951049, "ox": 0.08496732026143791, "oy": 0.1048951048951049, "term": "prepare", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 13, "s": 0.6969086021505376, "os": 0.020202020202020193, "bg": 2.645389671567314e-06}, {"x": 0.0065359477124183, "y": 0.13986013986013984, "ox": 0.0065359477124183, "oy": 0.13986013986013984, "term": "publications", "cat25k": 14, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 1, "s": 0.8998655913978495, "os": 0.13284632034632035, "bg": 6.820090557811958e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "regulatory", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 7.242582000400243e-07}, {"x": 0.3071895424836601, "y": 0.1678321678321678, "ox": 0.3071895424836601, "oy": 0.1678321678321678, "term": "documentation", "cat25k": 17, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 47, "s": 0.0974462365591398, "os": -0.13807720057720058, "bg": 3.2307563637482196e-06}, {"x": 0.326797385620915, "y": 0.11888111888111888, "ox": 0.326797385620915, "oy": 0.11888111888111888, "term": "standards", "cat25k": 12, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 50, "s": 0.053091397849462374, "os": -0.20616883116883117, "bg": 1.6289793183934776e-06}, {"x": 0.0784313725490196, "y": 0.1608391608391608, "ox": 0.0784313725490196, "oy": 0.1608391608391608, "term": "network", "cat25k": 16, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 12, "s": 0.8434139784946236, "os": 0.08225108225108224, "bg": 3.106975657467119e-07}, {"x": 0.3725490196078431, "y": 0.048951048951048945, "ox": 0.3725490196078431, "oy": 0.048951048951048945, "term": "pipeline", "cat25k": 5, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 58, "s": 0.021505376344086023, "os": -0.32106782106782106, "bg": 8.150548403379669e-06}, {"x": 0.11111111111111112, "y": 0.23776223776223773, "ox": 0.11111111111111112, "oy": 0.23776223776223773, "term": "strategies", "cat25k": 24, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 17, "s": 0.8958333333333334, "os": 0.12617243867243866, "bg": 2.76113203164907e-06}, {"x": 0.1633986928104575, "y": 0.18881118881118877, "ox": 0.1633986928104575, "oy": 0.18881118881118877, "term": "translate", "cat25k": 19, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 25, "s": 0.7103494623655915, "os": 0.025613275613275616, "bg": 9.59713178642949e-06}, {"x": 0.6209150326797385, "y": 0.9020979020979021, "ox": 0.6209150326797385, "oy": 0.9020979020979021, "term": "problems", "cat25k": 144, "ncat25k": 61, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 205, "ncat": 105, "s": 0.9737903225806451, "os": 0.27940115440115443, "bg": 5.11684632189202e-06}, {"x": 0.392156862745098, "y": 0.6643356643356643, "ox": 0.392156862745098, "oy": 0.6643356643356643, "term": "project", "cat25k": 75, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 61, "s": 0.9704301075268819, "os": 0.2705627705627705, "bg": 1.4277020842614812e-06}, {"x": 0.0522875816993464, "y": 0.12587412587412586, "ox": 0.0522875816993464, "oy": 0.12587412587412586, "term": "clearly", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.8245967741935484, "os": 0.07350288600288601, "bg": 1.6039899373074364e-06}, {"x": 0.09150326797385622, "y": 0.1048951048951049, "ox": 0.09150326797385622, "oy": 0.1048951048951049, "term": "identifying", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 14, "s": 0.6801075268817205, "os": 0.013708513708513698, "bg": 4.915758727442283e-06}, {"x": 0.1045751633986928, "y": 0.02797202797202797, "ox": 0.1045751633986928, "oy": 0.02797202797202797, "term": "risks", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 16, "s": 0.2110215053763441, "os": -0.07566738816738816, "bg": 2.0740172152761944e-06}, {"x": 0.039215686274509796, "y": 0.0979020979020979, "ox": 0.039215686274509796, "oy": 0.0979020979020979, "term": "scope", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.7930107526881721, "os": 0.058712121212121215, "bg": 1.707220698480326e-06}, {"x": 0.22875816993464052, "y": 0.24475524475524474, "ox": 0.22875816993464052, "oy": 0.24475524475524474, "term": "cross", "cat25k": 25, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 35, "s": 0.6875, "os": 0.01623376623376624, "bg": 1.8839613819136636e-06}, {"x": 0.6928104575163399, "y": 0.4475524475524475, "ox": 0.6928104575163399, "oy": 0.4475524475524475, "term": "database", "cat25k": 46, "ncat25k": 75, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 130, "s": 0.031586021505376344, "os": -0.24341630591630586, "bg": 3.7567805916160735e-06}, {"x": 0.11764705882352941, "y": 0.3356643356643356, "ox": 0.11764705882352941, "oy": 0.3356643356643356, "term": "subject", "cat25k": 34, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 18, "s": 0.9596774193548387, "os": 0.21690115440115443, "bg": 5.150247587618177e-07}, {"x": 0.09150326797385622, "y": 0.30769230769230765, "ox": 0.09150326797385622, "oy": 0.30769230769230765, "term": "matter", "cat25k": 31, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 14, "s": 0.9576612903225807, "os": 0.2150974025974026, "bg": 1.7387832777075078e-06}, {"x": 0.11111111111111112, "y": 0.2937062937062937, "ox": 0.11111111111111112, "oy": 0.2937062937062937, "term": "experts", "cat25k": 29, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 17, "s": 0.9435483870967741, "os": 0.18172799422799424, "bg": 3.6870001810379587e-06}, {"x": 0.1764705882352941, "y": 0.06293706293706293, "ox": 0.1764705882352941, "oy": 0.06293706293706293, "term": "complete", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 27, "s": 0.13978494623655915, "os": -0.11237373737373738, "bg": 4.819511728974597e-07}, {"x": 0.07189542483660129, "y": 0.12587412587412586, "ox": 0.07189542483660129, "oy": 0.12587412587412586, "term": "deliverables", "cat25k": 13, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 11, "s": 0.7741935483870969, "os": 0.05402236652236653, "bg": 4.646378548471341e-05}, {"x": 0.6535947712418302, "y": 0.5804195804195804, "ox": 0.6535947712418302, "oy": 0.5804195804195804, "term": "time", "cat25k": 59, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 84, "ncat": 118, "s": 0.21908602150537634, "os": -0.07251082251082253, "bg": 4.4454899121677493e-07}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "budget", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 3.3791269805802417e-07}, {"x": 0.9738562091503268, "y": 0.9090909090909091, "ox": 0.9738562091503268, "oy": 0.9090909090909091, "term": "design", "cat25k": 149, "ncat25k": 179, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 212, "ncat": 311, "s": 0.25336021505376344, "os": -0.06430375180375181, "bg": 3.954193114837979e-06}, {"x": 0.065359477124183, "y": 0.11188811188811187, "ox": 0.065359477124183, "oy": 0.11188811188811187, "term": "propose", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.7533602150537635, "os": 0.04662698412698413, "bg": 8.159612054383188e-06}, {"x": 0.2352941176470588, "y": 0.7972027972027973, "ox": 0.2352941176470588, "oy": 0.7972027972027973, "term": "algorithms", "cat25k": 101, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 144, "ncat": 36, "s": 0.9966397849462366, "os": 0.5583513708513709, "bg": 3.5341159502211914e-05}, {"x": 0.196078431372549, "y": 0.6713286713286714, "ox": 0.196078431372549, "oy": 0.6713286713286714, "term": "analyze", "cat25k": 77, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 110, "ncat": 30, "s": 0.9946236559139785, "os": 0.47231240981240985, "bg": 3.172692005371368e-05}, {"x": 0.09803921568627451, "y": 0.15384615384615385, "ox": 0.09803921568627451, "oy": 0.15384615384615385, "term": "leverage", "cat25k": 15, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 15, "s": 0.7795698924731183, "os": 0.05582611832611831, "bg": 1.794448799889617e-05}, {"x": 0.44444444444444436, "y": 0.37762237762237755, "ox": 0.44444444444444436, "oy": 0.37762237762237755, "term": "well", "cat25k": 39, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 69, "s": 0.24596774193548387, "os": -0.06610750360750361, "bg": 6.847739055902713e-07}, {"x": 0.8823529411764706, "y": 0.9370629370629371, "ox": 0.8823529411764706, "oy": 0.9370629370629371, "term": "new", "cat25k": 192, "ncat25k": 118, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 274, "ncat": 205, "s": 0.7768817204301075, "os": 0.05438311688311692, "bg": 6.175308196997871e-07}, {"x": 0.7712418300653595, "y": 0.32167832167832167, "ox": 0.7712418300653595, "oy": 0.32167832167832167, "term": "code", "cat25k": 32, "ncat25k": 90, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 157, "s": 0.006720430107526882, "os": -0.4463383838383838, "bg": 1.6218865002389006e-06}, {"x": 0.5686274509803921, "y": 0.9650349650349651, "ox": 0.5686274509803921, "oy": 0.9650349650349651, "term": "models", "cat25k": 279, "ncat25k": 54, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 398, "ncat": 94, "s": 0.9892473118279571, "os": 0.3938492063492064, "bg": 1.1355488506883896e-05}, {"x": 0.065359477124183, "y": 0.18881118881118877, "ox": 0.065359477124183, "oy": 0.18881118881118877, "term": "applying", "cat25k": 19, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 10, "s": 0.8951612903225807, "os": 0.12301587301587302, "bg": 5.096071273377366e-06}, {"x": 0.6535947712418302, "y": 0.7482517482517483, "ox": 0.6535947712418302, "oy": 0.7482517482517483, "term": "large", "cat25k": 91, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 129, "ncat": 118, "s": 0.8655913978494624, "os": 0.0941558441558441, "bg": 2.976899548755058e-06}, {"x": 0.2091503267973856, "y": 0.20979020979020974, "ox": 0.2091503267973856, "oy": 0.20979020979020974, "term": "structured", "cat25k": 21, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 32, "s": 0.6693548387096775, "os": 0.0009920634920635163, "bg": 1.4368541977695388e-05}, {"x": 0.1633986928104575, "y": 0.23776223776223773, "ox": 0.1633986928104575, "oy": 0.23776223776223773, "term": "unstructured", "cat25k": 24, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 25, "s": 0.8259408602150538, "os": 0.07422438672438672, "bg": 0.00013961787298086525}, {"x": 0.1633986928104575, "y": 0.41258741258741255, "ox": 0.1633986928104575, "oy": 0.41258741258741255, "term": "visualization", "cat25k": 42, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 25, "s": 0.967741935483871, "os": 0.24783549783549785, "bg": 4.0524288947715323e-05}, {"x": 0.5098039215686274, "y": 0.5454545454545454, "ox": 0.5098039215686274, "oy": 0.5454545454545454, "term": "products", "cat25k": 55, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 80, "s": 0.730510752688172, "os": 0.03562409812409817, "bg": 7.672666760435663e-07}, {"x": 0.1045751633986928, "y": 0.1748251748251748, "ox": 0.1045751633986928, "oy": 0.1748251748251748, "term": "share", "cat25k": 18, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 16, "s": 0.819220430107527, "os": 0.07016594516594517, "bg": 6.869125138840098e-07}, {"x": 0.4901960784313725, "y": 0.5664335664335663, "ox": 0.4901960784313725, "oy": 0.5664335664335663, "term": "across", "cat25k": 58, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 82, "ncat": 76, "s": 0.8286290322580646, "os": 0.07593795093795092, "bg": 4.121142657748113e-06}, {"x": 0.0261437908496732, "y": 0.1048951048951049, "ox": 0.0261437908496732, "oy": 0.1048951048951049, "term": "group", "cat25k": 11, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 4, "s": 0.8380376344086022, "os": 0.07864357864357864, "bg": 1.1804048190250392e-07}, {"x": 0.22222222222222218, "y": 0.1818181818181818, "ox": 0.22222222222222218, "oy": 0.1818181818181818, "term": "users", "cat25k": 18, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 34, "s": 0.39583333333333337, "os": -0.039772727272727265, "bg": 8.032088299852422e-07}, {"x": 0.0784313725490196, "y": 0.04195804195804195, "ox": 0.0784313725490196, "oy": 0.04195804195804195, "term": "continuously", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.4516129032258065, "os": -0.035804473304473304, "bg": 6.325923158659952e-06}, {"x": 0.4379084967320261, "y": 0.3706293706293706, "ox": 0.4379084967320261, "oy": 0.3706293706293706, "term": "industry", "cat25k": 38, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 68, "s": 0.2446236559139785, "os": -0.06655844155844154, "bg": 1.5165339586541504e-06}, {"x": 0.6078431372549019, "y": 0.45454545454545453, "ox": 0.6078431372549019, "oy": 0.45454545454545453, "term": "best", "cat25k": 46, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 102, "s": 0.0853494623655914, "os": -0.15205627705627706, "bg": 9.033876148300432e-07}, {"x": 0.5882352941176471, "y": 0.3356643356643356, "ox": 0.5882352941176471, "oy": 0.3356643356643356, "term": "practices", "cat25k": 34, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 98, "s": 0.02889784946236559, "os": -0.25063131313131315, "bg": 7.665786011368677e-06}, {"x": 0.5424836601307189, "y": 0.6363636363636364, "ox": 0.5424836601307189, "oy": 0.6363636363636364, "term": "create", "cat25k": 69, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 98, "ncat": 87, "s": 0.8635752688172044, "os": 0.09343434343434343, "bg": 2.697741518608718e-06}, {"x": 0.196078431372549, "y": 0.20979020979020974, "ox": 0.196078431372549, "oy": 0.20979020979020974, "term": "capabilities", "cat25k": 21, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 30, "s": 0.6821236559139786, "os": 0.01397907647907648, "bg": 6.646562292744956e-06}, {"x": 0.3071895424836601, "y": 0.2727272727272727, "ox": 0.3071895424836601, "oy": 0.2727272727272727, "term": "decisions", "cat25k": 27, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 47, "s": 0.459005376344086, "os": -0.03391053391053389, "bg": 5.477408365505676e-06}, {"x": 0.326797385620915, "y": 0.1048951048951049, "ox": 0.326797385620915, "oy": 0.1048951048951049, "term": "coding", "cat25k": 11, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 50, "s": 0.043010752688172046, "os": -0.22005772005772006, "bg": 1.1670798727918849e-05}, {"x": 0.4967320261437908, "y": 0.3356643356643356, "ox": 0.4967320261437908, "oy": 0.3356643356643356, "term": "testing", "cat25k": 34, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 78, "s": 0.0799731182795699, "os": -0.1597222222222222, "bg": 4.448792413255389e-06}, {"x": 0.261437908496732, "y": 0.0909090909090909, "ox": 0.261437908496732, "oy": 0.0909090909090909, "term": "monitoring", "cat25k": 9, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 40, "s": 0.07325268817204302, "os": -0.169011544011544, "bg": 2.5402631712645428e-06}, {"x": 0.4967320261437908, "y": 0.5524475524475524, "ox": 0.4967320261437908, "oy": 0.5524475524475524, "term": "production", "cat25k": 56, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 78, "s": 0.7788978494623656, "os": 0.05555555555555558, "bg": 3.484268653828879e-06}, {"x": 0.2352941176470588, "y": 0.12587412587412586, "ox": 0.2352941176470588, "oy": 0.12587412587412586, "term": "environments", "cat25k": 13, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 36, "s": 0.1478494623655914, "os": -0.10831529581529581, "bg": 8.05244527785596e-06}, {"x": 0.09803921568627451, "y": 0.020979020979020976, "ox": 0.09803921568627451, "oy": 0.020979020979020976, "term": "saas", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 15, "s": 0.20900537634408603, "os": -0.07611832611832613, "bg": 8.900360464598817e-05}, {"x": 0.4117647058823529, "y": 0.30769230769230765, "ox": 0.4117647058823529, "oy": 0.30769230769230765, "term": "platforms", "cat25k": 31, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 64, "s": 0.15591397849462366, "os": -0.10308441558441561, "bg": 1.6316046658152613e-05}, {"x": 0.5816993464052287, "y": 0.2797202797202797, "ox": 0.5816993464052287, "oy": 0.2797202797202797, "term": "maintain", "cat25k": 28, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 97, "s": 0.02486559139784946, "os": -0.2996933621933622, "bg": 8.297333303697115e-06}, {"x": 0.2549019607843137, "y": 0.4195804195804195, "ox": 0.2549019607843137, "oy": 0.4195804195804195, "term": "perform", "cat25k": 43, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 39, "s": 0.935483870967742, "os": 0.1638708513708514, "bg": 6.5660963946286975e-06}, {"x": 0.6274509803921569, "y": 0.4615384615384615, "ox": 0.6274509803921569, "oy": 0.4615384615384615, "term": "scale", "cat25k": 47, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 107, "s": 0.07526881720430109, "os": -0.16459235209235212, "bg": 7.501915251898148e-06}, {"x": 0.3594771241830065, "y": 0.5174825174825174, "ox": 0.3594771241830065, "oy": 0.5174825174825174, "term": "closely", "cat25k": 53, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 55, "s": 0.9287634408602151, "os": 0.15719696969696972, "bg": 1.732436739572713e-05}, {"x": 0.058823529411764705, "y": 0.14685314685314685, "ox": 0.058823529411764705, "oy": 0.14685314685314685, "term": "groups", "cat25k": 15, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 9, "s": 0.8514784946236559, "os": 0.08784271284271286, "bg": 4.7658857516625137e-07}, {"x": 0.5882352941176471, "y": 0.18881118881118877, "ox": 0.5882352941176471, "oy": 0.18881118881118877, "term": "like", "cat25k": 19, "ncat25k": 56, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 98, "s": 0.013440860215053765, "os": -0.3964646464646465, "bg": 4.801543875357482e-07}, {"x": 0.9084967320261438, "y": 0.8601398601398601, "ox": 0.9084967320261438, "oy": 0.8601398601398601, "term": "engineering", "cat25k": 126, "ncat25k": 129, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 180, "ncat": 223, "s": 0.33803763440860213, "os": -0.0479797979797979, "bg": 9.367139875745937e-06}, {"x": 0.4640522875816993, "y": 0.5034965034965034, "ox": 0.4640522875816993, "oy": 0.5034965034965034, "term": "customer", "cat25k": 51, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 72, "s": 0.7426075268817205, "os": 0.0394119769119769, "bg": 1.5719223277186271e-06}, {"x": 0.2418300653594771, "y": 0.39860139860139854, "ox": 0.2418300653594771, "oy": 0.39860139860139854, "term": "operations", "cat25k": 41, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 37, "s": 0.9274193548387097, "os": 0.15602453102453104, "bg": 3.053467943271258e-06}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "acquire", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 2.9265579165248876e-06}, {"x": 0.7712418300653595, "y": 0.4405594405594405, "ox": 0.7712418300653595, "oy": 0.4405594405594405, "term": "requirements", "cat25k": 45, "ncat25k": 90, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 64, "ncat": 157, "s": 0.020161290322580645, "os": -0.3282828282828283, "bg": 4.54198373934883e-06}, {"x": 0.934640522875817, "y": 0.7132867132867132, "ox": 0.934640522875817, "oy": 0.7132867132867132, "term": "systems", "cat25k": 84, "ncat25k": 141, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 120, "ncat": 244, "s": 0.04502688172043011, "os": -0.21978715728715725, "bg": 3.255279176341751e-06}, {"x": 0.261437908496732, "y": 0.5734265734265733, "ox": 0.261437908496732, "oy": 0.5734265734265733, "term": "solve", "cat25k": 58, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 83, "ncat": 40, "s": 0.9798387096774195, "os": 0.31015512265512263, "bg": 1.8178154336520643e-05}, {"x": 0.5163398692810457, "y": 0.3006993006993007, "ox": 0.5163398692810457, "oy": 0.3006993006993007, "term": "test", "cat25k": 30, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 82, "s": 0.048387096774193554, "os": -0.21392496392496385, "bg": 1.6120694634799397e-06}, {"x": 0.12418300653594772, "y": 0.034965034965034954, "ox": 0.12418300653594772, "oy": 0.034965034965034954, "term": "troubleshooting", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 19, "s": 0.1814516129032258, "os": -0.0882034632034632, "bg": 7.272586228630717e-06}, {"x": 0.7516339869281046, "y": 0.5314685314685315, "ox": 0.7516339869281046, "oy": 0.5314685314685315, "term": "management", "cat25k": 54, "ncat25k": 88, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 152, "s": 0.046370967741935484, "os": -0.21852453102453107, "bg": 1.5051830881046105e-06}, {"x": 0.1503267973856209, "y": 0.0909090909090909, "ox": 0.1503267973856209, "oy": 0.0909090909090909, "term": "may", "cat25k": 9, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 23, "s": 0.2728494623655914, "os": -0.05862193362193362, "bg": 8.696676007951574e-08}, {"x": 0.31372549019607837, "y": 0.3566433566433566, "ox": 0.31372549019607837, "oy": 0.3566433566433566, "term": "one", "cat25k": 36, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 48, "s": 0.7466397849462366, "os": 0.04292929292929293, "bg": 2.0128476340714656e-07}, {"x": 0.196078431372549, "y": 0.12587412587412586, "ox": 0.196078431372549, "oy": 0.12587412587412586, "term": "following", "cat25k": 13, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 30, "s": 0.23185483870967744, "os": -0.06935425685425686, "bg": 4.3480145993829704e-07}, {"x": 0.09803921568627451, "y": 0.6013986013986014, "ox": 0.09803921568627451, "oy": 0.6013986013986014, "term": "predictive", "cat25k": 65, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 15, "s": 0.995967741935484, "os": 0.5002705627705627, "bg": 8.182060793476374e-05}, {"x": 0.058823529411764705, "y": 0.5104895104895104, "ox": 0.058823529411764705, "oy": 0.5104895104895104, "term": "feature", "cat25k": 52, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 9, "s": 0.9932795698924731, "os": 0.4489538239538239, "bg": 2.578863069688135e-06}, {"x": 0.14379084967320263, "y": 0.2937062937062937, "ox": 0.14379084967320263, "oy": 0.2937062937062937, "term": "extraction", "cat25k": 29, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 22, "s": 0.9220430107526882, "os": 0.14926046176046176, "bg": 1.9907579064194474e-05}, {"x": 0.3333333333333333, "y": 0.2727272727272727, "ox": 0.3333333333333333, "oy": 0.2727272727272727, "term": "driven", "cat25k": 27, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 51, "s": 0.271505376344086, "os": -0.059884559884559874, "bg": 1.0109945659042082e-05}, {"x": 0.3790849673202614, "y": 0.26573426573426573, "ox": 0.3790849673202614, "oy": 0.26573426573426573, "term": "application", "cat25k": 27, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 59, "s": 0.14180107526881722, "os": -0.11228354978354982, "bg": 1.2691588524098181e-06}, {"x": 0.5424836601307189, "y": 0.958041958041958, "ox": 0.5424836601307189, "oy": 0.958041958041958, "term": "machine", "cat25k": 246, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 350, "ncat": 87, "s": 0.9899193548387096, "os": 0.41287878787878796, "bg": 1.2531032047053482e-05}, {"x": 0.673202614379085, "y": 0.979020979020979, "ox": 0.673202614379085, "oy": 0.979020979020979, "term": "learning", "cat25k": 318, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 453, "ncat": 125, "s": 0.9784946236559141, "os": 0.3038419913419913, "bg": 9.817086532106285e-06}, {"x": 0.14379084967320263, "y": 0.2587412587412587, "ox": 0.14379084967320263, "oy": 0.2587412587412587, "term": "making", "cat25k": 26, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 22, "s": 0.8850806451612904, "os": 0.11453823953823955, "bg": 9.494749918264688e-07}, {"x": 0.6405228758169935, "y": 0.5384615384615384, "ox": 0.6405228758169935, "oy": 0.5384615384615384, "term": "related", "cat25k": 55, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 111, "s": 0.16196236559139784, "os": -0.10119047619047628, "bg": 1.6348907130703434e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "utilities", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 4.1313276059132106e-07}, {"x": 0.196078431372549, "y": 0.6573426573426573, "ox": 0.196078431372549, "oy": 0.6573426573426573, "term": "results", "cat25k": 74, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 106, "ncat": 30, "s": 0.9939516129032259, "os": 0.4584235209235209, "bg": 1.0139362820745089e-06}, {"x": 0.0326797385620915, "y": 0.24475524475524474, "ox": 0.0326797385620915, "oy": 0.24475524475524474, "term": "form", "cat25k": 25, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.9556451612903226, "os": 0.21103896103896103, "bg": 3.9707024499224196e-07}, {"x": 0.43137254901960786, "y": 0.41258741258741255, "ox": 0.43137254901960786, "oy": 0.41258741258741255, "term": "internal", "cat25k": 42, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 67, "s": 0.57997311827957, "os": -0.018398268398268358, "bg": 4.8292805592595885e-06}, {"x": 0.6339869281045751, "y": 0.7062937062937062, "ox": 0.6339869281045751, "oy": 0.7062937062937062, "term": "technology", "cat25k": 84, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 119, "ncat": 108, "s": 0.8225806451612903, "os": 0.07196969696969702, "bg": 1.9015310411728087e-06}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "papers", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 5.087549984444816e-07}, {"x": 0.1633986928104575, "y": 0.9300699300699301, "ox": 0.1633986928104575, "oy": 0.9300699300699301, "term": "statistical", "cat25k": 159, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 226, "ncat": 25, "s": 1.0, "os": 0.7617243867243868, "bg": 2.730085872621372e-05}, {"x": 0.3790849673202614, "y": 0.21678321678321674, "ox": 0.3790849673202614, "oy": 0.21678321678321674, "term": "system", "cat25k": 22, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 59, "s": 0.07862903225806453, "os": -0.16089466089466092, "bg": 4.5333705714466224e-07}, {"x": 0.0522875816993464, "y": 0.04195804195804195, "ox": 0.0522875816993464, "oy": 0.04195804195804195, "term": "independent", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.6478494623655914, "os": -0.009830447330447328, "bg": 4.3579704134898354e-07}, {"x": 0.065359477124183, "y": 0.020979020979020976, "ox": 0.065359477124183, "oy": 0.020979020979020976, "term": "resolution", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.3763440860215054, "os": -0.04365079365079365, "bg": 4.6838520561371925e-07}, {"x": 0.09150326797385622, "y": 0.06993006993006992, "ox": 0.09150326797385622, "oy": 0.06993006993006992, "term": "priorities", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 14, "s": 0.5725806451612903, "os": -0.021013708513708512, "bg": 4.222099683597609e-06}, {"x": 0.4117647058823529, "y": 0.8041958041958042, "ox": 0.4117647058823529, "oy": 0.8041958041958042, "term": "modeling", "cat25k": 103, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 147, "ncat": 64, "s": 0.9885752688172044, "os": 0.38997113997113997, "bg": 2.712922814967285e-05}, {"x": 0.058823529411764705, "y": 0.12587412587412586, "ox": 0.058823529411764705, "oy": 0.12587412587412586, "term": "this", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 9, "s": 0.8138440860215054, "os": 0.06700937950937952, "bg": 1.6725773423086245e-08}, {"x": 0.3333333333333333, "y": 0.7062937062937062, "ox": 0.3333333333333333, "oy": 0.7062937062937062, "term": "model", "cat25k": 84, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 119, "ncat": 51, "s": 0.987231182795699, "os": 0.3706709956709957, "bg": 2.1080517383354244e-06}, {"x": 0.2483660130718954, "y": 0.2517482517482517, "ox": 0.2483660130718954, "oy": 0.2517482517482517, "term": "concepts", "cat25k": 25, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 38, "s": 0.6720430107526882, "os": 0.0036976911976911997, "bg": 6.456995797193789e-06}, {"x": 0.2091503267973856, "y": 0.0909090909090909, "ox": 0.2091503267973856, "oy": 0.0909090909090909, "term": "designs", "cat25k": 9, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 32, "s": 0.13239247311827956, "os": -0.11706349206349205, "bg": 2.724286166646946e-06}, {"x": 0.09803921568627451, "y": 0.2727272727272727, "ox": 0.09803921568627451, "oy": 0.2727272727272727, "term": "generation", "cat25k": 27, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 15, "s": 0.9388440860215054, "os": 0.1738816738816739, "bg": 2.700490341533764e-06}, {"x": 0.1372549019607843, "y": 0.05594405594405594, "ox": 0.1372549019607843, "oy": 0.05594405594405594, "term": "functions", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 21, "s": 0.1995967741935484, "os": -0.08035714285714285, "bg": 1.1176002363608887e-06}, {"x": 0.196078431372549, "y": 0.18881118881118877, "ox": 0.196078431372549, "oy": 0.18881118881118877, "term": "different", "cat25k": 19, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 30, "s": 0.6559139784946236, "os": -0.006854256854256863, "bg": 6.337743874924958e-07}, {"x": 0.08496732026143791, "y": 0.11888111888111888, "ox": 0.08496732026143791, "oy": 0.11888111888111888, "term": "uses", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 13, "s": 0.7278225806451614, "os": 0.03409090909090909, "bg": 1.0729073059229598e-06}, {"x": 0.6143790849673203, "y": 0.5454545454545454, "ox": 0.6143790849673203, "oy": 0.5454545454545454, "term": "ensure", "cat25k": 55, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 103, "s": 0.239247311827957, "os": -0.06827200577200576, "bg": 6.108587663989147e-06}, {"x": 0.09150326797385622, "y": 0.5174825174825174, "ox": 0.09150326797385622, "oy": 0.5174825174825174, "term": "accuracy", "cat25k": 53, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 14, "s": 0.991263440860215, "os": 0.423430735930736, "bg": 6.010150468844072e-06}, {"x": 0.0065359477124183, "y": 0.0909090909090909, "ox": 0.0065359477124183, "oy": 0.0909090909090909, "term": "feasibility", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.844758064516129, "os": 0.08423520923520923, "bg": 6.759408976591685e-06}, {"x": 0.1633986928104575, "y": 0.24475524475524474, "ox": 0.1633986928104575, "oy": 0.24475524475524474, "term": "deploy", "cat25k": 25, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 25, "s": 0.8420698924731183, "os": 0.08116883116883117, "bg": 3.0158293342179765e-05}, {"x": 0.14379084967320263, "y": 0.12587412587412586, "ox": 0.14379084967320263, "oy": 0.12587412587412586, "term": "ways", "cat25k": 13, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 22, "s": 0.6149193548387097, "os": -0.017406204906204897, "bg": 1.3746895156331325e-06}, {"x": 0.07189542483660129, "y": 0.1818181818181818, "ox": 0.07189542483660129, "oy": 0.1818181818181818, "term": "achieve", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.8823924731182796, "os": 0.10957792207792208, "bg": 2.6994235636330737e-06}, {"x": 0.6601307189542484, "y": 0.6153846153846153, "ox": 0.6601307189542484, "oy": 0.6153846153846153, "term": "performance", "cat25k": 67, "ncat25k": 69, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 95, "ncat": 120, "s": 0.3696236559139785, "os": -0.044282106782106756, "bg": 3.076014711619794e-06}, {"x": 0.1372549019607843, "y": 0.14685314685314685, "ox": 0.1372549019607843, "oy": 0.14685314685314685, "term": "improvement", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 21, "s": 0.6780913978494624, "os": 0.009920634920634941, "bg": 2.2572008334230393e-06}, {"x": 0.6666666666666666, "y": 0.5384615384615384, "ox": 0.6666666666666666, "oy": 0.5384615384615384, "term": "environment", "cat25k": 55, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 124, "s": 0.11290322580645162, "os": -0.12716450216450215, "bg": 3.959233613893383e-06}, {"x": 0.3986928104575163, "y": 0.4335664335664335, "ox": 0.3986928104575163, "oy": 0.4335664335664335, "term": "multiple", "cat25k": 44, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 62, "s": 0.7298387096774194, "os": 0.03490259740259738, "bg": 3.773474577807438e-06}, {"x": 0.7254901960784313, "y": 0.7622377622377622, "ox": 0.7254901960784313, "oy": 0.7622377622377622, "term": "including", "cat25k": 93, "ncat25k": 85, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 132, "ncat": 148, "s": 0.7311827956989247, "os": 0.03661616161616155, "bg": 2.613449480255135e-06}, {"x": 0.1764705882352941, "y": 0.3356643356643356, "ox": 0.1764705882352941, "oy": 0.3356643356643356, "term": "external", "cat25k": 34, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 27, "s": 0.9294354838709677, "os": 0.15845959595959597, "bg": 2.9594402206984792e-06}, {"x": 0.3006535947712418, "y": 0.3006993006993007, "ox": 0.3006535947712418, "oy": 0.3006993006993007, "term": "customers", "cat25k": 30, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 46, "s": 0.6686827956989247, "os": 0.0003607503607503948, "bg": 2.1090488944241286e-06}, {"x": 0.1764705882352941, "y": 0.13986013986013984, "ox": 0.1764705882352941, "oy": 0.13986013986013984, "term": "monitor", "cat25k": 14, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 27, "s": 0.4509408602150538, "os": -0.03598484848484848, "bg": 2.1398309893192157e-06}, {"x": 0.07189542483660129, "y": 0.13986013986013984, "ox": 0.07189542483660129, "oy": 0.13986013986013984, "term": "proactively", "cat25k": 14, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 11, "s": 0.8151881720430108, "os": 0.06791125541125542, "bg": 7.995491574428354e-05}, {"x": 0.0457516339869281, "y": 0.19580419580419578, "ox": 0.0457516339869281, "oy": 0.19580419580419578, "term": "evaluation", "cat25k": 20, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.922715053763441, "os": 0.14944083694083693, "bg": 1.4890426668158548e-06}, {"x": 0.22222222222222218, "y": 0.1748251748251748, "ox": 0.22222222222222218, "oy": 0.1748251748251748, "term": "role", "cat25k": 18, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 34, "s": 0.342741935483871, "os": -0.04671717171717171, "bg": 1.5995767357288442e-06}, {"x": 0.11111111111111112, "y": 0.28671328671328666, "ox": 0.11111111111111112, "oy": 0.28671328671328666, "term": "works", "cat25k": 29, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 17, "s": 0.9401881720430109, "os": 0.17478354978354982, "bg": 1.175728782073704e-06}, {"x": 0.2679738562091503, "y": 0.13986013986013984, "ox": 0.2679738562091503, "oy": 0.13986013986013984, "term": "wide", "cat25k": 14, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 41, "s": 0.11357526881720431, "os": -0.12689393939393936, "bg": 1.3904403125573057e-06}, {"x": 0.3660130718954248, "y": 0.23076923076923073, "ox": 0.3660130718954248, "oy": 0.23076923076923073, "term": "variety", "cat25k": 23, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 56, "s": 0.10013440860215055, "os": -0.13401875901875904, "bg": 3.5653384525718045e-06}, {"x": 0.47712418300653586, "y": 0.7202797202797203, "ox": 0.47712418300653586, "oy": 0.7202797202797203, "term": "company", "cat25k": 85, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 121, "ncat": 74, "s": 0.965725806451613, "os": 0.2417027417027417, "bg": 1.2023940331140612e-06}, {"x": 0.8039215686274509, "y": 0.5944055944055944, "ox": 0.8039215686274509, "oy": 0.5944055944055944, "term": "software", "cat25k": 63, "ncat25k": 100, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 174, "s": 0.051075268817204304, "os": -0.20797258297258303, "bg": 1.424725302438151e-06}, {"x": 0.058823529411764705, "y": 0.0769230769230769, "ox": 0.058823529411764705, "oy": 0.0769230769230769, "term": "driving", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6922043010752688, "os": 0.0183982683982684, "bg": 9.97149821735787e-07}, {"x": 0.5620915032679737, "y": 0.46853146853146843, "ox": 0.5620915032679737, "oy": 0.46853146853146843, "term": "based", "cat25k": 48, "ncat25k": 54, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 68, "ncat": 93, "s": 0.17473118279569894, "os": -0.09271284271284275, "bg": 1.275319149706384e-06}, {"x": 0.2026143790849673, "y": 0.22377622377622375, "ox": 0.2026143790849673, "oy": 0.22377622377622375, "term": "the", "cat25k": 22, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 31, "s": 0.698252688172043, "os": 0.02137445887445885, "bg": 5.446074173635616e-09}, {"x": 0.13071895424836602, "y": 0.08391608391608392, "ox": 0.13071895424836602, "oy": 0.08391608391608392, "term": "responsible", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 20, "s": 0.34408602150537637, "os": -0.04608585858585858, "bg": 9.785911025120813e-07}, {"x": 0.1764705882352941, "y": 0.14685314685314685, "ox": 0.1764705882352941, "oy": 0.14685314685314685, "term": "planning", "cat25k": 15, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 27, "s": 0.5047043010752689, "os": -0.029040404040404033, "bg": 9.234395240369528e-07}, {"x": 0.07189542483660129, "y": 0.048951048951048945, "ox": 0.07189542483660129, "oy": 0.048951048951048945, "term": "handling", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.564516129032258, "os": -0.022366522366522368, "bg": 1.1354966373686454e-06}, {"x": 0.0784313725490196, "y": 0.12587412587412586, "ox": 0.0784313725490196, "oy": 0.12587412587412586, "term": "many", "cat25k": 13, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 12, "s": 0.7553763440860216, "os": 0.04752886002886003, "bg": 1.8806011059344953e-07}, {"x": 0.08496732026143791, "y": 0.11888111888111888, "ox": 0.08496732026143791, "oy": 0.11888111888111888, "term": "social", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 13, "s": 0.7278225806451614, "os": 0.03409090909090909, "bg": 4.296552062083975e-07}, {"x": 0.7647058823529412, "y": 0.7902097902097902, "ox": 0.7647058823529412, "oy": 0.7902097902097902, "term": "etc", "cat25k": 98, "ncat25k": 89, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 140, "ncat": 155, "s": 0.7096774193548387, "os": 0.025432900432900363, "bg": 1.1432654187168905e-05}, {"x": 0.2483660130718954, "y": 0.1678321678321678, "ox": 0.2483660130718954, "oy": 0.1678321678321678, "term": "open", "cat25k": 17, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 38, "s": 0.2002688172043011, "os": -0.07963564213564212, "bg": 5.172035951138893e-07}, {"x": 0.2483660130718954, "y": 0.39160839160839156, "ox": 0.2483660130718954, "oy": 0.39160839160839156, "term": "problem", "cat25k": 40, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 38, "s": 0.9106182795698925, "os": 0.1425865800865801, "bg": 1.3466336034569304e-06}, {"x": 0.0522875816993464, "y": 0.11888111888111888, "ox": 0.0522875816993464, "oy": 0.11888111888111888, "term": "potential", "cat25k": 12, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.8118279569892474, "os": 0.06655844155844157, "bg": 7.585417720154763e-07}, {"x": 0.261437908496732, "y": 0.3566433566433566, "ox": 0.261437908496732, "oy": 0.3566433566433566, "term": "impact", "cat25k": 36, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 40, "s": 0.8662634408602151, "os": 0.0948773448773449, "bg": 3.01155155889202e-06}, {"x": 0.12418300653594772, "y": 0.13986013986013984, "ox": 0.12418300653594772, "oy": 0.13986013986013984, "term": "execute", "cat25k": 14, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 19, "s": 0.6868279569892474, "os": 0.015963203463203485, "bg": 8.884818578839095e-06}, {"x": 0.45751633986928103, "y": 0.48251748251748244, "ox": 0.45751633986928103, "oy": 0.48251748251748244, "term": "needs", "cat25k": 49, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 71, "s": 0.7056451612903226, "os": 0.02507215007215008, "bg": 2.280144523322082e-06}, {"x": 0.1503267973856209, "y": 0.4195804195804195, "ox": 0.1503267973856209, "oy": 0.4195804195804195, "term": "apply", "cat25k": 43, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 23, "s": 0.9697580645161291, "os": 0.26776695526695526, "bg": 2.1983193952904878e-06}, {"x": 0.1895424836601307, "y": 0.08391608391608392, "ox": 0.1895424836601307, "oy": 0.08391608391608392, "term": "long", "cat25k": 8, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 29, "s": 0.14986559139784947, "os": -0.10452741702741704, "bg": 3.246238046564696e-07}, {"x": 0.07189542483660129, "y": 0.048951048951048945, "ox": 0.07189542483660129, "oy": 0.048951048951048945, "term": "improving", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.564516129032258, "os": -0.022366522366522368, "bg": 2.007087919482769e-06}, {"x": 0.522875816993464, "y": 0.5524475524475524, "ox": 0.522875816993464, "oy": 0.5524475524475524, "term": "engineers", "cat25k": 56, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 83, "s": 0.7184139784946237, "os": 0.029581529581529598, "bg": 1.8516927055746683e-05}, {"x": 0.2091503267973856, "y": 0.11888111888111888, "ox": 0.2091503267973856, "oy": 0.11888111888111888, "term": "core", "cat25k": 12, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 32, "s": 0.17943548387096775, "os": -0.08928571428571427, "bg": 1.9093992398720596e-06}, {"x": 0.888888888888889, "y": 0.45454545454545453, "ox": 0.888888888888889, "oy": 0.45454545454545453, "term": "technologies", "cat25k": 46, "ncat25k": 122, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 211, "s": 0.00806451612903226, "os": -0.4312770562770563, "bg": 9.502059657189356e-06}, {"x": 0.15686274509803919, "y": 0.0909090909090909, "ox": 0.15686274509803919, "oy": 0.0909090909090909, "term": "focus", "cat25k": 9, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 24, "s": 0.24865591397849462, "os": -0.06511544011544011, "bg": 1.1262079969503505e-06}, {"x": 0.11111111111111112, "y": 0.02797202797202797, "ox": 0.11111111111111112, "oy": 0.02797202797202797, "term": "depth", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 17, "s": 0.19690860215053763, "os": -0.08216089466089466, "bg": 1.3141261900937976e-06}, {"x": 0.058823529411764705, "y": 0.04195804195804195, "ox": 0.058823529411764705, "oy": 0.04195804195804195, "term": "general", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.6250000000000001, "os": -0.016323953823953817, "bg": 9.62037172975265e-08}, {"x": 0.16993464052287582, "y": 0.048951048951048945, "ox": 0.16993464052287582, "oy": 0.048951048951048945, "term": "accurate", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 26, "s": 0.12634408602150538, "os": -0.11976911976911978, "bg": 2.6164556666958975e-06}, {"x": 0.0457516339869281, "y": 0.20979020979020974, "ox": 0.0457516339869281, "oy": 0.20979020979020974, "term": "phd", "cat25k": 21, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 7, "s": 0.9348118279569892, "os": 0.16332972582972582, "bg": 6.300130736226466e-06}, {"x": 0.16993464052287582, "y": 0.7972027972027973, "ox": 0.16993464052287582, "oy": 0.7972027972027973, "term": "statistics", "cat25k": 101, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 144, "ncat": 26, "s": 0.9986559139784947, "os": 0.6232864357864358, "bg": 4.907970500787656e-06}, {"x": 0.0457516339869281, "y": 0.2937062937062937, "ox": 0.0457516339869281, "oy": 0.2937062937062937, "term": "math", "cat25k": 29, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 7, "s": 0.9670698924731184, "os": 0.24666305916305914, "bg": 3.413158520177061e-06}, {"x": 0.29411764705882354, "y": 0.0979020979020979, "ox": 0.29411764705882354, "oy": 0.0979020979020979, "term": "background", "cat25k": 10, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 45, "s": 0.06048387096774194, "os": -0.194534632034632, "bg": 2.25644510039431e-06}, {"x": 0.07189542483660129, "y": 0.05594405594405594, "ox": 0.07189542483660129, "oy": 0.05594405594405594, "term": "willingness", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.6303763440860215, "os": -0.01542207792207792, "bg": 1.0172953596607589e-05}, {"x": 0.3790849673202614, "y": 0.3636363636363636, "ox": 0.3790849673202614, "oy": 0.3636363636363636, "term": "various", "cat25k": 37, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 59, "s": 0.6323924731182796, "os": -0.015061327561327553, "bg": 2.455152232865051e-06}, {"x": 0.13071895424836602, "y": 0.06293706293706293, "ox": 0.13071895424836602, "oy": 0.06293706293706293, "term": "components", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 20, "s": 0.24260752688172044, "os": -0.0669191919191919, "bg": 1.05367915785747e-06}, {"x": 0.8627450980392157, "y": 0.5244755244755244, "ox": 0.8627450980392157, "oy": 0.5244755244755244, "term": "big", "cat25k": 53, "ncat25k": 116, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 202, "s": 0.01814516129032258, "os": -0.33585858585858586, "bg": 2.5649154497303836e-06}, {"x": 0.11764705882352941, "y": 0.06993006993006992, "ox": 0.11764705882352941, "oy": 0.06993006993006992, "term": "libraries", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 18, "s": 0.34206989247311825, "os": -0.04698773448773448, "bg": 1.9869219378818657e-06}, {"x": 0.1895424836601307, "y": 0.1678321678321678, "ox": 0.1895424836601307, "oy": 0.1678321678321678, "term": "c", "cat25k": 17, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 29, "s": 0.5719086021505376, "os": -0.02119408369408371, "bg": 1.776425918398709e-07}, {"x": 0.2745098039215686, "y": 0.1818181818181818, "ox": 0.2745098039215686, "oy": 0.1818181818181818, "term": "familiarity", "cat25k": 18, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 42, "s": 0.1760752688172043, "os": -0.0917207792207792, "bg": 7.373056034141586e-05}, {"x": 0.7189542483660131, "y": 0.41258741258741255, "ox": 0.7189542483660131, "oy": 0.41258741258741255, "term": "spark", "cat25k": 42, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 146, "s": 0.024193548387096774, "os": -0.30411255411255406, "bg": 8.36873966983697e-05}, {"x": 0.2483660130718954, "y": 0.0769230769230769, "ox": 0.2483660130718954, "oy": 0.0769230769230769, "term": "mapreduce", "cat25k": 8, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 38, "s": 0.07190860215053765, "os": -0.1699134199134199, "bg": 0.0012173459374184815}, {"x": 0.15686274509803919, "y": 0.30769230769230765, "ox": 0.15686274509803919, "oy": 0.30769230769230765, "term": "training", "cat25k": 31, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 24, "s": 0.9240591397849464, "os": 0.15016233766233766, "bg": 7.718077865372161e-07}, {"x": 0.1045751633986928, "y": 0.11888111888111888, "ox": 0.1045751633986928, "oy": 0.11888111888111888, "term": "interest", "cat25k": 12, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 16, "s": 0.6841397849462365, "os": 0.014610389610389615, "bg": 5.483848466809443e-07}, {"x": 0.0326797385620915, "y": 0.12587412587412586, "ox": 0.0326797385620915, "oy": 0.12587412587412586, "term": "physics", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.862231182795699, "os": 0.09298340548340549, "bg": 1.543350708330873e-06}, {"x": 0.7124183006535947, "y": 0.30769230769230765, "ox": 0.7124183006535947, "oy": 0.30769230769230765, "term": "hadoop", "cat25k": 31, "ncat25k": 82, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 143, "s": 0.011424731182795701, "os": -0.4017857142857143, "bg": 0.0046378393125085255}, {"x": 0.673202614379085, "y": 0.11888111888111888, "ox": 0.673202614379085, "oy": 0.11888111888111888, "term": "aws", "cat25k": 12, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 125, "s": 0.0020161290322580645, "os": -0.5503246753246753, "bg": 0.00033896686718227825}, {"x": 0.065359477124183, "y": 0.1678321678321678, "ox": 0.065359477124183, "oy": 0.1678321678321678, "term": "conduct", "cat25k": 17, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 10, "s": 0.8750000000000001, "os": 0.10218253968253968, "bg": 2.263947740894777e-06}, {"x": 0.013071895424836598, "y": 0.1748251748251748, "ox": 0.013071895424836598, "oy": 0.1748251748251748, "term": "records", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 2, "s": 0.9321236559139786, "os": 0.16107503607503607, "bg": 5.301820360384366e-07}, {"x": 0.22222222222222218, "y": 0.0979020979020979, "ox": 0.22222222222222218, "oy": 0.0979020979020979, "term": "writing", "cat25k": 10, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 34, "s": 0.11626344086021506, "os": -0.1231060606060606, "bg": 1.1531916759885997e-06}, {"x": 0.1503267973856209, "y": 0.23776223776223773, "ox": 0.1503267973856209, "oy": 0.23776223776223773, "term": "clean", "cat25k": 24, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 23, "s": 0.8501344086021506, "os": 0.08721139971139971, "bg": 2.5075742489985596e-06}, {"x": 0.15686274509803919, "y": 0.06993006993006992, "ox": 0.15686274509803919, "oy": 0.06993006993006992, "term": "integrating", "cat25k": 7, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 24, "s": 0.1875, "os": -0.08594877344877344, "bg": 1.1172903474493658e-05}, {"x": 0.1830065359477124, "y": 0.06993006993006992, "ox": 0.1830065359477124, "oy": 0.06993006993006992, "term": "apis", "cat25k": 7, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 28, "s": 0.14247311827956988, "os": -0.11192279942279942, "bg": 3.746764102253137e-05}, {"x": 0.0326797385620915, "y": 0.0909090909090909, "ox": 0.0326797385620915, "oy": 0.0909090909090909, "term": "discover", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.790994623655914, "os": 0.058261183261183264, "bg": 1.3881919544019046e-06}, {"x": 0.0522875816993464, "y": 0.22377622377622375, "ox": 0.0522875816993464, "oy": 0.22377622377622375, "term": "interesting", "cat25k": 22, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 8, "s": 0.9381720430107527, "os": 0.17072510822510822, "bg": 1.699822483288514e-06}, {"x": 0.1372549019607843, "y": 0.2587412587412587, "ox": 0.1372549019607843, "oy": 0.2587412587412587, "term": "trends", "cat25k": 26, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 21, "s": 0.89247311827957, "os": 0.12103174603174605, "bg": 4.468343060582335e-06}, {"x": 0.1045751633986928, "y": 0.13286713286713286, "ox": 0.1045751633986928, "oy": 0.13286713286713286, "term": "validation", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 16, "s": 0.7163978494623656, "os": 0.02849927849927851, "bg": 8.035476398715287e-06}, {"x": 0.45751633986928103, "y": 0.13286713286713286, "ox": 0.45751633986928103, "oy": 0.13286713286713286, "term": "designing", "cat25k": 13, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 71, "s": 0.020833333333333332, "os": -0.32215007215007213, "bg": 1.8839280527876638e-05}, {"x": 0.0457516339869281, "y": 0.14685314685314685, "ox": 0.0457516339869281, "oy": 0.14685314685314685, "term": "visualizations", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 7, "s": 0.872983870967742, "os": 0.10082972582972584, "bg": 0.0001421915269084769}, {"x": 0.1503267973856209, "y": 0.28671328671328666, "ox": 0.1503267973856209, "oy": 0.28671328671328666, "term": "ideas", "cat25k": 29, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 23, "s": 0.9045698924731184, "os": 0.13582251082251084, "bg": 1.8993866687540134e-06}, {"x": 0.0784313725490196, "y": 0.18881118881118877, "ox": 0.0784313725490196, "oy": 0.18881118881118877, "term": "leaders", "cat25k": 19, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 12, "s": 0.883736559139785, "os": 0.11002886002886003, "bg": 2.239928313104531e-06}, {"x": 0.1045751633986928, "y": 0.05594405594405594, "ox": 0.1045751633986928, "oy": 0.05594405594405594, "term": "curious", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 16, "s": 0.33870967741935487, "os": -0.047889610389610385, "bg": 5.62023641056108e-06}, {"x": 0.9477124183006537, "y": 0.8251748251748251, "ox": 0.9477124183006537, "oy": 0.8251748251748251, "term": "you", "cat25k": 110, "ncat25k": 148, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 157, "ncat": 257, "s": 0.12163978494623658, "os": -0.1216630591630592, "bg": 2.7634433489756365e-07}, {"x": 0.13071895424836602, "y": 0.0909090909090909, "ox": 0.13071895424836602, "oy": 0.0909090909090909, "term": "know", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 20, "s": 0.39852150537634407, "os": -0.03914141414141413, "bg": 2.1555855958544823e-07}, {"x": 0.4836601307189542, "y": 0.1678321678321678, "ox": 0.4836601307189542, "oy": 0.1678321678321678, "term": "scala", "cat25k": 17, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 75, "s": 0.023521505376344086, "os": -0.3134018759018759, "bg": 0.0001686830539129783}, {"x": 0.058823529411764705, "y": 0.034965034965034954, "ox": 0.058823529411764705, "oy": 0.034965034965034954, "term": "constantly", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5571236559139785, "os": -0.023268398268398265, "bg": 2.6607994771148912e-06}, {"x": 0.3333333333333333, "y": 0.13286713286713286, "ox": 0.3333333333333333, "oy": 0.13286713286713286, "term": "source", "cat25k": 13, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 51, "s": 0.05913978494623656, "os": -0.19877344877344877, "bg": 7.775859300491473e-07}, {"x": 0.1830065359477124, "y": 0.13986013986013984, "ox": 0.1830065359477124, "oy": 0.13986013986013984, "term": "extract", "cat25k": 14, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 28, "s": 0.3850806451612903, "os": -0.042478354978354976, "bg": 8.693599256045242e-06}, {"x": 0.16993464052287582, "y": 0.034965034965034954, "ox": 0.16993464052287582, "oy": 0.034965034965034954, "term": "api", "cat25k": 4, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 26, "s": 0.10147849462365592, "os": -0.13365800865800867, "bg": 4.333650551893893e-06}, {"x": 0.11111111111111112, "y": 0.18881118881118877, "ox": 0.11111111111111112, "oy": 0.18881118881118877, "term": "bring", "cat25k": 19, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 17, "s": 0.8360215053763441, "os": 0.07756132756132757, "bg": 1.525432108113198e-06}, {"x": 0.22222222222222218, "y": 0.20279720279720279, "ox": 0.22222222222222218, "oy": 0.20279720279720279, "term": "transformation", "cat25k": 20, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 34, "s": 0.5793010752688172, "os": -0.018939393939393923, "bg": 1.0977325202213657e-05}, {"x": 0.11111111111111112, "y": 0.06293706293706293, "ox": 0.11111111111111112, "oy": 0.06293706293706293, "term": "two", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 17, "s": 0.34005376344086025, "os": -0.04743867243867243, "bg": 1.1778591856160634e-07}, {"x": 0.0784313725490196, "y": 0.02797202797202797, "ox": 0.0784313725490196, "oy": 0.02797202797202797, "term": "map", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.3279569892473118, "os": -0.04969336219336219, "bg": 1.0330676863268561e-07}, {"x": 0.2352941176470588, "y": 0.1818181818181818, "ox": 0.2352941176470588, "oy": 0.1818181818181818, "term": "tableau", "cat25k": 18, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 36, "s": 0.3030913978494624, "os": -0.052759740259740256, "bg": 0.00021192849805931605}, {"x": 0.2745098039215686, "y": 0.22377622377622375, "ox": 0.2745098039215686, "oy": 0.22377622377622375, "term": "better", "cat25k": 22, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 42, "s": 0.3266129032258065, "os": -0.050054112554112545, "bg": 9.417160124796716e-07}, {"x": 0.09803921568627451, "y": 0.0909090909090909, "ox": 0.09803921568627451, "oy": 0.0909090909090909, "term": "serve", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 15, "s": 0.6565860215053764, "os": -0.006673881673881679, "bg": 1.5226354165324165e-06}, {"x": 0.21568627450980388, "y": 0.24475524475524474, "ox": 0.21568627450980388, "oy": 0.24475524475524474, "term": "manage", "cat25k": 25, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 33, "s": 0.7170698924731184, "os": 0.02922077922077923, "bg": 3.72399124128213e-06}, {"x": 0.5032679738562091, "y": 0.30769230769230765, "ox": 0.5032679738562091, "oy": 0.30769230769230765, "term": "developing", "cat25k": 31, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 79, "s": 0.06115591397849463, "os": -0.19399350649350644, "bg": 5.740977190864248e-06}, {"x": 0.954248366013072, "y": 0.8811188811188811, "ox": 0.954248366013072, "oy": 0.8811188811188811, "term": "solutions", "cat25k": 138, "ncat25k": 153, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 197, "ncat": 266, "s": 0.21841397849462366, "os": -0.07260101010101017, "bg": 8.720658902850512e-06}, {"x": 0.2875816993464052, "y": 0.0769230769230769, "ox": 0.2875816993464052, "oy": 0.0769230769230769, "term": "batch", "cat25k": 8, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 44, "s": 0.05040322580645162, "os": -0.20887445887445888, "bg": 1.3896681207227437e-05}, {"x": 0.5947712418300654, "y": 0.7272727272727272, "ox": 0.5947712418300654, "oy": 0.7272727272727272, "term": "use", "cat25k": 88, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 100, "s": 0.8985215053763441, "os": 0.13176406926406925, "bg": 6.249471086430384e-07}, {"x": 0.13071895424836602, "y": 0.1818181818181818, "ox": 0.13071895424836602, "oy": 0.1818181818181818, "term": "cases", "cat25k": 18, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 20, "s": 0.7661290322580646, "os": 0.051136363636363646, "bg": 1.0656020159707567e-06}, {"x": 0.2745098039215686, "y": 0.3496503496503496, "ox": 0.2745098039215686, "oy": 0.3496503496503496, "term": "user", "cat25k": 35, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 42, "s": 0.8266129032258065, "os": 0.07494588744588748, "bg": 5.813094326696374e-07}, {"x": 0.16993464052287582, "y": 0.15384615384615385, "ox": 0.16993464052287582, "oy": 0.15384615384615385, "term": "passion", "cat25k": 15, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 26, "s": 0.6297043010752689, "os": -0.015602453102453118, "bg": 7.136978163300156e-06}, {"x": 0.2745098039215686, "y": 0.3636363636363636, "ox": 0.2745098039215686, "oy": 0.3636363636363636, "term": "solving", "cat25k": 37, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 42, "s": 0.8541666666666667, "os": 0.08883477633477638, "bg": 1.8969212967353985e-05}, {"x": 0.5424836601307189, "y": 0.6783216783216782, "ox": 0.5424836601307189, "oy": 0.6783216783216782, "term": "computer", "cat25k": 78, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 111, "ncat": 87, "s": 0.9038978494623656, "os": 0.13510101010101017, "bg": 1.765825662912915e-06}, {"x": 0.11764705882352941, "y": 0.3706293706293706, "ox": 0.11764705882352941, "oy": 0.3706293706293706, "term": "mathematics", "cat25k": 38, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 18, "s": 0.9684139784946237, "os": 0.25162337662337664, "bg": 5.7854799479483585e-06}, {"x": 0.12418300653594772, "y": 0.08391608391608392, "ox": 0.12418300653594772, "oy": 0.08391608391608392, "term": "discipline", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 19, "s": 0.39650537634408606, "os": -0.03959235209235208, "bg": 5.247365906949645e-06}, {"x": 0.013071895424836598, "y": 0.06293706293706293, "ox": 0.013071895424836598, "oy": 0.06293706293706293, "term": "graduate", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7573924731182795, "os": 0.04996392496392497, "bg": 5.272525836095578e-07}, {"x": 0.8431372549019608, "y": 0.7762237762237761, "ox": 0.8431372549019608, "oy": 0.7762237762237761, "term": "years", "cat25k": 95, "ncat25k": 115, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 136, "ncat": 199, "s": 0.2452956989247312, "os": -0.06637806637806631, "bg": 1.982705493264309e-06}, {"x": 0.22222222222222218, "y": 0.08391608391608392, "ox": 0.22222222222222218, "oy": 0.08391608391608392, "term": "equivalent", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 34, "s": 0.09811827956989248, "os": -0.1369949494949495, "bg": 3.812496767665784e-06}, {"x": 0.4117647058823529, "y": 0.2797202797202797, "ox": 0.4117647058823529, "oy": 0.2797202797202797, "term": "implementation", "cat25k": 28, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 64, "s": 0.10752688172043012, "os": -0.1308621933621934, "bg": 4.42598298847904e-06}, {"x": 0.065359477124183, "y": 0.08391608391608392, "ox": 0.065359477124183, "oy": 0.08391608391608392, "term": "especially", "cat25k": 8, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 10, "s": 0.6935483870967741, "os": 0.01884920634920635, "bg": 7.40530599599548e-07}, {"x": 0.3529411764705882, "y": 0.11188811188811187, "ox": 0.3529411764705882, "oy": 0.11188811188811187, "term": "similar", "cat25k": 11, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 54, "s": 0.033602150537634407, "os": -0.2390873015873016, "bg": 1.1741879175878765e-06}, {"x": 0.6862745098039216, "y": 0.6223776223776223, "ox": 0.6862745098039216, "oy": 0.6223776223776223, "term": "programming", "cat25k": 67, "ncat25k": 74, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 128, "s": 0.26142473118279574, "os": -0.06331168831168832, "bg": 8.834362815961825e-06}, {"x": 0.5098039215686274, "y": 0.3286713286713286, "ox": 0.5098039215686274, "oy": 0.3286713286713286, "term": "languages", "cat25k": 33, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 80, "s": 0.06720430107526883, "os": -0.17965367965367968, "bg": 6.78118142063081e-06}, {"x": 0.42483660130718953, "y": 0.20279720279720279, "ox": 0.42483660130718953, "oy": 0.20279720279720279, "term": "relational", "cat25k": 20, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 66, "s": 0.04233870967741936, "os": -0.2202380952380952, "bg": 6.054493629238743e-05}, {"x": 0.6405228758169935, "y": 0.28671328671328666, "ox": 0.6405228758169935, "oy": 0.28671328671328666, "term": "databases", "cat25k": 29, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 111, "s": 0.016129032258064516, "os": -0.3511904761904762, "bg": 1.4010243792528752e-05}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "oversee", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 7.975228938915727e-06}, {"x": 0.019607843137254898, "y": 0.2587412587412587, "ox": 0.019607843137254898, "oy": 0.2587412587412587, "term": "timelines", "cat25k": 26, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 3, "s": 0.9643817204301075, "os": 0.2379148629148629, "bg": 6.487482402703983e-05}, {"x": 0.6143790849673203, "y": 0.5454545454545454, "ox": 0.6143790849673203, "oy": 0.5454545454545454, "term": "a", "cat25k": 55, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 103, "s": 0.239247311827957, "os": -0.06827200577200576, "bg": 4.0082564206425604e-08}, {"x": 0.058823529411764705, "y": 0.048951048951048945, "ox": 0.058823529411764705, "oy": 0.048951048951048945, "term": "centric", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.6505376344086021, "os": -0.009379509379509376, "bg": 1.6228118664060702e-05}, {"x": 0.065359477124183, "y": 0.04195804195804195, "ox": 0.065359477124183, "oy": 0.04195804195804195, "term": "mindset", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5598118279569892, "os": -0.022817460317460313, "bg": 2.5222191745407194e-05}, {"x": 0.0, "y": 0.20979020979020974, "ox": 0.0, "oy": 0.20979020979020974, "term": "mathematical", "cat25k": 21, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 0, "s": 0.9543010752688172, "os": 0.2087842712842713, "bg": 5.428757867627339e-06}, {"x": 0.0065359477124183, "y": 0.06293706293706293, "ox": 0.0065359477124183, "oy": 0.06293706293706293, "term": "algorithmic", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7849462365591399, "os": 0.056457431457431456, "bg": 2.8669848064140187e-05}, {"x": 0.0261437908496732, "y": 0.21678321678321674, "ox": 0.0261437908496732, "oy": 0.21678321678321674, "term": "applied", "cat25k": 22, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 4, "s": 0.9469086021505376, "os": 0.18975468975468973, "bg": 1.5541500168170133e-06}, {"x": 0.0326797385620915, "y": 0.6433566433566432, "ox": 0.0326797385620915, "oy": 0.6433566433566432, "term": "ai", "cat25k": 72, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 5, "s": 0.9973118279569894, "os": 0.6068722943722944, "bg": 1.7519095002483497e-05}, {"x": 0.3529411764705882, "y": 0.8321678321678322, "ox": 0.3529411764705882, "oy": 0.8321678321678322, "term": "techniques", "cat25k": 113, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 161, "ncat": 54, "s": 0.9952956989247312, "os": 0.4761904761904762, "bg": 1.0654398085974551e-05}, {"x": 0.9607843137254901, "y": 0.7272727272727272, "ox": 0.9607843137254901, "oy": 0.7272727272727272, "term": "working", "cat25k": 88, "ncat25k": 158, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 125, "ncat": 275, "s": 0.03696236559139785, "os": -0.2318722943722944, "bg": 5.41430249456272e-06}, {"x": 0.8104575163398693, "y": 0.22377622377622375, "ox": 0.8104575163398693, "oy": 0.22377622377622375, "term": "cloud", "cat25k": 22, "ncat25k": 103, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 178, "s": 0.0013440860215053762, "os": -0.582521645021645, "bg": 3.678439893066e-05}, {"x": 0.4901960784313725, "y": 0.06993006993006992, "ox": 0.4901960784313725, "oy": 0.06993006993006992, "term": "agile", "cat25k": 7, "ncat25k": 44, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 76, "s": 0.009408602150537636, "os": -0.4171176046176046, "bg": 9.102961373276986e-05}, {"x": 0.47712418300653586, "y": 0.24475524475524474, "ox": 0.47712418300653586, "oy": 0.24475524475524474, "term": "issues", "cat25k": 25, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 74, "s": 0.038306451612903226, "os": -0.2305194805194805, "bg": 1.4515253423973502e-06}, {"x": 0.065359477124183, "y": 0.20279720279720279, "ox": 0.065359477124183, "oy": 0.20279720279720279, "term": "qa", "cat25k": 20, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 10, "s": 0.9065860215053764, "os": 0.13690476190476192, "bg": 1.5979323574746767e-05}, {"x": 0.12418300653594772, "y": 0.3006993006993007, "ox": 0.12418300653594772, "oy": 0.3006993006993007, "term": "collection", "cat25k": 30, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 19, "s": 0.9415322580645162, "os": 0.17568542568542572, "bg": 1.1566905110642064e-06}, {"x": 0.2418300653594771, "y": 0.0909090909090909, "ox": 0.2418300653594771, "oy": 0.0909090909090909, "term": "automation", "cat25k": 9, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 37, "s": 0.08669354838709678, "os": -0.14953102453102451, "bg": 7.890777125552442e-06}, {"x": 0.0261437908496732, "y": 0.20979020979020974, "ox": 0.0261437908496732, "oy": 0.20979020979020974, "term": "address", "cat25k": 21, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 4, "s": 0.9442204301075269, "os": 0.1828102453102453, "bg": 2.595882013406097e-07}, {"x": 0.065359477124183, "y": 0.0979020979020979, "ox": 0.065359477124183, "oy": 0.0979020979020979, "term": "engage", "cat25k": 10, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 10, "s": 0.7251344086021506, "os": 0.03273809523809525, "bg": 4.799793608874819e-06}, {"x": 0.0522875816993464, "y": 0.20979020979020974, "ox": 0.0522875816993464, "oy": 0.20979020979020974, "term": "prototype", "cat25k": 21, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 8, "s": 0.9280913978494624, "os": 0.15683621933621936, "bg": 1.292945569883623e-05}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "cognitive", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 2.8942129007128205e-06}, {"x": 0.392156862745098, "y": 0.13286713286713286, "ox": 0.392156862745098, "oy": 0.13286713286713286, "term": "modern", "cat25k": 13, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 61, "s": 0.027553763440860215, "os": -0.25721500721500723, "bg": 2.8015842889114182e-06}, {"x": 0.058823529411764705, "y": 0.04195804195804195, "ox": 0.058823529411764705, "oy": 0.04195804195804195, "term": "object", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.6250000000000001, "os": -0.016323953823953817, "bg": 4.291862071570234e-07}, {"x": 0.1764705882352941, "y": 0.11188811188811187, "ox": 0.1764705882352941, "oy": 0.11188811188811187, "term": "oriented", "cat25k": 11, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 27, "s": 0.25739247311827956, "os": -0.06376262626262627, "bg": 5.098924773131939e-06}, {"x": 0.47058823529411764, "y": 0.23076923076923073, "ox": 0.47058823529411764, "oy": 0.23076923076923073, "term": "functional", "cat25k": 23, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 73, "s": 0.0342741935483871, "os": -0.2379148629148629, "bg": 5.0544188053752404e-06}, {"x": 0.7058823529411765, "y": 0.2937062937062937, "ox": 0.7058823529411765, "oy": 0.2937062937062937, "term": "java", "cat25k": 29, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 135, "s": 0.010752688172043012, "os": -0.40918109668109665, "bg": 6.38519155033533e-06}, {"x": 0.3594771241830065, "y": 0.18881118881118877, "ox": 0.3594771241830065, "oy": 0.18881118881118877, "term": "frameworks", "cat25k": 19, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 55, "s": 0.07258064516129033, "os": -0.1691919191919192, "bg": 5.159377577722562e-05}, {"x": 0.22222222222222218, "y": 0.05594405594405594, "ox": 0.22222222222222218, "oy": 0.05594405594405594, "term": "apache", "cat25k": 6, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 34, "s": 0.07459677419354839, "os": -0.16477272727272727, "bg": 5.049938480928006e-06}, {"x": 0.0522875816993464, "y": 0.1678321678321678, "ox": 0.0522875816993464, "oy": 0.1678321678321678, "term": "tensorflow", "cat25k": 17, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 8, "s": 0.8864247311827957, "os": 0.11516955266955267, "bg": 0.0007951693462217032}, {"x": 0.08496732026143791, "y": 0.12587412587412586, "ox": 0.08496732026143791, "oy": 0.12587412587412586, "term": "scikit", "cat25k": 13, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 13, "s": 0.7439516129032259, "os": 0.041035353535353536, "bg": 0.0007703298751320121}, {"x": 0.22222222222222218, "y": 0.1048951048951049, "ox": 0.22222222222222218, "oy": 0.1048951048951049, "term": "linux", "cat25k": 11, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 34, "s": 0.13508064516129034, "os": -0.11616161616161616, "bg": 1.077696009975682e-06}, {"x": 0.08496732026143791, "y": 0.013986013986013983, "ox": 0.08496732026143791, "oy": 0.013986013986013983, "term": "bash", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.2271505376344086, "os": -0.07007575757575758, "bg": 6.246925991834851e-06}, {"x": 0.2679738562091503, "y": 0.13286713286713286, "ox": 0.2679738562091503, "oy": 0.13286713286713286, "term": "scripting", "cat25k": 13, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 41, "s": 0.10080645161290325, "os": -0.1338383838383838, "bg": 1.634811902629513e-05}, {"x": 0.196078431372549, "y": 0.11888111888111888, "ox": 0.196078431372549, "oy": 0.11888111888111888, "term": "oracle", "cat25k": 12, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 30, "s": 0.20833333333333334, "os": -0.07629870129870131, "bg": 5.4352046415259925e-06}, {"x": 0.09803921568627451, "y": 0.006993006993006992, "ox": 0.09803921568627451, "oy": 0.006993006993006992, "term": "postgresql", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.17809139784946237, "os": -0.09000721500721501, "bg": 6.809834933857351e-06}, {"x": 0.15686274509803919, "y": 0.04195804195804195, "ox": 0.15686274509803919, "oy": 0.04195804195804195, "term": "mysql", "cat25k": 4, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 24, "s": 0.13911290322580647, "os": -0.11372655122655123, "bg": 2.6334256948875244e-06}, {"x": 0.0457516339869281, "y": 0.11188811188811187, "ox": 0.0457516339869281, "oy": 0.11188811188811187, "term": "methodology", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.810483870967742, "os": 0.0661075036075036, "bg": 4.117849633856029e-06}, {"x": 0.15686274509803919, "y": 0.006993006993006992, "ox": 0.15686274509803919, "oy": 0.006993006993006992, "term": "ci", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 24, "s": 0.08736559139784947, "os": -0.14844877344877344, "bg": 5.664035042705125e-06}, {"x": 0.1503267973856209, "y": 0.006993006993006992, "ox": 0.1503267973856209, "oy": 0.006993006993006992, "term": "cd", "cat25k": 1, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 23, "s": 0.09475806451612905, "os": -0.14195526695526695, "bg": 2.513884972291017e-07}, {"x": 0.1895424836601307, "y": 0.04195804195804195, "ox": 0.1895424836601307, "oy": 0.04195804195804195, "term": "git", "cat25k": 4, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 29, "s": 0.08938172043010754, "os": -0.1461940836940837, "bg": 4.4301498276671715e-05}, {"x": 0.11764705882352941, "y": 0.006993006993006992, "ox": 0.11764705882352941, "oy": 0.006993006993006992, "term": "maven", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.14448924731182797, "os": -0.1094877344877345, "bg": 2.070515209568831e-05}, {"x": 0.196078431372549, "y": 0.006993006993006992, "ox": 0.196078431372549, "oy": 0.006993006993006992, "term": "jenkins", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 30, "s": 0.06250000000000001, "os": -0.18740981240981242, "bg": 1.5248907674012788e-05}, {"x": 0.1895424836601307, "y": 0.013986013986013983, "ox": 0.1895424836601307, "oy": 0.013986013986013983, "term": "restful", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 29, "s": 0.06922043010752689, "os": -0.17397186147186147, "bg": 0.00012944686519361702}, {"x": 0.09803921568627451, "y": 0.05594405594405594, "ox": 0.09803921568627451, "oy": 0.05594405594405594, "term": "collect", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 15, "s": 0.39112903225806456, "os": -0.0413961038961039, "bg": 2.870731401826197e-06}, {"x": 0.0784313725490196, "y": 0.11188811188811187, "ox": 0.0784313725490196, "oy": 0.11188811188811187, "term": "usage", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 12, "s": 0.7271505376344086, "os": 0.033639971139971137, "bg": 2.1942810140462195e-06}, {"x": 0.261437908496732, "y": 0.3846153846153845, "ox": 0.261437908496732, "oy": 0.3846153846153845, "term": "datasets", "cat25k": 39, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 40, "s": 0.894489247311828, "os": 0.12265512265512268, "bg": 9.928417146583253e-05}, {"x": 0.3464052287581699, "y": 0.6853146853146853, "ox": 0.3464052287581699, "oy": 0.6853146853146853, "term": "advanced", "cat25k": 79, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 112, "ncat": 53, "s": 0.9831989247311829, "os": 0.33685064935064934, "bg": 1.9019868154273953e-06}, {"x": 0.21568627450980388, "y": 0.49650349650349646, "ox": 0.21568627450980388, "oy": 0.49650349650349646, "term": "mining", "cat25k": 51, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 33, "s": 0.9731182795698925, "os": 0.27922077922077926, "bg": 1.229765885954674e-05}, {"x": 0.45751633986928103, "y": 0.2517482517482517, "ox": 0.45751633986928103, "oy": 0.2517482517482517, "term": "deliver", "cat25k": 25, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 71, "s": 0.05510752688172043, "os": -0.2040945165945166, "bg": 9.350030040248384e-06}, {"x": 0.15686274509803919, "y": 0.22377622377622375, "ox": 0.15686274509803919, "oy": 0.22377622377622375, "term": "effective", "cat25k": 22, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 24, "s": 0.8131720430107526, "os": 0.06682900432900432, "bg": 1.4728831946289422e-06}, {"x": 0.013071895424836598, "y": 0.3356643356643356, "ox": 0.013071895424836598, "oy": 0.3356643356643356, "term": "findings", "cat25k": 34, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 2, "s": 0.980510752688172, "os": 0.3207972582972583, "bg": 4.798931028515537e-06}, {"x": 0.15686274509803919, "y": 0.24475524475524474, "ox": 0.15686274509803919, "oy": 0.24475524475524474, "term": "recommendations", "cat25k": 25, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 24, "s": 0.8508064516129032, "os": 0.08766233766233766, "bg": 3.295819735865744e-06}, {"x": 0.22875816993464052, "y": 0.21678321678321674, "ox": 0.22875816993464052, "oy": 0.21678321678321674, "term": "able", "cat25k": 22, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 35, "s": 0.637768817204301, "os": -0.011544011544011551, "bg": 1.205814679547715e-06}, {"x": 0.1045751633986928, "y": 0.2517482517482517, "ox": 0.1045751633986928, "oy": 0.2517482517482517, "term": "critical", "cat25k": 25, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 16, "s": 0.915994623655914, "os": 0.14655483405483405, "bg": 2.2502752692497636e-06}, {"x": 0.0522875816993464, "y": 0.1818181818181818, "ox": 0.0522875816993464, "oy": 0.1818181818181818, "term": "strategic", "cat25k": 18, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.8971774193548387, "os": 0.12905844155844157, "bg": 2.1927525593856244e-06}, {"x": 0.1895424836601307, "y": 0.5174825174825174, "ox": 0.1895424836601307, "oy": 0.5174825174825174, "term": "analytic", "cat25k": 53, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 29, "s": 0.9818548387096774, "os": 0.32602813852813856, "bg": 7.770727668871361e-05}, {"x": 0.2026143790849673, "y": 0.1608391608391608, "ox": 0.2026143790849673, "oy": 0.1608391608391608, "term": "set", "cat25k": 16, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 31, "s": 0.3924731182795699, "os": -0.04112554112554115, "bg": 3.4444256386600597e-07}, {"x": 0.16993464052287582, "y": 0.1748251748251748, "ox": 0.16993464052287582, "oy": 0.1748251748251748, "term": "objectives", "cat25k": 18, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 26, "s": 0.6733870967741936, "os": 0.005230880230880225, "bg": 3.967156610508064e-06}, {"x": 0.13071895424836602, "y": 0.13286713286713286, "ox": 0.13071895424836602, "oy": 0.13286713286713286, "term": "plans", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 20, "s": 0.6713709677419355, "os": 0.0025252525252525415, "bg": 1.008062913671702e-06}, {"x": 0.2483660130718954, "y": 0.23776223776223773, "ox": 0.2483660130718954, "oy": 0.23776223776223773, "term": "methodologies", "cat25k": 24, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 38, "s": 0.646505376344086, "os": -0.010191197691197668, "bg": 4.558931310518626e-05}, {"x": 0.058823529411764705, "y": 0.1608391608391608, "ox": 0.058823529411764705, "oy": 0.1608391608391608, "term": "validate", "cat25k": 16, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.8736559139784946, "os": 0.10173160173160173, "bg": 1.685781852452997e-05}, {"x": 0.013071895424836598, "y": 0.24475524475524474, "ox": 0.013071895424836598, "oy": 0.24475524475524474, "term": "forecasting", "cat25k": 25, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.9616935483870969, "os": 0.2305194805194805, "bg": 2.102754210196994e-05}, {"x": 0.2418300653594771, "y": 0.3286713286713286, "ox": 0.2418300653594771, "oy": 0.3286713286713286, "term": "non", "cat25k": 33, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 37, "s": 0.849462365591398, "os": 0.08658008658008656, "bg": 8.563123870403542e-07}, {"x": 0.196078431372549, "y": 0.3846153846153845, "ox": 0.196078431372549, "oy": 0.3846153846153845, "term": "partners", "cat25k": 39, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 30, "s": 0.9455645161290323, "os": 0.18759018759018758, "bg": 2.904496033209872e-06}, {"x": 0.019607843137254898, "y": 0.0769230769230769, "ox": 0.019607843137254898, "oy": 0.0769230769230769, "term": "makers", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.788978494623656, "os": 0.05735930735930735, "bg": 2.098154672965127e-06}, {"x": 0.5163398692810457, "y": 0.19580419580419578, "ox": 0.5163398692810457, "oy": 0.19580419580419578, "term": "required", "cat25k": 20, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 82, "s": 0.022177419354838707, "os": -0.31809163059163054, "bg": 1.378795686645767e-06}, {"x": 0.0457516339869281, "y": 0.0979020979020979, "ox": 0.0457516339869281, "oy": 0.0979020979020979, "term": "masters", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.7721774193548387, "os": 0.05221861471861472, "bg": 2.707123641168957e-06}, {"x": 0.11111111111111112, "y": 0.0769230769230769, "ox": 0.11111111111111112, "oy": 0.0769230769230769, "term": "minimum", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 17, "s": 0.46034946236559143, "os": -0.03354978354978355, "bg": 1.1181632156489179e-06}, {"x": 0.09803921568627451, "y": 0.12587412587412586, "ox": 0.09803921568627451, "oy": 0.12587412587412586, "term": "delivering", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 15, "s": 0.7150537634408602, "os": 0.028048340548340545, "bg": 8.125099947962427e-06}, {"x": 0.08496732026143791, "y": 0.05594405594405594, "ox": 0.08496732026143791, "oy": 0.05594405594405594, "term": "package", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 13, "s": 0.5067204301075269, "os": -0.02840909090909091, "bg": 5.350206491856482e-07}, {"x": 0.1045751633986928, "y": 0.19580419580419578, "ox": 0.1045751633986928, "oy": 0.19580419580419578, "term": "sas", "cat25k": 20, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 16, "s": 0.8588709677419356, "os": 0.09099927849927851, "bg": 1.4358581757193448e-05}, {"x": 0.0065359477124183, "y": 0.0909090909090909, "ox": 0.0065359477124183, "oy": 0.0909090909090909, "term": "matlab", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.844758064516129, "os": 0.08423520923520923, "bg": 1.2027109103920236e-05}, {"x": 0.0784313725490196, "y": 0.048951048951048945, "ox": 0.0784313725490196, "oy": 0.048951048951048945, "term": "standard", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.5053763440860216, "os": -0.028860028860028863, "bg": 2.5805104840538547e-07}, {"x": 0.0, "y": 0.11888111888111888, "ox": 0.0, "oy": 0.11888111888111888, "term": "classification", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.8884408602150538, "os": 0.1185064935064935, "bg": 1.6990566837291936e-06}, {"x": 0.019607843137254898, "y": 0.21678321678321674, "ox": 0.019607843137254898, "oy": 0.21678321678321674, "term": "regression", "cat25k": 22, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 3, "s": 0.9502688172043011, "os": 0.19624819624819623, "bg": 1.0633659786688784e-05}, {"x": 0.0326797385620915, "y": 0.1678321678321678, "ox": 0.0326797385620915, "oy": 0.1678321678321678, "term": "selection", "cat25k": 17, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 5, "s": 0.903225806451613, "os": 0.13465007215007213, "bg": 7.704856184874039e-07}, {"x": 0.013071895424836598, "y": 0.06293706293706293, "ox": 0.013071895424836598, "oy": 0.06293706293706293, "term": "hypothesis", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7573924731182795, "os": 0.04996392496392497, "bg": 3.4351721341334785e-06}, {"x": 0.039215686274509796, "y": 0.0909090909090909, "ox": 0.039215686274509796, "oy": 0.0909090909090909, "term": "autonomy", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.7688172043010753, "os": 0.05176767676767677, "bg": 1.259051086329157e-05}, {"x": 0.058823529411764705, "y": 0.20279720279720279, "ox": 0.058823529411764705, "oy": 0.20279720279720279, "term": "thinking", "cat25k": 20, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 9, "s": 0.9112903225806452, "os": 0.1433982683982684, "bg": 1.8351342882454603e-06}, {"x": 0.11111111111111112, "y": 0.7342657342657343, "ox": 0.11111111111111112, "oy": 0.7342657342657343, "term": "ml", "cat25k": 88, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 126, "ncat": 17, "s": 0.997983870967742, "os": 0.6192279942279942, "bg": 1.4375177867694684e-05}, {"x": 0.8496732026143791, "y": 0.8881118881118881, "ox": 0.8496732026143791, "oy": 0.8881118881118881, "term": "knowledge", "cat25k": 139, "ncat25k": 115, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 198, "ncat": 200, "s": 0.739247311827957, "os": 0.038239538239538184, "bg": 8.86116339216066e-06}, {"x": 0.16993464052287582, "y": 0.14685314685314685, "ox": 0.16993464052287582, "oy": 0.14685314685314685, "term": "influence", "cat25k": 15, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 26, "s": 0.5638440860215054, "os": -0.022546897546897537, "bg": 3.541666729462176e-06}, {"x": 0.09803921568627451, "y": 0.034965034965034954, "ox": 0.09803921568627451, "oy": 0.034965034965034954, "term": "organizations", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.2647849462365591, "os": -0.06222943722943724, "bg": 7.376336910352685e-07}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "productivity", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 1.313399197119071e-06}, {"x": 0.07189542483660129, "y": 0.1608391608391608, "ox": 0.07189542483660129, "oy": 0.1608391608391608, "term": "effectiveness", "cat25k": 16, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 11, "s": 0.8528225806451613, "os": 0.08874458874458874, "bg": 4.557886668818257e-06}, {"x": 0.08496732026143791, "y": 0.06293706293706293, "ox": 0.08496732026143791, "oy": 0.06293706293706293, "term": "sharing", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 13, "s": 0.571236559139785, "os": -0.021464646464646464, "bg": 1.453992552848416e-06}, {"x": 0.0261437908496732, "y": 0.0769230769230769, "ox": 0.0261437908496732, "oy": 0.0769230769230769, "term": "prior", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.7647849462365592, "os": 0.050865800865800864, "bg": 4.7596739102980424e-07}, {"x": 0.065359477124183, "y": 0.11188811188811187, "ox": 0.065359477124183, "oy": 0.11188811188811187, "term": "art", "cat25k": 11, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 10, "s": 0.7533602150537635, "os": 0.04662698412698413, "bg": 2.4210462368803043e-07}, {"x": 0.2483660130718954, "y": 0.2727272727272727, "ox": 0.2483660130718954, "oy": 0.2727272727272727, "term": "contribute", "cat25k": 27, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 38, "s": 0.7029569892473119, "os": 0.02453102453102457, "bg": 6.894807797830624e-06}, {"x": 0.019607843137254898, "y": 0.08391608391608392, "ox": 0.019607843137254898, "oy": 0.08391608391608392, "term": "intellectual", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.8071236559139785, "os": 0.06430375180375181, "bg": 1.5249957478035231e-06}, {"x": 0.0, "y": 0.06993006993006992, "ox": 0.0, "oy": 0.06993006993006992, "term": "property", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8165322580645161, "os": 0.0698953823953824, "bg": 1.0424058138266241e-07}, {"x": 0.21568627450980388, "y": 0.15384615384615385, "ox": 0.21568627450980388, "oy": 0.15384615384615385, "term": "assist", "cat25k": 15, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 33, "s": 0.269489247311828, "os": -0.06105699855699856, "bg": 3.661261302521667e-06}, {"x": 0.13071895424836602, "y": 0.06293706293706293, "ox": 0.13071895424836602, "oy": 0.06293706293706293, "term": "career", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 20, "s": 0.24260752688172044, "os": -0.0669191919191919, "bg": 8.131302021932365e-07}, {"x": 0.0457516339869281, "y": 0.11188811188811187, "ox": 0.0457516339869281, "oy": 0.11188811188811187, "term": "actively", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 7, "s": 0.810483870967742, "os": 0.0661075036075036, "bg": 5.4048865108954876e-06}, {"x": 0.1633986928104575, "y": 0.08391608391608392, "ox": 0.1633986928104575, "oy": 0.08391608391608392, "term": "mentor", "cat25k": 8, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 25, "s": 0.20161290322580647, "os": -0.07855339105339106, "bg": 1.3171180673535021e-05}, {"x": 0.1503267973856209, "y": 0.12587412587412586, "ox": 0.1503267973856209, "oy": 0.12587412587412586, "term": "community", "cat25k": 13, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 23, "s": 0.5510752688172044, "os": -0.023899711399711393, "bg": 3.130731577290211e-07}, {"x": 0.14379084967320263, "y": 0.19580419580419578, "ox": 0.14379084967320263, "oy": 0.19580419580419578, "term": "managers", "cat25k": 20, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 22, "s": 0.771505376344086, "os": 0.05203823953823955, "bg": 4.026199283980719e-06}, {"x": 0.0326797385620915, "y": 0.0909090909090909, "ox": 0.0326797385620915, "oy": 0.0909090909090909, "term": "guide", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.790994623655914, "os": 0.058261183261183264, "bg": 1.6865043379069972e-07}, {"x": 0.33986928104575165, "y": 0.08391608391608392, "ox": 0.33986928104575165, "oy": 0.08391608391608392, "term": "hive", "cat25k": 8, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 52, "s": 0.028225806451612906, "os": -0.25387806637806637, "bg": 0.000121209251674913}, {"x": 0.09803921568627451, "y": 0.020979020979020976, "ox": 0.09803921568627451, "oy": 0.020979020979020976, "term": "impala", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 15, "s": 0.20900537634408603, "os": -0.07611832611832613, "bg": 3.2226386786465276e-05}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "presto", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.4042210885923084e-05}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "athena", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 6.931187173739119e-06}, {"x": 0.392156862745098, "y": 0.18881118881118877, "ox": 0.392156862745098, "oy": 0.18881118881118877, "term": "hands", "cat25k": 19, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 61, "s": 0.0571236559139785, "os": -0.20165945165945168, "bg": 3.579544360205156e-06}, {"x": 0.29411764705882354, "y": 0.1748251748251748, "ox": 0.29411764705882354, "oy": 0.1748251748251748, "term": "least", "cat25k": 18, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 45, "s": 0.12903225806451613, "os": -0.11814574314574314, "bg": 1.2577449915201933e-06}, {"x": 0.0457516339869281, "y": 0.19580419580419578, "ox": 0.0457516339869281, "oy": 0.19580419580419578, "term": "major", "cat25k": 20, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 7, "s": 0.922715053763441, "os": 0.14944083694083693, "bg": 5.706239171748841e-07}, {"x": 0.2352941176470588, "y": 0.1608391608391608, "ox": 0.2352941176470588, "oy": 0.1608391608391608, "term": "proven", "cat25k": 16, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 36, "s": 0.21370967741935484, "os": -0.0735930735930736, "bg": 9.075877954669372e-06}, {"x": 0.2091503267973856, "y": 0.20979020979020974, "ox": 0.2091503267973856, "oy": 0.20979020979020974, "term": "track", "cat25k": 21, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 32, "s": 0.6693548387096775, "os": 0.0009920634920635163, "bg": 1.1025578016149785e-06}, {"x": 0.09803921568627451, "y": 0.1608391608391608, "ox": 0.09803921568627451, "oy": 0.1608391608391608, "term": "leading", "cat25k": 16, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 15, "s": 0.7977150537634409, "os": 0.06277056277056275, "bg": 1.3503430839771435e-06}, {"x": 0.5555555555555556, "y": 0.20279720279720279, "ox": 0.5555555555555556, "oy": 0.20279720279720279, "term": "delivery", "cat25k": 20, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 90, "s": 0.016801075268817203, "os": -0.35010822510822504, "bg": 2.030715493337884e-06}, {"x": 0.1045751633986928, "y": 0.06293706293706293, "ox": 0.1045751633986928, "oy": 0.06293706293706293, "term": "significant", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 16, "s": 0.39314516129032256, "os": -0.04094516594516594, "bg": 8.558843305877876e-07}, {"x": 0.14379084967320263, "y": 0.13986013986013984, "ox": 0.14379084967320263, "oy": 0.13986013986013984, "term": "comfortable", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 22, "s": 0.6592741935483871, "os": -0.0035173160173160023, "bg": 4.285499354802767e-06}, {"x": 0.08496732026143791, "y": 0.04195804195804195, "ox": 0.08496732026143791, "oy": 0.04195804195804195, "term": "terms", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.38642473118279574, "os": -0.0422979797979798, "bg": 1.3679576223144097e-07}, {"x": 0.14379084967320263, "y": 0.11888111888111888, "ox": 0.14379084967320263, "oy": 0.11888111888111888, "term": "efforts", "cat25k": 12, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 22, "s": 0.5456989247311828, "os": -0.024350649350649345, "bg": 2.036391570529404e-06}, {"x": 0.013071895424836598, "y": 0.14685314685314685, "ox": 0.013071895424836598, "oy": 0.14685314685314685, "term": "risk", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 2, "s": 0.9012096774193549, "os": 0.1332972582972583, "bg": 5.023665176123367e-07}, {"x": 0.2352941176470588, "y": 0.21678321678321674, "ox": 0.2352941176470588, "oy": 0.21678321678321674, "term": "value", "cat25k": 22, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 36, "s": 0.6014784946236559, "os": -0.018037518037518047, "bg": 7.356906736515263e-07}, {"x": 0.22222222222222218, "y": 0.24475524475524474, "ox": 0.22222222222222218, "oy": 0.24475524475524474, "term": "appropriate", "cat25k": 25, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 34, "s": 0.7009408602150539, "os": 0.022727272727272735, "bg": 1.9784457227179075e-06}, {"x": 0.3071895424836601, "y": 0.7552447552447552, "ox": 0.3071895424836601, "oy": 0.7552447552447552, "term": "methods", "cat25k": 92, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 131, "ncat": 47, "s": 0.9926075268817205, "os": 0.44525613275613274, "bg": 4.046451443946644e-06}, {"x": 0.019607843137254898, "y": 0.08391608391608392, "ox": 0.019607843137254898, "oy": 0.08391608391608392, "term": "prescriptive", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.8071236559139785, "os": 0.06430375180375181, "bg": 4.659738153780679e-05}, {"x": 0.065359477124183, "y": 0.048951048951048945, "ox": 0.065359477124183, "oy": 0.048951048951048945, "term": "structure", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 10, "s": 0.6283602150537634, "os": -0.015873015873015872, "bg": 5.054109594620571e-07}, {"x": 0.0784313725490196, "y": 0.06293706293706293, "ox": 0.0784313725490196, "oy": 0.06293706293706293, "term": "facilitate", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 12, "s": 0.6330645161290323, "os": -0.014971139971139968, "bg": 3.6805132247850664e-06}, {"x": 0.058823529411764705, "y": 0.020979020979020976, "ox": 0.058823529411764705, "oy": 0.020979020979020976, "term": "appropriately", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4368279569892473, "os": -0.03715728715728715, "bg": 5.041558194391141e-06}, {"x": 0.261437908496732, "y": 0.23776223776223773, "ox": 0.261437908496732, "oy": 0.23776223776223773, "term": "effectively", "cat25k": 24, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 40, "s": 0.5591397849462366, "os": -0.02317821067821066, "bg": 7.555515554432665e-06}, {"x": 0.0522875816993464, "y": 0.034965034965034954, "ox": 0.0522875816993464, "oy": 0.034965034965034954, "term": "formal", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.6216397849462366, "os": -0.016774891774891776, "bg": 1.1630443724241642e-06}, {"x": 0.0522875816993464, "y": 0.034965034965034954, "ox": 0.0522875816993464, "oy": 0.034965034965034954, "term": "sessions", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.6216397849462366, "os": -0.016774891774891776, "bg": 1.2044647469845764e-06}, {"x": 0.065359477124183, "y": 0.2797202797202797, "ox": 0.065359477124183, "oy": 0.2797202797202797, "term": "evaluating", "cat25k": 28, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 10, "s": 0.956989247311828, "os": 0.21329365079365079, "bg": 1.2847208661279748e-05}, {"x": 0.09803921568627451, "y": 0.04195804195804195, "ox": 0.09803921568627451, "oy": 0.04195804195804195, "term": "provides", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.2956989247311828, "os": -0.05528499278499279, "bg": 3.842840065223609e-07}, {"x": 0.0457516339869281, "y": 0.0769230769230769, "ox": 0.0457516339869281, "oy": 0.0769230769230769, "term": "mentoring", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.7204301075268817, "os": 0.03138528138528138, "bg": 8.811489202866182e-06}, {"x": 0.1372549019607843, "y": 0.0769230769230769, "ox": 0.1372549019607843, "oy": 0.0769230769230769, "term": "deploying", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 21, "s": 0.2721774193548387, "os": -0.05952380952380952, "bg": 2.5027980500074693e-05}, {"x": 0.08496732026143791, "y": 0.12587412587412586, "ox": 0.08496732026143791, "oy": 0.12587412587412586, "term": "interpersonal", "cat25k": 13, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 13, "s": 0.7439516129032259, "os": 0.041035353535353536, "bg": 2.4490566589258042e-05}, {"x": 0.08496732026143791, "y": 0.04195804195804195, "ox": 0.08496732026143791, "oy": 0.04195804195804195, "term": "facing", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.38642473118279574, "os": -0.0422979797979798, "bg": 2.8220082182820385e-06}, {"x": 0.13071895424836602, "y": 0.0909090909090909, "ox": 0.13071895424836602, "oy": 0.0909090909090909, "term": "range", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 20, "s": 0.39852150537634407, "os": -0.03914141414141413, "bg": 5.140370632093697e-07}, {"x": 0.2352941176470588, "y": 0.1608391608391608, "ox": 0.2352941176470588, "oy": 0.1608391608391608, "term": "people", "cat25k": 16, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 36, "s": 0.21370967741935484, "os": -0.0735930735930736, "bg": 2.456368806323561e-07}, {"x": 0.2091503267973856, "y": 0.1748251748251748, "ox": 0.2091503267973856, "oy": 0.1748251748251748, "term": "must", "cat25k": 18, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 32, "s": 0.45967741935483875, "os": -0.03373015873015872, "bg": 3.8927083927376853e-07}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "follow", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 4.6334194879075284e-07}, {"x": 0.08496732026143791, "y": 0.08391608391608392, "ox": 0.08496732026143791, "oy": 0.08391608391608392, "term": "assigned", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.6612903225806452, "os": -0.0006313131313131354, "bg": 2.0388234390319207e-06}, {"x": 0.07189542483660129, "y": 0.02797202797202797, "ox": 0.07189542483660129, "oy": 0.02797202797202797, "term": "willing", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3803763440860215, "os": -0.043199855699855697, "bg": 1.6217002521689836e-06}, {"x": 0.16993464052287582, "y": 0.11188811188811187, "ox": 0.16993464052287582, "oy": 0.11188811188811187, "term": "us", "cat25k": 11, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 26, "s": 0.28830645161290325, "os": -0.057269119769119775, "bg": 6.833751244611182e-08}, {"x": 0.13071895424836602, "y": 0.14685314685314685, "ox": 0.13071895424836602, "oy": 0.14685314685314685, "term": "year", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 20, "s": 0.6881720430107527, "os": 0.016414141414141437, "bg": 1.817484331367839e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "beneficial", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 1.929811647626318e-06}, {"x": 0.013071895424836598, "y": 0.1678321678321678, "ox": 0.013071895424836598, "oy": 0.1678321678321678, "term": "neural", "cat25k": 17, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 2, "s": 0.926747311827957, "os": 0.15413059163059162, "bg": 9.677744147616866e-06}, {"x": 0.019607843137254898, "y": 0.21678321678321674, "ox": 0.019607843137254898, "oy": 0.21678321678321674, "term": "networks", "cat25k": 22, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 3, "s": 0.9502688172043011, "os": 0.19624819624819623, "bg": 1.111950597473949e-06}, {"x": 0.196078431372549, "y": 0.1748251748251748, "ox": 0.196078431372549, "oy": 0.1748251748251748, "term": "patterns", "cat25k": 18, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 30, "s": 0.5739247311827957, "os": -0.020743145743145758, "bg": 4.248691354782144e-06}, {"x": 0.0065359477124183, "y": 0.20279720279720279, "ox": 0.0065359477124183, "oy": 0.20279720279720279, "term": "series", "cat25k": 20, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 1, "s": 0.9489247311827957, "os": 0.19534632034632035, "bg": 3.712893320944894e-07}, {"x": 0.1503267973856209, "y": 0.0769230769230769, "ox": 0.1503267973856209, "oy": 0.0769230769230769, "term": "common", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 23, "s": 0.21975806451612903, "os": -0.07251082251082251, "bg": 6.300977514192652e-07}, {"x": 0.039215686274509796, "y": 0.1678321678321678, "ox": 0.039215686274509796, "oy": 0.1678321678321678, "term": "excellence", "cat25k": 17, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 6, "s": 0.8965053763440861, "os": 0.12815656565656564, "bg": 4.162848698419443e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "spotfire", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 0.00011444921316165952}, {"x": 0.4967320261437908, "y": 0.37762237762237755, "ox": 0.4967320261437908, "oy": 0.37762237762237755, "term": "plus", "cat25k": 39, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 78, "s": 0.1303763440860215, "os": -0.11805555555555558, "bg": 2.8055864246237336e-06}, {"x": 0.1372549019607843, "y": 0.0979020979020979, "ox": 0.1372549019607843, "oy": 0.0979020979020979, "term": "query", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 21, "s": 0.3998655913978495, "os": -0.03869047619047618, "bg": 2.2318011125974904e-06}, {"x": 0.5359477124183006, "y": 0.1048951048951049, "ox": 0.5359477124183006, "oy": 0.1048951048951049, "term": "nosql", "cat25k": 11, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 85, "s": 0.008736559139784947, "os": -0.42784992784992787, "bg": 0.002482806564540557}, {"x": 0.2549019607843137, "y": 0.034965034965034954, "ox": 0.2549019607843137, "oy": 0.034965034965034954, "term": "mongodb", "cat25k": 4, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 39, "s": 0.04704301075268817, "os": -0.21807359307359306, "bg": 0.0010931948619841486}, {"x": 0.11764705882352941, "y": 0.013986013986013983, "ox": 0.11764705882352941, "oy": 0.013986013986013983, "term": "cassandra", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 18, "s": 0.15994623655913978, "os": -0.10254329004329005, "bg": 3.273952028417904e-05}, {"x": 0.2875816993464052, "y": 0.06293706293706293, "ox": 0.2875816993464052, "oy": 0.06293706293706293, "term": "hbase", "cat25k": 6, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 44, "s": 0.04099462365591398, "os": -0.22276334776334775, "bg": 0.001316655694535879}, {"x": 0.3464052287581699, "y": 0.12587412587412586, "ox": 0.3464052287581699, "oy": 0.12587412587412586, "term": "security", "cat25k": 13, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 53, "s": 0.0456989247311828, "os": -0.2187049062049062, "bg": 6.17137623219784e-07}, {"x": 0.196078431372549, "y": 0.048951048951048945, "ox": 0.196078431372549, "oy": 0.048951048951048945, "term": "built", "cat25k": 5, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 30, "s": 0.0907258064516129, "os": -0.14574314574314576, "bg": 1.201622807840134e-06}, {"x": 0.058823529411764705, "y": 0.0769230769230769, "ox": 0.058823529411764705, "oy": 0.0769230769230769, "term": "communications", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 9, "s": 0.6922043010752688, "os": 0.0183982683982684, "bg": 6.117516575793507e-07}, {"x": 0.09803921568627451, "y": 0.1048951048951049, "ox": 0.09803921568627451, "oy": 0.1048951048951049, "term": "quickly", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 15, "s": 0.6747311827956989, "os": 0.007215007215007202, "bg": 1.490978017467404e-06}, {"x": 0.2745098039215686, "y": 0.32167832167832167, "ox": 0.2745098039215686, "oy": 0.32167832167832167, "term": "be", "cat25k": 32, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 42, "s": 0.7547043010752689, "os": 0.04716810966810969, "bg": 7.336987446561219e-08}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "exciting", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 1.4133767035826924e-06}, {"x": 0.07189542483660129, "y": 0.048951048951048945, "ox": 0.07189542483660129, "oy": 0.048951048951048945, "term": "getting", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.564516129032258, "os": -0.022366522366522368, "bg": 3.8476423000004347e-07}, {"x": 0.08496732026143791, "y": 0.020979020979020976, "ox": 0.08496732026143791, "oy": 0.020979020979020976, "term": "backend", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.2620967741935484, "os": -0.06313131313131314, "bg": 1.3585952804098203e-05}, {"x": 0.039215686274509796, "y": 0.18881118881118877, "ox": 0.039215686274509796, "oy": 0.18881118881118877, "term": "alongside", "cat25k": 19, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 6, "s": 0.9213709677419355, "os": 0.148989898989899, "bg": 1.368137649572685e-05}, {"x": 0.08496732026143791, "y": 0.08391608391608392, "ox": 0.08496732026143791, "oy": 0.08391608391608392, "term": "performing", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.6612903225806452, "os": -0.0006313131313131354, "bg": 2.382323691847002e-06}, {"x": 0.12418300653594772, "y": 0.1818181818181818, "ox": 0.12418300653594772, "oy": 0.1818181818181818, "term": "colleagues", "cat25k": 18, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 19, "s": 0.7903225806451614, "os": 0.05762987012987014, "bg": 9.143964404375366e-06}, {"x": 0.5032679738562091, "y": 0.5314685314685315, "ox": 0.5032679738562091, "oy": 0.5314685314685315, "term": "we", "cat25k": 54, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 79, "s": 0.715725806451613, "os": 0.02822871572871577, "bg": 2.243405914930384e-07}, {"x": 0.07189542483660129, "y": 0.12587412587412586, "ox": 0.07189542483660129, "oy": 0.12587412587412586, "term": "offer", "cat25k": 13, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 11, "s": 0.7741935483870969, "os": 0.05402236652236653, "bg": 4.5918970861491593e-07}, {"x": 0.326797385620915, "y": 0.13986013986013984, "ox": 0.326797385620915, "oy": 0.13986013986013984, "term": "competitive", "cat25k": 14, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 50, "s": 0.06518817204301075, "os": -0.18533549783549783, "bg": 5.50469090098972e-06}, {"x": 0.1633986928104575, "y": 0.05594405594405594, "ox": 0.1633986928104575, "oy": 0.05594405594405594, "term": "salary", "cat25k": 6, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 25, "s": 0.1485215053763441, "os": -0.10633116883116883, "bg": 3.1078204629070326e-06}, {"x": 0.08496732026143791, "y": 0.034965034965034954, "ox": 0.08496732026143791, "oy": 0.034965034965034954, "term": "equity", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.3326612903225806, "os": -0.04924242424242425, "bg": 1.1524020315823458e-06}, {"x": 0.11111111111111112, "y": 0.048951048951048945, "ox": 0.11111111111111112, "oy": 0.048951048951048945, "term": "options", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 17, "s": 0.26814516129032256, "os": -0.06132756132756133, "bg": 3.4843288282195614e-07}, {"x": 0.1045751633986928, "y": 0.06993006993006992, "ox": 0.1045751633986928, "oy": 0.06993006993006992, "term": "every", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 16, "s": 0.4583333333333333, "os": -0.03400072150072149, "bg": 2.745419808214476e-07}, {"x": 0.0784313725490196, "y": 0.034965034965034954, "ox": 0.0784313725490196, "oy": 0.034965034965034954, "term": "annual", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.38373655913978494, "os": -0.04274891774891775, "bg": 3.994165792512156e-07}, {"x": 0.14379084967320263, "y": 0.06993006993006992, "ox": 0.14379084967320263, "oy": 0.06993006993006992, "term": "reviews", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 22, "s": 0.2163978494623656, "os": -0.07296176046176045, "bg": 2.079511493117228e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "everyone", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.4891714817614676e-07}, {"x": 0.2549019607843137, "y": 0.13286713286713286, "ox": 0.2549019607843137, "oy": 0.13286713286713286, "term": "paid", "cat25k": 13, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 39, "s": 0.12432795698924731, "os": -0.12085137085137082, "bg": 1.921311049658603e-06}, {"x": 0.2091503267973856, "y": 0.06293706293706293, "ox": 0.2091503267973856, "oy": 0.06293706293706293, "term": "leave", "cat25k": 6, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 32, "s": 0.09139784946236561, "os": -0.14484126984126983, "bg": 1.154307795751429e-06}, {"x": 0.07189542483660129, "y": 0.048951048951048945, "ox": 0.07189542483660129, "oy": 0.048951048951048945, "term": "holidays", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.564516129032258, "os": -0.022366522366522368, "bg": 8.610204422038354e-07}, {"x": 0.065359477124183, "y": 0.034965034965034954, "ox": 0.065359477124183, "oy": 0.034965034965034954, "term": "contributions", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.5006720430107527, "os": -0.02976190476190476, "bg": 1.3661492710250258e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "made", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 5.5665052773270684e-08}, {"x": 0.11111111111111112, "y": 0.034965034965034954, "ox": 0.11111111111111112, "oy": 0.034965034965034954, "term": "towards", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 17, "s": 0.21236559139784947, "os": -0.07521645021645021, "bg": 1.1930864544610562e-06}, {"x": 0.22875816993464052, "y": 0.0769230769230769, "ox": 0.22875816993464052, "oy": 0.0769230769230769, "term": "get", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 35, "s": 0.08602150537634409, "os": -0.15043290043290042, "bg": 1.5179889745424799e-07}, {"x": 0.11764705882352941, "y": 0.11188811188811187, "ox": 0.11764705882352941, "oy": 0.11188811188811187, "term": "tech", "cat25k": 11, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 18, "s": 0.6579301075268817, "os": -0.005321067821067824, "bg": 7.274115422903645e-07}, {"x": 0.22222222222222218, "y": 0.0979020979020979, "ox": 0.22222222222222218, "oy": 0.0979020979020979, "term": "flexible", "cat25k": 10, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 34, "s": 0.11626344086021506, "os": -0.1231060606060606, "bg": 4.846053989483659e-06}, {"x": 0.09803921568627451, "y": 0.06293706293706293, "ox": 0.09803921568627451, "oy": 0.06293706293706293, "term": "start", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.4563172043010753, "os": -0.034451659451659455, "bg": 2.963074791600468e-07}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "week", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 1.434698564989388e-07}, {"x": 0.07189542483660129, "y": 0.02797202797202797, "ox": 0.07189542483660129, "oy": 0.02797202797202797, "term": "remote", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3803763440860215, "os": -0.043199855699855697, "bg": 5.909711778037845e-07}, {"x": 0.11764705882352941, "y": 0.06293706293706293, "ox": 0.11764705882352941, "oy": 0.06293706293706293, "term": "fully", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 18, "s": 0.3004032258064516, "os": -0.05393217893217893, "bg": 1.1088907513233842e-06}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "stocked", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 9.927385686153825e-06}, {"x": 0.196078431372549, "y": 0.0909090909090909, "ox": 0.196078431372549, "oy": 0.0909090909090909, "term": "office", "cat25k": 9, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 30, "s": 0.15053763440860216, "os": -0.10407647907647909, "bg": 3.2225413741431276e-07}, {"x": 0.09803921568627451, "y": 0.05594405594405594, "ox": 0.09803921568627451, "oy": 0.05594405594405594, "term": "snacks", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 15, "s": 0.39112903225806456, "os": -0.0413961038961039, "bg": 1.0030413959545159e-05}, {"x": 0.065359477124183, "y": 0.04195804195804195, "ox": 0.065359477124183, "oy": 0.04195804195804195, "term": "cycle", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5598118279569892, "os": -0.022817460317460313, "bg": 1.0786847973805219e-06}, {"x": 0.11764705882352941, "y": 0.0979020979020979, "ox": 0.11764705882352941, "oy": 0.0979020979020979, "term": "care", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 18, "s": 0.5772849462365591, "os": -0.019209956709956705, "bg": 2.839305616756517e-07}, {"x": 0.2352941176470588, "y": 0.13986013986013984, "ox": 0.2352941176470588, "oy": 0.13986013986013984, "term": "day", "cat25k": 14, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 36, "s": 0.1713709677419355, "os": -0.09442640692640691, "bg": 2.509429078938837e-07}, {"x": 0.0784313725490196, "y": 0.05594405594405594, "ox": 0.0784313725490196, "oy": 0.05594405594405594, "term": "home", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 12, "s": 0.5685483870967742, "os": -0.021915584415584416, "bg": 3.132506650836315e-08}, {"x": 0.2026143790849673, "y": 0.32167832167832167, "ox": 0.2026143790849673, "oy": 0.32167832167832167, "term": "take", "cat25k": 32, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 31, "s": 0.8897849462365591, "os": 0.11859668109668109, "bg": 5.823840209072536e-07}, {"x": 0.0326797385620915, "y": 0.11188811188811187, "ox": 0.0326797385620915, "oy": 0.11188811188811187, "term": "first", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.8387096774193549, "os": 0.07909451659451659, "bg": 7.263394684680284e-08}, {"x": 0.14379084967320263, "y": 0.06993006993006992, "ox": 0.14379084967320263, "oy": 0.06993006993006992, "term": "generous", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 22, "s": 0.2163978494623656, "os": -0.07296176046176045, "bg": 1.1074860171238104e-05}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "maternity", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 2.059130771405726e-06}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "paternity", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 1.329996151323637e-05}, {"x": 0.326797385620915, "y": 0.1048951048951049, "ox": 0.326797385620915, "oy": 0.1048951048951049, "term": "job", "cat25k": 11, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 50, "s": 0.043010752688172046, "os": -0.22005772005772006, "bg": 7.312102258197664e-07}, {"x": 0.0784313725490196, "y": 0.0909090909090909, "ox": 0.0784313725490196, "oy": 0.0909090909090909, "term": "important", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 12, "s": 0.6787634408602151, "os": 0.012806637806637808, "bg": 3.6715050396473344e-07}, {"x": 0.3006535947712418, "y": 0.5314685314685315, "ox": 0.3006535947712418, "oy": 0.5314685314685315, "term": "relevant", "cat25k": 54, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 77, "ncat": 46, "s": 0.9610215053763441, "os": 0.22952741702741702, "bg": 5.4277441924460975e-06}, {"x": 0.2810457516339869, "y": 0.08391608391608392, "ox": 0.2810457516339869, "oy": 0.08391608391608392, "term": "access", "cat25k": 8, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 43, "s": 0.05981182795698925, "os": -0.1954365079365079, "bg": 5.044309836679784e-07}, {"x": 0.3071895424836601, "y": 0.0979020979020979, "ox": 0.3071895424836601, "oy": 0.0979020979020979, "term": "modelling", "cat25k": 10, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 47, "s": 0.05174731182795699, "os": -0.207521645021645, "bg": 1.843940094920595e-05}, {"x": 0.7058823529411765, "y": 0.5734265734265733, "ox": 0.7058823529411765, "oy": 0.5734265734265733, "term": "e", "cat25k": 58, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 83, "ncat": 135, "s": 0.10685483870967744, "os": -0.1314033189033189, "bg": 7.35037714734567e-07}, {"x": 0.6078431372549019, "y": 0.5104895104895104, "ox": 0.6078431372549019, "oy": 0.5104895104895104, "term": "g", "cat25k": 52, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 102, "s": 0.16733870967741937, "os": -0.09650072150072153, "bg": 1.544860610387067e-06}, {"x": 0.07189542483660129, "y": 0.1818181818181818, "ox": 0.07189542483660129, "oy": 0.1818181818181818, "term": "b", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 11, "s": 0.8823924731182796, "os": 0.10957792207792208, "bg": 1.7625502539697962e-07}, {"x": 0.0065359477124183, "y": 0.1608391608391608, "ox": 0.0065359477124183, "oy": 0.1608391608391608, "term": "ph", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.9254032258064516, "os": 0.15367965367965367, "bg": 2.196113784498374e-06}, {"x": 0.058823529411764705, "y": 0.18881118881118877, "ox": 0.058823529411764705, "oy": 0.18881118881118877, "term": "d", "cat25k": 19, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 9, "s": 0.8978494623655914, "os": 0.12950937950937952, "bg": 1.9912922559684062e-07}, {"x": 0.0065359477124183, "y": 0.1608391608391608, "ox": 0.0065359477124183, "oy": 0.1608391608391608, "term": "list", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.9254032258064516, "os": 0.15367965367965367, "bg": 1.0155052439326211e-07}, {"x": 0.14379084967320263, "y": 0.06993006993006992, "ox": 0.14379084967320263, "oy": 0.06993006993006992, "term": "at", "cat25k": 7, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 22, "s": 0.2163978494623656, "os": -0.07296176046176045, "bg": 2.8164634954835006e-08}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "facilities", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 2.546848211666857e-07}, {"x": 0.0784313725490196, "y": 0.02797202797202797, "ox": 0.0784313725490196, "oy": 0.02797202797202797, "term": "without", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.3279569892473118, "os": -0.04969336219336219, "bg": 1.4593841421722908e-07}, {"x": 0.12418300653594772, "y": 0.0769230769230769, "ox": 0.12418300653594772, "oy": 0.0769230769230769, "term": "need", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 19, "s": 0.3434139784946237, "os": -0.04653679653679653, "bg": 1.878184602857991e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "ready", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 2.2311476272772908e-07}, {"x": 0.0261437908496732, "y": 0.06293706293706293, "ox": 0.0261437908496732, "oy": 0.06293706293706293, "term": "interdisciplinary", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.7318548387096774, "os": 0.03697691197691198, "bg": 6.5938569599681865e-06}, {"x": 0.31372549019607837, "y": 0.11888111888111888, "ox": 0.31372549019607837, "oy": 0.11888111888111888, "term": "life", "cat25k": 12, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 48, "s": 0.06182795698924731, "os": -0.19318181818181818, "bg": 4.2395029027615484e-07}, {"x": 0.09150326797385622, "y": 0.05594405594405594, "ox": 0.09150326797385622, "oy": 0.05594405594405594, "term": "balance", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 14, "s": 0.45497311827956993, "os": -0.03490259740259741, "bg": 1.0543418829117875e-06}, {"x": 0.15686274509803919, "y": 0.13286713286713286, "ox": 0.15686274509803919, "oy": 0.13286713286713286, "term": "professional", "cat25k": 13, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 24, "s": 0.5564516129032259, "os": -0.02344877344877344, "bg": 6.97807805944258e-07}, {"x": 0.08496732026143791, "y": 0.11888111888111888, "ox": 0.08496732026143791, "oy": 0.11888111888111888, "term": "extensive", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 13, "s": 0.7278225806451614, "os": 0.03409090909090909, "bg": 2.760579622339825e-06}, {"x": 0.039215686274509796, "y": 0.0909090909090909, "ox": 0.039215686274509796, "oy": 0.0909090909090909, "term": "education", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.7688172043010753, "os": 0.05176767676767677, "bg": 1.4241888834854245e-07}, {"x": 0.1895424836601307, "y": 0.20979020979020974, "ox": 0.1895424836601307, "oy": 0.20979020979020974, "term": "benefits", "cat25k": 21, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 29, "s": 0.6975806451612904, "os": 0.020472582972582976, "bg": 1.539234888600614e-06}, {"x": 0.039215686274509796, "y": 0.0769230769230769, "ox": 0.039215686274509796, "oy": 0.0769230769230769, "term": "collaboratively", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.7365591397849462, "os": 0.03787878787878787, "bg": 4.521727565914154e-05}, {"x": 0.0326797385620915, "y": 0.0769230769230769, "ox": 0.0326797385620915, "oy": 0.0769230769230769, "term": "recommendation", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.75, "os": 0.04437229437229437, "bg": 1.7746836168917268e-06}, {"x": 0.08496732026143791, "y": 0.05594405594405594, "ox": 0.08496732026143791, "oy": 0.05594405594405594, "term": "move", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 13, "s": 0.5067204301075269, "os": -0.02840909090909091, "bg": 5.983450204124691e-07}, {"x": 0.0065359477124183, "y": 0.06993006993006992, "ox": 0.0065359477124183, "oy": 0.06993006993006992, "term": "descriptive", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.8030913978494625, "os": 0.0634018759018759, "bg": 5.308847000477314e-06}, {"x": 0.7777777777777778, "y": 0.6433566433566432, "ox": 0.7777777777777778, "oy": 0.6433566433566432, "term": "implement", "cat25k": 72, "ncat25k": 91, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 158, "s": 0.10215053763440862, "os": -0.1333874458874459, "bg": 2.5090852197336186e-05}, {"x": 0.019607843137254898, "y": 0.15384615384615385, "ox": 0.019607843137254898, "oy": 0.15384615384615385, "term": "audiences", "cat25k": 15, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 3, "s": 0.9018817204301076, "os": 0.13374819624819623, "bg": 1.0089786996524674e-05}, {"x": 0.11111111111111112, "y": 0.13286713286713286, "ox": 0.11111111111111112, "oy": 0.13286713286713286, "term": "sales", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 17, "s": 0.7002688172043011, "os": 0.022005772005772015, "bg": 4.154052817973848e-07}, {"x": 0.22875816993464052, "y": 0.39860139860139854, "ox": 0.22875816993464052, "oy": 0.39860139860139854, "term": "client", "cat25k": 41, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 35, "s": 0.9375, "os": 0.16901154401154403, "bg": 2.6851422159589432e-06}, {"x": 0.14379084967320263, "y": 0.30769230769230765, "ox": 0.14379084967320263, "oy": 0.30769230769230765, "term": "present", "cat25k": 31, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 22, "s": 0.9341397849462365, "os": 0.16314935064935066, "bg": 1.3266163131354794e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "peer", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 1.009256734559928e-06}, {"x": 0.1764705882352941, "y": 0.04195804195804195, "ox": 0.1764705882352941, "oy": 0.04195804195804195, "term": "junior", "cat25k": 4, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 27, "s": 0.1028225806451613, "os": -0.13320707070707072, "bg": 2.2807360806158654e-06}, {"x": 0.3790849673202614, "y": 0.5244755244755244, "ox": 0.3790849673202614, "oy": 0.5244755244755244, "term": "level", "cat25k": 53, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 59, "s": 0.9119623655913979, "os": 0.14466089466089466, "bg": 1.3323843401197047e-06}, {"x": 0.0261437908496732, "y": 0.0769230769230769, "ox": 0.0261437908496732, "oy": 0.0769230769230769, "term": "adapt", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 4, "s": 0.7647849462365592, "os": 0.050865800865800864, "bg": 6.30777395293055e-06}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "industries", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 7.0070058592583e-07}, {"x": 0.38562091503267976, "y": 0.048951048951048945, "ox": 0.38562091503267976, "oy": 0.048951048951048945, "term": "azure", "cat25k": 5, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 60, "s": 0.01881720430107527, "os": -0.33405483405483405, "bg": 0.00011934683436975075}, {"x": 0.0522875816993464, "y": 0.11888111888111888, "ox": 0.0522875816993464, "oy": 0.11888111888111888, "term": "fields", "cat25k": 12, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 8, "s": 0.8118279569892474, "os": 0.06655844155844157, "bg": 1.226138967844064e-06}, {"x": 0.019607843137254898, "y": 0.08391608391608392, "ox": 0.019607843137254898, "oy": 0.08391608391608392, "term": "experimentation", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.8071236559139785, "os": 0.06430375180375181, "bg": 1.5427773592279943e-05}, {"x": 0.13071895424836602, "y": 0.14685314685314685, "ox": 0.13071895424836602, "oy": 0.14685314685314685, "term": "tests", "cat25k": 15, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 20, "s": 0.6881720430107527, "os": 0.016414141414141437, "bg": 1.933204849081299e-06}, {"x": 0.013071895424836598, "y": 0.22377622377622375, "ox": 0.013071895424836598, "oy": 0.22377622377622375, "term": "economics", "cat25k": 22, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 2, "s": 0.9549731182795699, "os": 0.20968614718614717, "bg": 2.197086475934359e-06}, {"x": 0.3333333333333333, "y": 0.21678321678321674, "ox": 0.3333333333333333, "oy": 0.21678321678321674, "term": "preferred", "cat25k": 22, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 51, "s": 0.13776881720430106, "os": -0.11544011544011545, "bg": 8.3160998222028e-06}, {"x": 0.196078431372549, "y": 0.0769230769230769, "ox": 0.196078431372549, "oy": 0.0769230769230769, "term": "google", "cat25k": 8, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 30, "s": 0.1310483870967742, "os": -0.11796536796536798, "bg": 9.687040773723322e-07}, {"x": 0.42483660130718953, "y": 0.21678321678321674, "ox": 0.42483660130718953, "oy": 0.21678321678321674, "term": "highly", "cat25k": 22, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 66, "s": 0.05241935483870968, "os": -0.20634920634920634, "bg": 4.5181609575743755e-06}, {"x": 0.07189542483660129, "y": 0.04195804195804195, "ox": 0.07189542483660129, "oy": 0.04195804195804195, "term": "desirable", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 11, "s": 0.5026881720430108, "os": -0.02931096681096681, "bg": 5.686491672132946e-06}, {"x": 0.6013071895424836, "y": 0.14685314685314685, "ox": 0.6013071895424836, "oy": 0.14685314685314685, "term": "platform", "cat25k": 15, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 101, "s": 0.006048387096774194, "os": -0.45111832611832614, "bg": 6.419186168737412e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "visualisation", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.219943696677993e-05}, {"x": 0.058823529411764705, "y": 0.034965034965034954, "ox": 0.058823529411764705, "oy": 0.034965034965034954, "term": "looker", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5571236559139785, "os": -0.023268398268398265, "bg": 0.00010564722128942434}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "climate", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 4.766331397620234e-07}, {"x": 0.0784313725490196, "y": 0.04195804195804195, "ox": 0.0784313725490196, "oy": 0.04195804195804195, "term": "low", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.4516129032258065, "os": -0.035804473304473304, "bg": 1.947041024965663e-07}, {"x": 0.019607843137254898, "y": 0.12587412587412586, "ox": 0.019607843137254898, "oy": 0.12587412587412586, "term": "developments", "cat25k": 13, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 3, "s": 0.8783602150537634, "os": 0.10597041847041846, "bg": 2.629726769510272e-06}, {"x": 0.1372549019607843, "y": 0.3006993006993007, "ox": 0.1372549019607843, "oy": 0.3006993006993007, "term": "questions", "cat25k": 30, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 21, "s": 0.933467741935484, "os": 0.16269841269841273, "bg": 8.164086400781508e-07}, {"x": 0.065359477124183, "y": 0.13286713286713286, "ox": 0.065359477124183, "oy": 0.13286713286713286, "term": "sciences", "cat25k": 13, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 10, "s": 0.814516129032258, "os": 0.06746031746031747, "bg": 1.3503337873791892e-06}, {"x": 0.0, "y": 0.0769230769230769, "ox": 0.0, "oy": 0.0769230769230769, "term": "pytorch", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.82997311827957, "os": 0.07683982683982683, "bg": 0.0002734107997265892}, {"x": 0.0065359477124183, "y": 0.08391608391608392, "ox": 0.0065359477124183, "oy": 0.08391608391608392, "term": "keras", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8333333333333334, "os": 0.07729076479076478, "bg": 0.0003231138230578995}, {"x": 0.16993464052287582, "y": 0.11888111888111888, "ox": 0.16993464052287582, "oy": 0.11888111888111888, "term": "control", "cat25k": 12, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 26, "s": 0.3239247311827957, "os": -0.05032467532467533, "bg": 3.9881107925002184e-07}, {"x": 0.11764705882352941, "y": 0.06293706293706293, "ox": 0.11764705882352941, "oy": 0.06293706293706293, "term": "via", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 18, "s": 0.3004032258064516, "os": -0.05393217893217893, "bg": 5.552835751468192e-07}, {"x": 0.11111111111111112, "y": 0.12587412587412586, "ox": 0.11111111111111112, "oy": 0.12587412587412586, "term": "independently", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 17, "s": 0.685483870967742, "os": 0.015061327561327567, "bg": 9.278198566173703e-06}, {"x": 0.0326797385620915, "y": 0.0769230769230769, "ox": 0.0326797385620915, "oy": 0.0769230769230769, "term": "oral", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 5, "s": 0.75, "os": 0.04437229437229437, "bg": 7.614050807561694e-07}, {"x": 0.1764705882352941, "y": 0.05594405594405594, "ox": 0.1764705882352941, "oy": 0.05594405594405594, "term": "english", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 27, "s": 0.12768817204301075, "os": -0.11931818181818182, "bg": 4.059099679513785e-07}, {"x": 0.0522875816993464, "y": 0.47552447552447547, "ox": 0.0522875816993464, "oy": 0.47552447552447547, "term": "optimization", "cat25k": 48, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 8, "s": 0.9905913978494624, "os": 0.4207251082251082, "bg": 9.536536110868054e-06}, {"x": 0.065359477124183, "y": 0.048951048951048945, "ox": 0.065359477124183, "oy": 0.048951048951048945, "term": "arise", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 10, "s": 0.6283602150537634, "os": -0.015873015873015872, "bg": 4.039173328530676e-06}, {"x": 0.019607843137254898, "y": 0.39160839160839156, "ox": 0.019607843137254898, "oy": 0.39160839160839156, "term": "analyses", "cat25k": 40, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 3, "s": 0.9865591397849462, "os": 0.36985930735930733, "bg": 1.1932485994244565e-05}, {"x": 0.2026143790849673, "y": 0.1048951048951049, "ox": 0.2026143790849673, "oy": 0.1048951048951049, "term": "full", "cat25k": 11, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 31, "s": 0.16666666666666666, "os": -0.0966810966810967, "bg": 2.967649277814968e-07}, {"x": 0.6274509803921569, "y": 0.1818181818181818, "ox": 0.6274509803921569, "oy": 0.1818181818181818, "term": "architecture", "cat25k": 18, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 107, "s": 0.007392473118279569, "os": -0.4423701298701299, "bg": 6.401152785048929e-06}, {"x": 0.11764705882352941, "y": 0.14685314685314685, "ox": 0.11764705882352941, "oy": 0.14685314685314685, "term": "ms", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 18, "s": 0.717741935483871, "os": 0.029401154401154414, "bg": 1.1396852610115951e-06}, {"x": 0.11764705882352941, "y": 0.08391608391608392, "ox": 0.11764705882352941, "oy": 0.08391608391608392, "term": "exposure", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 18, "s": 0.4610215053763441, "os": -0.0330988455988456, "bg": 2.1990017411695784e-06}, {"x": 0.0065359477124183, "y": 0.20279720279720279, "ox": 0.0065359477124183, "oy": 0.20279720279720279, "term": "linear", "cat25k": 20, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 1, "s": 0.9489247311827957, "os": 0.19534632034632035, "bg": 2.6058296926997164e-06}, {"x": 0.6078431372549019, "y": 0.18881118881118877, "ox": 0.6078431372549019, "oy": 0.18881118881118877, "term": "good", "cat25k": 19, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 102, "s": 0.010080645161290324, "os": -0.41594516594516595, "bg": 7.05155051583609e-07}, {"x": 0.2745098039215686, "y": 0.19580419580419578, "ox": 0.2745098039215686, "oy": 0.19580419580419578, "term": "structures", "cat25k": 20, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 42, "s": 0.20295698924731181, "os": -0.07783189033189031, "bg": 5.4803926231566555e-06}, {"x": 0.6013071895424836, "y": 0.3636363636363636, "ox": 0.6013071895424836, "oy": 0.3636363636363636, "term": "applications", "cat25k": 37, "ncat25k": 58, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 101, "s": 0.03494623655913979, "os": -0.23584054834054835, "bg": 3.0984422380450676e-06}, {"x": 0.0784313725490196, "y": 0.0979020979020979, "ox": 0.0784313725490196, "oy": 0.0979020979020979, "term": "energy", "cat25k": 10, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 12, "s": 0.696236559139785, "os": 0.019751082251082255, "bg": 4.4243453995924546e-07}, {"x": 0.13071895424836602, "y": 0.048951048951048945, "ox": 0.13071895424836602, "oy": 0.048951048951048945, "term": "hdfs", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 20, "s": 0.198252688172043, "os": -0.0808080808080808, "bg": 0.0004242481380220609}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "particularly", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 4.734773719363854e-07}, {"x": 0.058823529411764705, "y": 0.1608391608391608, "ox": 0.058823529411764705, "oy": 0.1608391608391608, "term": "packages", "cat25k": 16, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.8736559139784946, "os": 0.10173160173160173, "bg": 1.4028319362844256e-06}, {"x": 0.4836601307189542, "y": 0.08391608391608392, "ox": 0.4836601307189542, "oy": 0.08391608391608392, "term": "distributed", "cat25k": 8, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 75, "s": 0.012768817204301078, "os": -0.3967352092352092, "bg": 6.175589909029302e-06}, {"x": 0.1830065359477124, "y": 0.1748251748251748, "ox": 0.1830065359477124, "oy": 0.1748251748251748, "term": "computing", "cat25k": 18, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 28, "s": 0.655241935483871, "os": -0.007756132756132766, "bg": 2.9071690459559797e-06}, {"x": 0.2875816993464052, "y": 0.08391608391608392, "ox": 0.2875816993464052, "oy": 0.08391608391608392, "term": "meet", "cat25k": 8, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 44, "s": 0.05645161290322581, "os": -0.2019300144300144, "bg": 1.1997037331630952e-06}, {"x": 0.196078431372549, "y": 0.40559440559440557, "ox": 0.196078431372549, "oy": 0.40559440559440557, "term": "clients", "cat25k": 41, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 30, "s": 0.9536290322580645, "os": 0.2084235209235209, "bg": 4.105229769825533e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "require", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 3.3597279785043113e-07}, {"x": 0.3071895424836601, "y": 0.20279720279720279, "ox": 0.3071895424836601, "oy": 0.20279720279720279, "term": "intelligence", "cat25k": 20, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 47, "s": 0.15524193548387097, "os": -0.10335497835497834, "bg": 4.80801805334863e-06}, {"x": 0.31372549019607837, "y": 0.06293706293706293, "ox": 0.31372549019607837, "oy": 0.06293706293706293, "term": "bi", "cat25k": 6, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 48, "s": 0.03024193548387097, "os": -0.24873737373737373, "bg": 8.668838945899602e-06}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "pentaho", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 0.00017471657089610188}, {"x": 0.7843137254901961, "y": 0.39160839160839156, "ox": 0.7843137254901961, "oy": 0.39160839160839156, "term": "processes", "cat25k": 40, "ncat25k": 93, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 162, "s": 0.014784946236559142, "os": -0.3898809523809524, "bg": 1.1079880295806496e-05}, {"x": 0.0522875816993464, "y": 0.04195804195804195, "ox": 0.0522875816993464, "oy": 0.04195804195804195, "term": "act", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.6478494623655914, "os": -0.009830447330447328, "bg": 1.9836115009596465e-07}, {"x": 0.2483660130718954, "y": 0.19580419580419578, "ox": 0.2483660130718954, "oy": 0.19580419580419578, "term": "expert", "cat25k": 20, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 38, "s": 0.30376344086021506, "os": -0.051857864357864325, "bg": 3.5089842224372748e-06}, {"x": 0.3006535947712418, "y": 0.13986013986013984, "ox": 0.3006535947712418, "oy": 0.13986013986013984, "term": "deployment", "cat25k": 14, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 46, "s": 0.08064516129032259, "os": -0.15936147186147184, "bg": 1.339441864722257e-05}, {"x": 0.1830065359477124, "y": 0.13286713286713286, "ox": 0.1830065359477124, "oy": 0.13286713286713286, "term": "integrity", "cat25k": 13, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 28, "s": 0.33198924731182794, "os": -0.049422799422799424, "bg": 7.0530650857596434e-06}, {"x": 0.2026143790849673, "y": 0.06293706293706293, "ox": 0.2026143790849673, "oy": 0.06293706293706293, "term": "back", "cat25k": 6, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 31, "s": 0.09677419354838711, "os": -0.13834776334776336, "bg": 1.6389929434859274e-07}, {"x": 0.039215686274509796, "y": 0.14685314685314685, "ox": 0.039215686274509796, "oy": 0.14685314685314685, "term": "organize", "cat25k": 15, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.8803763440860215, "os": 0.10732323232323233, "bg": 6.8234363811222805e-06}, {"x": 0.11764705882352941, "y": 0.048951048951048945, "ox": 0.11764705882352941, "oy": 0.048951048951048945, "term": "bonus", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 18, "s": 0.24059139784946237, "os": -0.06782106782106782, "bg": 1.792116815048563e-06}, {"x": 0.15686274509803919, "y": 0.06293706293706293, "ox": 0.15686274509803919, "oy": 0.06293706293706293, "term": "401k", "cat25k": 6, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 24, "s": 0.17338709677419353, "os": -0.09289321789321789, "bg": 0.0}, {"x": 0.1895424836601307, "y": 0.24475524475524474, "ox": 0.1895424836601307, "oy": 0.24475524475524474, "term": "health", "cat25k": 25, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 29, "s": 0.7782258064516129, "os": 0.055194805194805185, "bg": 2.9058090025499814e-07}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "pto", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 2.1117242932762693e-05}, {"x": 0.2745098039215686, "y": 0.22377622377622375, "ox": 0.2745098039215686, "oy": 0.22377622377622375, "term": "great", "cat25k": 22, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 42, "s": 0.3266129032258065, "os": -0.050054112554112545, "bg": 4.90768319623665e-07}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "very", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 5.373072882859062e-08}, {"x": 0.065359477124183, "y": 0.04195804195804195, "ox": 0.065359477124183, "oy": 0.04195804195804195, "term": "friendly", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5598118279569892, "os": -0.022817460317460313, "bg": 4.951815661358078e-07}, {"x": 0.1045751633986928, "y": 0.08391608391608392, "ox": 0.1045751633986928, "oy": 0.08391608391608392, "term": "culture", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 16, "s": 0.5759408602150538, "os": -0.02011183261183261, "bg": 8.152936154022125e-07}, {"x": 0.08496732026143791, "y": 0.20279720279720279, "ox": 0.08496732026143791, "oy": 0.20279720279720279, "term": "scientist", "cat25k": 20, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 13, "s": 0.887768817204301, "os": 0.11742424242424243, "bg": 9.676136263497778e-06}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "valuable", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 1.484771413444708e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "undertake", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 3.1612745275387402e-06}, {"x": 0.12418300653594772, "y": 0.0909090909090909, "ox": 0.12418300653594772, "oy": 0.0909090909090909, "term": "amounts", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 19, "s": 0.46236559139784944, "os": -0.032647907647907634, "bg": 2.1574302470776763e-06}, {"x": 0.196078431372549, "y": 0.11188811188811187, "ox": 0.196078431372549, "oy": 0.11188811188811187, "term": "challenges", "cat25k": 11, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 30, "s": 0.1922043010752688, "os": -0.08324314574314576, "bg": 4.135803978715354e-06}, {"x": 0.0784313725490196, "y": 0.02797202797202797, "ox": 0.0784313725490196, "oy": 0.02797202797202797, "term": "as", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.3279569892473118, "os": -0.04969336219336219, "bg": 1.4237964918553211e-08}, {"x": 0.0065359477124183, "y": 0.08391608391608392, "ox": 0.0065359477124183, "oy": 0.08391608391608392, "term": "supervised", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8333333333333334, "os": 0.07729076479076478, "bg": 8.0693041497638e-06}, {"x": 0.1372549019607843, "y": 0.23076923076923073, "ox": 0.1372549019607843, "oy": 0.23076923076923073, "term": "senior", "cat25k": 23, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 21, "s": 0.8629032258064516, "os": 0.09325396825396826, "bg": 1.8942247035630418e-06}, {"x": 0.2810457516339869, "y": 0.11888111888111888, "ox": 0.2810457516339869, "oy": 0.11888111888111888, "term": "participate", "cat25k": 12, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 43, "s": 0.07930107526881722, "os": -0.1607142857142857, "bg": 4.895747904232317e-06}, {"x": 0.07189542483660129, "y": 0.05594405594405594, "ox": 0.07189542483660129, "oy": 0.05594405594405594, "term": "regular", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.6303763440860215, "os": -0.01542207792207792, "bg": 6.09080069505653e-07}, {"x": 0.0522875816993464, "y": 0.0909090909090909, "ox": 0.0522875816993464, "oy": 0.0909090909090909, "term": "limited", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 8, "s": 0.7405913978494624, "os": 0.038780663780663784, "bg": 3.953482794311097e-07}, {"x": 0.058823529411764705, "y": 0.08391608391608392, "ox": 0.058823529411764705, "oy": 0.08391608391608392, "term": "exploration", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.7063172043010753, "os": 0.025342712842712847, "bg": 3.8614685293532744e-06}, {"x": 0.09150326797385622, "y": 0.13286713286713286, "ox": 0.09150326797385622, "oy": 0.13286713286713286, "term": "include", "cat25k": 13, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 14, "s": 0.7452956989247312, "os": 0.04148629148629149, "bg": 3.613275265298988e-07}, {"x": 0.0457516339869281, "y": 0.12587412587412586, "ox": 0.0457516339869281, "oy": 0.12587412587412586, "term": "train", "cat25k": 13, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.840725806451613, "os": 0.0799963924963925, "bg": 1.567523422718744e-06}, {"x": 0.2745098039215686, "y": 0.0909090909090909, "ox": 0.2745098039215686, "oy": 0.0909090909090909, "term": "developers", "cat25k": 9, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 42, "s": 0.06586021505376345, "os": -0.181998556998557, "bg": 4.5565767350305064e-06}, {"x": 0.22875816993464052, "y": 0.13986013986013984, "ox": 0.22875816993464052, "oy": 0.13986013986013984, "term": "integrate", "cat25k": 14, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 35, "s": 0.1821236559139785, "os": -0.08793290043290042, "bg": 1.4421119230929576e-05}, {"x": 0.12418300653594772, "y": 0.0979020979020979, "ox": 0.12418300653594772, "oy": 0.0979020979020979, "term": "document", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 19, "s": 0.5094086021505376, "os": -0.025703463203463187, "bg": 6.681226408754051e-07}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "thorough", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 4.17317064796631e-06}, {"x": 0.1045751633986928, "y": 0.12587412587412586, "ox": 0.1045751633986928, "oy": 0.12587412587412586, "term": "way", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 16, "s": 0.6989247311827957, "os": 0.021554834054834063, "bg": 2.2251593452968631e-07}, {"x": 0.13071895424836602, "y": 0.08391608391608392, "ox": 0.13071895424836602, "oy": 0.08391608391608392, "term": "order", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 20, "s": 0.34408602150537637, "os": -0.04608585858585858, "bg": 1.9007360044806051e-07}, {"x": 0.08496732026143791, "y": 0.11188811188811187, "ox": 0.08496732026143791, "oy": 0.11188811188811187, "term": "promote", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 13, "s": 0.7137096774193549, "os": 0.02714646464646464, "bg": 1.7473266805577132e-06}, {"x": 0.0326797385620915, "y": 0.08391608391608392, "ox": 0.0326797385620915, "oy": 0.08391608391608392, "term": "smes", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.7668010752688171, "os": 0.051316738816738816, "bg": 1.712594419359973e-05}, {"x": 0.196078431372549, "y": 0.006993006993006992, "ox": 0.196078431372549, "oy": 0.006993006993006992, "term": "ingestion", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 30, "s": 0.06250000000000001, "os": -0.18740981240981242, "bg": 4.886310528186524e-05}, {"x": 0.1045751633986928, "y": 0.034965034965034954, "ox": 0.1045751633986928, "oy": 0.034965034965034954, "term": "curation", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.23655913978494625, "os": -0.06872294372294371, "bg": 0.0001808575230270382}, {"x": 0.019607843137254898, "y": 0.13286713286713286, "ox": 0.019607843137254898, "oy": 0.13286713286713286, "term": "experimental", "cat25k": 13, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.8844086021505377, "os": 0.11291486291486291, "bg": 2.262757814254119e-06}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "assessment", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 2.684853776654599e-07}, {"x": 0.12418300653594772, "y": 0.08391608391608392, "ox": 0.12418300653594772, "oy": 0.08391608391608392, "term": "review", "cat25k": 8, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 19, "s": 0.39650537634408606, "os": -0.03959235209235208, "bg": 1.8281090238106276e-07}, {"x": 0.1764705882352941, "y": 0.20279720279720279, "ox": 0.1764705882352941, "oy": 0.20279720279720279, "term": "presentation", "cat25k": 20, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 27, "s": 0.711021505376344, "os": 0.02651515151515152, "bg": 2.989240123417184e-06}, {"x": 0.0065359477124183, "y": 0.08391608391608392, "ox": 0.0065359477124183, "oy": 0.08391608391608392, "term": "discussion", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8333333333333334, "os": 0.07729076479076478, "bg": 2.3203112379821602e-07}, {"x": 0.0522875816993464, "y": 0.0769230769230769, "ox": 0.0522875816993464, "oy": 0.0769230769230769, "term": "conferences", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.7036290322580645, "os": 0.02489177489177489, "bg": 2.0315119581209156e-06}, {"x": 0.11111111111111112, "y": 0.048951048951048945, "ox": 0.11111111111111112, "oy": 0.048951048951048945, "term": "meetings", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 17, "s": 0.26814516129032256, "os": -0.06132756132756133, "bg": 1.2186415290031226e-06}, {"x": 0.09150326797385622, "y": 0.23776223776223773, "ox": 0.09150326797385622, "oy": 0.23776223776223773, "term": "levels", "cat25k": 24, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 14, "s": 0.9126344086021506, "os": 0.14565295815295815, "bg": 1.4234591555076486e-06}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "internally", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 3.7209233099101216e-06}, {"x": 0.196078431372549, "y": 0.04195804195804195, "ox": 0.196078431372549, "oy": 0.04195804195804195, "term": "guidance", "cat25k": 4, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 30, "s": 0.08467741935483873, "os": -0.1526875901875902, "bg": 3.0300044932441626e-06}, {"x": 0.09150326797385622, "y": 0.013986013986013983, "ox": 0.09150326797385622, "oy": 0.013986013986013983, "term": "ingest", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.20698924731182797, "os": -0.07656926406926408, "bg": 7.522774022920952e-05}, {"x": 0.0261437908496732, "y": 0.0979020979020979, "ox": 0.0261437908496732, "oy": 0.0979020979020979, "term": "segmentation", "cat25k": 10, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 4, "s": 0.821236559139785, "os": 0.0716991341991342, "bg": 1.4821457023332679e-05}, {"x": 0.5294117647058822, "y": 0.32167832167832167, "ox": 0.5294117647058822, "oy": 0.32167832167832167, "term": "scalable", "cat25k": 32, "ncat25k": 48, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 84, "s": 0.053763440860215055, "os": -0.20607864357864353, "bg": 8.215766245018797e-05}, {"x": 0.2483660130718954, "y": 0.1048951048951049, "ox": 0.2483660130718954, "oy": 0.1048951048951049, "term": "efficient", "cat25k": 11, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 38, "s": 0.09408602150537634, "os": -0.14213564213564212, "bg": 4.599857525922367e-06}, {"x": 0.2352941176470588, "y": 0.05594405594405594, "ox": 0.2352941176470588, "oy": 0.05594405594405594, "term": "automated", "cat25k": 6, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 36, "s": 0.0685483870967742, "os": -0.17775974025974026, "bg": 6.823054698646221e-06}, {"x": 0.1045751633986928, "y": 0.12587412587412586, "ox": 0.1045751633986928, "oy": 0.12587412587412586, "term": "ad", "cat25k": 13, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 16, "s": 0.6989247311827957, "os": 0.021554834054834063, "bg": 8.623460247883648e-07}, {"x": 0.09150326797385622, "y": 0.0909090909090909, "ox": 0.09150326797385622, "oy": 0.0909090909090909, "term": "hoc", "cat25k": 9, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 14, "s": 0.665994623655914, "os": -0.00018037518037518352, "bg": 1.2061435592268978e-05}, {"x": 0.039215686274509796, "y": 0.12587412587412586, "ox": 0.039215686274509796, "oy": 0.12587412587412586, "term": "behavior", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.8481182795698925, "os": 0.08648989898989899, "bg": 1.4902806226355184e-06}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "debugging", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 6.504149647475089e-06}, {"x": 0.16993464052287582, "y": 0.048951048951048945, "ox": 0.16993464052287582, "oy": 0.048951048951048945, "term": "optimizing", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 26, "s": 0.12634408602150538, "os": -0.11976911976911978, "bg": 2.35631814579183e-05}, {"x": 0.7320261437908497, "y": 0.1818181818181818, "ox": 0.7320261437908497, "oy": 0.1818181818181818, "term": "services", "cat25k": 18, "ncat25k": 86, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 149, "s": 0.002688172043010753, "os": -0.5462662337662337, "bg": 6.224574469548922e-07}, {"x": 0.09150326797385622, "y": 0.18881118881118877, "ox": 0.09150326797385622, "oy": 0.18881118881118877, "term": "media", "cat25k": 19, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 14, "s": 0.8682795698924732, "os": 0.09704184704184704, "bg": 3.7873013678785713e-07}, {"x": 0.22875816993464052, "y": 0.02797202797202797, "ox": 0.22875816993464052, "oy": 0.02797202797202797, "term": "transform", "cat25k": 3, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 35, "s": 0.058467741935483875, "os": -0.19904401154401152, "bg": 1.0304624517317516e-05}, {"x": 0.07189542483660129, "y": 0.048951048951048945, "ox": 0.07189542483660129, "oy": 0.048951048951048945, "term": "raw", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.564516129032258, "os": -0.022366522366522368, "bg": 1.434569065813406e-06}, {"x": 0.0522875816993464, "y": 0.034965034965034954, "ox": 0.0522875816993464, "oy": 0.034965034965034954, "term": "formats", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.6216397849462366, "os": -0.016774891774891776, "bg": 1.3638379112213909e-06}, {"x": 0.2549019607843137, "y": 0.0979020979020979, "ox": 0.2549019607843137, "oy": 0.0979020979020979, "term": "solid", "cat25k": 10, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 39, "s": 0.08198924731182797, "os": -0.15557359307359303, "bg": 2.8261428128348795e-06}, {"x": 0.039215686274509796, "y": 0.0909090909090909, "ox": 0.039215686274509796, "oy": 0.0909090909090909, "term": "experiment", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.7688172043010753, "os": 0.05176767676767677, "bg": 2.55687155192414e-06}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "sample", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 2.6072641507958155e-07}, {"x": 0.09150326797385622, "y": 0.048951048951048945, "ox": 0.09150326797385622, "oy": 0.048951048951048945, "term": "power", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.38911290322580644, "os": -0.041847041847041855, "bg": 1.852857991321151e-07}, {"x": 0.0326797385620915, "y": 0.08391608391608392, "ox": 0.0326797385620915, "oy": 0.08391608391608392, "term": "market", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 5, "s": 0.7668010752688171, "os": 0.051316738816738816, "bg": 2.092686037065126e-07}, {"x": 0.0, "y": 0.0769230769230769, "ox": 0.0, "oy": 0.0769230769230769, "term": "prediction", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.82997311827957, "os": 0.07683982683982683, "bg": 3.0462834303643836e-06}, {"x": 0.0, "y": 0.08391608391608392, "ox": 0.0, "oy": 0.08391608391608392, "term": "multivariate", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 0, "s": 0.8440860215053764, "os": 0.08378427128427128, "bg": 1.8240256093195547e-05}, {"x": 0.1895424836601307, "y": 0.020979020979020976, "ox": 0.1895424836601307, "oy": 0.020979020979020976, "term": "scalability", "cat25k": 2, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 29, "s": 0.07392473118279572, "os": -0.16702741702741702, "bg": 3.212011316317369e-05}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "vendor", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 4.80941920629193e-07}, {"x": 0.0326797385620915, "y": 0.24475524475524474, "ox": 0.0326797385620915, "oy": 0.24475524475524474, "term": "detection", "cat25k": 25, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.9556451612903226, "os": 0.21103896103896103, "bg": 4.535103172463398e-06}, {"x": 0.1503267973856209, "y": 0.1678321678321678, "ox": 0.1503267973856209, "oy": 0.1678321678321678, "term": "digital", "cat25k": 17, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 23, "s": 0.6915322580645162, "os": 0.017766955266955264, "bg": 4.643643167678378e-07}, {"x": 0.6797385620915033, "y": 0.1818181818181818, "ox": 0.6797385620915033, "oy": 0.1818181818181818, "term": "integration", "cat25k": 18, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 127, "s": 0.004704301075268817, "os": -0.49431818181818177, "bg": 9.4028672292101e-06}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "join", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 1.8350910140914807e-07}, {"x": 0.0326797385620915, "y": 0.13286713286713286, "ox": 0.0326797385620915, "oy": 0.13286713286713286, "term": "clustering", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.8716397849462366, "os": 0.09992784992784993, "bg": 1.7414909121385184e-05}, {"x": 0.09803921568627451, "y": 0.013986013986013983, "ox": 0.09803921568627451, "oy": 0.013986013986013983, "term": "bigquery", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1935483870967742, "os": -0.08306277056277057, "bg": 0.00042251245790408964}, {"x": 0.1830065359477124, "y": 0.05594405594405594, "ox": 0.1830065359477124, "oy": 0.05594405594405594, "term": "lake", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 28, "s": 0.114247311827957, "os": -0.12581168831168832, "bg": 8.43862322690022e-07}, {"x": 0.9019607843137255, "y": 0.13286713286713286, "ox": 0.9019607843137255, "oy": 0.13286713286713286, "term": "etl", "cat25k": 13, "ncat25k": 124, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 216, "s": 0.0, "os": -0.7637085137085138, "bg": 0.0008600481992969565}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "enjoys", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 3.4173288351522067e-06}, {"x": 0.16993464052287582, "y": 0.0979020979020979, "ox": 0.16993464052287582, "oy": 0.0979020979020979, "term": "managing", "cat25k": 10, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 26, "s": 0.22110215053763443, "os": -0.07115800865800866, "bg": 3.182823574298938e-06}, {"x": 0.11111111111111112, "y": 0.12587412587412586, "ox": 0.11111111111111112, "oy": 0.12587412587412586, "term": "collaborative", "cat25k": 13, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 17, "s": 0.685483870967742, "os": 0.015061327561327567, "bg": 6.90023842295248e-06}, {"x": 0.0784313725490196, "y": 0.05594405594405594, "ox": 0.0784313725490196, "oy": 0.05594405594405594, "term": "individual", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 12, "s": 0.5685483870967742, "os": -0.021915584415584416, "bg": 4.215160507621664e-07}, {"x": 0.0261437908496732, "y": 0.06293706293706293, "ox": 0.0261437908496732, "oy": 0.06293706293706293, "term": "skill", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.7318548387096774, "os": 0.03697691197691198, "bg": 1.6336231820679949e-06}, {"x": 0.019607843137254898, "y": 0.0769230769230769, "ox": 0.019607843137254898, "oy": 0.0769230769230769, "term": "experiences", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 3, "s": 0.788978494623656, "os": 0.05735930735930735, "bg": 1.15659675700183e-06}, {"x": 0.0, "y": 0.0909090909090909, "ox": 0.0, "oy": 0.0909090909090909, "term": "econometrics", "cat25k": 9, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.8568548387096775, "os": 0.09072871572871573, "bg": 3.094751777101694e-05}, {"x": 0.1503267973856209, "y": 0.06293706293706293, "ox": 0.1503267973856209, "oy": 0.06293706293706293, "term": "focused", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 23, "s": 0.18548387096774194, "os": -0.08639971139971139, "bg": 3.1984258946959255e-06}, {"x": 0.5359477124183006, "y": 0.3706293706293706, "ox": 0.5359477124183006, "oy": 0.3706293706293706, "term": "end", "cat25k": 38, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 85, "s": 0.07594086021505377, "os": -0.16396103896103897, "bg": 1.2585283346404748e-06}, {"x": 0.09803921568627451, "y": 0.0979020979020979, "ox": 0.09803921568627451, "oy": 0.0979020979020979, "term": "change", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 15, "s": 0.6673387096774194, "os": 0.00027056277056276834, "bg": 2.7529677502596133e-07}, {"x": 0.1045751633986928, "y": 0.47552447552447547, "ox": 0.1045751633986928, "oy": 0.47552447552447547, "term": "goals", "cat25k": 48, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 16, "s": 0.9852150537634409, "os": 0.3687770562770563, "bg": 4.613633689209134e-06}, {"x": 0.1764705882352941, "y": 0.1048951048951049, "ox": 0.1764705882352941, "oy": 0.1048951048951049, "term": "creating", "cat25k": 11, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 27, "s": 0.22513440860215053, "os": -0.07070707070707072, "bg": 2.156926576832729e-06}, {"x": 0.09803921568627451, "y": 0.14685314685314685, "ox": 0.09803921568627451, "oy": 0.14685314685314685, "term": "clear", "cat25k": 15, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 15, "s": 0.7560483870967742, "os": 0.04888167388167389, "bg": 8.133010874513289e-07}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "fashion", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 7.278658386768163e-07}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "logistic", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 1.2759469121021437e-05}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "competencies", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 6.1435543875217585e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "popular", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.2009314218233702e-07}, {"x": 0.2026143790849673, "y": 0.0909090909090909, "ox": 0.2026143790849673, "oy": 0.0909090909090909, "term": "supporting", "cat25k": 9, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 31, "s": 0.14381720430107528, "os": -0.11056998556998558, "bg": 3.582792742157603e-06}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "bayesian", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 1.652260228444041e-05}, {"x": 0.09803921568627451, "y": 0.13986013986013984, "ox": 0.09803921568627451, "oy": 0.13986013986013984, "term": "healthcare", "cat25k": 14, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 15, "s": 0.745967741935484, "os": 0.04193722943722944, "bg": 2.44668524306612e-06}, {"x": 0.1830065359477124, "y": 0.04195804195804195, "ox": 0.1830065359477124, "oy": 0.04195804195804195, "term": "ecosystem", "cat25k": 4, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 28, "s": 0.09610215053763442, "os": -0.13970057720057721, "bg": 1.449469366317572e-05}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "wider", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 2.6611289379869866e-06}, {"x": 0.07189542483660129, "y": 0.013986013986013983, "ox": 0.07189542483660129, "oy": 0.013986013986013983, "term": "ideal", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28897849462365593, "os": -0.05708874458874459, "bg": 9.95529460090679e-07}, {"x": 0.0784313725490196, "y": 0.04195804195804195, "ox": 0.0784313725490196, "oy": 0.04195804195804195, "term": "would", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.4516129032258065, "os": -0.035804473304473304, "bg": 6.285743410656492e-08}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "advantage", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 5.447293621110224e-07}, {"x": 0.0457516339869281, "y": 0.06993006993006992, "ox": 0.0457516339869281, "oy": 0.06993006993006992, "term": "curiosity", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.7016129032258064, "os": 0.024440836940836944, "bg": 1.1925765616614698e-05}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "organizational", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 2.3072345468972928e-06}, {"x": 0.09803921568627451, "y": 0.1748251748251748, "ox": 0.09803921568627451, "oy": 0.1748251748251748, "term": "player", "cat25k": 18, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 15, "s": 0.8293010752688172, "os": 0.07665945165945165, "bg": 8.621592785597725e-07}, {"x": 0.16993464052287582, "y": 0.08391608391608392, "ox": 0.16993464052287582, "oy": 0.08391608391608392, "term": "record", "cat25k": 8, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 26, "s": 0.1881720430107527, "os": -0.08504689754689755, "bg": 7.428023793328531e-07}, {"x": 0.0784313725490196, "y": 0.1048951048951049, "ox": 0.0784313725490196, "oy": 0.1048951048951049, "term": "previous", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 12, "s": 0.7123655913978495, "os": 0.02669552669552669, "bg": 2.67627271474696e-07}, {"x": 0.196078431372549, "y": 0.11188811188811187, "ox": 0.196078431372549, "oy": 0.11188811188811187, "term": "medical", "cat25k": 11, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 30, "s": 0.1922043010752688, "os": -0.08324314574314576, "bg": 5.922683347397882e-07}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "german", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 4.46169218971487e-07}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "contributing", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 2.873053072592594e-06}, {"x": 0.2352941176470588, "y": 0.1608391608391608, "ox": 0.2352941176470588, "oy": 0.1608391608391608, "term": "fast", "cat25k": 16, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 36, "s": 0.21370967741935484, "os": -0.0735930735930736, "bg": 1.2550256198058357e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "biologists", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.2058705222366832e-05}, {"x": 0.019607843137254898, "y": 0.0979020979020979, "ox": 0.019607843137254898, "oy": 0.0979020979020979, "term": "researchers", "cat25k": 10, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 3, "s": 0.8366935483870969, "os": 0.0781926406926407, "bg": 1.8327976443159689e-06}, {"x": 0.1830065359477124, "y": 0.13986013986013984, "ox": 0.1830065359477124, "oy": 0.13986013986013984, "term": "an", "cat25k": 14, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 28, "s": 0.3850806451612903, "os": -0.042478354978354976, "bg": 6.32266459773977e-08}, {"x": 0.09803921568627451, "y": 0.04195804195804195, "ox": 0.09803921568627451, "oy": 0.04195804195804195, "term": "compensation", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.2956989247311828, "os": -0.05528499278499279, "bg": 1.688768695021281e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "type", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 6.607509341861895e-08}, {"x": 0.16993464052287582, "y": 0.08391608391608392, "ox": 0.16993464052287582, "oy": 0.08391608391608392, "term": "term", "cat25k": 8, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 26, "s": 0.1881720430107527, "os": -0.08504689754689755, "bg": 6.531267981397161e-07}, {"x": 0.1503267973856209, "y": 0.06293706293706293, "ox": 0.1503267973856209, "oy": 0.06293706293706293, "term": "hours", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 23, "s": 0.18548387096774194, "os": -0.08639971139971139, "bg": 3.2270525428190797e-07}, {"x": 0.0784313725490196, "y": 0.1048951048951049, "ox": 0.0784313725490196, "oy": 0.1048951048951049, "term": "date", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 12, "s": 0.7123655913978495, "os": 0.02669552669552669, "bg": 1.1041864195478376e-07}, {"x": 0.039215686274509796, "y": 0.08391608391608392, "ox": 0.039215686274509796, "oy": 0.08391608391608392, "term": "possible", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 6, "s": 0.7513440860215054, "os": 0.04482323232323232, "bg": 2.9175303400666075e-07}, {"x": 0.1633986928104575, "y": 0.1678321678321678, "ox": 0.1633986928104575, "oy": 0.1678321678321678, "term": "future", "cat25k": 17, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 25, "s": 0.672715053763441, "os": 0.004779942279942273, "bg": 8.037736417919119e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "contact", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.8578593777690813e-08}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "atmosphere", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 8.062476669208139e-07}, {"x": 0.0784313725490196, "y": 0.048951048951048945, "ox": 0.0784313725490196, "oy": 0.048951048951048945, "term": "member", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.5053763440860216, "os": -0.028860028860028863, "bg": 1.3383138990315278e-07}, {"x": 0.0326797385620915, "y": 0.06993006993006992, "ox": 0.0326797385620915, "oy": 0.06993006993006992, "term": "university", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7338709677419355, "os": 0.037427849927849935, "bg": 9.632228511906903e-08}, {"x": 0.0784313725490196, "y": 0.013986013986013983, "ox": 0.0784313725490196, "oy": 0.013986013986013983, "term": "greater", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.25806451612903225, "os": -0.06358225108225109, "bg": 5.022031742935396e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "site", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 2.131714607161622e-08}, {"x": 0.6797385620915033, "y": 0.15384615384615385, "ox": 0.6797385620915033, "oy": 0.15384615384615385, "term": "infrastructure", "cat25k": 15, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 127, "s": 0.004032258064516129, "os": -0.5220959595959596, "bg": 1.0215668181659399e-05}, {"x": 0.13071895424836602, "y": 0.20279720279720279, "ox": 0.13071895424836602, "oy": 0.20279720279720279, "term": "financial", "cat25k": 20, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 20, "s": 0.8219086021505377, "os": 0.07196969696969699, "bg": 6.603294801535953e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "local", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 6.646397664650822e-08}, {"x": 0.0, "y": 0.0769230769230769, "ox": 0.0, "oy": 0.0769230769230769, "term": "inventory", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.82997311827957, "os": 0.07683982683982683, "bg": 9.717293885596002e-07}, {"x": 0.11111111111111112, "y": 0.20979020979020974, "ox": 0.11111111111111112, "oy": 0.20979020979020974, "term": "our", "cat25k": 21, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 17, "s": 0.8702956989247311, "os": 0.09839466089466091, "bg": 9.410930956291559e-08}, {"x": 0.261437908496732, "y": 0.39860139860139854, "ox": 0.261437908496732, "oy": 0.39860139860139854, "term": "optimize", "cat25k": 41, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 40, "s": 0.9059139784946237, "os": 0.13654401154401158, "bg": 3.522375892859389e-05}, {"x": 0.08496732026143791, "y": 0.0979020979020979, "ox": 0.08496732026143791, "oy": 0.0979020979020979, "term": "love", "cat25k": 10, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 13, "s": 0.6794354838709677, "os": 0.01325757575757576, "bg": 2.684643743822802e-07}, {"x": 0.1503267973856209, "y": 0.13986013986013984, "ox": 0.1503267973856209, "oy": 0.13986013986013984, "term": "keep", "cat25k": 14, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 23, "s": 0.6471774193548387, "os": -0.010010822510822498, "bg": 7.185648095033304e-07}, {"x": 0.4052287581699346, "y": 0.3356643356643356, "ox": 0.4052287581699346, "oy": 0.3356643356643356, "term": "make", "cat25k": 34, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 63, "s": 0.23588709677419353, "os": -0.06881313131313133, "bg": 5.479246395409268e-07}, {"x": 0.2026143790849673, "y": 0.0769230769230769, "ox": 0.2026143790849673, "oy": 0.0769230769230769, "term": "things", "cat25k": 8, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 31, "s": 0.11559139784946237, "os": -0.12445887445887448, "bg": 5.673743957538673e-07}, {"x": 0.196078431372549, "y": 0.020979020979020976, "ox": 0.196078431372549, "oy": 0.020979020979020976, "term": "no", "cat25k": 2, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 30, "s": 0.06989247311827958, "os": -0.17352092352092352, "bg": 7.042307570762223e-08}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "offices", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 7.159776764669365e-07}, {"x": 0.1633986928104575, "y": 0.1048951048951049, "ox": 0.1633986928104575, "oy": 0.1048951048951049, "term": "diverse", "cat25k": 11, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 25, "s": 0.2842741935483871, "os": -0.05772005772005773, "bg": 5.920301639368526e-06}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "inclusive", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 1.3201251667247367e-06}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "sustainable", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 1.1960629305248907e-06}, {"x": 0.07189542483660129, "y": 0.0909090909090909, "ox": 0.07189542483660129, "oy": 0.0909090909090909, "term": "successful", "cat25k": 9, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 11, "s": 0.6955645161290323, "os": 0.019300144300144303, "bg": 1.1131164212639189e-06}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "lunch", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 1.5319882253299986e-06}, {"x": 0.29411764705882354, "y": 0.1748251748251748, "ox": 0.29411764705882354, "oy": 0.1748251748251748, "term": "continuous", "cat25k": 18, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 45, "s": 0.12903225806451613, "os": -0.11814574314574314, "bg": 6.3771187521545544e-06}, {"x": 0.0784313725490196, "y": 0.02797202797202797, "ox": 0.0784313725490196, "oy": 0.02797202797202797, "term": "lunches", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.3279569892473118, "os": -0.04969336219336219, "bg": 2.3349040427404188e-05}, {"x": 0.08496732026143791, "y": 0.06993006993006992, "ox": 0.08496732026143791, "oy": 0.06993006993006992, "term": "progress", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.633736559139785, "os": -0.014520202020202017, "bg": 9.632446973012962e-07}, {"x": 0.11764705882352941, "y": 0.0979020979020979, "ox": 0.11764705882352941, "oy": 0.0979020979020979, "term": "creation", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 18, "s": 0.5772849462365591, "os": -0.019209956709956705, "bg": 2.1716616411362393e-06}, {"x": 0.039215686274509796, "y": 0.0979020979020979, "ox": 0.039215686274509796, "oy": 0.0979020979020979, "term": "broad", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.7930107526881721, "os": 0.058712121212121215, "bg": 1.8647497039709845e-06}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "workshops", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 1.425143957353992e-06}, {"x": 0.13071895424836602, "y": 0.22377622377622375, "ox": 0.13071895424836602, "oy": 0.22377622377622375, "term": "master", "cat25k": 22, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 20, "s": 0.8615591397849462, "os": 0.0928030303030303, "bg": 1.6832618208720355e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "qualifications", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.2562687812182792e-06}, {"x": 0.2875816993464052, "y": 0.034965034965034954, "ox": 0.2875816993464052, "oy": 0.034965034965034954, "term": "storage", "cat25k": 4, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 44, "s": 0.02956989247311828, "os": -0.2505411255411255, "bg": 1.2039167983419952e-06}, {"x": 0.12418300653594772, "y": 0.02797202797202797, "ox": 0.12418300653594772, "oy": 0.02797202797202797, "term": "together", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 19, "s": 0.17002688172043012, "os": -0.09514790764790763, "bg": 4.883525683024234e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "transfer", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.6190037663238413e-07}, {"x": 0.0065359477124183, "y": 0.08391608391608392, "ox": 0.0065359477124183, "oy": 0.08391608391608392, "term": "mine", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 1, "s": 0.8333333333333334, "os": 0.07729076479076478, "bg": 9.004034638659778e-07}, {"x": 0.013071895424836598, "y": 0.2517482517482517, "ox": 0.013071895424836598, "oy": 0.2517482517482517, "term": "recognition", "cat25k": 25, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 2, "s": 0.9637096774193549, "os": 0.23746392496392493, "bg": 3.4275800590247323e-06}, {"x": 0.08496732026143791, "y": 0.1608391608391608, "ox": 0.08496732026143791, "oy": 0.1608391608391608, "term": "utilize", "cat25k": 16, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 13, "s": 0.8272849462365591, "os": 0.07575757575757575, "bg": 1.1781178356731791e-05}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "domains", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 9.965810625680165e-07}, {"x": 0.1503267973856209, "y": 0.48251748251748244, "ox": 0.1503267973856209, "oy": 0.48251748251748244, "term": "marketing", "cat25k": 49, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 23, "s": 0.9825268817204301, "os": 0.33026695526695526, "bg": 1.6958925482481432e-06}, {"x": 0.0457516339869281, "y": 0.0909090909090909, "ox": 0.0457516339869281, "oy": 0.0909090909090909, "term": "supply", "cat25k": 9, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 7, "s": 0.7520161290322581, "os": 0.04527417027417027, "bg": 5.456952739637885e-07}, {"x": 0.065359477124183, "y": 0.034965034965034954, "ox": 0.065359477124183, "oy": 0.034965034965034954, "term": "chain", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.5006720430107527, "os": -0.02976190476190476, "bg": 8.331288464864568e-07}, {"x": 0.09150326797385622, "y": 0.04195804195804195, "ox": 0.09150326797385622, "oy": 0.04195804195804195, "term": "policy", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.334005376344086, "os": -0.048791486291486295, "bg": 1.040359871715513e-07}, {"x": 0.0326797385620915, "y": 0.13286713286713286, "ox": 0.0326797385620915, "oy": 0.13286713286713286, "term": "presentations", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 5, "s": 0.8716397849462366, "os": 0.09992784992784993, "bg": 3.178964789984671e-06}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "extracting", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 1.3731142124714681e-05}, {"x": 0.08496732026143791, "y": 0.1748251748251748, "ox": 0.08496732026143791, "oy": 0.1748251748251748, "term": "cleaning", "cat25k": 18, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 13, "s": 0.8548387096774194, "os": 0.08964646464646464, "bg": 2.5953664853837326e-06}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "command", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 4.670493513368404e-07}, {"x": 0.09803921568627451, "y": 0.034965034965034954, "ox": 0.09803921568627451, "oy": 0.034965034965034954, "term": "line", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.2647849462365591, "os": -0.06222943722943724, "bg": 1.4281120304332387e-07}, {"x": 0.0522875816993464, "y": 0.034965034965034954, "ox": 0.0522875816993464, "oy": 0.034965034965034954, "term": "ruby", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.6216397849462366, "os": -0.016774891774891776, "bg": 2.382852771500641e-06}, {"x": 0.11764705882352941, "y": 0.0769230769230769, "ox": 0.11764705882352941, "oy": 0.0769230769230769, "term": "leveraging", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 18, "s": 0.3951612903225807, "os": -0.04004329004329005, "bg": 2.6220899322596627e-05}, {"x": 0.15686274509803919, "y": 0.23776223776223773, "ox": 0.15686274509803919, "oy": 0.23776223776223773, "term": "acquisition", "cat25k": 24, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 24, "s": 0.8413978494623656, "os": 0.08071789321789322, "bg": 6.0245588023426385e-06}, {"x": 0.08496732026143791, "y": 0.06993006993006992, "ox": 0.08496732026143791, "oy": 0.06993006993006992, "term": "specific", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.633736559139785, "os": -0.014520202020202017, "bg": 4.264540339659142e-07}, {"x": 0.196078431372549, "y": 0.020979020979020976, "ox": 0.196078431372549, "oy": 0.020979020979020976, "term": "stream", "cat25k": 2, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 30, "s": 0.06989247311827958, "os": -0.17352092352092352, "bg": 1.706621035412671e-06}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "sure", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 1.6273510261166968e-07}, {"x": 0.0326797385620915, "y": 0.22377622377622375, "ox": 0.0326797385620915, "oy": 0.22377622377622375, "term": "select", "cat25k": 22, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 5, "s": 0.9475806451612904, "os": 0.1902056277056277, "bg": 4.3850932996153637e-07}, {"x": 0.09150326797385622, "y": 0.04195804195804195, "ox": 0.09150326797385622, "oy": 0.04195804195804195, "term": "defined", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.334005376344086, "os": -0.048791486291486295, "bg": 7.825698682780354e-07}, {"x": 0.1830065359477124, "y": 0.02797202797202797, "ox": 0.1830065359477124, "oy": 0.02797202797202797, "term": "optimal", "cat25k": 3, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 28, "s": 0.08400537634408603, "os": -0.1535894660894661, "bg": 6.14674193864399e-06}, {"x": 0.065359477124183, "y": 0.04195804195804195, "ox": 0.065359477124183, "oy": 0.04195804195804195, "term": "positive", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5598118279569892, "os": -0.022817460317460313, "bg": 5.692659706678592e-07}, {"x": 0.0261437908496732, "y": 0.06993006993006992, "ox": 0.0261437908496732, "oy": 0.06993006993006992, "term": "case", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 4, "s": 0.7493279569892473, "os": 0.04392135642135643, "bg": 1.1882357799962442e-07}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "qe", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 2.0772703006433307e-05}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "applicable", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 6.422649953841583e-07}, {"x": 0.0522875816993464, "y": 0.08391608391608392, "ox": 0.0522875816993464, "oy": 0.08391608391608392, "term": "beyond", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 8, "s": 0.7231182795698925, "os": 0.031836219336219336, "bg": 8.439581154558753e-07}, {"x": 0.0326797385620915, "y": 0.0909090909090909, "ox": 0.0326797385620915, "oy": 0.0909090909090909, "term": "hypotheses", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 5, "s": 0.790994623655914, "os": 0.058261183261183264, "bg": 1.8017792570163034e-05}, {"x": 0.0457516339869281, "y": 0.1048951048951049, "ox": 0.0457516339869281, "oy": 0.1048951048951049, "term": "stay", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.7950268817204301, "os": 0.05916305916305915, "bg": 5.447260374056685e-07}, {"x": 0.065359477124183, "y": 0.020979020979020976, "ox": 0.065359477124183, "oy": 0.020979020979020976, "term": "skilled", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.3763440860215054, "os": -0.04365079365079365, "bg": 3.289770862398248e-06}, {"x": 0.0784313725490196, "y": 0.05594405594405594, "ox": 0.0784313725490196, "oy": 0.05594405594405594, "term": "enjoy", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 12, "s": 0.5685483870967742, "os": -0.021915584415584416, "bg": 7.964648271475197e-07}, {"x": 0.0326797385620915, "y": 0.06993006993006992, "ox": 0.0326797385620915, "oy": 0.06993006993006992, "term": "supervision", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7338709677419355, "os": 0.037427849927849935, "bg": 3.0044140851739375e-06}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "quick", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 2.568141400464705e-07}, {"x": 0.1830065359477124, "y": 0.05594405594405594, "ox": 0.1830065359477124, "oy": 0.05594405594405594, "term": "detail", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 28, "s": 0.114247311827957, "os": -0.12581168831168832, "bg": 1.5798195951343665e-06}, {"x": 0.1372549019607843, "y": 0.12587412587412586, "ox": 0.1372549019607843, "oy": 0.12587412587412586, "term": "enhance", "cat25k": 13, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 21, "s": 0.6384408602150538, "os": -0.010912698412698402, "bg": 3.48355603007345e-06}, {"x": 0.1633986928104575, "y": 0.08391608391608392, "ox": 0.1633986928104575, "oy": 0.08391608391608392, "term": "efficiency", "cat25k": 8, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 25, "s": 0.20161290322580647, "os": -0.07855339105339106, "bg": 3.205346448571429e-06}, {"x": 0.08496732026143791, "y": 0.1818181818181818, "ox": 0.08496732026143791, "oy": 0.1818181818181818, "term": "content", "cat25k": 18, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 13, "s": 0.8676075268817205, "os": 0.09659090909090909, "bg": 4.127836164785506e-07}, {"x": 0.1764705882352941, "y": 0.20279720279720279, "ox": 0.1764705882352941, "oy": 0.20279720279720279, "term": "organization", "cat25k": 20, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 27, "s": 0.711021505376344, "os": 0.02651515151515152, "bg": 1.479825642714702e-06}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "publish", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 1.2555708983219157e-06}, {"x": 0.0522875816993464, "y": 0.0769230769230769, "ox": 0.0522875816993464, "oy": 0.0769230769230769, "term": "top", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.7036290322580645, "os": 0.02489177489177489, "bg": 7.846469469911767e-08}, {"x": 0.0, "y": 0.11888111888111888, "ox": 0.0, "oy": 0.11888111888111888, "term": "simulation", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.8884408602150538, "os": 0.1185064935064935, "bg": 2.092913542480236e-06}, {"x": 0.09803921568627451, "y": 0.1048951048951049, "ox": 0.09803921568627451, "oy": 0.1048951048951049, "term": "directly", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 15, "s": 0.6747311827956989, "os": 0.007215007215007202, "bg": 1.0304892397086457e-06}, {"x": 0.12418300653594772, "y": 0.0909090909090909, "ox": 0.12418300653594772, "oy": 0.0909090909090909, "term": "global", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 19, "s": 0.46236559139784944, "os": -0.032647907647907634, "bg": 6.842645173747111e-07}, {"x": 0.1503267973856209, "y": 0.26573426573426573, "ox": 0.1503267973856209, "oy": 0.26573426573426573, "term": "find", "cat25k": 27, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 23, "s": 0.8857526881720431, "os": 0.11498917748917747, "bg": 2.4296808877236637e-07}, {"x": 0.039215686274509796, "y": 0.13286713286713286, "ox": 0.039215686274509796, "oy": 0.13286713286713286, "term": "inform", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.864247311827957, "os": 0.09343434343434344, "bg": 4.266881489926745e-06}, {"x": 0.0457516339869281, "y": 0.0769230769230769, "ox": 0.0457516339869281, "oy": 0.0769230769230769, "term": "revenue", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.7204301075268817, "os": 0.03138528138528138, "bg": 1.1191006608662435e-06}, {"x": 0.07189542483660129, "y": 0.12587412587412586, "ox": 0.07189542483660129, "oy": 0.12587412587412586, "term": "audience", "cat25k": 13, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 11, "s": 0.7741935483870969, "os": 0.05402236652236653, "bg": 2.3622232283845064e-06}, {"x": 0.1045751633986928, "y": 0.11888111888111888, "ox": 0.1045751633986928, "oy": 0.11888111888111888, "term": "meaningful", "cat25k": 12, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 16, "s": 0.6841397849462365, "os": 0.014610389610389615, "bg": 9.635780631818137e-06}, {"x": 0.4183006535947712, "y": 0.23076923076923073, "ox": 0.4183006535947712, "oy": 0.23076923076923073, "term": "reporting", "cat25k": 23, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 65, "s": 0.06451612903225808, "os": -0.18596681096681095, "bg": 5.1475950764093515e-06}, {"x": 0.039215686274509796, "y": 0.0769230769230769, "ox": 0.039215686274509796, "oy": 0.0769230769230769, "term": "measure", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.7365591397849462, "os": 0.03787878787878787, "bg": 1.0570741046877692e-06}, {"x": 0.12418300653594772, "y": 0.2587412587412587, "ox": 0.12418300653594772, "oy": 0.2587412587412587, "term": "actionable", "cat25k": 26, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 19, "s": 0.9025537634408602, "os": 0.13401875901875904, "bg": 0.00016103453184242197}, {"x": 0.1503267973856209, "y": 0.05594405594405594, "ox": 0.1503267973856209, "oy": 0.05594405594405594, "term": "unit", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 23, "s": 0.17271505376344085, "os": -0.09334415584415584, "bg": 6.610171309412885e-07}, {"x": 0.196078431372549, "y": 0.06293706293706293, "ox": 0.196078431372549, "oy": 0.06293706293706293, "term": "events", "cat25k": 6, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 30, "s": 0.10483870967741936, "os": -0.13185425685425686, "bg": 3.874506772183328e-07}, {"x": 0.0065359477124183, "y": 0.06293706293706293, "ox": 0.0065359477124183, "oy": 0.06293706293706293, "term": "hbo", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7849462365591399, "os": 0.056457431457431456, "bg": 8.16179951355675e-06}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "promotion", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 5.985353167447026e-07}, {"x": 0.08496732026143791, "y": 0.05594405594405594, "ox": 0.08496732026143791, "oy": 0.05594405594405594, "term": "third", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 13, "s": 0.5067204301075269, "os": -0.02840909090909091, "bg": 4.3946976298924823e-07}, {"x": 0.15686274509803919, "y": 0.06293706293706293, "ox": 0.15686274509803919, "oy": 0.06293706293706293, "term": "party", "cat25k": 6, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 24, "s": 0.17338709677419353, "os": -0.09289321789321789, "bg": 4.55838922603434e-07}, {"x": 0.12418300653594772, "y": 0.05594405594405594, "ox": 0.12418300653594772, "oy": 0.05594405594405594, "term": "availability", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 19, "s": 0.24193548387096775, "os": -0.06737012987012986, "bg": 6.694403979114054e-07}, {"x": 0.039215686274509796, "y": 0.0979020979020979, "ox": 0.039215686274509796, "oy": 0.0979020979020979, "term": "consumer", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 6, "s": 0.7930107526881721, "os": 0.058712121212121215, "bg": 5.904382975520591e-07}, {"x": 0.039215686274509796, "y": 0.14685314685314685, "ox": 0.039215686274509796, "oy": 0.14685314685314685, "term": "engagement", "cat25k": 15, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.8803763440860215, "os": 0.10732323232323233, "bg": 4.637969545030648e-06}, {"x": 0.09150326797385622, "y": 0.04195804195804195, "ox": 0.09150326797385622, "oy": 0.04195804195804195, "term": "study", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.334005376344086, "os": -0.048791486291486295, "bg": 2.6133742511082083e-07}, {"x": 0.058823529411764705, "y": 0.08391608391608392, "ox": 0.058823529411764705, "oy": 0.08391608391608392, "term": "physical", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.7063172043010753, "os": 0.025342712842712847, "bg": 6.545060787797488e-07}, {"x": 0.07189542483660129, "y": 0.013986013986013983, "ox": 0.07189542483660129, "oy": 0.013986013986013983, "term": "conceptual", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28897849462365593, "os": -0.05708874458874459, "bg": 5.008473952660675e-06}, {"x": 0.0522875816993464, "y": 0.0909090909090909, "ox": 0.0522875816993464, "oy": 0.0909090909090909, "term": "manipulation", "cat25k": 9, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 8, "s": 0.7405913978494624, "os": 0.038780663780663784, "bg": 8.705460686761356e-06}, {"x": 0.09803921568627451, "y": 0.0979020979020979, "ox": 0.09803921568627451, "oy": 0.0979020979020979, "term": "preferably", "cat25k": 10, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 15, "s": 0.6673387096774194, "os": 0.00027056277056276834, "bg": 1.4546034562882884e-05}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "side", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 1.5576782997527867e-07}, {"x": 0.1764705882352941, "y": 0.23076923076923073, "ox": 0.1764705882352941, "oy": 0.23076923076923073, "term": "current", "cat25k": 23, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 27, "s": 0.7762096774193549, "os": 0.05429292929292928, "bg": 5.528230459360341e-07}, {"x": 0.11111111111111112, "y": 0.06293706293706293, "ox": 0.11111111111111112, "oy": 0.06293706293706293, "term": "workflows", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 17, "s": 0.34005376344086025, "os": -0.04743867243867243, "bg": 8.89583828734584e-05}, {"x": 0.013071895424836598, "y": 0.0769230769230769, "ox": 0.013071895424836598, "oy": 0.0769230769230769, "term": "artificial", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.8044354838709677, "os": 0.06385281385281386, "bg": 2.4335678107118543e-06}, {"x": 0.019607843137254898, "y": 0.06293706293706293, "ox": 0.019607843137254898, "oy": 0.06293706293706293, "term": "ip", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.7473118279569894, "os": 0.04347041847041847, "bg": 3.7357951060835056e-07}, {"x": 0.1633986928104575, "y": 0.034965034965034954, "ox": 0.1633986928104575, "oy": 0.034965034965034954, "term": "stack", "cat25k": 4, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 25, "s": 0.11223118279569894, "os": -0.12716450216450217, "bg": 5.37101974986624e-06}, {"x": 0.0326797385620915, "y": 0.22377622377622375, "ox": 0.0326797385620915, "oy": 0.22377622377622375, "term": "house", "cat25k": 22, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 5, "s": 0.9475806451612904, "os": 0.1902056277056277, "bg": 3.1980512838725977e-07}, {"x": 0.039215686274509796, "y": 0.13286713286713286, "ox": 0.039215686274509796, "oy": 0.13286713286713286, "term": "implemented", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.864247311827957, "os": 0.09343434343434344, "bg": 2.5981306138457706e-06}, {"x": 0.2418300653594771, "y": 0.2587412587412587, "ox": 0.2418300653594771, "oy": 0.2587412587412587, "term": "solution", "cat25k": 26, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 37, "s": 0.6901881720430109, "os": 0.017135642135642143, "bg": 1.9978100492194252e-06}, {"x": 0.2026143790849673, "y": 0.13286713286713286, "ox": 0.2026143790849673, "oy": 0.13286713286713286, "term": "collaboration", "cat25k": 13, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 31, "s": 0.23521505376344087, "os": -0.06890331890331891, "bg": 5.666403953291378e-06}, {"x": 0.0065359477124183, "y": 0.19580419580419578, "ox": 0.0065359477124183, "oy": 0.19580419580419578, "term": "thoroughly", "cat25k": 20, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 1, "s": 0.946236559139785, "os": 0.1884018759018759, "bg": 8.753860754449978e-06}, {"x": 0.09150326797385622, "y": 0.30769230769230765, "ox": 0.09150326797385622, "oy": 0.30769230769230765, "term": "domain", "cat25k": 31, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 14, "s": 0.9576612903225807, "os": 0.2150974025974026, "bg": 1.3155223253608108e-06}, {"x": 0.2418300653594771, "y": 0.2587412587412587, "ox": 0.2418300653594771, "oy": 0.2587412587412587, "term": "architectures", "cat25k": 26, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 37, "s": 0.6901881720430109, "os": 0.017135642135642143, "bg": 4.2841441220818974e-05}, {"x": 0.039215686274509796, "y": 0.21678321678321674, "ox": 0.039215686274509796, "oy": 0.21678321678321674, "term": "extend", "cat25k": 22, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 6, "s": 0.9422043010752689, "os": 0.17676767676767674, "bg": 4.875955366031709e-06}, {"x": 0.13071895424836602, "y": 0.2517482517482517, "ox": 0.13071895424836602, "oy": 0.2517482517482517, "term": "ownership", "cat25k": 25, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 20, "s": 0.8911290322580645, "os": 0.12058080808080807, "bg": 6.024717911411043e-06}, {"x": 0.11764705882352941, "y": 0.26573426573426573, "ox": 0.11764705882352941, "oy": 0.26573426573426573, "term": "text", "cat25k": 27, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 18, "s": 0.9206989247311829, "os": 0.14745670995670995, "bg": 5.362428594116296e-07}, {"x": 0.013071895424836598, "y": 0.21678321678321674, "ox": 0.013071895424836598, "oy": 0.21678321678321674, "term": "image", "cat25k": 22, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 2, "s": 0.9522849462365592, "os": 0.20274170274170272, "bg": 3.334094955125355e-07}, {"x": 0.0261437908496732, "y": 0.20979020979020974, "ox": 0.0261437908496732, "oy": 0.20979020979020974, "term": "anomaly", "cat25k": 21, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 4, "s": 0.9442204301075269, "os": 0.1828102453102453, "bg": 3.834236916371345e-05}, {"x": 0.15686274509803919, "y": 0.2937062937062937, "ox": 0.15686274509803919, "oy": 0.2937062937062937, "term": "next", "cat25k": 29, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 24, "s": 0.9052419354838711, "os": 0.13627344877344877, "bg": 3.0987089001659306e-07}, {"x": 0.08496732026143791, "y": 0.13986013986013984, "ox": 0.08496732026143791, "oy": 0.13986013986013984, "term": "generate", "cat25k": 14, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 13, "s": 0.7775537634408602, "os": 0.05492424242424243, "bg": 3.622794822630985e-06}, {"x": 0.1045751633986928, "y": 0.04195804195804195, "ox": 0.1045751633986928, "oy": 0.04195804195804195, "term": "own", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 16, "s": 0.26612903225806456, "os": -0.06177849927849927, "bg": 1.8957983159097905e-07}, {"x": 0.07189542483660129, "y": 0.04195804195804195, "ox": 0.07189542483660129, "oy": 0.04195804195804195, "term": "class", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 11, "s": 0.5026881720430108, "os": -0.02931096681096681, "bg": 1.778538090024388e-07}, {"x": 0.0261437908496732, "y": 0.2587412587412587, "ox": 0.0261437908496732, "oy": 0.2587412587412587, "term": "outcomes", "cat25k": 26, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 4, "s": 0.9623655913978495, "os": 0.23142135642135642, "bg": 5.7158877561362485e-06}, {"x": 0.09803921568627451, "y": 0.19580419580419578, "ox": 0.09803921568627451, "oy": 0.19580419580419578, "term": "challenging", "cat25k": 20, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 15, "s": 0.8689516129032258, "os": 0.09749278499278499, "bg": 9.528654047700222e-06}, {"x": 0.0784313725490196, "y": 0.05594405594405594, "ox": 0.0784313725490196, "oy": 0.05594405594405594, "term": "ongoing", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 12, "s": 0.5685483870967742, "os": -0.021915584415584416, "bg": 2.3441316539349068e-06}, {"x": 0.2745098039215686, "y": 0.04195804195804195, "ox": 0.2745098039215686, "oy": 0.04195804195804195, "term": "search", "cat25k": 4, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 42, "s": 0.03763440860215054, "os": -0.2306096681096681, "bg": 9.3734107308876e-08}, {"x": 0.09803921568627451, "y": 0.048951048951048945, "ox": 0.09803921568627451, "oy": 0.048951048951048945, "term": "cost", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 15, "s": 0.33736559139784944, "os": -0.04834054834054835, "bg": 2.767857315848225e-07}, {"x": 0.0065359477124183, "y": 0.0769230769230769, "ox": 0.0065359477124183, "oy": 0.0769230769230769, "term": "rigorous", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 1, "s": 0.8198924731182796, "os": 0.07034632034632034, "bg": 7.739189319918739e-06}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "algebra", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 2.3127249305676615e-06}, {"x": 0.09803921568627451, "y": 0.02797202797202797, "ox": 0.09803921568627451, "oy": 0.02797202797202797, "term": "comfort", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.23252688172043012, "os": -0.06917388167388168, "bg": 1.3570924993176146e-06}, {"x": 0.0457516339869281, "y": 0.06993006993006992, "ox": 0.0457516339869281, "oy": 0.06993006993006992, "term": "simple", "cat25k": 7, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 7, "s": 0.7016129032258064, "os": 0.024440836940836944, "bg": 3.9674022825608874e-07}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "connect", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 5.308094288652001e-07}, {"x": 0.09803921568627451, "y": 0.020979020979020976, "ox": 0.09803921568627451, "oy": 0.020979020979020976, "term": "front", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 15, "s": 0.20900537634408603, "os": -0.07611832611832613, "bg": 3.565527941036864e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "elastic", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 3.6818876669879883e-06}, {"x": 0.1372549019607843, "y": 0.0909090909090909, "ox": 0.1372549019607843, "oy": 0.0909090909090909, "term": "passionate", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 21, "s": 0.34543010752688175, "os": -0.045634920634920625, "bg": 1.7708605418677007e-05}, {"x": 0.08496732026143791, "y": 0.0909090909090909, "ox": 0.08496732026143791, "oy": 0.0909090909090909, "term": "paced", "cat25k": 9, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 13, "s": 0.6740591397849462, "os": 0.006313131313131312, "bg": 1.8335677596728635e-05}, {"x": 0.1372549019607843, "y": 0.06293706293706293, "ox": 0.1372549019607843, "oy": 0.06293706293706293, "term": "desire", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 21, "s": 0.2157258064516129, "os": -0.0734126984126984, "bg": 3.3587732416612157e-06}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "view", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 3.652302168325812e-08}, {"x": 0.07189542483660129, "y": 0.04195804195804195, "ox": 0.07189542483660129, "oy": 0.04195804195804195, "term": "short", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 11, "s": 0.5026881720430108, "os": -0.02931096681096681, "bg": 3.1824564132407676e-07}, {"x": 0.039215686274509796, "y": 0.0769230769230769, "ox": 0.039215686274509796, "oy": 0.0769230769230769, "term": "cleansing", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.7365591397849462, "os": 0.03787878787878787, "bg": 1.1468199357780835e-05}, {"x": 0.0065359477124183, "y": 0.0979020979020979, "ox": 0.0065359477124183, "oy": 0.0979020979020979, "term": "inference", "cat25k": 10, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.8595430107526882, "os": 0.09117965367965368, "bg": 1.0623515363727918e-05}, {"x": 0.0457516339869281, "y": 0.0769230769230769, "ox": 0.0457516339869281, "oy": 0.0769230769230769, "term": "communicating", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 7, "s": 0.7204301075268817, "os": 0.03138528138528138, "bg": 8.541802752216303e-06}, {"x": 0.22875816993464052, "y": 0.1748251748251748, "ox": 0.22875816993464052, "oy": 0.1748251748251748, "term": "features", "cat25k": 18, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 35, "s": 0.3024193548387097, "os": -0.05321067821067821, "bg": 7.451490702383848e-07}, {"x": 0.22875816993464052, "y": 0.020979020979020976, "ox": 0.22875816993464052, "oy": 0.020979020979020976, "term": "governance", "cat25k": 2, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 35, "s": 0.05443548387096774, "os": -0.205988455988456, "bg": 5.529485325691779e-06}, {"x": 0.09150326797385622, "y": 0.020979020979020976, "ox": 0.09150326797385622, "oy": 0.020979020979020976, "term": "flow", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 14, "s": 0.22849462365591397, "os": -0.06962481962481963, "bg": 7.270437745786236e-07}, {"x": 0.0522875816993464, "y": 0.12587412587412586, "ox": 0.0522875816993464, "oy": 0.12587412587412586, "term": "graph", "cat25k": 13, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 8, "s": 0.8245967741935484, "os": 0.07350288600288601, "bg": 3.331790031104823e-06}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "hortonworks", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 0.00019885163182620369}, {"x": 0.08496732026143791, "y": 0.006993006993006992, "ox": 0.08496732026143791, "oy": 0.006993006993006992, "term": "cloudera", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.2049731182795699, "os": -0.07702020202020202, "bg": 0.00034796440821196}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "rdbms", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 3.090813244478176e-05}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "logical", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 2.8956566417223944e-06}, {"x": 0.5163398692810457, "y": 0.034965034965034954, "ox": 0.5163398692810457, "oy": 0.034965034965034954, "term": "warehouse", "cat25k": 4, "ncat25k": 47, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 82, "s": 0.005376344086021506, "os": -0.47781385281385275, "bg": 1.1909043652462622e-05}, {"x": 0.1045751633986928, "y": 0.020979020979020976, "ox": 0.1045751633986928, "oy": 0.020979020979020976, "term": "schemas", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 16, "s": 0.19623655913978494, "os": -0.08261183261183261, "bg": 2.7245475279391595e-05}, {"x": 0.09803921568627451, "y": 0.06293706293706293, "ox": 0.09803921568627451, "oy": 0.06293706293706293, "term": "associated", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.4563172043010753, "os": -0.034451659451659455, "bg": 7.350800796238743e-07}, {"x": 0.0457516339869281, "y": 0.020979020979020976, "ox": 0.0457516339869281, "oy": 0.020979020979020976, "term": "js", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5470430107526881, "os": -0.024170274170274175, "bg": 3.3490235084705177e-06}, {"x": 0.1503267973856209, "y": 0.020979020979020976, "ox": 0.1503267973856209, "oy": 0.020979020979020976, "term": "pig", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 23, "s": 0.10954301075268819, "os": -0.12806637806637805, "bg": 7.314341271128424e-06}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "communicator", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 1.0866823903571426e-05}, {"x": 0.1045751633986928, "y": 0.3426573426573426, "ox": 0.1045751633986928, "oy": 0.3426573426573426, "term": "latest", "cat25k": 34, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 16, "s": 0.9630376344086021, "os": 0.23683261183261184, "bg": 9.846046311015152e-07}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "operation", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 3.732847945459155e-07}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "along", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 3.1128321561021157e-07}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "cluster", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 1.3556016204048408e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "according", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.2028388617727838e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "protocols", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.345842878804061e-06}, {"x": 0.0326797385620915, "y": 0.06993006993006992, "ox": 0.0326797385620915, "oy": 0.06993006993006992, "term": "completed", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7338709677419355, "os": 0.037427849927849935, "bg": 5.465367179672142e-07}, {"x": 0.039215686274509796, "y": 0.0909090909090909, "ox": 0.039215686274509796, "oy": 0.0909090909090909, "term": "studies", "cat25k": 9, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 6, "s": 0.7688172043010753, "os": 0.05176767676767677, "bg": 3.515090296518309e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "comparable", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.03765406415834e-06}, {"x": 0.1045751633986928, "y": 0.04195804195804195, "ox": 0.1045751633986928, "oy": 0.04195804195804195, "term": "proof", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 16, "s": 0.26612903225806456, "os": -0.06177849927849927, "bg": 1.66758036173987e-06}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "several", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 2.600537427064677e-07}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "identification", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 9.181845788061964e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "iot", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 7.940919558484873e-05}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "spoken", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 1.5427780534422177e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "welcome", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 1.4022072775749583e-07}, {"x": 0.039215686274509796, "y": 0.12587412587412586, "ox": 0.039215686274509796, "oy": 0.12587412587412586, "term": "practical", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 6, "s": 0.8481182795698925, "os": 0.08648989898989899, "bg": 1.6427446539613145e-06}, {"x": 0.2418300653594771, "y": 0.013986013986013983, "ox": 0.2418300653594771, "oy": 0.013986013986013983, "term": "scrum", "cat25k": 1, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 37, "s": 0.03965053763440861, "os": -0.2259199134199134, "bg": 0.00015403969089368693}, {"x": 0.0326797385620915, "y": 0.06993006993006992, "ox": 0.0326797385620915, "oy": 0.06993006993006992, "term": "measurement", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 5, "s": 0.7338709677419355, "os": 0.037427849927849935, "bg": 1.4603113656423005e-06}, {"x": 0.0065359477124183, "y": 0.06993006993006992, "ox": 0.0065359477124183, "oy": 0.06993006993006992, "term": "attribution", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 1, "s": 0.8030913978494625, "os": 0.0634018759018759, "bg": 7.1875224610076905e-06}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "play", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 1.4586073940601181e-07}, {"x": 0.058823529411764705, "y": 0.08391608391608392, "ox": 0.058823529411764705, "oy": 0.08391608391608392, "term": "teach", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.7063172043010753, "os": 0.025342712842712847, "bg": 2.1920372888507037e-06}, {"x": 0.0065359477124183, "y": 0.15384615384615385, "ox": 0.0065359477124183, "oy": 0.15384615384615385, "term": "extracted", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.9173387096774194, "os": 0.14673520923520922, "bg": 9.673624526255269e-06}, {"x": 0.07189542483660129, "y": 0.02797202797202797, "ox": 0.07189542483660129, "oy": 0.02797202797202797, "term": "retention", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3803763440860215, "os": -0.043199855699855697, "bg": 3.2235426927606e-06}, {"x": 0.08496732026143791, "y": 0.15384615384615385, "ox": 0.08496732026143791, "oy": 0.15384615384615385, "term": "proficient", "cat25k": 15, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 13, "s": 0.8158602150537635, "os": 0.0688131313131313, "bg": 3.0703599295220804e-05}, {"x": 0.196078431372549, "y": 0.06293706293706293, "ox": 0.196078431372549, "oy": 0.06293706293706293, "term": "providing", "cat25k": 6, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 30, "s": 0.10483870967741936, "os": -0.13185425685425686, "bg": 1.3439437578145808e-06}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "ux", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 8.089371374950454e-06}, {"x": 0.058823529411764705, "y": 0.034965034965034954, "ox": 0.058823529411764705, "oy": 0.034965034965034954, "term": "emphasis", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 9, "s": 0.5571236559139785, "os": -0.023268398268398265, "bg": 1.813311545963368e-06}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "wellness", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 2.7474170581207623e-06}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "designed", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 2.6303219618098534e-07}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "worked", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 6.804734200146548e-07}, {"x": 0.0, "y": 0.06993006993006992, "ox": 0.0, "oy": 0.06993006993006992, "term": "automotive", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8165322580645161, "os": 0.0698953823953824, "bg": 4.62864132899031e-07}, {"x": 0.09150326797385622, "y": 0.048951048951048945, "ox": 0.09150326797385622, "oy": 0.048951048951048945, "term": "reduce", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.38911290322580644, "os": -0.041847041847041855, "bg": 1.0523355798944074e-06}, {"x": 0.07189542483660129, "y": 0.020979020979020976, "ox": 0.07189542483660129, "oy": 0.020979020979020976, "term": "evolve", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.3245967741935484, "os": -0.050144300144300144, "bg": 9.154368480986213e-06}, {"x": 0.2875816993464052, "y": 0.1678321678321678, "ox": 0.2875816993464052, "oy": 0.1678321678321678, "term": "write", "cat25k": 17, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 44, "s": 0.12836021505376344, "os": -0.11859668109668109, "bg": 1.0731842789266543e-06}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "add", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 6.196551435811325e-08}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "capture", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 1.2288222264002175e-06}, {"x": 0.31372549019607837, "y": 0.11188811188811187, "ox": 0.31372549019607837, "oy": 0.11188811188811187, "term": "enterprise", "cat25k": 11, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 48, "s": 0.05779569892473118, "os": -0.20012626262626262, "bg": 2.2656670968248054e-06}, {"x": 0.2091503267973856, "y": 0.020979020979020976, "ox": 0.2091503267973856, "oy": 0.020979020979020976, "term": "server", "cat25k": 2, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 32, "s": 0.06384408602150538, "os": -0.1865079365079365, "bg": 4.5493234496378484e-07}, {"x": 0.1764705882352941, "y": 0.06293706293706293, "ox": 0.1764705882352941, "oy": 0.06293706293706293, "term": "principles", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 27, "s": 0.13978494623655915, "os": -0.11237373737373738, "bg": 2.3128198738298938e-06}, {"x": 0.14379084967320263, "y": 0.05594405594405594, "ox": 0.14379084967320263, "oy": 0.05594405594405594, "term": "involved", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 22, "s": 0.18481182795698925, "os": -0.08685064935064934, "bg": 1.0085968254717497e-06}, {"x": 0.12418300653594772, "y": 0.23076923076923073, "ox": 0.12418300653594772, "oy": 0.23076923076923073, "term": "programs", "cat25k": 23, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 19, "s": 0.8790322580645161, "os": 0.10624098124098125, "bg": 7.294215214613555e-07}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "file", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 8.266351261278843e-08}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "xml", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 3.14014399863006e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "files", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 1.4330646435318178e-07}, {"x": 0.08496732026143791, "y": 0.11188811188811187, "ox": 0.08496732026143791, "oy": 0.11188811188811187, "term": "develops", "cat25k": 11, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 13, "s": 0.7137096774193549, "os": 0.02714646464646464, "bg": 9.60363003969114e-06}, {"x": 0.07189542483660129, "y": 0.15384615384615385, "ox": 0.07189542483660129, "oy": 0.15384615384615385, "term": "assess", "cat25k": 15, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 11, "s": 0.842741935483871, "os": 0.08180014430014429, "bg": 5.19898893844208e-06}, {"x": 0.2875816993464052, "y": 0.13986013986013984, "ox": 0.2875816993464052, "oy": 0.13986013986013984, "term": "it", "cat25k": 14, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 44, "s": 0.08870967741935484, "os": -0.14637445887445885, "bg": 4.5499068749232225e-08}, {"x": 0.13071895424836602, "y": 0.048951048951048945, "ox": 0.13071895424836602, "oy": 0.048951048951048945, "term": "requests", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 20, "s": 0.198252688172043, "os": -0.0808080808080808, "bg": 1.4867710416648313e-06}, {"x": 0.019607843137254898, "y": 0.0909090909090909, "ox": 0.019607843137254898, "oy": 0.0909090909090909, "term": "output", "cat25k": 9, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 3, "s": 0.8205645161290324, "os": 0.07124819624819625, "bg": 5.724491050706361e-07}, {"x": 0.09803921568627451, "y": 0.06293706293706293, "ox": 0.09803921568627451, "oy": 0.06293706293706293, "term": "rapidly", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 15, "s": 0.4563172043010753, "os": -0.034451659451659455, "bg": 3.986839111855174e-06}, {"x": 0.07189542483660129, "y": 0.05594405594405594, "ox": 0.07189542483660129, "oy": 0.05594405594405594, "term": "comprehensive", "cat25k": 6, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 11, "s": 0.6303763440860215, "os": -0.01542207792207792, "bg": 1.0099702133205928e-06}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "thrive", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 1.2609923063706909e-05}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "startup", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 2.9902998411029418e-06}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "if", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 1.9382092341482316e-08}, {"x": 0.11111111111111112, "y": 0.020979020979020976, "ox": 0.11111111111111112, "oy": 0.020979020979020976, "term": "done", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 17, "s": 0.18010752688172044, "os": -0.0891053391053391, "bg": 3.887529797672928e-07}, {"x": 0.08496732026143791, "y": 0.06993006993006992, "ox": 0.08496732026143791, "oy": 0.06993006993006992, "term": "fun", "cat25k": 7, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.633736559139785, "os": -0.014520202020202017, "bg": 4.240799725587069e-07}, {"x": 0.16993464052287582, "y": 0.06293706293706293, "ox": 0.16993464052287582, "oy": 0.06293706293706293, "term": "activities", "cat25k": 6, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 26, "s": 0.14919354838709678, "os": -0.10588023088023088, "bg": 5.272446955210465e-07}, {"x": 0.14379084967320263, "y": 0.11188811188811187, "ox": 0.14379084967320263, "oy": 0.11188811188811187, "term": "analyzing", "cat25k": 11, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 22, "s": 0.4771505376344086, "os": -0.03129509379509379, "bg": 1.7155938223272345e-05}, {"x": 0.0457516339869281, "y": 0.020979020979020976, "ox": 0.0457516339869281, "oy": 0.020979020979020976, "term": "difference", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5470430107526881, "os": -0.024170274170274175, "bg": 4.4815608308356657e-07}, {"x": 0.0, "y": 0.1048951048951049, "ox": 0.0, "oy": 0.1048951048951049, "term": "algorithm", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.8763440860215055, "os": 0.10461760461760461, "bg": 1.8142506119920876e-06}, {"x": 0.013071895424836598, "y": 0.0769230769230769, "ox": 0.013071895424836598, "oy": 0.0769230769230769, "term": "bias", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.8044354838709677, "os": 0.06385281385281386, "bg": 2.925374484501197e-06}, {"x": 0.11764705882352941, "y": 0.06293706293706293, "ox": 0.11764705882352941, "oy": 0.06293706293706293, "term": "experienced", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 18, "s": 0.3004032258064516, "os": -0.05393217893217893, "bg": 2.2054593696184495e-06}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "xgboost", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 0.00019885163182620369}, {"x": 0.1503267973856209, "y": 0.048951048951048945, "ox": 0.1503267973856209, "oy": 0.048951048951048945, "term": "resources", "cat25k": 5, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 23, "s": 0.1646505376344086, "os": -0.10028860028860029, "bg": 2.7605357272306547e-07}, {"x": 0.0457516339869281, "y": 0.0979020979020979, "ox": 0.0457516339869281, "oy": 0.0979020979020979, "term": "excel", "cat25k": 10, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 7, "s": 0.7721774193548387, "os": 0.05221861471861472, "bg": 2.9548451122957227e-06}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "ensures", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 3.781990365525005e-06}, {"x": 0.11111111111111112, "y": 0.06293706293706293, "ox": 0.11111111111111112, "oy": 0.06293706293706293, "term": "free", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 17, "s": 0.34005376344086025, "os": -0.04743867243867243, "bg": 5.127255544297636e-08}, {"x": 0.09150326797385622, "y": 0.1818181818181818, "ox": 0.09150326797385622, "oy": 0.1818181818181818, "term": "four", "cat25k": 18, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 14, "s": 0.8555107526881721, "os": 0.09009740259740259, "bg": 6.1896698680774e-07}, {"x": 0.0, "y": 0.0769230769230769, "ox": 0.0, "oy": 0.0769230769230769, "term": "probability", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.82997311827957, "os": 0.07683982683982683, "bg": 1.6910728189023201e-06}, {"x": 0.1633986928104575, "y": 0.0909090909090909, "ox": 0.1633986928104575, "oy": 0.0909090909090909, "term": "tasks", "cat25k": 9, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 25, "s": 0.22043010752688172, "os": -0.07160894660894661, "bg": 3.733934562305489e-06}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "demonstrate", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 1.7464212470146274e-06}, {"x": 0.0457516339869281, "y": 0.1048951048951049, "ox": 0.0457516339869281, "oy": 0.1048951048951049, "term": "thought", "cat25k": 11, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 7, "s": 0.7950268817204301, "os": 0.05916305916305915, "bg": 5.389403181533729e-07}, {"x": 0.0065359477124183, "y": 0.06293706293706293, "ox": 0.0065359477124183, "oy": 0.06293706293706293, "term": "manufacturing", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7849462365591399, "os": 0.056457431457431456, "bg": 4.894532491509638e-07}, {"x": 0.5424836601307189, "y": 0.30769230769230765, "ox": 0.5424836601307189, "oy": 0.30769230769230765, "term": "web", "cat25k": 31, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 87, "s": 0.035618279569892476, "os": -0.23295454545454541, "bg": 4.228178596198229e-07}, {"x": 0.0065359477124183, "y": 0.22377622377622375, "ox": 0.0065359477124183, "oy": 0.22377622377622375, "term": "semantic", "cat25k": 22, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 1, "s": 0.9590053763440861, "os": 0.21617965367965367, "bg": 1.4125191572910707e-05}, {"x": 0.065359477124183, "y": 0.006993006993006992, "ox": 0.065359477124183, "oy": 0.006993006993006992, "term": "linked", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.2849462365591398, "os": -0.057539682539682536, "bg": 9.584350672222397e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "ontologies", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.4760438089802505e-05}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "task", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 4.5097880439706335e-07}, {"x": 0.08496732026143791, "y": 0.02797202797202797, "ox": 0.08496732026143791, "oy": 0.02797202797202797, "term": "elasticsearch", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 13, "s": 0.29368279569892475, "os": -0.05618686868686869, "bg": 0.00042251245790408964}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "assemble", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 5.221141445942912e-06}, {"x": 0.1045751633986928, "y": 0.11188811188811187, "ox": 0.1045751633986928, "oy": 0.11188811188811187, "term": "concept", "cat25k": 11, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 16, "s": 0.6760752688172043, "os": 0.007665945165945168, "bg": 2.01499667572775e-06}, {"x": 0.09150326797385622, "y": 0.06993006993006992, "ox": 0.09150326797385622, "oy": 0.06993006993006992, "term": "framework", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 14, "s": 0.5725806451612903, "os": -0.021013708513708512, "bg": 1.4768177003000924e-06}, {"x": 0.065359477124183, "y": 0.020979020979020976, "ox": 0.065359477124183, "oy": 0.020979020979020976, "term": "fluent", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.3763440860215054, "os": -0.04365079365079365, "bg": 1.613567871629505e-05}, {"x": 0.1045751633986928, "y": 0.06293706293706293, "ox": 0.1045751633986928, "oy": 0.06293706293706293, "term": "pandas", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 16, "s": 0.39314516129032256, "os": -0.04094516594516594, "bg": 0.00010370216737529814}, {"x": 0.12418300653594772, "y": 0.06293706293706293, "ox": 0.12418300653594772, "oy": 0.06293706293706293, "term": "commercial", "cat25k": 6, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 19, "s": 0.27083333333333337, "os": -0.06042568542568541, "bg": 6.178600342687242e-07}, {"x": 0.039215686274509796, "y": 0.06993006993006992, "ox": 0.039215686274509796, "oy": 0.06993006993006992, "term": "topics", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.7190860215053764, "os": 0.03093434343434344, "bg": 3.3608923706986987e-07}, {"x": 0.0326797385620915, "y": 0.23776223776223773, "ox": 0.0326797385620915, "oy": 0.23776223776223773, "term": "nlp", "cat25k": 24, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 5, "s": 0.9529569892473119, "os": 0.2040945165945166, "bg": 6.412786108918706e-05}, {"x": 0.2875816993464052, "y": 0.15384615384615385, "ox": 0.2875816993464052, "oy": 0.15384615384615385, "term": "vision", "cat25k": 15, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 44, "s": 0.10349462365591398, "os": -0.13248556998556998, "bg": 3.0340579905020192e-06}, {"x": 0.2026143790849673, "y": 0.0979020979020979, "ox": 0.2026143790849673, "oy": 0.0979020979020979, "term": "dental", "cat25k": 10, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 31, "s": 0.15255376344086022, "os": -0.10362554112554113, "bg": 3.859148622073833e-06}, {"x": 0.47058823529411764, "y": 0.0979020979020979, "ox": 0.47058823529411764, "oy": 0.0979020979020979, "term": "insurance", "cat25k": 10, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 73, "s": 0.015456989247311828, "os": -0.36985930735930733, "bg": 8.99913884447561e-07}, {"x": 0.09150326797385622, "y": 0.06293706293706293, "ox": 0.09150326797385622, "oy": 0.06293706293706293, "term": "space", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 14, "s": 0.508736559139785, "os": -0.02795815295815296, "bg": 3.7833382212418225e-07}, {"x": 0.1764705882352941, "y": 0.06293706293706293, "ox": 0.1764705882352941, "oy": 0.06293706293706293, "term": "vacation", "cat25k": 6, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 27, "s": 0.13978494623655915, "os": -0.11237373737373738, "bg": 1.3084509100657711e-06}, {"x": 0.013071895424836598, "y": 0.06993006993006992, "ox": 0.013071895424836598, "oy": 0.06993006993006992, "term": "credit", "cat25k": 7, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 2, "s": 0.7883064516129032, "os": 0.056908369408369415, "bg": 1.3636595923378287e-07}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "forecast", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 5.383971016468827e-07}, {"x": 0.1372549019607843, "y": 0.1048951048951049, "ox": 0.1372549019607843, "oy": 0.1048951048951049, "term": "changes", "cat25k": 11, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 21, "s": 0.46370967741935487, "os": -0.031746031746031744, "bg": 5.51087574773112e-07}, {"x": 0.07189542483660129, "y": 0.048951048951048945, "ox": 0.07189542483660129, "oy": 0.048951048951048945, "term": "run", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 11, "s": 0.564516129032258, "os": -0.022366522366522368, "bg": 3.0692097331427e-07}, {"x": 0.13071895424836602, "y": 0.02797202797202797, "ox": 0.13071895424836602, "oy": 0.02797202797202797, "term": "metadata", "cat25k": 3, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 20, "s": 0.16129032258064516, "os": -0.10164141414141413, "bg": 7.231574137420097e-06}, {"x": 0.392156862745098, "y": 0.04195804195804195, "ox": 0.392156862745098, "oy": 0.04195804195804195, "term": "engineer", "cat25k": 4, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 61, "s": 0.017473118279569894, "os": -0.347492784992785, "bg": 5.906179631340675e-06}, {"x": 0.1895424836601307, "y": 0.18881118881118877, "ox": 0.1895424836601307, "oy": 0.18881118881118877, "term": "program", "cat25k": 19, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 29, "s": 0.6653225806451613, "os": -0.00036075036075036704, "bg": 3.6509735404070347e-07}, {"x": 0.0, "y": 0.1608391608391608, "ox": 0.0, "oy": 0.1608391608391608, "term": "profitable", "cat25k": 16, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 0, "s": 0.9301075268817205, "os": 0.16017316017316016, "bg": 9.571608113394425e-06}, {"x": 0.21568627450980388, "y": 0.11888111888111888, "ox": 0.21568627450980388, "oy": 0.11888111888111888, "term": "plan", "cat25k": 12, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 33, "s": 0.16868279569892472, "os": -0.09577922077922077, "bg": 6.217871171529253e-07}, {"x": 0.1045751633986928, "y": 0.048951048951048945, "ox": 0.1045751633986928, "oy": 0.048951048951048945, "term": "collaborating", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 16, "s": 0.29838709677419356, "os": -0.05483405483405483, "bg": 3.010437711098764e-05}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "exceed", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.059311293302565e-06}, {"x": 0.058823529411764705, "y": 0.08391608391608392, "ox": 0.058823529411764705, "oy": 0.08391608391608392, "term": "utilizing", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.7063172043010753, "os": 0.025342712842712847, "bg": 8.901296071095077e-06}, {"x": 0.1633986928104575, "y": 0.14685314685314685, "ox": 0.1633986928104575, "oy": 0.14685314685314685, "term": "robust", "cat25k": 15, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 25, "s": 0.6276881720430108, "os": -0.016053391053391042, "bg": 1.270279424193383e-05}, {"x": 0.1895424836601307, "y": 0.1048951048951049, "ox": 0.1895424836601307, "oy": 0.1048951048951049, "term": "hand", "cat25k": 11, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 29, "s": 0.19153225806451613, "os": -0.08369408369408371, "bg": 7.250128502348616e-07}, {"x": 0.1045751633986928, "y": 0.2517482517482517, "ox": 0.1045751633986928, "oy": 0.2517482517482517, "term": "growing", "cat25k": 25, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 16, "s": 0.915994623655914, "os": 0.14655483405483405, "bg": 2.6797335283286094e-06}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "sqoop", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 0.00019885163182620369}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "center", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 8.152185432435939e-08}, {"x": 0.2026143790849673, "y": 0.11188811188811187, "ox": 0.2026143790849673, "oy": 0.11188811188811187, "term": "used", "cat25k": 11, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 31, "s": 0.17876344086021506, "os": -0.08973665223665225, "bg": 2.2300318676298633e-07}, {"x": 0.07189542483660129, "y": 0.006993006993006992, "ox": 0.07189542483660129, "oy": 0.006993006993006992, "term": "mentorship", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.2540322580645161, "os": -0.06403318903318903, "bg": 5.407171260884749e-05}, {"x": 0.2418300653594771, "y": 0.13986013986013984, "ox": 0.2418300653594771, "oy": 0.13986013986013984, "term": "improvements", "cat25k": 14, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 37, "s": 0.16263440860215053, "os": -0.10091991341991341, "bg": 6.061210892761601e-06}, {"x": 0.0784313725490196, "y": 0.034965034965034954, "ox": 0.0784313725490196, "oy": 0.034965034965034954, "term": "looking", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.38373655913978494, "os": -0.04274891774891775, "bg": 2.2562780555318742e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "coordinate", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 2.0970698691253643e-06}, {"x": 0.0065359477124183, "y": 0.06293706293706293, "ox": 0.0065359477124183, "oy": 0.06293706293706293, "term": "communicates", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7849462365591399, "os": 0.056457431457431456, "bg": 1.685057666886005e-05}, {"x": 0.0522875816993464, "y": 0.04195804195804195, "ox": 0.0522875816993464, "oy": 0.04195804195804195, "term": "discussions", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.6478494623655914, "os": -0.009830447330447328, "bg": 8.73194615557671e-07}, {"x": 0.08496732026143791, "y": 0.04195804195804195, "ox": 0.08496732026143791, "oy": 0.04195804195804195, "term": "operate", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 13, "s": 0.38642473118279574, "os": -0.0422979797979798, "bg": 2.0730527420077677e-06}, {"x": 0.1045751633986928, "y": 0.034965034965034954, "ox": 0.1045751633986928, "oy": 0.034965034965034954, "term": "manager", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.23655913978494625, "os": -0.06872294372294371, "bg": 4.798783631746646e-07}, {"x": 0.2875816993464052, "y": 0.06293706293706293, "ox": 0.2875816993464052, "oy": 0.06293706293706293, "term": "service", "cat25k": 6, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 44, "s": 0.04099462365591398, "os": -0.22276334776334775, "bg": 2.0399611884682249e-07}, {"x": 0.07189542483660129, "y": 0.02797202797202797, "ox": 0.07189542483660129, "oy": 0.02797202797202797, "term": "parental", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3803763440860215, "os": -0.043199855699855697, "bg": 4.783477486084864e-06}, {"x": 0.13071895424836602, "y": 0.04195804195804195, "ox": 0.13071895424836602, "oy": 0.04195804195804195, "term": "policies", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 20, "s": 0.1827956989247312, "os": -0.08775252525252525, "bg": 5.917676987675722e-07}, {"x": 0.08496732026143791, "y": 0.08391608391608392, "ox": 0.08496732026143791, "oy": 0.08391608391608392, "term": "right", "cat25k": 8, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 13, "s": 0.6612903225806452, "os": -0.0006313131313131354, "bg": 1.826812097034252e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "discounted", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.4804097901191316e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "feel", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 1.9564268098991372e-07}, {"x": 0.1503267973856209, "y": 0.02797202797202797, "ox": 0.1503267973856209, "oy": 0.02797202797202797, "term": "unix", "cat25k": 3, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 23, "s": 0.12298387096774195, "os": -0.12112193362193362, "bg": 2.2937772160383455e-06}, {"x": 0.15686274509803919, "y": 0.0769230769230769, "ox": 0.15686274509803919, "oy": 0.0769230769230769, "term": "familiar", "cat25k": 8, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 24, "s": 0.20094086021505375, "os": -0.07900432900432901, "bg": 4.249838794507727e-06}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "json", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 0.00011423023103064226}, {"x": 0.2483660130718954, "y": 0.14685314685314685, "ox": 0.2483660130718954, "oy": 0.14685314685314685, "term": "implementing", "cat25k": 15, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 38, "s": 0.1639784946236559, "os": -0.10046897546897543, "bg": 8.632905519214031e-06}, {"x": 0.0, "y": 0.06993006993006992, "ox": 0.0, "oy": 0.06993006993006992, "term": "envision", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8165322580645161, "os": 0.0698953823953824, "bg": 1.5320141159780644e-05}, {"x": 0.0065359477124183, "y": 0.12587412587412586, "ox": 0.0065359477124183, "oy": 0.12587412587412586, "term": "types", "cat25k": 13, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 1, "s": 0.8904569892473119, "os": 0.11895743145743146, "bg": 4.925392423836551e-07}, {"x": 0.065359477124183, "y": 0.04195804195804195, "ox": 0.065359477124183, "oy": 0.04195804195804195, "term": "function", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5598118279569892, "os": -0.022817460317460313, "bg": 3.4057278616753415e-07}, {"x": 0.058823529411764705, "y": 0.04195804195804195, "ox": 0.058823529411764705, "oy": 0.04195804195804195, "term": "input", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.6250000000000001, "os": -0.016323953823953817, "bg": 5.23814858406558e-07}, {"x": 0.0, "y": 0.06993006993006992, "ox": 0.0, "oy": 0.06993006993006992, "term": "attempt", "cat25k": 7, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 0, "s": 0.8165322580645161, "os": 0.0698953823953824, "bg": 7.895037061277725e-07}, {"x": 0.0, "y": 0.0769230769230769, "ox": 0.0, "oy": 0.0769230769230769, "term": "wireless", "cat25k": 8, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 0, "s": 0.82997311827957, "os": 0.07683982683982683, "bg": 2.5756908011509446e-07}, {"x": 0.1045751633986928, "y": 0.034965034965034954, "ox": 0.1045751633986928, "oy": 0.034965034965034954, "term": "orchestration", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.23655913978494625, "os": -0.06872294372294371, "bg": 8.363984681959482e-05}, {"x": 0.0, "y": 0.13286713286713286, "ox": 0.0, "oy": 0.13286713286713286, "term": "reinforcement", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 0, "s": 0.8991935483870968, "os": 0.1323953823953824, "bg": 1.717140815940128e-05}, {"x": 0.019607843137254898, "y": 0.08391608391608392, "ox": 0.019607843137254898, "oy": 0.08391608391608392, "term": "conference", "cat25k": 8, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 3, "s": 0.8071236559139785, "os": 0.06430375180375181, "bg": 2.93921238513558e-07}, {"x": 0.0065359477124183, "y": 0.15384615384615385, "ox": 0.0065359477124183, "oy": 0.15384615384615385, "term": "billion", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.9173387096774194, "os": 0.14673520923520922, "bg": 1.4525951876152996e-06}, {"x": 0.08496732026143791, "y": 0.020979020979020976, "ox": 0.08496732026143791, "oy": 0.020979020979020976, "term": "entire", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.2620967741935484, "os": -0.06313131313131314, "bg": 5.677333672885198e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "advising", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 3.854547679670706e-06}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "mission", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 5.303353024114896e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "operationalize", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 9.99692951450626e-05}, {"x": 0.0784313725490196, "y": 0.11888111888111888, "ox": 0.0784313725490196, "oy": 0.11888111888111888, "term": "success", "cat25k": 12, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 12, "s": 0.7432795698924731, "os": 0.040584415584415584, "bg": 9.819343381862447e-07}, {"x": 0.1045751633986928, "y": 0.0909090909090909, "ox": 0.1045751633986928, "oy": 0.0909090909090909, "term": "roadmap", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 16, "s": 0.6364247311827956, "os": -0.013167388167388161, "bg": 2.1571273906457286e-05}, {"x": 0.08496732026143791, "y": 0.020979020979020976, "ox": 0.08496732026143791, "oy": 0.020979020979020976, "term": "practice", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 13, "s": 0.2620967741935484, "os": -0.06313131313131314, "bg": 3.804167179832969e-07}, {"x": 0.0522875816993464, "y": 0.1748251748251748, "ox": 0.0522875816993464, "oy": 0.1748251748251748, "term": "answer", "cat25k": 18, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 8, "s": 0.8938172043010754, "os": 0.12211399711399712, "bg": 1.0196192509985124e-06}, {"x": 0.09803921568627451, "y": 0.02797202797202797, "ox": 0.09803921568627451, "oy": 0.02797202797202797, "term": "volume", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.23252688172043012, "os": -0.06917388167388168, "bg": 5.110003310206354e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "sub", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 2.936920280781326e-07}, {"x": 0.11111111111111112, "y": 0.06993006993006992, "ox": 0.11111111111111112, "oy": 0.06993006993006992, "term": "recommend", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 17, "s": 0.394489247311828, "os": -0.040494227994227985, "bg": 1.3098113211940667e-06}, {"x": 0.14379084967320263, "y": 0.020979020979020976, "ox": 0.14379084967320263, "oy": 0.020979020979020976, "term": "ensuring", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 22, "s": 0.12231182795698926, "os": -0.12157287157287157, "bg": 4.779274922819489e-06}, {"x": 0.14379084967320263, "y": 0.04195804195804195, "ox": 0.14379084967320263, "oy": 0.04195804195804195, "term": "reliable", "cat25k": 4, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 22, "s": 0.16330645161290322, "os": -0.10073953823953824, "bg": 2.740760382856818e-06}, {"x": 0.4379084967320261, "y": 0.04195804195804195, "ox": 0.4379084967320261, "oy": 0.04195804195804195, "term": "redshift", "cat25k": 4, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 68, "s": 0.014112903225806453, "os": -0.39294733044733043, "bg": 0.0002092493885110775}, {"x": 0.065359477124183, "y": 0.034965034965034954, "ox": 0.065359477124183, "oy": 0.034965034965034954, "term": "javascript", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.5006720430107527, "os": -0.02976190476190476, "bg": 1.1606899837677504e-06}, {"x": 0.019607843137254898, "y": 0.06993006993006992, "ox": 0.019607843137254898, "oy": 0.06993006993006992, "term": "do", "cat25k": 7, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 3, "s": 0.7600806451612904, "os": 0.05041486291486292, "bg": 2.7344467615620446e-08}, {"x": 0.14379084967320263, "y": 0.0769230769230769, "ox": 0.14379084967320263, "oy": 0.0769230769230769, "term": "approach", "cat25k": 8, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 22, "s": 0.24663978494623656, "os": -0.06601731601731602, "bg": 1.0274159085764227e-06}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "something", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 1.6677194145470997e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "strive", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 3.3075642577046164e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "grasp", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.5815710262842544e-06}, {"x": 0.07189542483660129, "y": 0.23776223776223773, "ox": 0.07189542483660129, "oy": 0.23776223776223773, "term": "throughout", "cat25k": 24, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 11, "s": 0.9361559139784946, "os": 0.16513347763347763, "bg": 1.7564020611183028e-06}, {"x": 0.065359477124183, "y": 0.04195804195804195, "ox": 0.065359477124183, "oy": 0.04195804195804195, "term": "call", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.5598118279569892, "os": -0.022817460317460313, "bg": 1.602489154416e-07}, {"x": 0.013071895424836598, "y": 0.1608391608391608, "ox": 0.013071895424836598, "oy": 0.1608391608391608, "term": "noise", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.9186827956989247, "os": 0.14718614718614717, "bg": 1.9800412632679097e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "attitude", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 1.4502710516310192e-06}, {"x": 0.09150326797385622, "y": 0.1048951048951049, "ox": 0.09150326797385622, "oy": 0.1048951048951049, "term": "produce", "cat25k": 11, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 14, "s": 0.6801075268817205, "os": 0.013708513708513698, "bg": 1.6354818809192107e-06}, {"x": 0.196078431372549, "y": 0.0909090909090909, "ox": 0.196078431372549, "oy": 0.0909090909090909, "term": "around", "cat25k": 9, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 30, "s": 0.15053763440860216, "os": -0.10407647907647909, "bg": 4.807409313394063e-07}, {"x": 0.11111111111111112, "y": 0.05594405594405594, "ox": 0.11111111111111112, "oy": 0.05594405594405594, "term": "taking", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 17, "s": 0.29905913978494625, "os": -0.05438311688311688, "bg": 7.13491638862611e-07}, {"x": 0.019607843137254898, "y": 0.11888111888111888, "ox": 0.019607843137254898, "oy": 0.11888111888111888, "term": "message", "cat25k": 12, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 3, "s": 0.8709677419354839, "os": 0.09902597402597402, "bg": 1.0719213221755042e-07}, {"x": 0.1764705882352941, "y": 0.11888111888111888, "ox": 0.1764705882352941, "oy": 0.11888111888111888, "term": "jobs", "cat25k": 12, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 27, "s": 0.2923387096774194, "os": -0.05681818181818182, "bg": 4.857688951279769e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "young", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.0262263505679375e-07}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "fwd", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 2.5116109682050982e-06}, {"x": 0.2026143790849673, "y": 0.08391608391608392, "ox": 0.2026143790849673, "oy": 0.08391608391608392, "term": "leads", "cat25k": 8, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 31, "s": 0.13172043010752688, "os": -0.11751443001443003, "bg": 2.8115787482177945e-06}, {"x": 0.0784313725490196, "y": 0.02797202797202797, "ox": 0.0784313725490196, "oy": 0.02797202797202797, "term": "huge", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.3279569892473118, "os": -0.04969336219336219, "bg": 4.524886557912432e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "windows", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 8.303056337245477e-08}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "contribution", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 8.759466367469249e-07}, {"x": 0.09150326797385622, "y": 0.04195804195804195, "ox": 0.09150326797385622, "oy": 0.04195804195804195, "term": "coverage", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.334005376344086, "os": -0.048791486291486295, "bg": 9.152892478775359e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "number", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 3.6470844922617277e-08}, {"x": 0.0065359477124183, "y": 0.14685314685314685, "ox": 0.0065359477124183, "oy": 0.14685314685314685, "term": "pioneer", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.907258064516129, "os": 0.1397907647907648, "bg": 3.0756833399460526e-06}, {"x": 0.0065359477124183, "y": 0.14685314685314685, "ox": 0.0065359477124183, "oy": 0.14685314685314685, "term": "enormous", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.907258064516129, "os": 0.1397907647907648, "bg": 6.336210260801294e-06}, {"x": 0.0, "y": 0.14685314685314685, "ox": 0.0, "oy": 0.14685314685314685, "term": "correlations", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133064516129032, "os": 0.1462842712842713, "bg": 1.8472857654316536e-05}, {"x": 0.0, "y": 0.1608391608391608, "ox": 0.0, "oy": 0.1608391608391608, "term": "signals", "cat25k": 16, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 0, "s": 0.9301075268817205, "os": 0.16017316017316016, "bg": 3.582332992804028e-06}, {"x": 0.0065359477124183, "y": 0.14685314685314685, "ox": 0.0065359477124183, "oy": 0.14685314685314685, "term": "amongst", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.907258064516129, "os": 0.1397907647907648, "bg": 5.686070986977864e-06}, {"x": 0.0261437908496732, "y": 0.3006993006993007, "ox": 0.0261437908496732, "oy": 0.3006993006993007, "term": "push", "cat25k": 30, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 4, "s": 0.9711021505376345, "os": 0.2730880230880231, "bg": 4.507172349667582e-06}, {"x": 0.0, "y": 0.14685314685314685, "ox": 0.0, "oy": 0.14685314685314685, "term": "zoominfo", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133064516129032, "os": 0.1462842712842713, "bg": 0.00024732506168404807}, {"x": 0.0, "y": 0.1748251748251748, "ox": 0.0, "oy": 0.1748251748251748, "term": "stages", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 0, "s": 0.9395161290322581, "os": 0.17406204906204906, "bg": 3.9328907258110495e-06}, {"x": 0.013071895424836598, "y": 0.1748251748251748, "ox": 0.013071895424836598, "oy": 0.1748251748251748, "term": "sourcing", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 2, "s": 0.9321236559139786, "os": 0.16107503607503607, "bg": 1.2937029011287559e-05}, {"x": 0.013071895424836598, "y": 0.1608391608391608, "ox": 0.013071895424836598, "oy": 0.1608391608391608, "term": "ground", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.9186827956989247, "os": 0.14718614718614717, "bg": 8.229144410259848e-07}, {"x": 0.013071895424836598, "y": 0.1608391608391608, "ox": 0.013071895424836598, "oy": 0.1608391608391608, "term": "truth", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.9186827956989247, "os": 0.14718614718614717, "bg": 1.4113766669628677e-06}, {"x": 0.039215686274509796, "y": 0.1608391608391608, "ox": 0.039215686274509796, "oy": 0.1608391608391608, "term": "normalization", "cat25k": 16, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 6, "s": 0.8931451612903226, "os": 0.1212121212121212, "bg": 4.4826933258106524e-05}, {"x": 0.07189542483660129, "y": 0.1748251748251748, "ox": 0.07189542483660129, "oy": 0.1748251748251748, "term": "release", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 11, "s": 0.8756720430107527, "os": 0.10263347763347763, "bg": 5.740650059731463e-07}, {"x": 0.0, "y": 0.14685314685314685, "ox": 0.0, "oy": 0.14685314685314685, "term": "elevate", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133064516129032, "os": 0.1462842712842713, "bg": 4.757961372150746e-05}, {"x": 0.0326797385620915, "y": 0.15384615384615385, "ox": 0.0326797385620915, "oy": 0.15384615384615385, "term": "backgrounds", "cat25k": 15, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 5, "s": 0.8918010752688172, "os": 0.12076118326118325, "bg": 7.336372402958135e-06}, {"x": 0.0, "y": 0.14685314685314685, "ox": 0.0, "oy": 0.14685314685314685, "term": "impressive", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.9133064516129032, "os": 0.1462842712842713, "bg": 4.643843722717419e-06}, {"x": 0.013071895424836598, "y": 0.15384615384615385, "ox": 0.013071895424836598, "oy": 0.15384615384615385, "term": "fortune", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 2, "s": 0.9092741935483871, "os": 0.14024170274170272, "bg": 4.4022246642340704e-06}, {"x": 0.0457516339869281, "y": 0.020979020979020976, "ox": 0.0457516339869281, "oy": 0.020979020979020976, "term": "engines", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5470430107526881, "os": -0.024170274170274175, "bg": 7.921716645503141e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "gives", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 2.6241205751914853e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "city", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 3.5838137930630553e-08}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "water", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 6.503794645954534e-08}, {"x": 0.058823529411764705, "y": 0.02797202797202797, "ox": 0.058823529411764705, "oy": 0.02797202797202797, "term": "efficiently", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.4952956989247312, "os": -0.030212842712842705, "bg": 4.052851679969374e-06}, {"x": 0.0, "y": 0.1048951048951049, "ox": 0.0, "oy": 0.1048951048951049, "term": "verizon", "cat25k": 11, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.8763440860215055, "os": 0.10461760461760461, "bg": 3.2021501370787104e-06}, {"x": 0.0065359477124183, "y": 0.05594405594405594, "ox": 0.0065359477124183, "oy": 0.05594405594405594, "term": "represent", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 1, "s": 0.756720430107527, "os": 0.04951298701298701, "bg": 6.891417222248895e-07}, {"x": 0.039215686274509796, "y": 0.0769230769230769, "ox": 0.039215686274509796, "oy": 0.0769230769230769, "term": "relationship", "cat25k": 8, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 6, "s": 0.7365591397849462, "os": 0.03787878787878787, "bg": 7.077402028096162e-07}, {"x": 0.013071895424836598, "y": 0.0769230769230769, "ox": 0.013071895424836598, "oy": 0.0769230769230769, "term": "partnership", "cat25k": 8, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 2, "s": 0.8044354838709677, "os": 0.06385281385281386, "bg": 8.302555600059381e-07}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "systematically", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 8.550000641250048e-06}, {"x": 0.013071895424836598, "y": 0.06293706293706293, "ox": 0.013071895424836598, "oy": 0.06293706293706293, "term": "practicing", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7573924731182795, "os": 0.04996392496392497, "bg": 4.946461301584846e-06}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "chatbot", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 0.00010582135004526802}, {"x": 0.0522875816993464, "y": 0.0769230769230769, "ox": 0.0522875816993464, "oy": 0.0769230769230769, "term": "shell", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 8, "s": 0.7036290322580645, "os": 0.02489177489177489, "bg": 1.8375427488604697e-06}, {"x": 0.019607843137254898, "y": 0.06293706293706293, "ox": 0.019607843137254898, "oy": 0.06293706293706293, "term": "script", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.7473118279569894, "os": 0.04347041847041847, "bg": 7.35510669654883e-07}, {"x": 0.0784313725490196, "y": 0.02797202797202797, "ox": 0.0784313725490196, "oy": 0.02797202797202797, "term": "cutting", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.3279569892473118, "os": -0.04969336219336219, "bg": 1.6112112915298523e-06}, {"x": 0.1045751633986928, "y": 0.034965034965034954, "ox": 0.1045751633986928, "oy": 0.034965034965034954, "term": "edge", "cat25k": 4, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.23655913978494625, "os": -0.06872294372294371, "bg": 1.0011944488153997e-06}, {"x": 0.2352941176470588, "y": 0.0769230769230769, "ox": 0.2352941176470588, "oy": 0.0769230769230769, "term": "have", "cat25k": 8, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 36, "s": 0.08131720430107527, "os": -0.15692640692640691, "bg": 6.009141882706254e-08}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "commitment", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 7.820027115509577e-07}, {"x": 0.2091503267973856, "y": 0.034965034965034954, "ox": 0.2091503267973856, "oy": 0.034965034965034954, "term": "maintaining", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 32, "s": 0.07123655913978495, "os": -0.17261904761904762, "bg": 5.504613498398343e-06}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "tier", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 2.78947147465549e-06}, {"x": 0.1895424836601307, "y": 0.05594405594405594, "ox": 0.1895424836601307, "oy": 0.05594405594405594, "term": "architect", "cat25k": 6, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 29, "s": 0.10416666666666669, "os": -0.13230519480519481, "bg": 8.190565685846425e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "landscape", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 9.301405054745229e-07}, {"x": 0.058823529411764705, "y": 0.020979020979020976, "ox": 0.058823529411764705, "oy": 0.020979020979020976, "term": "store", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4368279569892473, "os": -0.03715728715728715, "bg": 8.23640186524851e-08}, {"x": 0.1045751633986928, "y": 0.05594405594405594, "ox": 0.1045751633986928, "oy": 0.05594405594405594, "term": "corporate", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 16, "s": 0.33870967741935487, "os": -0.047889610389610385, "bg": 5.661836015700884e-07}, {"x": 0.019607843137254898, "y": 0.06293706293706293, "ox": 0.019607843137254898, "oy": 0.06293706293706293, "term": "behaviors", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 3, "s": 0.7473118279569894, "os": 0.04347041847041847, "bg": 4.707663880645028e-06}, {"x": 0.07189542483660129, "y": 0.020979020979020976, "ox": 0.07189542483660129, "oy": 0.020979020979020976, "term": "context", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.3245967741935484, "os": -0.050144300144300144, "bg": 6.888259176262114e-07}, {"x": 0.058823529411764705, "y": 0.08391608391608392, "ox": 0.058823529411764705, "oy": 0.08391608391608392, "term": "finance", "cat25k": 8, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 9, "s": 0.7063172043010753, "os": 0.025342712842712847, "bg": 4.3462675945837597e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "assurance", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.1657559198542806e-06}, {"x": 0.0065359477124183, "y": 0.06293706293706293, "ox": 0.0065359477124183, "oy": 0.06293706293706293, "term": "economic", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 1, "s": 0.7849462365591399, "os": 0.056457431457431456, "bg": 2.1476563109900837e-07}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "cox", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 1.6695246842354923e-06}, {"x": 0.0457516339869281, "y": 0.08391608391608392, "ox": 0.0457516339869281, "oy": 0.08391608391608392, "term": "schedule", "cat25k": 8, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 7, "s": 0.7399193548387096, "os": 0.038329725829725825, "bg": 5.883720223033872e-07}, {"x": 0.07189542483660129, "y": 0.02797202797202797, "ox": 0.07189542483660129, "oy": 0.02797202797202797, "term": "event", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 11, "s": 0.3803763440860215, "os": -0.043199855699855697, "bg": 2.388616256210432e-07}, {"x": 0.0522875816993464, "y": 0.04195804195804195, "ox": 0.0522875816993464, "oy": 0.04195804195804195, "term": "department", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.6478494623655914, "os": -0.009830447330447328, "bg": 1.418628054637426e-07}, {"x": 0.29411764705882354, "y": 0.06293706293706293, "ox": 0.29411764705882354, "oy": 0.06293706293706293, "term": "enable", "cat25k": 6, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 45, "s": 0.038978494623655914, "os": -0.22925685425685424, "bg": 2.7126466979868522e-06}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "roadmaps", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 7.486405368433231e-05}, {"x": 0.11764705882352941, "y": 0.02797202797202797, "ox": 0.11764705882352941, "oy": 0.02797202797202797, "term": "scaling", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 18, "s": 0.18077956989247312, "os": -0.08865440115440115, "bg": 1.1397221668190615e-05}, {"x": 0.09803921568627451, "y": 0.0769230769230769, "ox": 0.09803921568627451, "oy": 0.0769230769230769, "term": "overall", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 15, "s": 0.5745967741935484, "os": -0.020562770562770574, "bg": 6.576191508206903e-07}, {"x": 0.058823529411764705, "y": 0.020979020979020976, "ox": 0.058823529411764705, "oy": 0.020979020979020976, "term": "informed", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4368279569892473, "os": -0.03715728715728715, "bg": 6.268299516902156e-07}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "addressing", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 1.9029823645338218e-06}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "consistent", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 1.2345097138058176e-06}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "completion", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 1.2640337370604421e-06}, {"x": 0.09150326797385622, "y": 0.04195804195804195, "ox": 0.09150326797385622, "oy": 0.04195804195804195, "term": "tables", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.334005376344086, "os": -0.048791486291486295, "bg": 1.1058883995486315e-06}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "views", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 3.100222400912726e-07}, {"x": 0.09803921568627451, "y": 0.0909090909090909, "ox": 0.09803921568627451, "oy": 0.0909090909090909, "term": "custom", "cat25k": 9, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 15, "s": 0.6565860215053764, "os": -0.006673881673881679, "bg": 8.398925189543494e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "pl", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.022744822098652e-06}, {"x": 0.09803921568627451, "y": 0.02797202797202797, "ox": 0.09803921568627451, "oy": 0.02797202797202797, "term": "scripts", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.23252688172043012, "os": -0.06917388167388168, "bg": 2.1817900795268224e-06}, {"x": 0.2679738562091503, "y": 0.048951048951048945, "ox": 0.2679738562091503, "oy": 0.048951048951048945, "term": "maintenance", "cat25k": 5, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 41, "s": 0.047715053763440866, "os": -0.21717171717171715, "bg": 1.8430658955765328e-06}, {"x": 0.0784313725490196, "y": 0.04195804195804195, "ox": 0.0784313725490196, "oy": 0.04195804195804195, "term": "resolve", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.4516129032258065, "os": -0.035804473304473304, "bg": 3.4213302645268022e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "acceptance", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 3.7893777246302516e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "fixes", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.5801998847507553e-06}, {"x": 0.1503267973856209, "y": 0.020979020979020976, "ox": 0.1503267973856209, "oy": 0.020979020979020976, "term": "tuning", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 23, "s": 0.10954301075268819, "os": -0.12806637806637805, "bg": 6.877289839941658e-06}, {"x": 0.12418300653594772, "y": 0.013986013986013983, "ox": 0.12418300653594772, "oy": 0.013986013986013983, "term": "developer", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 19, "s": 0.14583333333333334, "os": -0.10903679653679653, "bg": 1.1676168148497505e-06}, {"x": 0.0457516339869281, "y": 0.020979020979020976, "ox": 0.0457516339869281, "oy": 0.020979020979020976, "term": "objects", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5470430107526881, "os": -0.024170274170274175, "bg": 5.895248102842839e-07}, {"x": 0.08496732026143791, "y": 0.006993006993006992, "ox": 0.08496732026143791, "oy": 0.006993006993006992, "term": "informatica", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.2049731182795699, "os": -0.07702020202020202, "bg": 4.089077635520065e-05}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "talend", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 0.00022370530554416317}, {"x": 0.0065359477124183, "y": 0.0979020979020979, "ox": 0.0065359477124183, "oy": 0.0979020979020979, "term": "elements", "cat25k": 10, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 1, "s": 0.8595430107526882, "os": 0.09117965367965368, "bg": 6.778029081992327e-07}, {"x": 0.0, "y": 0.0979020979020979, "ox": 0.0, "oy": 0.0979020979020979, "term": "conjunction", "cat25k": 10, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.8696236559139785, "os": 0.09767316017316018, "bg": 3.206235119633793e-06}, {"x": 0.0326797385620915, "y": 0.1048951048951049, "ox": 0.0326797385620915, "oy": 0.1048951048951049, "term": "delivered", "cat25k": 11, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 5, "s": 0.823252688172043, "os": 0.07215007215007214, "bg": 1.3693523203161561e-06}, {"x": 0.0, "y": 0.0909090909090909, "ox": 0.0, "oy": 0.0909090909090909, "term": "replicable", "cat25k": 9, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.8568548387096775, "os": 0.09072871572871573, "bg": 0.00014907316625671545}, {"x": 0.0326797385620915, "y": 0.0979020979020979, "ox": 0.0326797385620915, "oy": 0.0979020979020979, "term": "abreast", "cat25k": 10, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 5, "s": 0.8098118279569892, "os": 0.06520562770562771, "bg": 4.546188678793824e-05}, {"x": 0.0065359477124183, "y": 0.0909090909090909, "ox": 0.0065359477124183, "oy": 0.0909090909090909, "term": "promising", "cat25k": 9, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 1, "s": 0.844758064516129, "os": 0.08423520923520923, "bg": 5.269780780883644e-06}, {"x": 0.13071895424836602, "y": 0.006993006993006992, "ox": 0.13071895424836602, "oy": 0.006993006993006992, "term": "ssis", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.11962365591397851, "os": -0.12247474747474746, "bg": 0.0002028613104840657}, {"x": 0.11111111111111112, "y": 0.006993006993006992, "ox": 0.11111111111111112, "oy": 0.006993006993006992, "term": "logic", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15658602150537634, "os": -0.102994227994228, "bg": 1.49561510577904e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "sap", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 2.3150300625945547e-06}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "adherence", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 1.2181658989683657e-05}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "target", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 5.355742473943309e-07}, {"x": 0.058823529411764705, "y": 0.048951048951048945, "ox": 0.058823529411764705, "oy": 0.048951048951048945, "term": "owners", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.6505376344086021, "os": -0.009379509379509376, "bg": 5.275202448669849e-07}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "selected", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 5.446926535106029e-07}, {"x": 0.09150326797385622, "y": 0.020979020979020976, "ox": 0.09150326797385622, "oy": 0.020979020979020976, "term": "running", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 14, "s": 0.22849462365591397, "os": -0.06962481962481963, "bg": 4.660094298104615e-07}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "enhancements", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 3.7573214931783482e-06}, {"x": 0.07189542483660129, "y": 0.013986013986013983, "ox": 0.07189542483660129, "oy": 0.013986013986013983, "term": "consistency", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28897849462365593, "os": -0.05708874458874459, "bg": 4.074364038785438e-06}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "factory", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 7.465811900884925e-07}, {"x": 0.11111111111111112, "y": 0.0979020979020979, "ox": 0.11111111111111112, "oy": 0.0979020979020979, "term": "manner", "cat25k": 10, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 17, "s": 0.6370967741935484, "os": -0.012716450216450209, "bg": 2.186999073594247e-06}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "managed", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 5.866523679929508e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "regarding", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 3.565049820184829e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "exchange", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 2.3328176510314744e-07}, {"x": 0.0784313725490196, "y": 0.013986013986013983, "ox": 0.0784313725490196, "oy": 0.013986013986013983, "term": "excited", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.25806451612903225, "os": -0.06358225108225109, "bg": 2.8890944731829093e-06}, {"x": 0.065359477124183, "y": 0.006993006993006992, "ox": 0.065359477124183, "oy": 0.006993006993006992, "term": "chance", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.2849462365591398, "os": -0.057539682539682536, "bg": 5.052580946767776e-07}, {"x": 0.0457516339869281, "y": 0.020979020979020976, "ox": 0.0457516339869281, "oy": 0.020979020979020976, "term": "commuter", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5470430107526881, "os": -0.024170274170274175, "bg": 1.0837717641690966e-05}, {"x": 0.196078431372549, "y": 0.0909090909090909, "ox": 0.196078431372549, "oy": 0.0909090909090909, "term": "employee", "cat25k": 9, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 30, "s": 0.15053763440860216, "os": -0.10407647907647909, "bg": 1.877716179225739e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "drinks", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 1.6045938452059013e-06}, {"x": 0.0784313725490196, "y": 0.020979020979020976, "ox": 0.0784313725490196, "oy": 0.020979020979020976, "term": "meeting", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 12, "s": 0.29301075268817206, "os": -0.05663780663780664, "bg": 2.481496822339923e-07}, {"x": 0.09150326797385622, "y": 0.05594405594405594, "ox": 0.09150326797385622, "oy": 0.05594405594405594, "term": "direction", "cat25k": 6, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 14, "s": 0.45497311827956993, "os": -0.03490259740259741, "bg": 1.2960599453645929e-06}, {"x": 0.09150326797385622, "y": 0.020979020979020976, "ox": 0.09150326797385622, "oy": 0.020979020979020976, "term": "workflow", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 14, "s": 0.22849462365591397, "os": -0.06962481962481963, "bg": 6.3749549768804765e-06}, {"x": 0.058823529411764705, "y": 0.048951048951048945, "ox": 0.058823529411764705, "oy": 0.048951048951048945, "term": "assistance", "cat25k": 5, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.6505376344086021, "os": -0.009379509379509376, "bg": 5.82755984535186e-07}, {"x": 0.0457516339869281, "y": 0.034965034965034954, "ox": 0.0457516339869281, "oy": 0.034965034965034954, "term": "classes", "cat25k": 4, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.6397849462365591, "os": -0.010281385281385287, "bg": 4.906225673297703e-07}, {"x": 0.058823529411764705, "y": 0.04195804195804195, "ox": 0.058823529411764705, "oy": 0.04195804195804195, "term": "upon", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.6250000000000001, "os": -0.016323953823953817, "bg": 2.884568538112876e-07}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "updates", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 3.6780380364303536e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "evangelize", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 7.171580052762338e-05}, {"x": 0.065359477124183, "y": 0.020979020979020976, "ox": 0.065359477124183, "oy": 0.020979020979020976, "term": "rapid", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.3763440860215054, "os": -0.04365079365079365, "bg": 1.357621601514145e-06}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "easy", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 1.751188133388828e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "construct", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.4031749639910223e-06}, {"x": 0.09150326797385622, "y": 0.048951048951048945, "ox": 0.09150326797385622, "oy": 0.048951048951048945, "term": "disparate", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.38911290322580644, "os": -0.041847041847041855, "bg": 3.094900708953327e-05}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "break", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 3.896855381890463e-07}, {"x": 0.08496732026143791, "y": 0.034965034965034954, "ox": 0.08496732026143791, "oy": 0.034965034965034954, "term": "compliance", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 13, "s": 0.3326612903225806, "os": -0.04924242424242425, "bg": 9.629251354594935e-07}, {"x": 0.058823529411764705, "y": 0.020979020979020976, "ox": 0.058823529411764705, "oy": 0.020979020979020976, "term": "central", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4368279569892473, "os": -0.03715728715728715, "bg": 2.106696931474784e-07}, {"x": 0.0784313725490196, "y": 0.013986013986013983, "ox": 0.0784313725490196, "oy": 0.013986013986013983, "term": "single", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.25806451612903225, "os": -0.06358225108225109, "bg": 2.0766172225836482e-07}, {"x": 0.013071895424836598, "y": 0.06293706293706293, "ox": 0.013071895424836598, "oy": 0.06293706293706293, "term": "variables", "cat25k": 6, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 2, "s": 0.7573924731182795, "os": 0.04996392496392497, "bg": 9.128149678419435e-07}, {"x": 0.12418300653594772, "y": 0.04195804195804195, "ox": 0.12418300653594772, "oy": 0.04195804195804195, "term": "also", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 19, "s": 0.19758064516129034, "os": -0.08125901875901875, "bg": 8.104907050972656e-08}, {"x": 0.1503267973856209, "y": 0.048951048951048945, "ox": 0.1503267973856209, "oy": 0.048951048951048945, "term": "bs", "cat25k": 5, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 23, "s": 0.1646505376344086, "os": -0.10028860028860029, "bg": 4.543240060110094e-06}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "possess", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 3.991428588534693e-06}, {"x": 0.1633986928104575, "y": 0.02797202797202797, "ox": 0.1633986928104575, "oy": 0.02797202797202797, "term": "attention", "cat25k": 3, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 25, "s": 0.09946236559139786, "os": -0.13410894660894662, "bg": 1.2936903487465758e-06}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "aggregation", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 6.600238928649218e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "bases", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.3723974055741981e-06}, {"x": 0.0522875816993464, "y": 0.034965034965034954, "ox": 0.0522875816993464, "oy": 0.034965034965034954, "term": "base", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.6216397849462366, "os": -0.016774891774891776, "bg": 3.171693001050672e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "tackling", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 1.0262977392371301e-05}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "particular", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.9407413751269956e-07}, {"x": 0.09803921568627451, "y": 0.013986013986013983, "ox": 0.09803921568627451, "oy": 0.013986013986013983, "term": "defining", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1935483870967742, "os": -0.08306277056277057, "bg": 4.143044711372963e-06}, {"x": 0.0261437908496732, "y": 0.06293706293706293, "ox": 0.0261437908496732, "oy": 0.06293706293706293, "term": "proper", "cat25k": 6, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 4, "s": 0.7318548387096774, "os": 0.03697691197691198, "bg": 9.226385660294629e-07}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "advances", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 2.4048352053882303e-06}, {"x": 0.11764705882352941, "y": 0.006993006993006992, "ox": 0.11764705882352941, "oy": 0.006993006993006992, "term": "administration", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.14448924731182797, "os": -0.1094877344877345, "bg": 4.897181862570437e-07}, {"x": 0.11764705882352941, "y": 0.08391608391608392, "ox": 0.11764705882352941, "oy": 0.08391608391608392, "term": "microsoft", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 18, "s": 0.4610215053763441, "os": -0.0330988455988456, "bg": 5.868540927360922e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "read", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 4.962590934981536e-08}, {"x": 0.09150326797385622, "y": 0.020979020979020976, "ox": 0.09150326797385622, "oy": 0.020979020979020976, "term": "operating", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 14, "s": 0.22849462365591397, "os": -0.06962481962481963, "bg": 5.301946475540437e-07}, {"x": 0.13071895424836602, "y": 0.006993006993006992, "ox": 0.13071895424836602, "oy": 0.006993006993006992, "term": "manual", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.11962365591397851, "os": -0.12247474747474746, "bg": 8.437323174046247e-07}, {"x": 0.3006535947712418, "y": 0.05594405594405594, "ox": 0.3006535947712418, "oy": 0.05594405594405594, "term": "streaming", "cat25k": 6, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 46, "s": 0.03225806451612903, "os": -0.24269480519480519, "bg": 1.137598668251159e-05}, {"x": 0.0, "y": 0.05594405594405594, "ox": 0.0, "oy": 0.05594405594405594, "term": "fusion", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 0, "s": 0.780241935483871, "os": 0.056006493506493504, "bg": 1.4658484789073566e-06}, {"x": 0.1633986928104575, "y": 0.06993006993006992, "ox": 0.1633986928104575, "oy": 0.06993006993006992, "term": "tool", "cat25k": 7, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 25, "s": 0.1754032258064516, "os": -0.09244227994227994, "bg": 1.0754303795562528e-06}, {"x": 0.0, "y": 0.0909090909090909, "ox": 0.0, "oy": 0.0909090909090909, "term": "llnl", "cat25k": 9, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 0, "s": 0.8568548387096775, "os": 0.09072871572871573, "bg": 6.361836809098406e-05}, {"x": 0.0, "y": 0.06293706293706293, "ox": 0.0, "oy": 0.06293706293706293, "term": "sponsor", "cat25k": 6, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 0, "s": 0.7983870967741936, "os": 0.06295093795093795, "bg": 5.2834582699991e-07}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "vendors", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 1.4915065144597828e-06}, {"x": 0.039215686274509796, "y": 0.06993006993006992, "ox": 0.039215686274509796, "oy": 0.06993006993006992, "term": "s", "cat25k": 7, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 6, "s": 0.7190860215053764, "os": 0.03093434343434344, "bg": 1.0021323042344838e-08}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "competing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.7939346173619683e-06}, {"x": 0.065359477124183, "y": 0.020979020979020976, "ox": 0.065359477124183, "oy": 0.020979020979020976, "term": "person", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.3763440860215054, "os": -0.04365079365079365, "bg": 1.7707563203315888e-07}, {"x": 0.058823529411764705, "y": 0.020979020979020976, "ox": 0.058823529411764705, "oy": 0.020979020979020976, "term": "flexibility", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4368279569892473, "os": -0.03715728715728715, "bg": 1.8778296545607166e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "http", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 9.529684866254617e-07}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "com", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 2.763741731541128e-07}, {"x": 0.12418300653594772, "y": 0.006993006993006992, "ox": 0.12418300653594772, "oy": 0.006993006993006992, "term": "disability", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.1364247311827957, "os": -0.11598124098124096, "bg": 1.8279137974132276e-06}, {"x": 0.07189542483660129, "y": 0.020979020979020976, "ox": 0.07189542483660129, "oy": 0.020979020979020976, "term": "compute", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.3245967741935484, "os": -0.050144300144300144, "bg": 5.4037331690598445e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "applicants", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.1210124087265692e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "receive", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.5835420394612118e-07}, {"x": 0.1503267973856209, "y": 0.020979020979020976, "ox": 0.1503267973856209, "oy": 0.020979020979020976, "term": "stored", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 23, "s": 0.10954301075268819, "os": -0.12806637806637805, "bg": 2.8150208647181095e-06}, {"x": 0.4509803921568627, "y": 0.048951048951048945, "ox": 0.4509803921568627, "oy": 0.048951048951048945, "term": "amazon", "cat25k": 5, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 70, "s": 0.012096774193548388, "os": -0.398989898989899, "bg": 2.6218046606731875e-06}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "speed", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 2.403377812926911e-07}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "repeatable", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 3.8697711444986364e-05}, {"x": 0.13071895424836602, "y": 0.05594405594405594, "ox": 0.13071895424836602, "oy": 0.05594405594405594, "term": "feedback", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 20, "s": 0.21303763440860216, "os": -0.07386363636363635, "bg": 3.7064354593992063e-07}, {"x": 0.11764705882352941, "y": 0.048951048951048945, "ox": 0.11764705882352941, "oy": 0.048951048951048945, "term": "timely", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 18, "s": 0.24059139784946237, "os": -0.06782106782106782, "bg": 5.173840524230078e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "recruiters", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 4.107701889696908e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "perl", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 7.380457802417026e-07}, {"x": 0.09150326797385622, "y": 0.034965034965034954, "ox": 0.09150326797385622, "oy": 0.034965034965034954, "term": "values", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 14, "s": 0.29502688172043007, "os": -0.05573593073593074, "bg": 5.426497188781712e-07}, {"x": 0.07189542483660129, "y": 0.006993006993006992, "ox": 0.07189542483660129, "oy": 0.006993006993006992, "term": "live", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.2540322580645161, "os": -0.06403318903318903, "bg": 1.4450388939744557e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "or", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 5.403693815641641e-09}, {"x": 0.07189542483660129, "y": 0.013986013986013983, "ox": 0.07189542483660129, "oy": 0.013986013986013983, "term": "3rd", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28897849462365593, "os": -0.05708874458874459, "bg": 0.0}, {"x": 0.08496732026143791, "y": 0.006993006993006992, "ox": 0.08496732026143791, "oy": 0.006993006993006992, "term": "spirit", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.2049731182795699, "os": -0.07702020202020202, "bg": 7.556916196309359e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "conversation", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 8.818594889183331e-07}, {"x": 0.0784313725490196, "y": 0.013986013986013983, "ox": 0.0784313725490196, "oy": 0.013986013986013983, "term": "someone", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.25806451612903225, "os": -0.06358225108225109, "bg": 3.670080282612959e-07}, {"x": 0.261437908496732, "y": 0.048951048951048945, "ox": 0.261437908496732, "oy": 0.048951048951048945, "term": "stores", "cat25k": 5, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 40, "s": 0.04973118279569892, "os": -0.21067821067821066, "bg": 6.652939246618612e-07}, {"x": 0.196078431372549, "y": 0.013986013986013983, "ox": 0.196078431372549, "oy": 0.013986013986013983, "term": "flows", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 30, "s": 0.06653225806451614, "os": -0.18046536796536797, "bg": 5.883622678864887e-06}, {"x": 0.1633986928104575, "y": 0.006993006993006992, "ox": 0.1633986928104575, "oy": 0.006993006993006992, "term": "columnar", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 25, "s": 0.08333333333333334, "os": -0.15494227994227994, "bg": 0.00016052454482030513}, {"x": 0.1372549019607843, "y": 0.048951048951048945, "ox": 0.1372549019607843, "oy": 0.048951048951048945, "term": "postgres", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 21, "s": 0.18413978494623656, "os": -0.0873015873015873, "bg": 6.782255681350251e-05}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "keen", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 2.41370154925158e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "deployed", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 3.2207551905992597e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "lakes", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 1.06468457887566e-06}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "resource", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 1.7991985290232615e-07}, {"x": 0.33986928104575165, "y": 0.020979020979020976, "ox": 0.33986928104575165, "oy": 0.020979020979020976, "term": "warehousing", "cat25k": 2, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 52, "s": 0.0228494623655914, "os": -0.31637806637806637, "bg": 3.3719131284824585e-05}, {"x": 0.1633986928104575, "y": 0.04195804195804195, "ox": 0.1633986928104575, "oy": 0.04195804195804195, "term": "warehouses", "cat25k": 4, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 25, "s": 0.12500000000000003, "os": -0.12022005772005773, "bg": 3.6538334606281764e-05}, {"x": 0.1764705882352941, "y": 0.013986013986013983, "ox": 0.1764705882352941, "oy": 0.013986013986013983, "term": "daily", "cat25k": 1, "ncat25k": 16, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 27, "s": 0.07795698924731183, "os": -0.16098484848484848, "bg": 5.475921805347219e-07}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "catered", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 1.6442684911071234e-05}, {"x": 0.09803921568627451, "y": 0.02797202797202797, "ox": 0.09803921568627451, "oy": 0.02797202797202797, "term": "responsibility", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.23252688172043012, "os": -0.06917388167388168, "bg": 6.843283653855331e-07}, {"x": 0.11764705882352941, "y": 0.05594405594405594, "ox": 0.11764705882352941, "oy": 0.05594405594405594, "term": "necessary", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 18, "s": 0.2701612903225807, "os": -0.060876623376623376, "bg": 7.154169673653609e-07}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "guidelines", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 3.5329656943952435e-07}, {"x": 0.058823529411764705, "y": 0.006993006993006992, "ox": 0.058823529411764705, "oy": 0.006993006993006992, "term": "pm", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.31317204301075274, "os": -0.05104617604617604, "bg": 3.307655184733212e-08}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "coordination", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 1.6851106710475798e-06}, {"x": 0.058823529411764705, "y": 0.020979020979020976, "ox": 0.058823529411764705, "oy": 0.020979020979020976, "term": "streams", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.4368279569892473, "os": -0.03715728715728715, "bg": 2.6776235020370025e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "affinity", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.5998621252806023e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "teradata", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 7.309288481373497e-05}, {"x": 0.09803921568627451, "y": 0.013986013986013983, "ox": 0.09803921568627451, "oy": 0.013986013986013983, "term": "traditional", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1935483870967742, "os": -0.08306277056277057, "bg": 7.093964525920992e-07}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "dynamodb", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 0.00019885163182620369}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "app", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 1.4709169083124375e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "advice", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.6818692934437665e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "inspire", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.2877577054081414e-06}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "disciplines", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 3.0282332277937974e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "phone", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 4.674275597906847e-08}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "pro", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 1.3637073693996206e-07}, {"x": 0.065359477124183, "y": 0.02797202797202797, "ox": 0.065359477124183, "oy": 0.02797202797202797, "term": "mobile", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.44086021505376344, "os": -0.0367063492063492, "bg": 1.9234666760806622e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "responsibilities", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 8.339412665888044e-07}, {"x": 0.039215686274509796, "y": 0.020979020979020976, "ox": 0.039215686274509796, "oy": 0.020979020979020976, "term": "collaborates", "cat25k": 2, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.6021505376344086, "os": -0.01767676767676768, "bg": 4.598758335249483e-05}, {"x": 0.11764705882352941, "y": 0.013986013986013983, "ox": 0.11764705882352941, "oy": 0.013986013986013983, "term": "architects", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 18, "s": 0.15994623655913978, "os": -0.10254329004329005, "bg": 5.726160954081486e-06}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "premise", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 6.226353476935874e-06}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "sdlc", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 6.098655923997194e-05}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "accessing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 1.8677644140716302e-06}, {"x": 0.12418300653594772, "y": 0.006993006993006992, "ox": 0.12418300653594772, "oy": 0.006993006993006992, "term": "emr", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.1364247311827957, "os": -0.11598124098124096, "bg": 5.444384253751861e-05}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "informatics", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 4.3923571886827625e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "visualizing", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 2.571293709697267e-05}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "rest", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 4.4950155334626375e-07}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "located", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.4121782800562805e-07}, {"x": 0.0522875816993464, "y": 0.020979020979020976, "ox": 0.0522875816993464, "oy": 0.020979020979020976, "term": "gain", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.48723118279569894, "os": -0.030663780663780664, "bg": 6.861991593873151e-07}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "node", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 8.047791361903141e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "never", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 8.019395281727303e-08}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "roles", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 1.2073677504075054e-06}, {"x": 0.0784313725490196, "y": 0.013986013986013983, "ox": 0.0784313725490196, "oy": 0.013986013986013983, "term": "unlimited", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 12, "s": 0.25806451612903225, "os": -0.06358225108225109, "bg": 1.172755319094576e-06}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "messaging", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 1.3670430193058942e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "producing", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 1.3281405987346363e-06}, {"x": 0.09150326797385622, "y": 0.02797202797202797, "ox": 0.09150326797385622, "oy": 0.02797202797202797, "term": "handle", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.26411290322580644, "os": -0.06268037518037518, "bg": 1.1111407072355044e-06}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "stakeholder", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 5.929876980995856e-06}, {"x": 0.0522875816993464, "y": 0.02797202797202797, "ox": 0.0522875816993464, "oy": 0.02797202797202797, "term": "align", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.5517473118279571, "os": -0.023719336219336216, "bg": 4.317238607841652e-06}, {"x": 0.1372549019607843, "y": 0.006993006993006992, "ox": 0.1372549019607843, "oy": 0.006993006993006992, "term": "faster", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 21, "s": 0.1081989247311828, "os": -0.12896825396825395, "bg": 1.3914490145252096e-06}, {"x": 0.1045751633986928, "y": 0.02797202797202797, "ox": 0.1045751633986928, "oy": 0.02797202797202797, "term": "go", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 16, "s": 0.2110215053763441, "os": -0.07566738816738816, "bg": 9.497424051664164e-08}, {"x": 0.09150326797385622, "y": 0.020979020979020976, "ox": 0.09150326797385622, "oy": 0.020979020979020976, "term": "in", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 14, "s": 0.22849462365591397, "os": -0.06962481962481963, "bg": 4.014411528638413e-09}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "providers", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 4.6310514624821924e-07}, {"x": 0.0457516339869281, "y": 0.02797202797202797, "ox": 0.0457516339869281, "oy": 0.02797202797202797, "term": "monthly", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.6155913978494624, "os": -0.017225829725829728, "bg": 4.753740983476839e-07}, {"x": 0.07189542483660129, "y": 0.034965034965034954, "ox": 0.07189542483660129, "oy": 0.034965034965034954, "term": "gaps", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.446236559139785, "os": -0.036255411255411256, "bg": 5.624766550216422e-06}, {"x": 0.11111111111111112, "y": 0.006993006993006992, "ox": 0.11111111111111112, "oy": 0.006993006993006992, "term": "loading", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15658602150537634, "os": -0.102994227994228, "bg": 2.1026301976361414e-06}, {"x": 0.0784313725490196, "y": 0.006993006993006992, "ox": 0.0784313725490196, "oy": 0.006993006993006992, "term": "salesforce", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22580645161290325, "os": -0.07052669552669552, "bg": 0.00011339994853386951}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "bachelors", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 8.665479977325328e-06}, {"x": 0.12418300653594772, "y": 0.013986013986013983, "ox": 0.12418300653594772, "oy": 0.013986013986013983, "term": "tooling", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 19, "s": 0.14583333333333334, "os": -0.10903679653679653, "bg": 2.8685409166627392e-05}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "option", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.9418137138813546e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "tested", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 5.356266485862916e-07}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "maintainable", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 7.616404041771744e-05}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "status", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 1.7044346398821682e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "clarify", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 2.7465046804363333e-06}, {"x": 0.1633986928104575, "y": 0.04195804195804195, "ox": 0.1633986928104575, "oy": 0.04195804195804195, "term": "specifications", "cat25k": 4, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 25, "s": 0.12500000000000003, "os": -0.12022005772005773, "bg": 1.9672314450650776e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "audits", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 4.6591206168675695e-06}, {"x": 0.065359477124183, "y": 0.020979020979020976, "ox": 0.065359477124183, "oy": 0.020979020979020976, "term": "minimal", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.3763440860215054, "os": -0.04365079365079365, "bg": 2.1906964156499308e-06}, {"x": 0.08496732026143791, "y": 0.013986013986013983, "ox": 0.08496732026143791, "oy": 0.013986013986013983, "term": "video", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.2271505376344086, "os": -0.07007575757575758, "bg": 8.208147995403634e-08}, {"x": 0.1372549019607843, "y": 0.020979020979020976, "ox": 0.1372549019607843, "oy": 0.020979020979020976, "term": "reliability", "cat25k": 2, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 21, "s": 0.13844086021505375, "os": -0.11507936507936507, "bg": 2.826938033871665e-06}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "fit", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 5.185833519139391e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "globally", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.5156232833870684e-06}, {"x": 0.09803921568627451, "y": 0.013986013986013983, "ox": 0.09803921568627451, "oy": 0.013986013986013983, "term": "sense", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1935483870967742, "os": -0.08306277056277057, "bg": 6.450376193528283e-07}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "functionality", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 1.8088602951622862e-06}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "performant", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 0.00011465823655632177}, {"x": 0.058823529411764705, "y": 0.013986013986013983, "ox": 0.058823529411764705, "oy": 0.013986013986013983, "term": "accountable", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.3702956989247312, "os": -0.0441017316017316, "bg": 6.141785919676607e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "update", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 1.7849059251945331e-07}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "documented", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 3.0657264935101764e-06}, {"x": 0.2483660130718954, "y": 0.006993006993006992, "ox": 0.2483660130718954, "oy": 0.006993006993006992, "term": "s3", "cat25k": 1, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 38, "s": 0.03293010752688172, "os": -0.23935786435786433, "bg": 0.0}, {"x": 0.065359477124183, "y": 0.013986013986013983, "ox": 0.065359477124183, "oy": 0.013986013986013983, "term": "t", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.3185483870967742, "os": -0.050595238095238096, "bg": 6.177153281765566e-08}, {"x": 0.11111111111111112, "y": 0.006993006993006992, "ox": 0.11111111111111112, "oy": 0.006993006993006992, "term": "architecting", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15658602150537634, "os": -0.102994227994228, "bg": 0.00016604629924310563}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "pyspark", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 0.00017399734032636928}, {"x": 0.11111111111111112, "y": 0.013986013986013983, "ox": 0.11111111111111112, "oy": 0.013986013986013983, "term": "gcp", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 17, "s": 0.16801075268817203, "os": -0.09604978354978355, "bg": 0.00012678161533123363}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "bonuses", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 3.1873032409410005e-06}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "transformations", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 5.839296715103633e-06}, {"x": 0.07189542483660129, "y": 0.006993006993006992, "ox": 0.07189542483660129, "oy": 0.006993006993006992, "term": "profile", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.2540322580645161, "os": -0.06403318903318903, "bg": 1.1885000085497721e-07}, {"x": 0.065359477124183, "y": 0.006993006993006992, "ox": 0.065359477124183, "oy": 0.006993006993006992, "term": "board", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.2849462365591398, "os": -0.057539682539682536, "bg": 1.0355790895192411e-07}, {"x": 0.0326797385620915, "y": 0.013986013986013983, "ox": 0.0326797385620915, "oy": 0.013986013986013983, "term": "recognize", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.5806451612903225, "os": -0.018127705627705628, "bg": 9.735431986498903e-07}, {"x": 0.09150326797385622, "y": 0.013986013986013983, "ox": 0.09150326797385622, "oy": 0.013986013986013983, "term": "your", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.20698924731182797, "os": -0.07656926406926408, "bg": 1.5517807283475562e-08}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "steps", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.1303189335879884e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "succeed", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.5612670462714015e-06}, {"x": 0.07189542483660129, "y": 0.013986013986013983, "ox": 0.07189542483660129, "oy": 0.013986013986013983, "term": "foundation", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.28897849462365593, "os": -0.05708874458874459, "bg": 4.0102427151588913e-07}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "library", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 9.866989466871574e-08}, {"x": 0.33986928104575165, "y": 0.006993006993006992, "ox": 0.33986928104575165, "oy": 0.006993006993006992, "term": "devops", "cat25k": 1, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 52, "s": 0.019489247311827957, "os": -0.33026695526695526, "bg": 0.001316655694535879}, {"x": 0.0457516339869281, "y": 0.013986013986013983, "ox": 0.0457516339869281, "oy": 0.013986013986013983, "term": "personnel", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.4778225806451613, "os": -0.03111471861471862, "bg": 5.395950339270378e-07}, {"x": 0.07189542483660129, "y": 0.006993006993006992, "ox": 0.07189542483660129, "oy": 0.006993006993006992, "term": "fix", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.2540322580645161, "os": -0.06403318903318903, "bg": 7.404089050089618e-07}, {"x": 0.0522875816993464, "y": 0.013986013986013983, "ox": 0.0522875816993464, "oy": 0.013986013986013983, "term": "querying", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.4321236559139785, "os": -0.037608225108225105, "bg": 2.309671518516637e-05}, {"x": 0.13071895424836602, "y": 0.006993006993006992, "ox": 0.13071895424836602, "oy": 0.006993006993006992, "term": "architectural", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.11962365591397851, "os": -0.12247474747474746, "bg": 4.075218045996985e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "satisfy", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.3172001199091177e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "journal", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.0470047458369368e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "administrators", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 1.2225854853603248e-06}, {"x": 0.11111111111111112, "y": 0.006993006993006992, "ox": 0.11111111111111112, "oy": 0.006993006993006992, "term": "load", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.15658602150537634, "os": -0.102994227994228, "bg": 9.47949763718255e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "bugs", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 7.930462532331504e-07}, {"x": 0.22875816993464052, "y": 0.006993006993006992, "ox": 0.22875816993464052, "oy": 0.006993006993006992, "term": "airflow", "cat25k": 1, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 35, "s": 0.04435483870967742, "os": -0.21987734487734487, "bg": 6.414053904778024e-05}, {"x": 0.5490196078431372, "y": 0.006993006993006992, "ox": 0.5490196078431372, "oy": 0.006993006993006992, "term": "kafka", "cat25k": 1, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 89, "s": 0.0033602150537634405, "os": -0.5380591630591631, "bg": 0.000312175359305167}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "setup", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 7.622173597459716e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "aptitude", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.0957243011561717e-05}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "workload", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 4.818751275678351e-06}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "infrastructures", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 9.576185394949246e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "less", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 1.2370231971400573e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "memory", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 9.960138529590724e-08}, {"x": 0.0457516339869281, "y": 0.020979020979020976, "ox": 0.0457516339869281, "oy": 0.020979020979020976, "term": "challenge", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.5470430107526881, "os": -0.024170274170274175, "bg": 5.405582913060523e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "poc", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.0421379808055553e-05}, {"x": 0.0784313725490196, "y": 0.006993006993006992, "ox": 0.0784313725490196, "oy": 0.006993006993006992, "term": "logging", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.22580645161290325, "os": -0.07052669552669552, "bg": 2.870571136561791e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "referral", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 1.1514108092719658e-06}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "configuration", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.1855340227908503e-07}, {"x": 0.07189542483660129, "y": 0.006993006993006992, "ox": 0.07189542483660129, "oy": 0.006993006993006992, "term": "checks", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.2540322580645161, "os": -0.06403318903318903, "bg": 1.0634065928904712e-06}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "integrations", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 3.481485027438454e-05}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "ec2", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 0.0}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "offerings", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 1.8961477506769546e-06}, {"x": 0.0522875816993464, "y": 0.006993006993006992, "ox": 0.0522875816993464, "oy": 0.006993006993006992, "term": "purpose", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3575268817204301, "os": -0.04455266955266955, "bg": 2.795230064128013e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "helpful", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 2.5923975306808624e-07}, {"x": 0.0326797385620915, "y": 0.006993006993006992, "ox": 0.0326797385620915, "oy": 0.006993006993006992, "term": "curate", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.5100806451612904, "os": -0.025072150072150072, "bg": 3.8516473816822076e-05}, {"x": 0.039215686274509796, "y": 0.013986013986013983, "ox": 0.039215686274509796, "oy": 0.013986013986013983, "term": "forms", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.5315860215053764, "os": -0.024621212121212124, "bg": 2.514060432921835e-07}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "meets", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 7.847155350134467e-07}, {"x": 0.065359477124183, "y": 0.006993006993006992, "ox": 0.065359477124183, "oy": 0.006993006993006992, "term": "marts", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.2849462365591398, "os": -0.057539682539682536, "bg": 6.36075299750485e-05}, {"x": 0.0457516339869281, "y": 0.006993006993006992, "ox": 0.0457516339869281, "oy": 0.006993006993006992, "term": "facilitating", "cat25k": 1, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.42338709677419356, "os": -0.03805916305916306, "bg": 5.770867004238341e-06}, {"x": 0.039215686274509796, "y": 0.006993006993006992, "ox": 0.039215686274509796, "oy": 0.006993006993006992, "term": "representations", "cat25k": 1, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.46438172043010756, "os": -0.03156565656565657, "bg": 2.090827643668608e-06}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "teammates", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 1.420096495556873e-05}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "implementations", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 3.705372101406507e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "error", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 1.5646490174853549e-07}, {"x": 0.21568627450980388, "y": 0.0, "ox": 0.21568627450980388, "oy": 0.0, "term": "docker", "cat25k": 0, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 33, "s": 0.049059139784946235, "os": -0.21383477633477632, "bg": 0.0003604548284562703}, {"x": 0.065359477124183, "y": 0.0, "ox": 0.065359477124183, "oy": 0.0, "term": "etls", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2493279569892473, "os": -0.06448412698412698, "bg": 0.00024855836150328094}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "luigi", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 8.114155346856475e-06}, {"x": 0.09150326797385622, "y": 0.0, "ox": 0.09150326797385622, "oy": 0.0, "term": "troubleshoot", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.176747311827957, "os": -0.09045815295815296, "bg": 2.783784455546442e-05}, {"x": 0.065359477124183, "y": 0.0, "ox": 0.065359477124183, "oy": 0.0, "term": "recommended", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2493279569892473, "os": -0.06448412698412698, "bg": 3.803948163446219e-07}, {"x": 0.1372549019607843, "y": 0.0, "ox": 0.1372549019607843, "oy": 0.0, "term": "snowflake", "cat25k": 0, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 21, "s": 0.09879032258064517, "os": -0.1359126984126984, "bg": 4.411797145567247e-05}, {"x": 0.12418300653594772, "y": 0.0, "ox": 0.12418300653594772, "oy": 0.0, "term": "elt", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.11760752688172044, "os": -0.12292568542568541, "bg": 4.2812173979661965e-05}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "dw", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 3.2033731519289815e-06}, {"x": 0.07189542483660129, "y": 0.0, "ox": 0.07189542483660129, "oy": 0.0, "term": "terraform", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2217741935483871, "os": -0.07097763347763347, "bg": 0.00019152251695409554}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "mentality", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 7.290768611053899e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "containerization", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 0.00013503737641668676}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "log", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.0786438318965946e-07}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "accessibility", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 7.336473029369632e-07}, {"x": 0.1045751633986928, "y": 0.0, "ox": 0.1045751633986928, "oy": 0.0, "term": "automating", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 16, "s": 0.1532258064516129, "os": -0.10344516594516594, "bg": 2.9102047238079297e-05}, {"x": 0.065359477124183, "y": 0.0, "ox": 0.065359477124183, "oy": 0.0, "term": "fault", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2493279569892473, "os": -0.06448412698412698, "bg": 1.51594072307947e-06}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "tolerant", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 8.076968121552985e-06}, {"x": 0.1633986928104575, "y": 0.0, "ox": 0.1633986928104575, "oy": 0.0, "term": "kubernetes", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 25, "s": 0.07728494623655915, "os": -0.16188672438672438, "bg": 0.0006212800854881397}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "pub", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 5.116654390249362e-07}, {"x": 0.2745098039215686, "y": 0.0, "ox": 0.2745098039215686, "oy": 0.0, "term": "kinesis", "cat25k": 0, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 42, "s": 0.026881720430107527, "os": -0.2722763347763348, "bg": 0.00032446338005863516}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "sqs", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 8.777576218620165e-05}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "massively", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.1414428408229423e-05}, {"x": 0.065359477124183, "y": 0.0, "ox": 0.065359477124183, "oy": 0.0, "term": "oozie", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2493279569892473, "os": -0.06448412698412698, "bg": 0.00024855836150328094}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "eks", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 6.747903932341016e-05}, {"x": 0.07189542483660129, "y": 0.0, "ox": 0.07189542483660129, "oy": 0.0, "term": "rds", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2217741935483871, "os": -0.07097763347763347, "bg": 1.619967423927802e-05}, {"x": 0.1045751633986928, "y": 0.0, "ox": 0.1045751633986928, "oy": 0.0, "term": "lambda", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 16, "s": 0.1532258064516129, "os": -0.10344516594516594, "bg": 3.4188041493171256e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "container", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 1.141701182060319e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "alerting", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 1.241490249424259e-05}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "upgrades", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.2233134177908917e-06}, {"x": 0.07189542483660129, "y": 0.0, "ox": 0.07189542483660129, "oy": 0.0, "term": "secure", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2217741935483871, "os": -0.07097763347763347, "bg": 3.527436909345721e-07}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "considerations", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 1.8742079519103228e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "makes", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 1.5925367539566777e-07}, {"x": 0.065359477124183, "y": 0.0, "ox": 0.065359477124183, "oy": 0.0, "term": "technically", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2493279569892473, "os": -0.06448412698412698, "bg": 5.129803259220501e-06}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "segregation", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 9.871013275964467e-06}, {"x": 0.08496732026143791, "y": 0.0, "ox": 0.08496732026143791, "oy": 0.0, "term": "tdd", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18951612903225806, "os": -0.08396464646464646, "bg": 1.661156100755507e-05}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "timescales", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 3.952187316113507e-05}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "sophos", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 2.177220946800816e-05}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "possesses", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 8.49637723914942e-06}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "inception", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 7.2720338585896465e-06}, {"x": 0.08496732026143791, "y": 0.0, "ox": 0.08496732026143791, "oy": 0.0, "term": "kanban", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18951612903225806, "os": -0.08396464646464646, "bg": 0.00015833094821969028}, {"x": 0.11764705882352941, "y": 0.0, "ox": 0.11764705882352941, "oy": 0.0, "term": "hg", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.13306451612903225, "os": -0.11643217893217893, "bg": 7.211658366915956e-06}, {"x": 0.12418300653594772, "y": 0.0, "ox": 0.12418300653594772, "oy": 0.0, "term": "svn", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.11760752688172044, "os": -0.12292568542568541, "bg": 9.817015988818938e-06}, {"x": 0.11764705882352941, "y": 0.0, "ox": 0.11764705882352941, "oy": 0.0, "term": "sbt", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.13306451612903225, "os": -0.11643217893217893, "bg": 0.00011633656167291975}, {"x": 0.12418300653594772, "y": 0.0, "ox": 0.12418300653594772, "oy": 0.0, "term": "bamboo", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.11760752688172044, "os": -0.12292568542568541, "bg": 9.010771428740804e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "outputs", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.9596351094105107e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "glue", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 3.1000501322392814e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "consume", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 4.433310501784249e-06}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "latency", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 6.1772351724907125e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "when", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.8441631769797388e-08}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "conventions", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 2.3783068684759144e-06}, {"x": 0.0784313725490196, "y": 0.0, "ox": 0.0784313725490196, "oy": 0.0, "term": "schema", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.20430107526881722, "os": -0.07747113997113997, "bg": 3.6010153062656013e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "redis", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 0.00012720758157186166}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "flink", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 0.0001330032881368456}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "ssms", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 0.00011761131420842685}, {"x": 0.07189542483660129, "y": 0.0, "ox": 0.07189542483660129, "oy": 0.0, "term": "studio", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2217741935483871, "os": -0.07097763347763347, "bg": 4.314236431138119e-07}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "react", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 3.3279159615644718e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "adobe", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 4.344801255213082e-07}, {"x": 0.11764705882352941, "y": 0.0, "ox": 0.11764705882352941, "oy": 0.0, "term": "ideally", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 18, "s": 0.13306451612903225, "os": -0.11643217893217893, "bg": 8.719453755287075e-06}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "enough", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 1.9122040664023378e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "really", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 7.402205302014602e-08}, {"x": 0.09150326797385622, "y": 0.0, "ox": 0.09150326797385622, "oy": 0.0, "term": "want", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.176747311827957, "os": -0.09045815295815296, "bg": 1.0820385682368282e-07}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "moving", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 3.086042391537038e-07}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "dbt", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 4.9326961020739246e-05}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "storm", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 8.063567070309422e-07}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "accenture", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 1.8722459596263534e-05}, {"x": 0.065359477124183, "y": 0.0, "ox": 0.065359477124183, "oy": 0.0, "term": "weeks", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.2493279569892473, "os": -0.06448412698412698, "bg": 2.7154649107420575e-07}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "near", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 2.2022866342123025e-07}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "lineage", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 7.30704825696453e-06}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "parquet", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 3.97959462854215e-05}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "awesome", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 7.776773421891772e-07}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "jvm", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 1.970279429407077e-05}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "databricks", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 0.00014914243102162565}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "micro", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 5.381071328252885e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "dataflow", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 3.597618376634668e-05}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "chef", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.22418200158654e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "kimball", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 9.621245910111449e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "biological", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 5.96304107143922e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "raise", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 5.665614577777393e-07}, {"x": 0.15686274509803919, "y": 0.0, "ox": 0.15686274509803919, "oy": 0.0, "term": "ads", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 24, "s": 0.08266129032258066, "os": -0.1553932178932179, "bg": 8.792124718047549e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "standardized", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 2.886507573594517e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "backlog", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 1.2259226600420142e-05}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "employment", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 2.0205689371521988e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "growflow", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 0.00014914243102162565}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "mapping", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 6.063545655290159e-07}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "provisioning", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 8.250411783945288e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "breakfast", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 3.8811510595833476e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "regardless", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 8.464426133069244e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "adf", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.0729095698169082e-05}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "populate", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 2.0214748006320478e-05}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "exclusive", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 5.488862069698943e-07}, {"x": 0.08496732026143791, "y": 0.0, "ox": 0.08496732026143791, "oy": 0.0, "term": "devex", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.18951612903225806, "os": -0.08396464646464646, "bg": 0.0003231138230578995}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "highlighted", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 2.6644853413338946e-06}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "recruiter", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 5.385143801850941e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "sla", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 7.33821528489092e-06}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "eco", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.5351206828728505e-06}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "recovery", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 3.7110384042110745e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "zynga", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 0.00014914243102162565}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "women", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 4.9463951939340374e-08}, {"x": 0.14379084967320263, "y": 0.0, "ox": 0.14379084967320263, "oy": 0.0, "term": "screened", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 22, "s": 0.09206989247311828, "os": -0.1424062049062049, "bg": 1.2824967179743081e-05}, {"x": 0.14379084967320263, "y": 0.0, "ox": 0.14379084967320263, "oy": 0.0, "term": "scams", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 22, "s": 0.09206989247311828, "os": -0.1424062049062049, "bg": 1.6040456948871772e-05}, {"x": 0.14379084967320263, "y": 0.0, "ox": 0.14379084967320263, "oy": 0.0, "term": "junk", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 22, "s": 0.09206989247311828, "os": -0.1424062049062049, "bg": 6.712684150666195e-06}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "watson", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 1.9178411246028573e-06}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "flume", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 6.288517167651867e-05}, {"x": 0.0457516339869281, "y": 0.0, "ox": 0.0457516339869281, "oy": 0.0, "term": "routines", "cat25k": 0, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.3461021505376344, "os": -0.045003607503607504, "bg": 2.2788835879524544e-06}, {"x": 0.07189542483660129, "y": 0.0, "ox": 0.07189542483660129, "oy": 0.0, "term": "eg", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.2217741935483871, "os": -0.07097763347763347, "bg": 3.4015463986544965e-07}, {"x": 0.058823529411764705, "y": 0.0, "ox": 0.058823529411764705, "oy": 0.0, "term": "chembl", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.27352150537634407, "os": -0.05799062049062048, "bg": 0.00022370530554416317}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "demonstrable", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 2.7180252640448296e-05}, {"x": 0.0522875816993464, "y": 0.0, "ox": 0.0522875816993464, "oy": 0.0, "term": "faculty", "cat25k": 0, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.30443548387096775, "os": -0.051497113997114, "bg": 3.0247909789087564e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "nweh", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 0.00014914243102162565}, {"x": 0.1045751633986928, "y": 0.0, "ox": 0.1045751633986928, "oy": 0.0, "term": "ed", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 16, "s": 0.1532258064516129, "os": -0.10344516594516594, "bg": 6.169340452392338e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "los", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 1.824745455231946e-07}, {"x": 0.039215686274509796, "y": 0.0, "ox": 0.039215686274509796, "oy": 0.0, "term": "angeles", "cat25k": 0, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.4005376344086022, "os": -0.03851010101010101, "bg": 2.6050550311362686e-07}], "docs": {"categories": ["data scientist", "data engineer"], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "texts": ["", "Lead effort determine overpayments contracts sources information Automate procedures obtaining reading parsing understanding contracts Improve pricing procedures contracts available Support initiatives data process quality Interface others Data Science Data Analytics teams Explain insights team members Equian employees Provide technical expertise aspects claim scoring needed Bachelor degree quantitative field Strong understanding Natural Language Processing Experience Python R Proficiency basic SQL commands queries Strong analytical quantitative skills Self starter eager learn Excellent communication skills written verbal Demonstrated ability build relationships work part team", "Analysis Deep dive analysis Airbnb vast data uncover opportunities Partner data scientists analysts help tell story behind data Product Strategy Partnering leadership prioritize areas opportunity drive growth quality Evaluate define product business metrics Product Execution Communicate state business stakeholders Digest interpret experiments Decision Tools Democratize data building socializing decision tools dashboards reports Build key data sets pipelines empower operational exploratory analysis", "", "Lead projects Identify candidate therapeutic targets biomarkers immunology integrated analysis high throughput molecular data real world data clinical phenotypes Explore opportunities position existing drugs novel indications Collaborate wet lab computational scientific teams champion projects discovery early clinical development Develop innovative creative computational approaches understand complex immune conditions using multi dimensional omics clinical data Establish interactions collaborators within outside Immunology Inflammation Research Early Development Therapeutic Area Prepare publications research clinical regulatory documentation accordance Sanofi quality standards", "Network business stakeholders develop pipeline data science projects aligned business strategies Translate complex ambiguous business problems project charters clearly identifying technical risks project scope Work cross disciplinary project team database specialists data scientists business subject matter experts complete project deliverables time within budget Design strategies propose algorithms analyze leverage data existing well new data sources Develop code models applying algorithms large structured well unstructured data sets complex projects Develop visualization products share analysis across large group business users Continuously seek industry best practices develop skills create new capabilities data analytics Cargill improve business decisions", "Design coding testing monitoring new classifiers production environments drive analytical SaaS product platforms Maintain data assets Perform large scale data analysis Work closely groups like product engineering customer operations acquire requirements develop new systems solve problems", "", "", "", "Analysis design development test troubleshooting documentation complex data data management systems may involve one following predictive score models feature extraction feature engineering data driven analysis application machine learning algorithms decision making related utilities Complete written documentation reports results form business reports internal technology white papers statistical system documentation Independent resolution problems changing priorities Devise new approaches solve difficult statistical modeling problems This includes new model concepts designs creative targets generation objective functions modification new different uses data sources Provide insights data experiments ensure accuracy completeness feasibility provide analytical opinions help business decision Identify deploy new algorithms combinations algorithms creative ways achieve performance improvement Work team environment interact multiple departments including internal external customers Monitor system performance proactively prevent system degradation", "", "Safety evaluation fundamental importance Waymo measuring subtle This role works wide variety teams across company onboard software engineering product safety systems engineering evaluate driving safety performance Waymo self driving car This based wide variety data real simulated driving Waymo self driving system well external data sources The Planner Team responsible motion planning self driving car This includes gracefully handling many complex situations involving social interactions merging negotiating narrow roads etc dealing noisy uncertain environment Evaluation onboard decision making system complex open ended problem tremendous potential impact The quality road maneuvers execute subjective contextual geometrically subtle evaluation needs apply wide variety conditions extremely long tail Analysis improvement existing data sources metrics Development novel approaches measuring improving Collaborate engineers data scientists evaluate improve core Waymo technologies focus bringing statistical depth general analytical rigor accurate causal interpretation data PhD statistics math quantitative area Progressive statistics background either academia industry Data science system evaluation experience Willingness understand complex system various components Experience tools manipulating big data Experience R Python statistical libraries Experience C Familiarity Spark MapReduce Training interest geometry classical physics", "Using Big Data tools Hadoop Spark H2O AWS conduct analysis billions customer transaction records Writing software clean investigate large messy data sets numerical textual data Integrating external data sources APIs discover interesting trends Building machine learning models development testing validation million customers production Designing rich data visualizations communicate complex ideas customers company leaders Curious You ask explore afraid blurt disruptive idea You probably know Python Scala R constantly exploring new open source tools Wrangler You know programmatically extract data database API bring transformation two model human readable form ROC curve map d3 visualization Tableau etc", "", "Apply analytics data science expertise better serve users Create manage key data pipelines including developing new solutions batch real time data analytics use cases Demonstrated ability data science data engineering user analytics Passion persistence solving hard problems Degree Engineering Computer Science Stats Mathematics related quantitative discipline followed graduate degree years equivalent experience Experience requirements analysis design implementation testing software solutions especially data related using Python R similar programming languages Experience relational databases SQL Ability oversee work simultaneously different projects variety timelines A team customer centric mindset Demonstrated ability solve hard mathematical algorithmic statistical problems Experience applied Machine Learning AI techniques Experience working cloud platforms like AWS real time data processing agile software development Drive understand key issues subtleties complex systems", "", "", "Lead projects Identify candidate therapeutic targets biomarkers immunology integrated analysis high throughput molecular data real world data clinical phenotypes Explore opportunities position existing drugs novel indications Collaborate wet lab computational scientific teams champion projects discovery early clinical development Develop innovative creative computational approaches understand complex immune conditions using multi dimensional omics clinical data Establish interactions collaborators within outside Immunology Inflammation Research Early Development Therapeutic Area Prepare publications research clinical regulatory documentation accordance Sanofi quality standards", "Work team data scientists machine learning engineers software engineers QA engineers Perform data collection preprocessing feature engineering data visualization analysis Build automation data collection preprocessing Build models address business problems Engage lines business users analysts explore prototype opportunities use cases exploiting data application cognitive machine learning technologies Modern object oriented functional programming experience Java C Python Scala SQL R Data Science Machine Learning Frameworks R Apache Spark MLlib TensorFlow Scikit learn etc Natural Language Processing NLTK CoreNLP Gensim Spacy etc Experience Big Data technologies cloud AWS Linux Bash scripting Relational databases Oracle PostgreSQL MySQL etc Agile development methodology CI CD Development environments tools GIT Maven Jenkins etc RESTful Microservice APIs", "Identify customers needs requirements quantify statistical problems You collect cleanse large amount real world costumers usage commerce data various product environments You develop predictive models large scale datasets address various business problems demonstrating advanced statistical modeling machine learning data mining techniques You deliver informative effective findings results recommendations machine learning models stakeholders able build dashboards critical statistics metrics", "", "", "Leverage large sets structured unstructured data develop tactical strategic insights Collaborate analytic data teams set objectives approaches work plans Research evaluate new analytical methodologies approaches solutions Develop validate statistical forecasting models tools Interpret communicate analytic results analytical non analytical business partners executive decision makers Travel required Monday Friday Advanced degree Masters minimum quantitative field one following Statistics Computer Science Data Science Engineering Physics Mathematics Computational Social Science similar academic pedigree years experience advanced analytics machine learning model development validation years experience working analytic based projects delivering results within scope funding duration Experience statistical package various programming languages R Python Java C SAS Matlab etc Experience standard classification regression techniques including dimension reduction feature selection hypothesis testing Strong communication skills ability explain modeling results non technical users Capacity high degree autonomy box thinking", "", "Serve technical lead demanding projects involves data science ML knowledge experience Exert technical influence multiple teams across organizations increasing productivity effectiveness sharing knowledge experience Leverage knowledge internet industry prior art design decisions Contribute intellectual property patents Assist career development actively mentor individuals community advanced technical issues helping managers guide career growth team members years industry experience predictive modeling data science analysis Experience handling terabyte size datasets using big data platforms Spark Hadoop Hive Impala Presto Athena etc Deep hands technical expertise least one major technical area statistical analysis ML Deep Learning DP established tools R Scikit learn MLlib SparkML Tensorflow etc Proven track records leading delivery statistical analysis ML solutions solving real customer use cases significant business impact Proven problem solving diving data discover hidden platforms Comfortable ambiguity", "", "Work engineers geoscientists management frame open ended problems terms deliverables generated using data science techniques Work subject matter experts evaluate multiple internal external datasets applicability usefulness achieving model objectives Work management user community scope prioritize project efforts using risk vs value criteria Explore examine data variety angles using appropriate profiling techniques understand data determine hidden weaknesses trends opportunities Employ appropriate visual machine learning statistical methods prepare data use predictive prescriptive modeling Create data driven solutions provide structure assist decision making process facilitate automation Devise new algorithms solve problems tools automate work efforts Appropriately evaluate tune models insure business objectives achieved Effectively communicate recommendations stakeholders management Periodically evaluate existing models insure continued effectiveness Help champion appropriate use machine learning statistical methods effective means decision making automation Creates training materials delivers formal informal training sessions increase awareness utilization quantitative methods evaluating data decision making Provides mentoring seeking machine learning statistical methods Assist evaluation tools visual quantitative analysis well tools techniques deploying predictive prescriptive solutions Excellent interpersonal customer facing communication skills Ability interact wide range people Excellent analytical problem solving skills Strong mentoring training skills Must self starter ability follow projects assigned Must willing travel within US time year degree equivalent experience required advanced degree beneficial years experience developing models decision support process automation Excellent understanding machine learning techniques algorithms Regression Decision Trees k NN Na\u00efve Bayes Neural Networks Experience identifying patterns high frequency time series data Working knowledge common data science tools R Weka NumPy SAS MatLab excellence least one Experience using visual analytics tools required knowledge Tibco Spotfire plus Proficiency using query languages SQL Hive Experience NoSQL databases MongoDB Cassandra HBase plus", "", "", "Use machine learning eliminate advanced security threats models built detect anomalies intricacies human human communications Brainstorm ideas team quickly mock prototypes Jupyter Notebooks Be thrown exciting unknown like focus new problems little prior research rely Be responsible getting work production see Work closely backend engineering team deploy monitor iterate models You work alongside amazing high performing colleagues We offer competitive salary equity options every role annual salary reviews Everyone gets days paid annual leave days including bank holidays Company contributions made towards pension You get high spec tech kit work get choose OS Flexible morning start times engineering teams One week remote working abroad per year Fully stocked kitchen plenty office snacks including fruit nuts bread cereals amazing coffee We offer cycle work scheme eye care vouchers Parents carers guaranteed one day per week work home give extra day annual leave take child first day nursery primary school Generous enhanced Maternity Paternity leave", "", "A cover letter explaining motivation apply position interest announced position topic job related experience think important including applied methods A detailed curriculum vitae including detailed account relevant technical skills prior experience access one recent statistics modelling projects relevant position e g paper GitHub repository Copies certificates high school B Sc M Sc Ph D transcript covered subjects grades B Sc M Sc publication list At least one relevant recent publication Two references Excellent technical facilities without parallel The freedom need bridge difficult gap basic research close ready application Work interdisciplinary multinational teams excellent links national international research networks including project partners A vibrant region high quality life wide cultural offering balance family professional life Interesting career opportunities extensive range training education courses Remuneration TV\u00f6D public sector pay grade including attractive public sector social security benefits", "", "", "Work collaboratively business stakeholders team members deliver value advanced analytic modeling techniques Analyze visualize model data using advanced statistical techniques tools structured unstructured data Perform machine learning natural language statistical analysis classification regression recommendation sentiment analysis deep learning Work organizations move descriptive analytics predictive prescriptive analysis Work variety tools implement advanced analytic solutions Create quality deliverables communicate technical solutions appropriate audiences Work sales team create deliver client proposals demonstrations Present client industry internal peer groups Provide mentoring leadership junior team members High level understanding advanced analytics techniques Ability quickly understand adapt new industries situations business requirements Development experience R Python Azure Machine Learning SAS SPSS related languages tools Advanced knowledge data analysis transactional SQL Knowledge Big Data techniques methodologies Degree equivalent experience Statistics Mathematics Computers Science related fields Eagerness learn new tools technologies Strong communication skills", "", "", "", "", "", "", "Working within analytics data science team tasked optimising product performance Analyse combination product customer user behaviour data Implement statistical techniques increase quality analysis Conduct statistical segmentations Provide strategic insights recommendations improvement Identify optimisation opportunities generating experimentation A B tests ideas analysing results Degree educated Maths Stats Engineering Economics Physics Chemistry preferred Hands experience using SQL analyse manipulate large data sets Experience Google Analytics highly desirable Experience working online platform online product highly desirable Experience using R Python highly desirable Experience using visualisation tools e g Tableau Looker highly desirable", "Apply interpretable machine learning better understand climate impacts floods crop failure low carbon uptake wildfires Follow new developments interpretable machine learning translate apply pressing research questions Earth system sciences Support group expertise state art data science machine learning methods Excellent data science machine learning skills including deep learning e g CNNs Experience common machine learning toolboxes TensorFlow PyTorch Keras similar Interest interpretable machine learning Experience code sharing version control e g via GitHub Self driven personality able work independently team Prepared work highly interdisciplinary environment PhD Computer Science Applied Mathematics Engineering Environmental Sciences related field Excellent oral written communication skills English Excellent technical facilities without parallel Work interdisciplinary multinational teams excellent links national international research networks A vibrant region high quality life wide cultural offering balance family professional life Interesting career opportunities extensive range training education courses Remuneration public sector pay grade TV\u00f6D EG13 including attractive public sector social security benefits", "Develop maintain forecasting optimization algorithms AutoGrid machine learning predictive control systems Implement deploy algorithms big data platforms production Benchmark debug critical issues algorithms software arise Prototype new methods analyses customer data sets Work closely product management engineering QA teams manage full product lifecycle including requirements architecture algorithmics QA MS Ph D computer science operations research statistics related field Exposure large forecasting optimization problems techniques including large scale time series forecasting stochastic linear programming Good understanding algorithms data structures performance optimization techniques Excellent programming skills Python Java C Experience building production python applications Specifically knowledge Celery Django frameworks plus Hands experience energy industry Understanding experience Big Data NoSQL frameworks Spark HBase Hadoop MapReduce HDFS etc Knowledge relational databases SQL MS Ph D computer science operations research statistics related field Exposure large forecasting optimization problems techniques including large scale time series forecasting stochastic linear programming Good understanding algorithms data structures performance optimization techniques Excellent programming skills Python Java C Experience building production python applications Specifically knowledge Celery Django frameworks plus Hands experience energy industry Understanding experience Big Data NoSQL frameworks Spark HBase Hadoop MapReduce HDFS etc", "Advanced knowledge statistical machine learning methods particularly areas modeling business analytics Strong programming skills Experience statistical languages packages R SAS Matlab Mahout Experience working relational databases distributed computing platforms query interfaces SQL MapReduce Hive Excellent written verbal communications skills proven ability translate complex methodologies analytical results higher level business insights key take aways Ability travel needed meet customers This role support US government clients require US citizenship Given US citizenship required apply A proven passion generating insights data strong familiarity higher level trends data growth open source platforms public data sets Experience working hands large scale data sets Familiarity visualization software techniques including Tableau business intelligence BI software Microstrategy Cognos Pentaho etc", "", "", "Maintain team data collection validation distribution processes Act subject matter expert data problems Work automate streamline processes involving data deployment Experience data integrity regression back testing Programming skills R Java Python PHP Database knowledge within SQL MySQL etc Ability problem solve organize multitask Competitive salary bonus structure 401K health benefits educational reimbursements generous PTO", "Excellent health retirement benefits Anual Bonus Great work life balance Very friendly company culture Experience using Machine Learning years experience using R Python", "years proven experience Data Scientist Data Analyst Knowledge R SQL Python Identify valuable data sources automate collection processes Undertake preprocessing structured unstructured data Analyze large amounts information discover trends patterns Build predictive models machine learning algorithms Combine models ensemble modeling Present information using data visualization techniques Propose solutions strategies business challenges Collaborate engineering product development teams Requirements", "", "", "", "As PhD student supervised jointly Prof Breteler senior PostDoctoral researcher group Additionally participate Clinical Population Science Graduate School BIGS CPS participate regular trainings scientific retreats The position initially limited three years possibility extension", "Develop maintain forecasting optimization algorithms AutoGrid machine learning predictive control systems Implement deploy algorithms big data platforms production Benchmark debug critical issues algorithms software arise Prototype new methods analyses customer data sets Work closely product management engineering QA teams manage full product lifecycle including requirements architecture algorithmics QA MS Ph D computer science operations research statistics related field Exposure large forecasting optimization problems techniques including large scale time series forecasting stochastic linear programming Good understanding algorithms data structures performance optimization techniques Excellent programming skills Python Java C Experience building production python applications Specifically knowledge Celery Django frameworks plus Hands experience energy industry Understanding experience Big Data NoSQL frameworks Spark HBase Hadoop MapReduce HDFS etc Knowledge relational databases SQL MS Ph D computer science operations research statistics related field Exposure large forecasting optimization problems techniques including large scale time series forecasting stochastic linear programming Good understanding algorithms data structures performance optimization techniques Excellent programming skills Python Java C Experience building production python applications Specifically knowledge Celery Django frameworks plus Hands experience energy industry Understanding experience Big Data NoSQL frameworks Spark HBase Hadoop MapReduce HDFS etc", "", "", "Prepare Data Data pipelines model development Develop novel approaches exploration analysis feature selection large scale multidimensional data include neurophysiological data behavioral wearable clinical patient data Build train test deploy machine learning models Develop visualization tools interpret communicate results Work closely software developers integrate models software products Document code results details approaches thorough systematic way order promote knowledge sharing code use Collaborate data scientists SMEs ideate solve complex issues pertinent data ingestion curation model performance model generalization", "literature research aim critically evaluating new experimental theoretical methods making usable work evaluating problems difficulties disadvantages arise deriving new approaches solutions independent research expansion scientific knowledge highest international level field uncertainty assessment interpretability traceability neural networks review development existing methods robustness analysis uncertainty modeling machine learning models New development approaches visual analysis model components self explanatory models research development procedures verification stabilization machine learning models e g prior knowledge combination multiple data sources perform experiments validate developed methods Evaluation interpretation experimental results compare different methods implementation procedures prototypical algorithms detailed preparation presentation new approaches scientific users presentation discussion research results conferences meetings etc national international levels publication research results suitable publications participation topic related scientific committees working groups interest groups Representation presentation scientific results internally externally provide guidance doctoral students part research project", "", "", "", "", "", "Identify ingest enrich wide range structured unstructured data datasets analysis Learn apply statistical advanced techniques e g segmentation machine learning develop prototypes scalable e efficient automated data analyses Conduct basic ad hoc analysis building models forecasting viewing behavior Extensive code development debugging optimizing productionizing Support development machine learning pipeline architecture Help build frameworks quickly rolling new data analysis standalone data driven products services support network media brands Be able transform unstructured raw data formats suitable modeling Be able work teams collaborate stakeholders define requirements", "", "", "", "Solid understanding basic statistics significance testing correlation analysis confidence intervals descriptive statistics Experiment Design sample size power randomization matched market Forecasting Prediction Models linear nonlinear Nonparametric models Large scale multivariate optimization Methodology writeups interpretation results Applications generalization Use cases automation scalability evaluation POVs pager vendor evaluation assessment Lead commercialization tools techniques Data prep cleanup missing data outlier detection duplications internet scale digital advertisement survey response data Data Integration merge join subset Data visualization Tableau R Matplotlib ggPlot etc Data mining data reduction Classification Decision Tree clustering bagging boosting logit regression Prediction Neural Network validation cross validation Utilization statistical programming tools R SAS SciPy coding languages Python Java C Google tools BigQuery TensorFlow Comfortable data retrieval processing SQL NoSQL Solid understanding relational non relational database technology cloud based data lake ETL data pipeline Understanding code version management system Some management experience enjoys mentoring managing direct reports Highly collaborative individual great communication skill Develop communication styles focusing technical details non technical audiences Degree equivalent experiences applied quantitative field Statistics Mathematics Econometrics Engineering CS At least years industry experience graduate degree data focused role Experience building end end data products ability use story create new analytical products change existing product way improves company goals metrics Knowledge data science lifecycle acquiring data different sources exploratory data analysis creating optimizing modes including validation interpretation finally onward communication results clear succinct fashion Hands Experience statistical software packages R Python SciPy programming languages SQL Python JAVA Experience statistical models multivariate testing time series analysis logistic regression linear non linear regression Baseline understanding five core competencies programming statistics machine learning data munging data visualization Ability communicate findings effectively non technical audiences Preferred Knowledge popular ad serving technologies supporting analytical research tools Doubleclick Atlas comScore Compete etc Preferred Experience machine learning algorithms random forest SVM etc Bayesian methods", "", "Develop maintain forecasting optimization algorithms AutoGrid machine learning predictive control systems Implement deploy algorithms big data platforms production Benchmark debug critical issues algorithms software arise Prototype new methods analyses customer data sets Work closely product management engineering QA teams manage full product lifecycle including requirements architecture algorithmics QA MS Ph D computer science operations research statistics related field Exposure large forecasting optimization problems techniques including large scale time series forecasting stochastic linear programming Good understanding algorithms data structures performance optimization techniques Excellent programming skills Python Java C Experience building production python applications Specifically knowledge Celery Django frameworks plus Hands experience energy industry Understanding experience Big Data NoSQL frameworks Spark HBase Hadoop MapReduce HDFS etc Knowledge relational databases SQL MS Ph D computer science operations research statistics related field Exposure large forecasting optimization problems techniques including large scale time series forecasting stochastic linear programming Good understanding algorithms data structures performance optimization techniques Excellent programming skills Python Java C Experience building production python applications Specifically knowledge Celery Django frameworks plus Hands experience energy industry Understanding experience Big Data NoSQL frameworks Spark HBase Hadoop MapReduce HDFS etc", "", "Shaping innovating personalized digital healthcare solutions Understanding evaluating new benchmarks new ecosystem digital medicine Tailoring multimodal longitudinal data patients Parkinson disease individual patient needs Shaping us predictive individualized reference modelling Integration analysis multi parametric data Development data model contribute new solutions digital healthcare Engage publications knowledge sharing Interacting group wider research community especially PhD students engage grant writing Hold Ph D degree life science data science engineering Ideal candidates would good understanding proven experience biomedical statistics machine learning advantage Analytical skills ability translate complex research data meet patients needs A cross disciplinary curiosity strong organizational interpersonal skills seeking collaborative interdisciplinary digital medicine research Team player good communication skills A track record previous publications medical data science advantageous Working knowledge modern programming languages Python R Linux Excellent working knowledge English required Knowledge German French Luxembourgish advantageous Contributing innovative research project fast developing research group A highly interdisciplinary research environment integrating biologists physicists mathematicians clinical researchers working area systems biomedicine An international environment competitive compensation benefits State art research facilities computational equipment Contract Type Fixed Term Contract Month possibility renewal years total Work Hours Full Time Hours per Week Starting date soon possible Location Belval Internal Title Postdoctoral researcher Research associate Job Reference UOL04177 A detailed Curriculum vitae A motivation letter includes description past research experience future interests well would contribute research group Copy PhD degree Name contact details three references Multilingual international character Modern institution personal atmosphere Staff coming countries Member University Greater Region UniGR A modern dynamic university High quality equipment Close ties business world Luxembourg labour market A unique urban site excellent infrastructure A partner society industry Cooperation European institutions innovative companies Financial Centre numerous non academic partners ministries local governments associations NGOs", "", "", "Build Machine Learning models power apps inventory classification Our preferred language Python Research develop predictive algorithms using various regression techniques Work internal external stakeholders optimize products Use statistics describe data Communicate teams data visualization Bachelor degree Statistics Math Economics Experience building Machine Learning models using messy data Strong programming ability Python Jupyter Spark Experience Data Visualization A self starter You love keep motivated A learner You insatiable thirst knowledge greater understanding A conversationalist You speak language statistics explain grandmas An innovator You interested solutions make things interesting efficient With others We collaborate cross functionally solve problems deliver best products customers With transparency We open team room No cubicles private offices With agility We believe following process process sake We ship frequently focus delivering incremental value With open minds We committed building diverse team people unique perspectives This encourages healthy inclusive environment builds sustainable successful company With pride We value people We invest applying strengths interests company needs Lunch learns Annual stipend continuous education Tech hands lunches every Friday Hack days Team outings Nordstrom discount Flexible work environment", "", "", "Integrate interdisciplinary research teams creatively develop apply modern data science statistics machine learning techniques advance research Coding algorithmic prototyping relevant analysis methods including setting clear goals measuring progress creation appropriate documentation Collaborate educate convene support broad community researchers campus best leverage data science teaching research This may include contributing mini courses workshops data science Communicate results impact stakeholders This may include presenting research academic conferences workshops", "", "Master degree Healthcare Science Computer Engineering related scientific technical clinical discipline At least years professional experience years degree requirement met Experience visualization dashboard creation Additional qualifications Experience VA OIT", "Perform scientific work area quantum physics quantum information Develop simulations materials energy storage together partners Transfer theoretical concepts experiments Present obtained results scientific journals well national international conferences", "", "Retrieve process prepare rich data variety data sources social media news internal external documents emails financial data operational data Analyze model structured data implement algorithms support analysis using advanced statistical mathematical methods statistics machine learning data mining econometrics operations research Perform Statistical Natural Language Processing mine unstructured data using methods document clustering topic analysis named entity recognition document classification sentiment analysis Utilize diverse array technologies tools needed deliver insights R SAS Python Spark Hadoop Qlikview Tableau Participate client engagements focused Big Data advanced business analytics diverse domains risk management product development marketing research supply chain public policy communicate results educate others reports presentations years professional experience working Data Scientist Master degree PhD Computer Science Statistics Mathematics Econometrics related fields accredited college university Strong mathematical background strong knowledge least one following fields statistics data mining machine learning statistics operations research econometrics natural language processing information retrieval Deep experience extracting cleaning preparing modeling data Experience command line scripting data structures algorithms ability work Linux environment proficiency analysis e g R SAS Matlab packages well proficiency programming languages e g Python Ruby Java Scala", "Identify strong AI ML use cases cloud security data security adjacent domains leveraging Netskope rich set data sources Define scalable data acquisition labeling strategy specific use cases Work data engineers retrieve clean normalize data Ensure scalable continuous high quality data stream Work closely threat research development team feature engineering Make sure high quality feature sets chosen systematic way Select ML models defined use cases Implement KPIs ensure optimal algorithms results Conduct strict internal testing ensure high efficacy low false positive false negative rate Interpret results communicate findings Document use case data acquisition feature engineering training validation deployment future improvement opportunity important aspects Work closely development QE team productization Be evangelist AI ML within Netskope Promote AI ML wherever applicable beyond security use cases Collaborate data analytics team define new platform requirements continuously improve horizontally scalable data lake", "", "Combine synthesize data multiple sources Select appropriate analytical methods explore develop test hypotheses Develop apply predictive models Communicate results effectively clearly range audiences Stay date techniques tools Analytical articulate inquisitive insightful Skilled problem solver Receptive peer review enjoy working collaboratively Self motivated proactive work productively without supervision Critical thinker quick learner Attentive detail Interested understanding challenges healthcare delivery building data products meet needs healthcare system", "Drive implementation database curation tools enhance efficiency effectiveness team Lead outsourced content curation Drive implementation bioinformatics software pipelines Contribute analyses public internal data Collaborate closely interdisciplinary teams across organization understand business needs address technical solutions Publish scientific methodological results top journals", "", "Data curation Develop statistical modeling techniques pattern recognition problems Develop code R Python Java languages statistical analysis optimization simulation Build models maximize performance accuracy", "Work directly biomarker scientists benefit future clinical trials patients Share discoveries propose new hypotheses cancer immunotherapy cancer research community publications Work vibrant global data science learn implement production level analysis pipelines libraries clinical data", "", "Master data flowing NPR digital platforms find opportunities analyses inform key decisions content revenue digital product development Serve diverse needs across organization foundational trend analyses big questions require data science expertise Uncover insights audience usage across NPR platforms including website apps podcasts social media Drive evolution personalization algorithms A B testing NPR platforms Innovate use behavioral data engage deeply audiences drive loyalty donations stations Independently perform data mining techniques statistical analyses large data sets uncover critical patterns predictive models Assure data quality integrity NPR digital metrics systems Create meaningful dashboards executives content leaders digital newsroom staff including historical reporting Develop metrics measure business performance Build models understand media behavior online discover actionable insights Communicate effectively small teams formal presentations business unit events engaging storytelling Develop written communications address varied styles information needs Build strong working relationships levels internal clients build problem solving partnerships clients colleagues", "Lead participate application analytic machine learning approaches HBO understanding drivers content performance help HBO make smarter decisions acquisition scheduling promotion content Mine HBO third party data better understand consumers make choices regards content consumption change based various factors including availability seasonality promotion etc Develop compelling data driven stories influence key content decisions levels within organization Provide analytic consultation related components insights environment including limited audience modeling promotional effectiveness consumer engagement product optimization Bachelor Degree higher quantitative field study mathematics physical sciences computer science operations research etc accredited institution Inquisitive conceptual thinker comfortable working complex analytic problems related people choose watch HBO content Must capable telling compelling data centric stories inform key strategic tactical decisions content development scheduling promotion varying internal external stakeholders varying seniority Prior experience application analytic programming R SAS SQL manipulation data execution analyses preferably entertainment media company years relevant experience Position based New York Seattle", "Collaborate SMEs client side understand current workflows help design autonomous solutions empowered Beyond Limits Artificial Intelligence IP stack Act house reservoir engineering expert ensure accuracy O G knowledge physics implemented agents Contribute solution design usually collaboration data scientists", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Build next level data science capability Chipotle hardware software best practices hands deep diving analysis models Analyze existing data generate insights internal marketing product coaching ops member ops sales audiences Partner internal customers define experimental questions scope analytics investigation Own top notch first class computational science projects studying internal data start finish Perform exploratory targeted data analyses using descriptive statistics methods Translate data analytics outcomes clear visualizations understandable laymen Report analytics findings senior leadership inform key business decisions Solve challenging analytical problems requires working large complex data sets applying analytical methods including machine learning statistical approaches needed End end analysis including data gathering requirements specification processing analysis ongoing deliverables presentations Prototype build analytical pipelines using various data sources provide insights scale Interact cross functionally wide variety teams work closely identify opportunities improve platform Developing analytical solutions forecasting optimization methods improve quality user experience Areas include search quality product recommendation end user behavioral modeling user engagement pricing etc Make business recommendations e g cost benefit forecasting experiment analysis based rigorous analysis Work alongside software developers software engineers data engineers translate algorithms viable solutions Hold advanced scientific degree CS Engineering Applied Mathematics including training statistical analysis experimental design Skilled critically thinking research question optimal methodologies clear operationalization hypotheses Expertise hands experience statistical modeling machine learning software Python R etc Advanced knowledge database query languages SQL understanding relational algebra set theory Comfort common applied ML strategies ensemble methodologies e g train test validation sets feature engineering accuracy assessment Experience building simple backend data infrastructure pipelines Know connect dots data sources e g SaaS APIs ETL storage solutions better built analysis environments e g R Python scripting preferred required front end reporting e g dashboards apps Strong knowledge variety persistence layers e g SQL MS Oracle NoSQL MongoDb Neo4j prem cloud elastic otherwise Passionate storytelling data Ability port analytics outcomes interactive presentation based media e g D3 Shiny PowerPoint Keynote Understand technical basics data visualization common pitfalls Ability make clean elegant informative graphs knowledge ggplot2 visualization package Exceptional written verbal communication skills Ability present data science findings sizable audiences laymen specialists alike Outstanding leadership organizational skills Ability work collaboratively part fast paced customer oriented team Desire use create standards consider reusability long term view understanding short term needs Self starter provides ideas solutions understands various approaches within BI discipline", "", "Data collection data cleansing entity extraction Building dashboards reports Data Visualizations Statistical inference Doing ad hoc analysis presenting complex results non technical partners Working stakeholders understand questions apply data part solution Communicating results decision makers aid decision making Selecting features building optimizing classifiers using machine learning techniques Data mining using state art methods algorithms Integrating third party sources information needed Creating data applications measured KP driven Work team drive value data help define appropriate data governance security models serve multiple business outcomes A minimum years progressively responsible experience directly related area professional management capabilities clearly proven Experience common data science toolkits Python Tensor Flow Experience R Weka NumPy MatLab Julia required desirable Understanding machine learning techniques image processing algorithms k NN Naive Bayes SVM Decision Forests Waterfalls Experience NoSQL Graph database platforms CosmoDB preferred HortonWorks Cloudera MongoDB Cassandra HBase Neo4J etc Extensive expertise RDBMS systems like Oracle MySQL database well data modeling logical physical Extensive experience multidimensional data modeling Data Warehouse BI systems including star schemas snowflakes normalized de normalized models handling slow changing dimensions attributes Knowledge Time Series Databases associated algorithms technologies Great written oral communication skills Excellent verbal written visual presentation skills Extremely strong analytical problem solving skills Experience data visualization tools D3 js GGplot Tableau PowerBI etc Proficiency using query languages SQL Hive Pig Good scripting programming skills Good negotiating consensus building abilities Demonstrated skills work effectively across internal functional areas ambiguous situations Structured thinker effective communicator comfortable interacting staff levels organization Ability work independently establishing strategic objectives project plans milestone goals Minimum Bachelor Degree Computer Science Mathematics Statistics Data Science Advanced degree Applied Mathematics Business Analytics Statistics Machine Learning Computer Science Data Science plus", "", "Research develop AI Deep Learning algorithms electronic health care applications Implement neural network models e g CNNs RNNs machine learning algorithms statistical analyses Python using PyTorch TensorFlow scipy sklearn etc Work expertly Big Databases Maintain breadth knowledge latest developments AI Deep Learning industry best practices Clearly communicate design operation implemented algorithms along results using concise reports visualizations Contribute maintain documentation use developed tools machine learning cluster infrastructure software algorithms Use internal ticketing system tools report document work according CCI standards protocols", "completed scientific university studies diploma master data science computer science physics mathematics electrical engineering technical cybernetics geophysics comparable course studies doctorate comparable proof scientific qualification including relevant publications German English field data science applications respective methods engineering problems deep knowledge data science methods e g supervised unsupervised learning time series analysis time frequency domain deep neural networks time series prediction Bayesian networks profound experience application data science methods large heterogeneous data sets time location reference preferably Python several years professional experience research development e g field condition monitoring cyber physical systems system identification anomaly detection IoT concepts etc project experience science preferably experience acquiring managing nationally internationally e g EU funded research projects good German English language skills written spoken preferably good self organisation high level independent initiative processing complex scientific technical issues ability quickly familiarize unknown circumstances complex interrelationships including new tools methods data science welcome preferably practical experience agile working e g SCRUM", "", "Develop implement refine core marketing measurement frameworks models including attribution churn CLV ROI experiment methodology segmentation Play foundational role deciding best engage grow Etsy users determining helping execute opportunities growth analysis behavioral transactional data Transform raw data meaningful impactful analysis characterized strong data governance technique transparency aggressive documentation communication Identify key metrics drive performance marketing channels marketing products make accessible partners dashboards reports drive metrics statistical analyses experiments Continually evaluate refine technical toolkit teach learn team You years experience data scientist analyst quantitative role extracted meaning big data sets little engineering support Experience acquisition retention marketing operations channel attribution churn lifetime value models analytical tools infrastructure finding cost effective growth strategies plus You proficient SQL either R Python Bonus points experience Hadoop ecosystem additional scripting languages Scala PHP Ruby etc You communicate insights verbally visually writing You care deeply quality data integrity work You strong independent analytical thinker enjoys thinking solving complex nuanced problems You care reproducible work picked tools end teach others make work better efficient", "", "Real World Impact No project ever work across multiple sectors providing unique learning development opportunities internationally Fusing Tech Leadership We work latest technologies methodologies offer first class learning programmes levels Multidisciplinary Teamwork Our teams include data scientists engineers project managers UX visual designers work collaboratively enhance performance Innovative Work Culture Creativity insight passion come balanced We cultivate modern work environment emphasis wellness insightful talks training sessions Striving Diversity With colleagues nationalities recognise benefits working people walks life Healthcare Efficiency We helped healthcare provider improve clinical trial practices identifying congestion diagnostic testing key indicator admissions breaches Environmental Impact We designed built first data driven application state art centre excellence urban innovation collecting real time data environmental sensors across London deploying proprietary analytics find unexpected patterns air pollution Product Development We worked CEO elite automotive organisation reduce month car development timeframe improving processes designs team structures Learn successful projections real world problems across variety industries completed referencing past deliveries end end machine learning pipelines Build products alongside Core engineering team evolve engineering process scale data handling complex problems advanced client situations Be able focus modelling working alongside Data Engineering team focuses wrangling clean transformation data Learn best practices software development productionise machine learning working Machine Learning Engineering teams optimise code model development scale Work UX Visual Design teams interpret complex models stunning user focused visualisations Learn use new technologies problem solving skills multicultural creative environment Work complex extremely varied data sets world largest organisations solve real world problems Develop data science products solutions clients well data science team Write highly optimized code advance internal Data Science Toolbox Work multi disciplinary environment specialists machine learning engineering design Add real world impact academic expertise encouraged write black papers present meetings conferences wish Attend conferences NIPS ICML one global team well Data Science retrospectives opportunity share learn co workers Work within one largest advanced data science teams support Lead Data Scientists develop data science products", "Work customers capture data analytics requirements Develop verify validate analytics address customer needs opportunities Work alongside software developers software engineers translate algorithms commercially viable products services Work technical teams development deployment application applied analytics predictive analytics prescriptive analytics Perform exploratory targeted data analyses using descriptive statistics methods Work data engineers data quality assessment data cleansing data analytics Generate reports annotated code projects artifacts document archive communicate work outcomes Communicate methods findings hypotheses stakeholders Qualifications Requirements", "", "", "Demonstrated knowledge following software business intelligence data analysis data mining compiler decompiler user interface enterprise application integration operation job scheduling server network monitoring Knowledge design techniques tools principles involved production precision data models queries programs Advance knowledge SQL relational database management systems Demonstrated knowledge programming data modeling simulation well statistics mathematics Ability parse data common file formats csv xml files applying regular expressions mathematical transforming data information Ability communicate verbally writing information concepts methods non technical users Analyzes problems develops solutions involving computer software programs applications designing programs queries retrieve complex data databases one programming languages Evaluates project plans proposals assess feasibility issues Coordinates stakeholders including executives project sponsors IT Teams clinicians clinical operational staff determine analytic requirements project requests Applies theoretical expertise innovation create apply new technology", "", "", "Work team data scientists cross functional partners collaboratively develop solutions Apply ML statistical approaches generate insights structured unstructured data Provide ML expertise design delivery data products broad consumption business partners Mentor support training technical non technical teams data science machine learning Participate broader data science community stay current methodology software data development availability Communicate visualize output analyses including written verbal communication business leaders non technical audiences Conceptualize deploy data science solutions business questions", "", "Develop implement machine learning models Collaborate closely product business engineering teams identify data science opportunities solutions Conduct depth statistical analyses including A B testing Effectively communicate written verbally technical concepts across multiple teams Think creatively able test new ideas iterate rapidly", "Our biggest investment people like At Meredith offer comprehensive health benefits 401k matching plus career boosting opportunities like tuition reimbursement learning sessions attendance tech conferences help thrive For role Bizrate Insights expect fast paced excitement startup rock solid support Meredith industry leader nice perks side If passionate technologist get lot done fun come join team like minded skilled professionals learn share knowledge Participating activities across data science lifecycle business case definition data collection cleaning integration analysis visualization feature engineering modeling machine learning algorithms interpreting models results reporting Communicating coordinating effectively product managers software engineers analytical groups customer facing teams agile environment Working data processing pipelines analyzing gigabyte terabyte scale data multiple sources Designing actionable data products based statistical analysis machine learning methods working deployment enterprise systems move needle business Meredith Bizrate Insights Learning continuously areas data science machine learning ad technology marketing operations data ecosystems content operations social media analysis industry standard measurements Degree computer science engineering statistics quantitative fields Master degree preferred Solid understanding popular machine learning algorithms including supervised un supervised semi supervised ones What difference algorithms What algorithm would choose given problem What pros cons algorithms What cross validation hold sample RMSE overfitting bias Proficient one core data science programming languages Python R ability become functional emergent languages APIs needed Java Scala Practical experience building full cycle machine learning engine cleansing data training hyper parameter optimization methods evaluating accuracy identifying ways improve accuracy model Exposed experienced state art machine learning toolkits Spark ML TensorFlow scikit learn xgboost preferably Big Data frameworks e g Apache Spark A self learner capability switch independent contributor leader project collaborative contributor team vice versa depending demand project availability resources Collaborate stakeholders partners clients ensure parties aligned supportive project priorities requirements timeline objectives Familiarity MapReduce paradigm Apache Spark project Proficient SQL means gathering data relational databases Practiced data manipulation mining structured unstructured data Experience visualizations business stakeholders using variety visualization tools MS Excel libraries R Python", "", "", "", "Developing next generation AI protects privacy customers ensures modeling practices ethical bias free Bachelor degree four years work experience Coding proficiency Python Java R etc Knowledge AI ML models frameworks keras pytorch etc libraries packages apis e g scikit Bachelor Master degree PHD mathematics statistics physics engineering computer science economics Advanced knowledge math probability statistics models", "", "", "", "Manages project tasks timelines deliverables Manages mentors junior analysts contributing technical career development Manages relationships Merkle business units Develops analytical solutions client business problems Predictive statistical models Customer profiling Segmentation Analysis Campaign Analysis Data Analysis Mining Presents ideas findings clients Participates sales process solution development responses RFPs Demonstrate thought leadership", "Design implement effective data parsing acquisition using Big Data processing methodology Select refine develop features algorithms machine learning deep learning techniques solve various problems semiconductor manufacturing supplier chain environment Define execute data collection cleaning procedures Perform adhoc analysis deliver clear presentations Design test review monitor performance algorithms team shared ownership environment Develop software tools implement automation data management visualization advanced analytics", "", "Employ web retrieval text mining techniques facilitate collection publically available unstructured data Perform natural language machine learning statistical analysis methods classification sentiment analysis topic modelling regression statistical inference Develop statistical predictive models using software Apache Spark MLlib Python R Write regular research reports describe novel ways unstructured data machine learning approaches may enhance company macroeconomic insights A PhD Information Retrieval Natural Language Processing solid background statistical learning techniques e g CNNs CRFs HMMs LDA SVMs Experience Semantic Web RDF OWL SPARQL linked data taxonomies ontologies Strong programming skills least one object oriented programming language Java Scala C Python etc Excellent time management skills ability prioritize multi task work shifting deadlines fast paced environment Excellent oral written communication skills including ability present complex technical information generalist audience A track record publications one following ACL EMNLP COLING NIPS NAACL SIGIR ICML KDD WWW etc Work cloud big data technologies including Elasticsearch Hadoop AWS C2S others", "", "Define problems research solutions figure data use solution Query assemble clean refine datasets preparation analysis Analyze data using variety techniques order build proof concept solution Build high quality prototypes collaborate engineering team implement solution scalable framework Communicate findings stakeholders broader audience technical non technical co workers clients Extremely fluent Python Pandas Scikit Learn years work experience machine learning commercial setting Deep expertise statistics algebra geometry mathematical modeling Fluent data fundamentals including SQL data manipulation using procedural language Multiple pieces evidence successfully able scope deliver sell research prototypes Excellent written verbal communication skills quantitative topics variety audiences including clients technical non technical product managers engineers Experience NLP computer vision plus Willingness learn new tools become needed Bachelor degree higher natural sciences computer science mathematics Stock options Full benefits including medical dental vision insurance Working Environment Open office space SF equipped snacks working remotely optional company paid holidays generous vacation policy", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Develop test pricing strategies optimize profitability improvement revenue credit risk cost per customer acquisition Leverage pricing models simulate profitability pricing strategies forecast changes SoFi customer credit portfolio Build intuitive analytical models help decision makers understand price elasticity customer segments Run significance tests continuously discover trends customer behavior inform decision making high level confidence Build reporting pipelines dashboards deliver results pricing strategies business", "Collecting combining data multiple sources Uncovering exploring anomalous data including metadata Applying scientific process data evaluation performing statistical inference data mining Developing analytic plans engineer supporting algorithms design implement solutions execute analytic plans Designing developing tools techniques analysis Analyzing data using mathematical statistical methods Evaluating documenting communicating research processes analyses results customers peers leadership Completed degree program fields mathematics statistics computer science computational sciences passion rigorous analysis data Tenacity integrity persistence willingness learn Ability solve complex problems Use critical thinking reasoning make analytic determinations Works effectively collaborative environment Strong communications skills technical non technical audiences", "", "Marketing Strategy Qualitative quantitative market research services deliver unique reports marketing strategy insights advertisers Media Optimization Data science solutions facilitate efficient transparent profitable advertising placement media publishers Marketing Media Activation Connects Schireson audience segmentation capabilities campaign optimization technology allow clients better plan execute advertising campaigns Conducting data mining Building predictive models Developing proprietary algorithms assets clients Drawing key insights drive critical decision making Creating high quality data driven presentations Collaborating peers managers consistently exceed client expectations Participating full cycle strategic client analysis projects client facing kick code delivery implementation Answering key strategic client questions utilizing robust data analysis years hand experience quantitative analysis modeling technical coding work A desire work complex data sets predictive modeling machine learning projects alone Superb communication presentation skills Ability think strategically analytically proactively diverse business problems Passion managing quality accuracy analytics including checking others work Experience multi tasking fast paced fast growing small company professional services consulting firm environment plus Proficiency Python Experience software engineering best practices OOP Version Control functional programming Expertise SQL relational databases Familiarity cloud tools AWS plus Exposure big data tools Hadoop Hive Sqoop Pig Impala Spark big plus Experience advertising media entertainment plus", "Leverage data business principles create drive large scale FB Data Center programs Define develop program metrics creation data collection modeling reporting operational performance Facebooks data centers Work cross functionally define problem statements collect data build analytical models make recommendations Be self starter motivated passion developing best possible solutions problems Identify implement streamlined processes data reporting communication Use analytical models identify insights used drive key decisions across organization Routinely communicate metrics trends key indicators leadership Provide leadership mentorship members team Lead support various ad hoc projects needed support Facebooks Data Center strategy Build maintain data driven optimization models experiments forecasting algorithms capacity constraint models Leverage tools like R Tableau PHP Python Hadoop SQL drive efficient analytics", "", "", "Defines opportunities predictive analytics provide continuous improvement inspection process Defines opportunities introduction use real time closed loop predictive analytics drive continuous improvement inspection process Public Accounting Firms driving continuous audit improvement Designs develops implements advanced predictive analytical models applications address improvements business processes lead insightful forward looking solutions Utilizes machine learning techniques design build data models execute analyses Uses existing tools coordinate acquisition new tools identify analyze interpret trends patterns large complex datasets Identifies problematic areas conduct research determine best course action develops recommendations Uses experience visualization techniques drive model results depict relevant process trends Demonstrates analytic capabilities understanding correlation optimization time series analysis Effectively communicates complex findings actionable understandable result high level discussions management executives Demonstrates strong understanding analysis computational complexity ability programmatically solve", "Creatively leverage new existing data increase effectiveness efficiency risk infrastructure Work engineers design machine learning solutions operate effectively scale Partner operatives quickly respond rapidly evolving threats Apply good software development practices actively contribute production code", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Collaborating actively across functional partners senior management accomplish various project objectives Executing research analytics including data source identification processing model algorithm development Turning insights meaningful recommendations drive business value creation Ongoing coaching career development work passionate people access best class training P G Leadership Academy well day day mentoring manager We provide market competitive salary benchmarked finest companies able spend generous vacation time things love people love We offer suite benefits including limited flexible working arrangements generous paid vacation increasing service generous parental leave policies group life insurance health insurance dedicated support help find right child care elder care Additional perks include discounted P G products company shops discount platform offering unbeatable savings everything groceries exotic holidays What financial package might include things like interest free loans tax advantageous share purchase plan contributory pension plan financial education advisement topics including purchasing real estate generating wealth", "You comfortable data analysis wrangling curation You degree quantitative field coursework statistics e g Math Linguistics Physics Chemistry Engineering You feel home command line text processing You eager learn new technologies skills data processing analysis years full time work experience domain relevant internships Proficient Unix commands Ruby Python scripting Familiar regular expressions DOM CSS HTML parsing JSON information extraction Experience reporting analytics databases Experience implementing machine learning pipelines Experience Spark MapReduce", "", "", "", "Envision new network engineering AI ML projects present well formed ideas team Utilize considerable creativity merge ideation knowledge AI ML model uses insights SMEs translators constraints propose ML projects meaningful actionable Provide timelines milestones project plans types AI ML models attempted new prototype projects Adapt communicate needed changes datasets labels models may function expected Partner team non team data scientists teach complex concepts assess project feasibility select input features validate project output Envision test corner cases Utilize large amounts GPU effectively train attempt multiple models documenting progress Guide AI ML projects high autonomy accepted business SMEs use Pull sample data sets NS tools VGRID attempt prototypes new topics Learn wireless domain 5G LTE xLPT datasets RF Planning Orchestration etc toolsets better communicate understand needs engineers AI ML automation Publish blogs create documentation perform presentations submit intellectual property write ups lead efforts external publications Bachelor degree Electrical Engineering Computer Science four years work experience Six years relevant work experience A Degree Masters PhD Statistics Math Economics Engineering Computer Science Business Analytics Data Science related field Broad experience training multiple types AI ML models KNN XGBoost CNN RNN reinforcement learning deep learning models GANs etc Knowledge Relational databases SQL R Previous experience wireless networking plus Recognized contributor Software Organized Networks SON platforms Experience advanced AI ML ensembles deep learning reinforcement learning NLP Four trained models moved production completed research environment Experience Spark Big Data deployments Attached examples filing IP Published Papers Conference Presentation titles resume", "", "", "Work team data scientists cross functional partners collaboratively develop solutions Apply ML statistical approaches generate insights structured unstructured data Provide ML expertise design delivery data products broad consumption business partners Mentor support training technical non technical teams data science machine learning Participate broader data science community stay current methodology software data development availability Communicate visualize output analyses including written verbal communication business leaders non technical audiences Conceptualize deploy data science solutions business questions", "Analyze one interesting datasets world consists billion user actions trillion events per day Work product marketing engineering operations teams launch evolve high impact products Define metrics use analyze optimize new products Analyze feature adoption ensure developing products drive impact customers Help craft dashboards product scorecards forecasts keep entire team date Report senior management product adoption forecasting Research latest data science techniques apply product analytics methodologies", "Support conduct projects increase graduation retention rates reduce time degree support improved academic advising improve student learning course performance Help improve ability assess outcomes strategic planning decisions supporting development business intelligence dashboards monitor strategic goals key performance indicators Work campus stakeholders identify prioritize conduct analytical projects support university mission goals strategic plan Use appropriate data methods uncover fact patterns test hypotheses support progress towards campus goals Prepare high quality clear written reports exhibits visualizations present interpret findings Work campus stakeholders improve data quality usefulness including support development business intelligence dashboards monitor strategic goals key performance indicators Support operationalize assess student success initiatives including supporting committees charged student success persistence development assessment student advising tools case management tools nudging campaigns first year alerts Support implementation training effective use vendor provided analytics systems Support collaborative innovative use learning analytics tools Perform duties assigned", "", "Individuals role identifies addresses complex business problems Designs drives creation new data products analytical capabilities embedded multiple business applications Gathers analyzes large volumes data evaluates scenarios make predictions future outcomes supports decision making Comfortable using advanced statistical data modeling techniques tools Uses business acumen solution strategy roadmap problems make best practice recommendations Responsible using analytic techniques like advanced data visualizations machine learning Natural Language Processing large scale optimization improve Asurion wireless retail customers experiences Work closely product managers identify answer important product questions help improve outcomes Interpret problems provide solutions using appropriate data modeling techniques Develop prototypes new data product ideas using Machine Learning Deep Learning AI Design large scale models using Logistic Regression Decision Trees Conjoint Analysis Spatial models Time series models Machine Learning algorithms Utilize Natural Language Processing analyze speech social data Communicate findings product managers development groups Drive collection new data refinement existing data Analyze interpret results product experiments Regularly invents new novel approaches problems Takes initiative breaks barriers solve problems recognized within team source solutions Comfortable manipulating analyzing complex high volume high dimensionality data multiple sources A strong passion empirical research answering hard questions data", "Statistical Computing R NumPy SciPy Modern Programming Python Scala Java C Machine Learning Sub Specialties graph analytics natural language processing computer vision", "Partner cross functional teams focused product improvements insight Dive deep wide range data behavioral financial etc identify opportunities recommend solutions Contribute data team initiatives focused ensuring fast reliable comprehensive data Serve trusted consultant promote data literacy across company Work Redshift Python R JavaScript CoffeeScript MongoDB unblock data questions Experience requirement applying get exposure job Do best work fun respectful supportive environment You experience analyzing data business setting You know pick right approach question statistical models A B tests data visualizations You know sometimes simplest solution best You creative resourceful passion deriving insights data telling story You great collaborating non technical consumers data You curious business side well business You eager get guts customer data order answer questions across company You self motivated thrive working autonomously You know run something ask help You extremely proficient comes writing SQL Python R know write code answer complicated questions expose useful data You strive master technology language needed useful You fast learner adept prioritizing bring good opinions table You experience working ina consulting capacity", "", "Are passionate data science sample Do enjoy working even harder find solution within grasp Do enjoy balancing innovation applied project work clients Are naturally curious Do love puzzles Are intrigued becoming student successful individuals throughout world Do others call perfectionist Do seek find signal among noise Are dependable reliable Do upbeat positive attitude Do produce twice work around Do verify verify verify verify An opportunity work appreciate reward performance An opportunity work diverse portfolio clients variety industries A stimulating growth environment highly engaged company culture five time winner Great Place Work one best small medium workplaces United States An opportunity make difference clients insightful consultative research An opportunity express creativity", "", "", "", "Build production web apps improve merchandisers make decisions manage risk Innovate algorithms optimize merchandise portfolio Partner across business make impact bottom line client experience Contribute culture technical collaboration scalable development You experience working collaboratively modern web ecosystem You experience finding creative uses toolkits financial engineering operations research optimization etc desire grow DS ML generalist You generous ideas experience eager seek ideas experience others You believe things rarely black white multiple good paths follow multiple valuable perspectives consider You believe working better perfect innovative better incremental You relevant degree relevant industry experience We group bright kind goal oriented people You authentic self empowered encourage others We successful fast growing company forefront tech fashion redefining retail next generation We technologically data driven business We committed clients connected vision Transforming way people find love We love solving problems thinking creatively trying new things We believe autonomy taking initiative We challenged developed meaningful impact We take seriously We take seriously We smart experienced leadership team wants right open new ideas We offer competitive compensation packages comprehensive health benefits", "Begin developing relationships one level two levels Identify support opportunities expand project scope beyond current deliverable actively share information clients emerging support needs trends team members management", "", "", "", "", "Analyze results WSJ ongoing A B testing efforts Source query clean data required larger scale data science projects Experiment using techniques like machine learning power personalization products", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Previous message Jobs Please circulate widely Apply IIASA Young Scientists Summer Program fwd Next message Jobs Brown U Demography postdoc Messages sorted date thread subject author Previous message Jobs Please circulate widely Apply IIASA Young Scientists Summer Program fwd Next message Jobs Brown U Demography postdoc", "", "", "", "", "Previous message Jobs Northwestern University Postdoctoral Fellowships Global Comparative International Affairs Next message Jobs Fwd URGENT Demographer Vacancy Announcement posted Messages sorted date thread subject author Previous message Jobs Northwestern University Postdoctoral Fellowships Global Comparative International Affairs Next message Jobs Fwd URGENT Demographer Vacancy Announcement posted", "", "Assist building scalable machine learning pipeline Assist designing building robust data processing pipelines Build high volume services reliable scale Collaborate explain complex technical issues Product Project Leads Optimize enhance existing products years Python experience data science environment Experience building machine learning pipelines Knowledge machine learning algorithms applicability specific use cases Familiar primary component analysis feature extraction Comfortable Linux UNIX environment Honest Hungry Humble Happy Knowledge neural networks including TensorFlow Experience H2O Familiarity Java high performance SQL Collaborative work environment friendly people Beautiful office open floor plan huge windows Transparency share detailed financial info company meetings Investment portfolio Acorns contribution SteelHouse Competitive compensation healthcare coverage Open ended vacation policy annual vacation stipend 401k plan Flexible Spending Account FSA dependent medical dental care Weekly food deliveries food trucks outdoor BBQ", "", "Texas Health Resources E Lamar Arlington TX Full Time 1st Shift Minimum Maximum Please note salary determine number years experience", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Merchant ranking Determining merchants call first based multiple objectives User behavior modeling Predicting user propensity click conversion attrition Personalization Recommendation algorithms users merchants Forecasting Modeling historical data forecast demand purchase propensity merchant risk etc Marketing CRM Customer life stage modeling large scale advertising search engines social media customer acquisition continued engagement Exposure Being household name gives employees opportunity work startup like environment focus deeply interesting work get anywhere else large scale impact short amount time Professional Development Participate weekly lunch learns engineering business functions skill building workshops networking sessions Fun Explore city planned intern social outings develop network getting know peers Groupon leaders You currently enrolled MS PhD program Computer Science Software Engineering Math Statistics related field Preferred emphasis algorithms machine learning data mining statistics applied mathematics similar field You strong coding skills one programming language Java C Python Ruby etc You understanding applied math topics probability statistics linear algebra basic optimization techniques Experience SQL Hadoop R scripting preferred You excellent efficient communicator", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Helping farmers India get need insurance tough years Giving produce growers California tools optimize yields minimize waste Helping irrigated farmers Nebraska manage water efficiently sustainably protect water supply Perform deep dive analyses understand optimize environmental measurements Provide expertise statistical mathematical concepts production environment Develop implement state art analytical algorithms time series segmentation classification recognition Research develop prototype measurements real time crop monitoring decision agriculture Collaborate team members prototyping production rolling big data capabilities analytic frameworks best practices inside data science Advanced degree relevant computer physical science engineering discipline using technologies Substantial experience working strategy full life cycle data science Python well experience working data mining tools Python R Experience machine learning libraries xgboost sklearn Tensorflow Keras etc An understanding large datasets application calibrations analytics wide range environments implications scalability across globe Team experience specifically cross group collaborations outstanding communication skills Ability obtain work authorization United States", "Lead large portfolios data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development deployment AI solutions drive business growth better customer experience Contribute development AI ML strategy roadmap company Contribute research development AI ML techniques technology fuels business innovation growth Verizon Represent Verizon AI ML research industry publications conference speeches collaboration leading researchers universities Build strong influence among AI ML community senior business leaders actively promote effective applications AI ML technology Lead engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Work closely engineers deploy models production real time batch process systematically track model performance Bachelor degree four years work experience Master degree quantitative field related field Ph D Statistics Math Economics Engineering Computer Science Business Analytics Data Science Eight years experience practicing machine learning data science business Accomplished researcher expert machine learning neural networks reinforcement learning chatbot technology NLP Strong communication interpersonal influencing skills Excellent problem solving critical thinking capabilities Experience leading large scale data science projects delivering end end Strong computing programming skills Proficient Python Spark SQL Linux shell script Strong experiencein Big Data Cloud technology", "Be immersed team innovative high caliber computer scientists engineers machine intelligence experts collegial fun environment Work solving unique hard problems data science machine learning Artificial Intelligence value real world Create world class products solutions cutting edge concepts Build Comcast tools platforms vast technical resources Work highly visible dynamic team provides continuous opportunities learning growth Have access wide range data exciting projects technologies Join company strong commitment teams maintaining healthy work life balance providing top tier benefits program Research analyses architect develop emerging concepts related AI ML HMI Engage partner top tier universities companies research labs start ups across technology landscape explore new concepts theories Design develop analyze data systems methods tools technologies Apply machine learning deep learning NLP methods massive data sets complex systems Be respected SME acting internal technology consultant Senior leadership Comcast Design implement deploy full stack solutions millions Comcast customers Investigate solve exciting difficult challenges data science machine learning classification content analysis deep learning Research develop innovative scalable dynamic solutions hard problems Review analyze report technologies methods research trends Be valued contributor shaping future products services Collaborate witha cross functional agile team software engineers data engineers ML experts others address challenges head Help drive thought strategy process across Technology Products Experiences Bring experience designing developing implementing data pipelines systems solutions scale", "Working warehouse team ensure inventory management software implemented data well formatted easily accessible Working engineering team extract demand data online store Predict supply levels variable supply data Marry supply data demand data forecast shortfalls overages Produce business analytics dashboard view current future supply levels Identify patterns propose solutions Regularly meet supply team communicate understocked areas affect inbound supply Regularly communicate senior management analysis results", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Develop implement effective strategic business solutions research analysis data Harvest insights data present findings impact business objectives Facilitate effectiveness measurement test design implementation support post analysis new existing programs initiatives Assist managing day day results reporting visualization major corporate initiatives Analyze review forecast trend complex data Develop statistical models forecasts conduct statistical analysis better understand trends population segments predict behaviors outcomes", "", "Conduct recurring web reporting monitor drive KPIs Proactively develop site performance reports analysis segmentation analysis highlight observations business context deliver actionable recommendations business leads data Perform short term ad hoc analyses Translate business requirements Solution Design Document used implementation support guidance documentation Participate design set evaluation A B multivariate testing Ensure quality timeliness deliverables meet expectations balancing business needs appropriate level analytical rigor Provide insights support senior management decision making Complete maintain distribution web analytics reports including contextualization Bachelor degree business finance economics statistics related field Expert proficiency web analytics tools including Omniture Google Analytics Strong business acumen ability present senior management Strong quality assurance skills Must detailed oriented logical problem solving skills Outstanding organizational skills dedication quality integrity Clear understanding basic financial statistical economic concepts Ability work collaboratively acting subject matter expert within team environment help define meet measurement criteria goals Must self motivator enjoy sifting large amounts data Ability contribute effectively fast paced environment limited supervision Benefits package Veterinary discounts Website retail discount Bring pet work Friday High tech environment Bachelor Required Union City CA Preferred", "The Senior Data Scientist anticipates future business needs identifies opportunities complex analysis The Senior Data Scientist gather analyze data solve address highly complex business problems evaluate scenarios make predictions future outcomes provide prescriptive solutions support decision making The Senior Data Scientist involved phases Big Data analytics projects including question formulation research development implementation testing The Senior Data Scientist able explore understand data build advanced analytical models present discuss resulting models level audience The Senior Data Scientist design drive creation new standards best practices use statistical data modeling big data optimization tools Cox Automotive Identify direct special studies analyses unique business problems scenarios Proactively identify algorithms products high intellectual property content assist evaluation potential patents appropriate assist patent applications contribute intellectual property protection Cox Automotive Development research exploration areas statistics machine learning experimental design optimization simulation operational research Interprets problems develops solutions business problems using data analysis data mining optimization tools machine learning techniques statistics Leverage big data solve strategic tactical structure unstructured business problems Collaborate client Enterprise Data Products team set analytic objectives approaches work schedule Research evaluate new analytical methodologies approaches solutions Analyze customer economic trends impact business performance recommend ways improve outcomes Developing advanced statistical models utilizing typical atypical methodologies Developing updating data models statistical modeling purposes tracking results forecasts specifying required Design deploy data science technology based algorithmic solutions address business needs Cox Automotive Identify understand evaluate new commerce analytic data technologies determine effectiveness solution feasibility integration Cox Automotive current platforms Design large scale models using Logistic Regression Linear Models Family Poisson models Survival models Hierarchical Models Na\u00efve Bayesian estimators Conjoint Analysis Spatial Models Time Series Models Design large scale models using linear mixed integer optimization non linear methods heuristics Design large scale discrete event Monte Carlo simulation models Interpret communicate analytic results analytical non analytical business partners executive decision makers Develop coach mentor team members within department", "", "Synthesize raw data recommendations products evolve Build machine learning algorithms enable decision making order optimize customer experience reduce cost maximize profits customer value Work product management engineering operations teams develop KPIs new features instrumentation quantify adoption new product initiatives Quantify value existing product improvements potential value new product hypotheses Design support large scale multivariate tests prove new ideas Create product scorecards teams managers track product performance business impact discover opportunities Deep dive analyses identify root causes product operational issues", "", "", "Work within administrative existing legacy systems extract data consumption models Create bounds model accuracy may result data vintage manage perceived accuracy model based historical data quality Identify opportunities enhance Operations businesses using econometrics advanced analytical methods perform analyses deliver findings recommendations Work complex models answer business optimization questions LTC Life Insurance Operations forecasting basis well ad hoc basis meet changing business demands Work closely partners Actuaries Ops Leaders Senior Leaders Shared Services model business problems dealing Operations make clear recommendations Explain clearly impact Ops decisions process changes partner areas vice versa Collaborate thought leaders functional leadership within Insurance Operations around effective use models address business needs coach teach partner areas creation ownership models address business needs", "Collaborate AI Center Data Scientist teams senior leaders ensure model architecture data architecture roadmaps designed scale beyond business area Anticipate connections dependencies risks scaling plan design phase opposed solving road solutions production could far expensive Design develop deploy large scale AI products closed loop optimization production environment Find ways optimize improve AI products looking across various business applications making connections may visible teams working specific problem space Experiment bring relevant data science techniques algorithms tools organization embed ways working data science teams Operate thought partner Enterprise Architecture team overall enterprise northstar architecture ensure AI products platforms requirements accurately represented business technology applications roadmap Lead designing AI protects privacy customers ensures modeling practices ethical bias free Bring thought leadership space introducing ways identifying testing biases models Introduce new methods protect confidentiality data Operate expert within Verizon externally within industry Serve technical thought leader advisor organization Serve trusted advisor business partners Attract top talent deep technical expertise Verizon Partner academia startup ecosystem identify potentially bring new AI applications value Verizon Stay informed latest advancements AI technology space finding ways deliver value applying customizing specific problem space Help create environment scientists engaged pioneering work shared world number ways like open source publications etc", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Design develop tools enhance current future data requirements multiple data sources Interact project stakeholders provide solutions addressing business data analytic requirements Ensure development requirements clearly understood work consistent requirements Provide accurate completion deadlines Work SQL database access tools methods address business technical requirements Design implement support data movement solutions across multiple platforms data sources Database tables views external files using custom code ETL tools frameworks Contribute UI modernization actively developing functionalities participating design discussions Conduct PL SQL shell scripting advanced SQL developments Ability generate modify version database object scripts DML DDL Become proficient data model data structures able efficiently respond business queries requests Provide ongoing maintenance support database schemas views Conduct root cause analysis resolve production problems data issues Work within defined software development lifecycle following rigorous change control policies Create maintain date documentation data model data flow field level mappings Bachelor degree years experience database developments various transactional analytical database systems Significant experience development process hands experience basic design structured programming work reviews testing cycles unit acceptance testing release build implementation post release bug issue fixes Excellent information management skills architecture design development support ETL Database Reporting Big Data Extensive experience programming analytic models using R Python Java MATLAB SAS programming language Experience modeling behavior financial instruments either statistical machine learning methods Advanced proficiency Oracle SQL e g complex views joins analytical functions regular expressions table partitioning materialized views distributed transactions Oracle SQL Performance Tuning Advanced proficiency PL SQL Packages Functions Procedures programming debugging Oracle Collection Object Types Experience Oracle Data Pump SQL Loader external tables framework Experience working large volumes data relational dimensional data models Good understanding Oracle Database Architecture Experience Oracle development tools TOAD Oracle SQL Developer Source Control Software Advanced knowledge scripting Unix Linux environment Experience business intelligence reporting tools SQL query Business Objects data integration tools Informatica etc metadata repository processes tools Experience financial reporting plus Experience Oracle Application Express plus Experience Snaplogic Talend ETL plus Must resourceful self motivated comfortable working pressure unstructured environment little supervision Strong written oral communications skills Proven ability initiative learn research new concepts ideas technologies quickly Strong systems process orientation demonstrated analytical thinking organization skills problem solving skills", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "Prototyping optimization solutions advertising applications Developing implementing machine learning algorithms Researching building new analytical solutions Developing methods advertising attribution analyses Data visualization analysis Data processing statistical algorithm development Graduate degree Statistics Operations Research Applied Math Engineering Computer Science quantitative discipline preferred Experience implementing optimization methods Experience conducting attribution analyses Experience Media Math Economics Experience R Python plus Experience Machine Learning Implementation Experience working relational databases SQL experience plus Excellent oral written communication skills must Must highly analytical ability resolve complex issues independently Must detail oriented passion providing informative documentation Strong interest TV advertising advanced media platforms", "", "", "Build data products solve hard problems reveal opportunities creative sourcing model development visualization Four years experience data science role concentration social sciences demographic analysis anthropology Experience creation visual data displays Expert Python R Scala MA Applied Mathematics Computer Science Engineering related field experience Self taught coders welcome Bring portfolio impress us Credentials machine learning AI probabilities statistical inference linear models Familiarity Unix shell scripting", "", "Lead large portfolios data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development deployment AI solutions drive business growth better customer experience Contribute development AI ML strategy roadmap company Contribute research development AI ML techniques technology fuels business innovation growth Verizon Represent Verizon AI ML research industry publications conference speeches collaboration leading researchers universities Build strong influence among AI ML community senior business leaders actively promote effective applications AI ML technology Lead engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Work closely engineers deploy models production real time batch process systematically track model performance Bachelor degree four years work experience Master degree quantitative field related field Ph D Statistics Math Economics Engineering Computer Science Business Analytics Data Science Eight years experience practicing machine learning data science business Accomplished researcher expert machine learning neural networks reinforcement learning chatbot technology NLP Strong communication interpersonal influencing skills Excellent problem solving critical thinking capabilities Experience leading large scale data science projects delivering end end Strong computing programming skills Proficient Python Spark SQL Linux shell script Strong experiencein Big Data Cloud technology", "Analysis validation data within Oracle SSIS platforms Understand write debug complex SQL logic large tables Develop maintain Webi Reports Dashboards SAP Business Objects Ensures adherence locally defined standards developed components Performs extensive data analysis validations Source Target tables columns Maintain technical documentation supported platform applications Supports development design internal data integration framework Participates design development reviews Works System owners resolve source data issues refine transformation rules Collaborate proactively DBA teams triage performance issues Ensures performance metrics met tracked Write maintain unit tests Support QA Reviews", "", "", "", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "Ideation data models modeling e historical well predictive models Working Games team partners understand needed KPIs addition selected BI tools provide Help design monitor A B tests Ideate cross promotion campaigns Work across teams identify needs translate modeling mandates Explain player behavior campaign results", "", "Design experiments create case studies apply findings production code Employ predictive modeling data mining graph algorithms data science techniques project end goal increasing platform ability drive customer value sustaining healthy margins Responsible implementing data mining statistical machine learning solutions various business problems lead scoring demand forecasting target marketing segmentation Model train improve machine learning algorithms audience discovery apply bid optimization strategies real time bidding Provide analytical technical leadership team environment design developed analysis systems extract meaning large scale data Advanced degree MA MS years experience applied Machine Learning techniques advanced analytics statistical modeling Strong background foundational mathematics statistics probability linear algebra Experience manipulating large data sets using SQL Hadoop MapReduce Hive Understanding programming concepts experience programming languages Python MATLAB R Java Scala C", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Work team data scientists cross functional partners collaboratively develop solutions Apply ML statistical approaches generate insights structured unstructured data Provide ML expertise design delivery data products broad consumption business partners Mentor support training technical non technical teams data science machine learning Participate broader data science community stay current methodology software data development availability Communicate visualize output analyses including written verbal communication business leaders non technical audiences Conceptualize deploy data science solutions business questions", "", "Be Subject Matter Expert aspects data analytics applications descriptive prescriptive solutions Design build deploy maintain highly scalable analytical solutions support running operations make efficient effective decisions Deep broad knowledge application advanced analytical techniques solve business problems Identify additional sensors include missing information relevant building analytics systems Use data analysis identify interactions process product quality leverage process capability optimize quality identify process enhancements improve product quality consistency Validates mathematical modeling results design experiments empirical data manufacturing processes pilot studies lab experiments Works cross functional teams scientists engineers operations R D manufacturing process data infrastructure Communicates factory floor business leaders sees options uniquely across businesses ability interface collaborate business sensitive manner flexible unbiased Understands manufacturing data manufacturing process fundamental meaning process measurements Understands applied physics enthalpy heat transfer etc Develops solutions understood used operations extract insights make decisions Standardizes simplifies improves user friendliness analytics platform widespread use tools across OC Stays current respect statistical mathematical methodology maintain proficiency applying different methods justifying methods selected Leads assessment introduction new statistical mathematical technology methodology apply broader practice Commits evolving projects managed match business project needs change management challenges communication requirements successfully deliver impactful results Defines develops communicates technology standards manufacturing analytics Builds community growing capability within Owens Corning personal development mentoring teaching opportunities peers", "", "Own definition tracking important metrics regarding adoption utilization impact Xpring initiatives How know making impact Be expert technical deep dives emerging blockchain technologies DeFi instruments novel scaling technologies digital exchange performance Understand model digital assets liquidity support different applications use cases BTC ETH XRP beyond Identify collect important datasets support analytics data science research across Xpring Ripple You significant hands experience applying quantitative methods real world data big small You experienced Python R SQL cloud data services You love delivering tools APIs addition presentations dashboards You collaborative coder comfortable Git code reviews You excellent written verbal communication skills You excited blockchains technical details potential benefits The chance work fast paced start environment experienced industry leaders A learning environment dive deep latest technologies make impact Competitive salary equity paid medical dental paid vision insurance employees starting first day 401k match fully paid parental leave commuter benefits Generous wellness reimbursement weekly onsite programs Flexible vacation policy work manager take time need Employee giving match Modern office San Francisco Financial District Fully stocked kitchen organic snacks beverages coffee drinks Weekly company meeting ask anything style discussion Leadership Team", "Use data inform influence direction team roadmaps inform business decisions Work partner teams define goals identify metrics improving existing features new releases Deepen understanding patterns trends user behavior design experiments uncover new opportunities Build dashboards reports drive awareness understanding metrics experiment results Work closely Data Engineering IT author develop core data sets empower operational exploratory analyses", "Develop data science methodologies classify images Utilize data science tools Python TensorFlow Keras etc Work part team evaluate refine new modeling approaches Implement train deploy Deep Learning Convolutional Neural Network models Student pursuing MS PhD degree Computer Science Computer Science Engineering Electrical Engineering experience applied image processing machine learning Strong interpersonal communication skills Highly analytical imaginative thinking ability Demonstrated skills Python TensorFlow Keras Experience Linux systems", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Work across massive enterprise email datasets Deploy models production impacts directly customers products Improve ML workflow Collaborate engineering product leaders develop product suite continually detecting advanced threats customers Make mark leading ambitious projects Mentor knowledge share data scientists Tessian years Data Science experience Has strong NLP experience production Has worked understands principles behind working data scale Can balance longer term projects alongside impactful quick wins Cares deeply impact team hasIs creative heart encourage novel ways thinking team This always core part data science team DNA Reads Rules Machine Learning Martin Zinkevich resonates think machine learning data science It important us Tessians part journey offer equity options every role benchmark provide market rate salaries plenty days paid holiday plus bank holidays additional day every year worked Tessian Private health insurance provided Vitality Health mental health support Employee Assistance Program Classpass subsided access gym time classes across London Choice First Do best work way works best Flexible working hours working home already remote Work home subsidy upon joining kit home office Enhanced pension contributions matched We family friendly policies built support stages life High quality tech kit provided work plus Tessian ANC headphones If relocating join team provide contribution help costs Elite membership Tessian House System Every Wednesday stop share team updates drinks", "", "Advocate evangelize build data driven design manufacturing processes drive efficiency Develop process design space exploration framework enables design parameter sensitivities used inform initial design electromagnetic induction heating processes large scale deformation structural mechanics Research design implement validate cutting edge algorithms analyze diverse sources data achieve targeted outcomes Provide insight leading analytic practices design lead iterative learning development cycles ultimately produce new creative analytic solutions become part core deliverables Construction isogeometric sensitivity analysis surrogate models enable rapid design space exploration Develop numerical techniques optimize steady non steady forming processes utilizing inverse problem formulation together evolutionary search schemes Application isogeometric analysis model elastic deformation electromagnetic heating multi phase metallic materials Clear communicator strong ability communicate insights clear concise valid data driven way others company effectively act insights Strong desire fast paced data driven collaborative iterative engineering environment Strong love learning especially complementary physical sciences including Mechanics Physics Material Science Excel making complex concepts simple easy understand around Passionate asking answering questions large datasets able communicate passion product managers Manufacturing managers engineers", "", "", "Applies machine learning deep learning artificial intelligence techniques Develops implements Markov decision process models healthcare economic models Uses advanced analytics methods extract value business data Performs large scale experimentation builds data driven models answer business questions Creates hypotheses experiments identify hidden relationships construct new analytics methods Articulates vision roadmap utilization data valued corporate asset Visualizes information develops reports results data analysis Influences product teams presentation data based recommendations Spreads best practices analytics product teams", "Explore client data holdings establish relationships identify connections across disparate systems Determine statistically significant patterns data identify measurements statistical validity Develop statistical models predict relevant results Design execute experiments compare performance different systems", "Utilize analytical statistical programming skills collect analyze interpret large data sets Able understand break explain complex algorithms Conducts advanced analytics leveraging predictive modeling machine learning simulation techniques deliver insights develop analytical solutions better manage compliance risk Lead projects enhance data analysis drive predictive analytics Research explore data sources times work data systems experts understand data represents Build strong working relationships translate complex analytical technical concepts non technical employees enable understanding drive informed decisions Familiarity Machine Learning Algorithms willingness learn skills interest data mining large datasets insights extraction Present findings format understood actionable business partners stakeholders", "", "Bringing relevant data central system create single version truth Creating key metrics reports dashboards support business health Opportunity analysis hypothesis generation stages throughout end end user lifecycle Building advanced analytical models behavior segmentation churn prediction purchase propensity recommendation engine etc spans engineering marketing partners finance Design prototype implement test descriptive predictive analytics models Work data engineers architect develop operational models run scale Partner teams identify explore opportunities application machine learning predictive analysis", "", "Working directly clients develop analytic questions define data technology opportunities Driving exploration data sources analytic techniques create new variables modeling capabilities Working large complex data sets solve difficult non routine problems Applying variety statistical methods approaches develop analytic models", "", "Parameterization Abstraction Use data driven methodologies discover develop abstract key security malware metrics across multiple models network segments support network traffic due malware security threat evaluation analysis including evaluation alternate threat detection network bandwidth consumption due malware models future business scenarios Modeling Architect Design Build Support data models using real time historical analysis approaches support security business technical planning decision making", "years financial industry experience Natural programmer demonstrated industry experience statistics data modeling Extensive experience written oral communications presentations ability produce variety business documents business requirements technical specs slide presentations etc demonstrate command language clarity thought orderliness presentation You responsible providing Project Program Management oversight direction Artificial Intelligence analytics projects across client Institutional You work variety partners stakeholders help identify execute track progress multiple projects time As part Product organization also opportunity work closely Product Managers various aspects product rollout", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "", "BA BS degree years experience data scientist MS PhD years industry experience quantitative role Fluency SQL Python R data analysis Solid understanding statistical inference experimental design analysis Enthusiasm clean code sharing reproducible results Communication skills work partners engineering product business teams An eye great data visualization Matplotlib Plotly ggplot Tableau Define key metrics track Yelp performance inform product decisions Assess frame questions partners actionable deliverables Design execute analyze complex experiments impacting millions users Devise evaluate models diverse business needs identifying growth opportunities personalizing user experience matching consumers businesses Own analyses start finish communicate key insights stakeholders", "Interact advise Management regarding credit risk issues formulate product strategy recommendations evaluate risk overall loan portfolio Autonomous end end statistical model creation Including limited identifying objectives compiling data sampling prepping data feature selection model comparison selection deployment monitoring Ensure adequate internal control processes around model development implementation validation established Develop monitor maintain custom risk scorecards Recommend implement model changes improve performance credit functions Apply intermediate advanced knowledge financial processes procedures routine modeling theories techniques create effective modeling solutions single multiple business functions", "", "", "", "You currently enrolled BS MS MBA program equivalent Computer Science Mathematics Economics Information Management Statistics related field You earned minimum GPA You know work problem beginning end data science tools techniques including data manipulation SQL Hadoop etc programming R Python XML Javascript ETL frameworks You experience Oracle databases strong knowledge experience reporting packages Business Objects etc knowledge statistics analyzing large datasets Excel SPSS SAS etc You possess technical knowledge regarding data models database design development data mining segmentation techniques You strong analytical skills ability collect organize analyze disseminate significant amounts information attention detail accuracy You proficient queries report writing presenting findings", "", "Works stakeholders throughout organization identify opportunities leveraging data drive business solutions Munges cleans prepares data Mines analyzes data state databases drive optimization improvement program development marketing techniques business strategies Assesses effectiveness accuracy new data sources data gathering techniques Performs exploratory data analysis order understand state data Interprets results multiple sources using variety techniques ranging simple data aggregation via statistical analysis complex data mining independently Develops analytic predicative models business needs Coordinates different functional teams implement models monitor outcomes Create meaningful visualizations communicate information answers Performs related duties assigned", "", "", "Design develop maintain software solutions business problems using data engineering machine learning techniques Collaborate business engineering teams Assist train mentor members data team needed Other duties assigned Chief Technology Officer Experience recommendation systems machine learning algorithms linear regression SVM decision trees clustering algorithms Experience Python R Experience reading cross disciplinary research applying unique problems Proficiency SQL Bachelor higher degree Mathematics Engineering Economics Statistics Knowledge key KPI important ecommerce business Ability work team individual environment Experience Java Scala object oriented development plus Knowledge distributed computing Spark Hadoop plus We relatively small rapidly growing team We like cats Dogs okay We generally build buy technology focused group built maintains manages Wantable homegrown software solutions solve business needs We work reasonable weekly sprints following agile process New emerging technologies modern techniques finding new solutions excite us We hard workers let get way good time We make consistent meaningful impact direction health business We believe transforming way people shop combination technology experienced stylists First days Become familiar Wantable business model data structure existing machine learning initiatives Begin working python R code bases First days Establish standards practices data analysts use Tableau Complete project within python code base Become fully involved integrated active machine learning projects First months This things start get interesting Once established successes position move building tuning maintaining reporting success recommender systems natural language processing data science able use improve customer employee experience One year beyond", "", "", "Develop statistical algorithms large data sets Apply machine learning recommendation systems techniques digital marketing use cases Design A B testing experiments end end data analysis Effectively communicate teams tell stories data Experience statistical analysis algorithm development data mining machine learning Experience Python R SQL Excellent communication collaboration skills Masters degree relevant field", "Conduct financial data analysis large datasets generate profitable trading alphas Define key metrics model validation Back testing production performance monitoring Contribute house data analysis packages research framework development Acquisition new data sets cleaning maintenance new existing data sets", "You move needle We tools data truly interesting work Data Science We tackling problems drive global business There exciting journey ahead going fun along way years experience applying advanced AI techniques machine learning predictive analytics optimization semantic analysis time series analysis advanced visualization real world problems You demonstrated ability iteratively conceptualize design build data driven analytical models taken project leadership role shaping solutions Strong capabilities modern analytics languages tools Balancing breadth also bring particular depth least one following areas Deep Learning Optimization Operations Research NLP Image Processing Machine Learning Given problems tackling place Noodle experts various fields You advanced degree relevant field Computer Science Operations Research Statistics Applied Math Electrical Engineering Computational Science You experience manipulating preparing large heterogeneous data sets Big Data support advanced analytics Collaborative open respectful working style Intellectual curiosity hunger humility always keep learning share learning You enjoy mentoring intellectually curious data scientists Experience excitement interdisciplinary collaboration", "Leads organization planning GEODIS data model data architecture continual redesign data model transition common model Recommends strategies management approaches optimize business success based changing business needs Evaluates applicability leading edge technologies uses information significantly influence future business strategy Analyzes complex business competitive issues discerns implications systems support including identifying defining directing performing project issue analysis technical economic feasibility proposed data solutions Designs projects proper tools methodologies successfully address cross technology cross platform issues within business future architecture organization Proposes leads projects required support development organization data infrastructure needs provides intelligence advances database technologies Develops partnerships senior users understand business needs define future data requirements Interacts internal external audiences develop effective networks ensure business needs met Other duties required assigned Master degree Data Science Computer Science Statistics Business Administration Minimum years related experience training equivalent combination education experience Experience analytical data mining efforts could include limited clustering segmentation logistic multivariate regression decision CART trees neural networks time series analysis sentiment analysis topic modeling Experience optimization algorithms Bayesian Optimization Genetic Algorithm Particle Swarm Optimization etc Knowledge Experience modeling ability implement models model components scratch Experience using statistical computer languages Python R etc PC literate experience Microsoft Outlook Word Access Excel Excellent written oral communication skills demonstrate proven ability develop relationships range internal external customers Excellent planning organizational skills Ability read interpret documents safety rules operating maintenance instructions procedure manual Ability write routine reports correspondence", "Build production fraud credit machine learning models models decide lend real time Passion drive change consumer banking better years industry experience PhD related field years experience Deep understanding experience machine learning data analysis Strong programming ability preferably python", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "", "", "", "Collaborate scientists researchers one following areas data intensive applications text processing graph analysis machine learning statistical learning information visualization low level data management data integration data streaming scientific data mining data fusion massive scale knowledge fusion using semantic graphs database technology programming models scalable parallel computing application performance modeling analysis scalable tool development novel architectures e g FPGAs GPUs embedded systems HPC architecture simulation evaluation Work LLNL scientists application developers bring research results practical use LLNL programs Assess requirements data sciences research LLNL programs external government sponsors Carry development data analysis algorithms address program sponsor data sciences requirements Engage developers frequently share relevant knowledge opinions recommendations working fulfill deliverables team Contribute technical solutions participate member multidisciplinary team analyze sponsor requirements designs implement software perform analyses address requirements Develop integrate components web based user interfaces access control mechanisms commercial indexing products creating operational information knowledge discovery system", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "You using heavy statistical econometric background continue enhancing state art credit model Work directly vendors source evaluate cutting edge data inclusion model Aggregate data variety datasets automate data collection processing Develop optimization algorithms guide integration existing technology Communicate analytical findings credit model finance team executive leadership Work Data Engineers ensure models implemented correctly efficiently years experience quantitative analysis proven experience delivering results research analytics projects B S quantitative discipline Math Statistics Economics Econometrics Graduate degree heavily preferred Proven fluency SQL Experience least one statistical package R SAS MATLAB etc Strong understanding econometrics predictive modeling methods knowledge machine learning techniques plus Knowledge programming languages e g Python Java Net experience large data platforms e g Hadoop plus Ability prioritize manage projects completion without guidance Strong communication skills A full time salaried position Medical employee medical fully paid Carvana Dental Vision benefits A 401K company match All perks heart desires gym snacks iced coffee tap", "Master degree equivalent experience technical field data science urban planning computer science economics finance mathematics years experience demonstrated skill data science similar field using advanced analytic methods tools Solid understanding proficiency Python pandas numpy scipy scikit learn R SQL Efficient manager workflows processes proven ability manage multiple competing projects priorities deadlines Proficient statistical analysis data management presentation Ability write clearly succinctly variety formats e g email technical documentation instructions Effective variety communication settings one one groups peers supervisors staff clients person long distance Experience applying interpreting travel forecasts particularly around transit usage Proficiency using network analysis software TransCAD Cube EMME VISUM associated scripting languages Solid understanding experience applying open source GIS packages e g OSGeo QGIS GeoPandas etc Experience software development tools GitHub version control issue tracking testing etc Understanding statistical analysis database software packages e g SQL SPSS scikit learn R SAS Demonstrated ability manage projects successfully Solid background formal informal urban planning theory Spanish Proficiency Fluency employee owned company employee stock ownership plan Award winning culture workplace flexibility http www whenworkworks org workplace awards rsg http reviews greatplacetowork com rsg Competitive k matching medical HSA dental vision disability insurance coverage", "Build scalable backend data applications support growing needs business Build data pipelines collect process compute business metrics marketplace activity Leverage best practices continuous integration delivery Collaborate engineers Expertise building data training pipelines using Spark Hadoop Experience building production grade API expose results Extensive programming experience Python Scala Java A BS MS Computer Science related technical fields At least years experience production environment Experience interest ML Qualified applicants receive questions answer Selected candidates invited schedule minute intro call CTO Next Candidates invited schedule behavioral interview CEO Next candidates invited schedule technical interview CTO The technical interview split technical discussion technical test Next candidates invited review technical test CTO software engineers Candidates invited schedule additional interview CEO CTO COO Successful candidates subsequently made offer via email", "Define key metrics track Yelp performance inform product decisions Assess frame questions internal partners stakeholders actionable deliverables Design execute analyze complex experiments impacting millions users Devise evaluate models diverse business needs identifying growth opportunities personalizing user experience matching consumers businesses Own analyses start finish communicate key insights stakeholders Share technical skills develop maintain high quality reusable analysis tools BA BS degree years experience data scientist MS PhD years industry experience related quantitative role Fluency SQL Python R data analysis Solid understanding statistical inference experimental design analysis Enthusiasm clean code sharing reproducible results Communication skills work partners engineering product business teams", "Design build launch new data models TV usage associated online activity Apply expertise quantitative analysis data mining presentation data see beyond numbers understand users interact products Partner Product Engineering teams solve problems build products based data identify trends opportunities Mine massive amounts data extract actionable insights inform influence support help execute product decisions product launches Design implement reporting metrics track monitor performance products quality data overall health business Data Infrastructure Work SQL Map Reduce SQL databases Design build launch new ETL processes Build data sets empower operational exploratory analysis Automate analyses Product Operations Leadership Design evaluate experiments monitoring key product metrics understand root causes changes metrics Build analyze dashboards reports Influence product teams presentation findings Communicate state business experiment results etc product management teams Exploratory Analysis Statistical Modeling Understand ecosystems user behaviors long term trends Identify levers help move key metrics Evaluate define metrics Build models user behaviors analysis powering production systems Identify correct biases errors data sets", "", "Create predictive analytics using latest machine learning algorithms relatively big data hundreds features million records Provide analytic insights partners throughout lifecycle campaign build review recommend custom models provide real time visualizations analysis campaign work produce post campaign analytics help partners constantly learn efforts Work Senior Data Engineer help build optimize predictive analytics pipeline perform well large datasets stored Amazon Web Services AWS Help develop maintain Python analytics code base build automate custom analytic products Analyze performance production machine learning models Work closely data engineering team analyze augment data PhD political science sociology mathematics statistics related field years experience building production level machine learning models large datasets Excellent general data science skills data cleaning munging data visualization Excellent Python skills additional experience R desired Experience Amazon Web Services particular applying analytics setting Experience identifying appropriate statistical techniques applying variety real world datasets Strong problem solving skills well ability manage several tasks projects concurrently prioritize work effectively Experience databases knowledge SQL preferred", "", "Identify critical business problems create analytical modeling solutions Maintain right balance speed market analytical soundness designing solutions Translate complex findings results compelling narrative Define evaluate key metrics understand moves Investigate challenging questions around user experience understand voice user Ownership conceptualizing developing maintaining dashboards visualizations Communicate analyses recommendations cross functional stakeholders decision making Strategize execute making analyses easily repeatable accessible", "", "", "Develop scalable solutions using state art machine learning models Draw meaningful insights large datasets define key metrics track performance develop actionable solutions tackle business problems Use Dow Jones terabyte scale data lake identify opportunities enhance understanding Dow Jones customer base content consumption Use visualization tools present complex model business insights simple engaging manner business stakeholders Work closely engineers develop deploy scalable machine learning applications Identify opportunities new data projects Ask smart questions diagnose real problem develop long term solution", "Work data analytics team define implement machine learning solutions advanced analytics use cases Fortune clients Always curious data constantly asking questions extracting key insights understand domain Manage data received multiple sources cleanse data needed Integrate data multiple sources develop data storage solutions Develop data analytics model descriptive predictive analytics using client provided data Manage data quality feedback loop issues corrected timely manner Bachelor Master Degree Computer Science Engineering Mathematics related field preferred At least year Data Analytics IT implementation project experience Software experience R Python Understanding machine learning techniques algorithms k NN Naive Bayes SVM Decision Forests etc Able apply statistics skills distributions statistical testing regressionetc understanding key features data Familiar data cleaning processing techniques via R Python Automotive IoT architecture solution experience An entrepreneurial mindset ability work independently within team The ability think critically understand complex processes", "At least years experience Data Analysis Predictive Analytics Machine Learning At least year experience least one programming language Java Python etc At least year experience least one statistics data analysis package Python R", "Lead large portfolios data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development deployment AI solutions drive business growth better customer experience Contribute development AI ML strategy roadmap company Contribute research development AI ML techniques technology fuels business innovation growth Verizon Represent Verizon AI ML research industry publications conference speeches collaboration leading researchers universities Build strong influence among AI ML community senior business leaders actively promote effective applications AI ML technology Lead engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Work closely engineers deploy models production real time batch process systematically track model performance Bachelor degree four years work experience Master degree quantitative field related field Ph D Statistics Math Economics Engineering Computer Science Business Analytics Data Science Eight years experience practicing machine learning data science business Accomplished researcher expert machine learning neural networks reinforcement learning chatbot technology NLP Strong communication interpersonal influencing skills Excellent problem solving critical thinking capabilities Experience leading large scale data science projects delivering end end Strong computing programming skills Proficient Python Spark SQL Linux shell script Strong experiencein Big Data Cloud technology", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Collaborate scientists researchers one following areas data intensive applications text processing graph analysis machine learning statistical learning information visualization low level data management data integration data streaming scientific data mining data fusion massive scale knowledge fusion using semantic graphs database technology programming models scalable parallel computing application performance modeling analysis scalable tool development novel architectures e g FPGAs GPUs embedded systems HPC architecture simulation evaluation Work LLNL scientists application developers bring research results practical use LLNL programs Assess requirements data sciences research LLNL programs external government sponsors Carry development data analysis algorithms address program sponsor data sciences requirements Engage developers frequently share relevant knowledge opinions recommendations working fulfill deliverables team Contribute technical solutions participate member multidisciplinary team analyze sponsor requirements designs implement software perform analyses address requirements Develop integrate components web based user interfaces access control mechanisms commercial indexing products creating operational information knowledge discovery system", "", "You build lead great team Data Scientists You create Machine Learning services Statistics Data mining You promote Machine Learning architecture incredible advantages could bring You help bring Data Science Coolblue next level delivering high performing scalable data driven solutions By using technical skills closely collaborate team take challenges rapidly growing e commerce company By contributing solid data science infrastructure selecting right tools job By coaching providing feedback team members By selecting interviewing best candidates team together recruiters By working leads incorporating Data Science IT software development process A Master degree PhD Econometrics Computer Science Machine Learning Mathematics Statistics another related field Relevant experience data driven environment leveraging analytics large amounts streaming data drive significant business impact Expertise Statistics Data mining solutions implementing Machine Learning services Prior experience managing projects seeing completion managerial experience huge plus Experience writing deploying robust structured code within extended production environments preferably Java Scala Perl Experience preparing scripts languages like Python R Matlab EViews Julia Octave You lead way ego fear politics core values nurture autonomy within data scientist teams You stop smiling tell us one pet projects Money Travel allowance pension plan Plenty space creativity Over training courses Coolblue University leave days As long promise come back Discount new bicycle Because roll Relocation assistance A Z live abroad An office best possible location It short stumble away Rotterdam Central Station Or minute walk", "Contribute development data model Business Intelligence tool collaboration key users subsidiaries IT partner Support data team understanding analysing data base process data coming different data sources Work consistency data coming various sources Support data team extracting combining verifying data needed Business Intelligence tool Support data IT business teams specifying documenting processes needed two tasks You 3rd 4th year studies focus IT Business Analyst You good team spirit perseverance appreciate collaboration different countries cultures You strong analytical data manipulation skills You interested business intelligence applied logistics Working language English Written spoken English professional level key French German conversation level needed well Skills advanced excel ETL Pentaho programming Workplace Paris Bern", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Bogota Boston Buenos Aires Chicago Denver Lima", "", "We immature data infrastructure mature datasets like someone interrogate data relentlessly We need create models scratch ensuring sustainability addition new variables simple updating query dirty work already done Think Jupyter notebooks meetings internal business partners We need work directly product building data scientific solutions empower retail stores company whole We need build machine learning models productionize This Key These models predict customer retention analyze sales employee data real time sales demo actions offer insight stores compare variables improve sales employee operational flows much The sky limit We hope outcome contributions offer true dashboards resulting data consolidation variety sources driving Retail vertical forward 21st century From store country level customer employee data points need dash boarded minutia state region country level To accomplish goals need design implement statistical experiments around business decisions build pipelines pull new sources data models years Data Science experience large progressive technology driven company Relative fluency Python Pandas Scikit Learn deep desire improve We value Data Scientists build models well put production Experience building predictive models Strong understanding probability simulation statistical inference Experience writing SQL A git based workflow Experience working Qualtrics plus NLP word2vec Time Series Analysis Data visualization exploration tools especially Looker Data Flow Tools Think Qualtrics Columnar databases especially Redshift Document databases especially Elasticsearch AWS Postgres Apache Spark", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Work coordinate Supervision team members Model Risk Management Compliance Risk Audit build sophisticated models determine proper empirical methodology organize data collection write unique programs prepare written reports summarize results analytic studies formal informal presentations Design develop test implement advanced predictive models risk analytics tools utilizing data Supervision Compliance key partners across GWIM Work Supervision leads understand business data requirements translate predictive models analytical tools Conduct research apply techniques natural language processing financial engineering methodologies applied mathematics suggest process improvements risk mitigation analytics applicable Develop state art software tools collect analyze large volumes structured unstructured data streamline business processes improvements enhance quality efficiency supervision oversight", "Design build launch new data models TV usage associated online activity Apply expertise quantitative analysis data mining presentation data see beyond numbers understand users interact products Partner Product Engineering teams solve problems build products based data identify trends opportunities Mine massive amounts data extract actionable insights inform influence support help execute product decisions product launches Design implement reporting metrics track monitor performance products quality data overall health business Data Infrastructure Work SQL Map Reduce SQL databases Design build launch new ETL processes Build data sets empower operational exploratory analysis Automate analyses Product Operations Leadership Design evaluate experiments monitoring key product metrics understand root causes changes metrics Build analyze dashboards reports Influence product teams presentation findings Communicate state business experiment results etc product management teams Exploratory Analysis Statistical Modeling Understand ecosystems user behaviors long term trends Identify levers help move key metrics Evaluate define metrics Build models user behaviors analysis powering production systems Identify correct biases errors data sets", "", "", "Interviewing key project stakeholders documenting findings making detailed recommendations Assess frame business problem success criteria risks Acquire blend disparate data sources could format internal structured unstructured external high volume sources Exploratory data mining analysis Guide clients processes identify meaningful predictors engineer representative features data set Understand best practices core set predictive use cases familiar model libraries", "Lead guide data engineers implementing methodologies different platforms R Spark H2O tensorflow evaluating models making trade decisions best solutions Creative use existing data well frequent integration new types data feature engineering results predictive accurate insightful models Deep dive internal mechanics machine learning algorithms suggesting modifications current implementation pipeline Lead guide data engineers comparing performance different machine learning methods kernel methods XGBoost regularized regression deep nets Develop causal models Develop implement practical approaches deal real life data challenges incomplete data high signal noise ratio Accelerate research production cycle developing new novel experimental frameworks metrics Integrate learning rank techniques collaborative filtering methods Ability write clean concise code especially R Python Solid understanding statistics Keen eye detail thoughtful investigation data relying upon Intuition data science best practices stemming proven experience Steadfast focus creating impactful change ability prioritize many tasks maximize improvement business years industry experience Ph D quantitative field", "Research develop operate extend advanced marketing attribution models Machine Learning algorithms deployed production Collaborate closely client services engineering deliver insights clients time Define implement features marketing analytics products Python Degree Computer Science Statistics Mathematics Physics Engineering Experience predictive modeling Machine Learning algorithms Understanding probability statistics linear algebra Solid coding skills Python Excellent teamwork communication ability", "Retrieve prepare process rich variety data sources social media CRM data paid media data sales data account data Create attribution models quantify return investment marketing spend channel selection correlate closely actual sales campaign performance Mine data generate new insights around channel audience account performance Partner Brand Consumer Planning Sales teams develop hypotheses run experiments build forecasts report relevant performance variables Collaborate Data Architect Growth Manager IT teams create data lakes sustainable data infrastructure Consult teams exploration Data Science choices around pricing distribution A P resourcing etc Extend first party data second third party data needed Enhance data collection procedures include information relevant building analytic systems Process cleanse verify integrity data used analysis Understanding marketing channels including digital media social media offline media CRM trade media Ability translate project objectives project plan milestones resource technology requirements teach lead manage projects successful execution Experience tools R SQL Spark Python Hadoop RedShift Matlab Tableau Oracle various DMPs data analysis warehousing visualization Ability communicate complex concepts simple jargon free English Familiarity machine learning techniques preferred years Data Science experience working Data Scientist Statistician Predictive Analyst related Masters degree Applied Mathematics Economics Computer Science Experience attribution modeling large data sets source disparate sources e g Data Warehouses 3rd Party APIs etc", "Degree CS Applied Math Statistics scientific field Graduate degree plus Experience building models real world problems Software development experience python Familiarity hardware robotics Understanding supply chains Previous experience start company paid Medical Dental Vision dependents Ownership via Stock Options Flexible Time Off Daily catered lunch Free discounted pizza The opportunity work incredibly supportive team thinkers innovators", "Research develop lead implementation AI solutions application projects Microsoft products services Select apply appropriate statistical machine learning large scale high dimensional data Stay current latest research technology communicate knowledge throughout enterprise Take responsibility preparing data analysis review data preparation ETL code provide critical feedback issues data integrity Share knowledge clearly articulating results ideas customers managers key decision makers Patent publish relevant IP scientific research Research develop implement AI solutions application projects Microsoft products services Select apply appropriate statistical machine learning large scale high dimensional data Integrate AI solutions overall product service Stay current latest research technology communicate knowledge throughout enterprise Take responsibility preparing data analysis review data preparation ETL code provide critical feedback issues data integrity Share knowledge clearly articulating results ideas customers managers key decision makers Lead architecture design implementation AI solutions application projects Microsoft products services Test review new modified code data pipelines Build new API infrastructure endpoints data pipelines necessary Build reporting monitoring mechanisms solutions Analyze technology industry market trends choose potential impact solutions Develop patterns standards guidelines necessary uphold design principles maintain integrity product architecture Participate key project design reviews Patent publish relevant IP scientific research Collaborate PM development marketing teams across Microsoft translate business needs technical solutions Create manage end end project plans Ensure timely high quality delivery Provide hands program management analysis design development testing implementation post implementation phase Perform risk planning management projects Provide day day coordination Scrum teams Share knowledge clearly articulating results ideas customers managers key decision makers Evangelize group work stakeholders", "Previous message Jobs Fwd inedinfo Fwd JOB Statistician fte Next message Jobs NPD Group", "", "Develop models drive personalization digital experiences Nike consumers e g suggesting products aesthetic match products consumer shown interest computer vision consumer behavior patterns new approach devise Develop models help us understand describe customers e g learning extract deep interests tendencies event streams Develop production code batch real time models collaboration data service engineering teams Manage experimentation portfolio validates feeds back core customer understandings Build processes support fast iterative experimentation model creation customer facing products", "", "You design develop processes systems consolidate analyze unstructured diverse data sources generate actionable insights You work product service teams identify questions issues data analysis experimentation You use machine learning build models address areas like user segmentation churn affinity conversion rates You develop software programs algorithms automated processes cleanse integrate evaluate large datasets multiple disparate sources MS PhD program Computer Science Software Engineering Math Statistics related field Understanding applied math topics probability statistics linear algebra basic optimization techniques etc You expert SQL Excel experience data warehouse technologies Teradata You hands experience predictive analytics tools machine learning techniques algorithms Hands experience tools R Python familiarity Teradata Hadoop environment Experience designing analyzing multivariate tests Experience data visualization tools Tableau Splunk Kibana plus", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Independently lead data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development machine learning statistical models ensure best performance Work closely engineers deploy models production real time batch process systematically track model performance Assist engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Be subject matter expert machine learning predictive modeling Bachelor degree four years work experience Master degree quantitative field relevant A Ph D statistics Math Economics Engineering Computer Science Business Analytics Data Science relevant field Five years experience practicing machine learning data science business Strong foundational quantitative knowledge skills extensive training math statistics physical science engineering relevant fields Experience leading data science projects delivering end end Strong technical experience machine learning statistical modeling Strong computing programming skills proficiency Python R Linux shell script Experience data management data analysis relational database Hadoop Strong communication interpersonal skills Excellent problem solving critical thinking capabilities Experience NLP chatbot technology Experience Hadoop Spark C scala Java", "Work stakeholders throughout organization identify opportunities leveraging company data drive business solutions Generate reports analysis key product metrics Develop custom data models algorithms apply data sets Help identify assess metrics KPIs tracked measure impacts business outcomes Excellent problem solving communication skills Strong statistics mathematical fundamentals Familiar reporting analytics platforms tools Looker Google Analytics Excel B S Computer Science Statistics Mathematics similar field Ability lift pounds Ability stand extended periods Ability work computer front display monitor extended periods Ability consistently operate computer office productivity machinery including limited calculator copy machine computer printer Occasional night weekend work natural part position", "least years experience Extensive experience machine learning statistical modeling imperative text mining topic modeling plus The team heavily investing modern machine learning methods technologies experience Deep Learning Neural Networks corresponding technologies Tensorflow Caffe PyTorch Scikit Learn Numpy etc plus Python preferred R languages must Typical data sources truly big thus working knowledge Big Data technologies NoSQL databases addition traditional Relational Databases must Experience using streaming data processing techniques plus Bachelor degree four year college university four years related experience equivalent combination education experience", "Develop thorough understanding business problems interviewing stakeholders subject matter experts Derive critical extraneous solving business problem Understand data requirements formulating solution Research new data sets determine quality feasibility Manipulate data support various statistical techniques Build machine learning models appropriate business problem Collaborate data engineers ensure solutions successfully operationalized Effective story telling data Interpret communicate analytic results analytical non analytical business partners executive decision makers Passionate machine learning A thought leader researches latest tools techniques trends looks innovative approaches Always learning data science industry market business etc Critical thinking skills Ability assess situation multiple points view High personal professional standards unassailable integrity ethics High energy self starter personality Strong structured coding skills Team Oriented able train develop mentor data scientists", "Engage broadly business frame structure prioritize business problems analytic projects tools biggest impact Nordstrom business Perform large scale statistical research analysis modeling areas web analytics supply chain optimization forecasting Communicate insights recommend areas data discovery Master degree Computer Science Mathematics Statistics equivalent education experience year corporate experience Data Science Analytics years corporate experience using SQL variety RDMS environments Scripting skills Bash least one analytic programming language e g R Python Experience working NoSQL data environments tools Hadoop Spark DynamoDB Fluency statistical machine learning algorithms decision trees neural networks collaborative filtering clustering survival analysis graph theory etc Commuter Benefits Paid Parental Leave Charitable Giving Volunteer Match Merchandise Discount", "", "Collaborate data scientists engineers design develop operationalize machine learning based decision making systems Solve complex problems propose solutions collaboration data science team Keep abreast developments machine learning data science game monetization attending meetings conferences reading current publications media Develop implement machine learning models pipelines Python Spark TensorFlow Keras PyTorch", "", "BA BS Data Science Machine Learning Statistics related STEM field Proficiency Python R SQL another programming language Experience coursework machine learning natural language processing Statistical modeling software e g SPSS programming language e g Python Understanding database analytical technologies industry Demonstrated ability think strategically business product technical challenges enterprise environment Excellent oral written communication skills Ability collaborate team environment MS Data Science Machine Learning Statistics related STEM field Quantitative data analysis Experience coursework Deep Learning Advanced MS Excel skills e g VLOOKUP Pivot tables Professional experience Spark TensorFlow Hands experience deploying analytical models solve business problems Ability develop experimental analytical plans data modeling processes", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "A top shopping app iTunes Google Play Geekwire App Year Million Downloads Train machine learning deep learning models solve problems products Collaborate stakeholders across company make data driven algorithms core part marketplace Make individual technical contributions possible Develop end end machine learning solutions offline training production implementation A B testing communication results", "Work every aspect modeling statistical problems data wrangling feature engineering target definition machine learning models specification estimation comparison performance evaluation policy development implementation monitoring Dive quantitative problems every part organization including credit growth engineering operations drive end end implementation business outcomes Extract actionable insights analyzing complex multi dimensional data sets interpreting implications business Provide advice education usage interpretation solutions end users including boarding training new team members Deep understanding statistical methods machine learning Proficiency Python scikit learn Pandas NumPy R similar Experience working Git Experience SQL structured data tools dealing unstructured data A good understanding software engineering write unit tests refactor code use appropriate design patterns Expertise working data cleaning data building datasets integrating machine learning models performing statistical analysis data visualization An analytical mindset A detail oriented nature The ability manage projects across multiple stakeholders prioritize vigilantly Superb communication collaboration skills A proven track record solving business problems fact based methods The ability persuade inspire motivate others An ability summarize key insights technical non technical audiences Experience working large volumes data Experience credit bureau financial services data", "supervised learning methods classification binary multinomial Logit Na\u00efve Bayes support vector machines unsupervised learning methods clustering Gaussian mixtures K Means linear nonlinear regression modeling", "Analyze client proprietary public domain datasets leveraging advanced analytics statistics work scheduling network optimization macroeconomic forecasting pricing routing Source clean process diverse set data streams Identify understand business needs contextualize results analysis drive impact organization whole Dynamically collaborate operations product teams refine project goals scope timelines Provide consulting services internal external analytic questions Bachelor degree mathematics statistics engineering computer science technical disciplines Expert knowledge mathematical programming language choice preferred R Python Matlab Experience ETL processes manipulating data sets Detail oriented proactive problem solving skills Ability communicate complex findings clear precise actionable manner Able translate high level ideas well defined problems years work experience non academic Experience API integration Corporate headquarters sunny Austin TX Incredible growth opportunity one fastest growing companies Austin The kitchen fully stocked local coffee drinks snacks Company sponsored healthcare insurance dental insurance Team building events office competitions Partial cell phone reimbursement Receive elite technology package include brand new MacBook Pro Competitive salary Opportunity put thumbprint high growth company early stages", "A successful candidate hands experience graph database web applications desire work high performing faced paced team Excellent analytical problem solving skills must well track record taking complex concepts implementing practical solutions Lead design development testing deployment graph database solution Design develop graph data models accordance leading practices use cases Work system owners data engineers identify integrate data sources used hydrate knowledge graph Create manage queries pull insights knowledge graph Optimize high performance efficiency scalability stability database Define database architecture development best practices Assume role trusted advisor offering technical insights team business stakeholders", "", "Develop data assets using statistics machine learning packages Microsoft BI platform solve real world ecommerce CRM inventory offer management problems Collaborate IT teams integrate data assets business applications Develop analytical insights complex data analysis financial modeling identify product improvement opportunities support subsequent business case development Perform A B test design implementation analysis web mobile site order determine effectiveness efforts Stay date latest data science technologies build business case demonstrate benefits onboarding respective technology techniques years professional experience data scientist role especially ecommerce CRM inventory management Excellent written oral communication skills including ability communicate across business areas Ability conceptualize business problems solving data analysis Expert data visualization presentation skills ability present analysis product owners Expert SQL data manipulation skills required including cleaning managing data Experience statistics machine learning packages R SAS etc Degree qualified quantitative field Computer Science Mathematics Statistics Machine Learning AI Willingness work unstructured messy data", "Working interdisciplinary field together computer scientists business spocs telecom network engineers Integrating multiple data sources models software tools business line specific decision support data analysis This include use distributed computing utilization cloud internal computing resources Developing new automation methods analyze evaluate business strategies across various business geographies technologies drivers scales Modeling complex systems integrating large varied datasets economic demographic telecom related information Creating walking executive presentations explain complex data algorithms used simple easy understand visually striking layman terms leave residual impact executive audience Solving complex geospatial problems utilizing robust repeatable solutions Keeping current latest computing trends analysis methodologies tech journals ongoing research Work involve challenging existing constructs business paradigms creatively solving problems result business transformation process program current approach Bachelor degree four years work experience Six years relevant work experience Experience using Python automation analytics tasks Experience using development Web Front Ends JavaScript html nginx php Experience management data SQL NoSQL databases Postgres Oracle Teradata Hadoop Casandra MongoDB Leadership experience one areas team task project lead responsibilities Experience project management Masters Science degree Geo Information Systems Experience parallel computing distributed environments familiarity GIS spatial databases e g Postgres PostGIS Oracle ESRI sde Demonstrated complete understanding wide application technical procedures principles theories concepts Telecommunications field General knowledge related disciplines Good interpersonal communication skills Ability convey complex information upper level executives Ability adapt quickly changing business environment requests come related upper level executive requests Programming experience one programming languages e g Python JavaScript Experience analysis large spatial non spatial datasets Experience multitude databases general Oracle Postgres Knowledge R SPSS SAS statistical analysis", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Builds best class predictive prescriptive models taking account business constraints Performs data integration data set cleansing analysis deploys analytical models development scripts custom workflows Monitors performance accuracy existing models Advises IT Data Engineers build automate maintain set enterprise wide data services data lake connectors internal external data sources rapidly deployed business analysts Creates proactive reactive custom reports based client needs Troubleshoots issues prepares summaries documents processes workflows proposes solutions collaborates communicates internal team members Builds maintains trusted relationships information managers architects business units knowledge sharing mentoring training", "Working directly clients develop analytic questions define data technology opportunities Driving exploration data sources analytic techniques create new variables modeling capabilities Working large complex data sets solve difficult non routine problems Applying variety statistical methods approaches develop analytic models", "", "Build deploy predictive models enable smart decision making Manage production scoring process support IT Provide technical analytical support marketing programs consumer research Serve department expert data access manipulation large complex structured unstructured data Support projects advance L L Bean data infrastructure environment premise cloud Investigate onboard new data sources provide valuable customer insights Help build advanced analytical capabilities awareness industry best practices", "", "Work stakeholders throughout organization Operations Supply Chain Sales Marketing Finance etc identify opportunities leveraging company data drive business solutions Develop scalable statistical machine learning mathematical solutions complex business problems Pricing Inventory Reordering Demand Forecasting Perform analytical studies generate actionable recommendations Use statistical techniques hypothesis testing validate findings", "Operations Maintenance O M Data Science Consultant key leading requirements design new client determining optimum data reporting visualizations assist business objectives Work collaboration team data team members SDLC development requirements design user acceptance testing delivery Works general direction Develops implements maintains complex business accounting management information systems Translates designs computer software programs code Tests debugs refines computer software produce required product Prepares required documentation including required relevant SDLC process Enhances software reduce operating time improve efficiency Works Business Analysts users define existing new system scope objectives Performs modifications maintenance operational programs procedures", "Cash Vouchers top winners contest Top participants get Certificates Merit", "Collaborate scientists researchers one following areas data intensive applications text processing graph analysis machine learning statistical learning information visualization low level data management data integration data streaming scientific data mining data fusion massive scale knowledge fusion using semantic graphs database technology programming models scalable parallel computing application performance modeling analysis scalable tool development novel architectures e g FPGAs GPUs embedded systems HPC architecture simulation evaluation Work LLNL scientists application developers bring research results practical use LLNL programs Assess requirements data sciences research LLNL programs external government sponsors Carry development data analysis algorithms address program sponsor data sciences requirements Engage developers frequently share relevant knowledge opinions recommendations working fulfill deliverables team Design technical solutions independently participating member multidisciplinary team analyze sponsor requirements designs implementing software performing analyses address requirements Develop integrate components web based user interfaces accessing control mechanisms commercial indexing products creating operational information knowledge discovery system Lead multiple parallel tasks priorities customers partners ensure complex deadlines met Manage various complex projects using team members skills complete complex projects tasks solve abstract complex problems ideas convert useable algorithms software modules", "Verifiable experience working health data EMR preferably Epic Clinical databases clinical enterprise data warehouses Verifiable experience working optimizing SPARQL queries graph RDF based database triple stores Verifiable experience working R Python analytic languages Formal Healthcare Economics education degree Experience clinical informatics clinical terminologies ICD9 LOINC CPT etc Excellent interpersonal writing skills communicate document user interactions requirements Strong analytical skills analyze categorize translate business user requirements functional requirements Excellent presentation skills demo user training writing skills user documentations", "", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "", "Leverage data perform intensive analysis across areas business drive product development Design experiments interpret results draw detailed actionable conclusions Generate ideas exploratory analysis shape future projects provide recommendations actions Perform time series analyses hypothesis testing causal analyses statistically assess relative impact extract trends Build models enhance understanding user behavior predict future performance cohorts Create dashboards reports regularly communicate results monitor key metrics Present findings senior management strengthen business decisions Minimum years experience quantitative analysis role MS PhD Math Economics Statistics Engineering Computer Science quantitative field advanced degrees plus SQL skills ability use tools Python R work efficiently scale large data sets Advanced knowledge experimentation statistical methods Experience modeling machine learning Working knowledge big data technology Work closely cross functional teams execute decisions Self driven ability work self guided manner", "Building efficient machine learning models solve automate various business problems Developing strong relationships business teams product owners Extracting cleansing mining analyzing visualizing healthcare data Clear concise presentation findings recommendations business teams senior leadership Participating assisting business conversations identify potential use cases business opportunities application data science data analysis", "Pursuing B S M S Ph D scientific quantitative field Excellent statistical intuition knowledge various analytical approaches Curiosity passion Quora Superb communication skills ability explain analysis clearly Proficiency SQL Familiarity Python similar scripting language Analyze external traffic patterns time better understand changes outside products cause increases decreases visits Quora Study usage differences English Spanish Quora products understand various mechanics Quora translate across different languages Develop methodology prototype features internal experimentation platform identify capture novelty effects experiments Investigate effect satisfying question page user engagement clickthrough patterns rest product Understand effects positional bias home feed ranking identify opportunities improve ranking feed engagement Improve accuracy user engagement prediction model understand factors important determining new users long term usage Build recommender system suggest Quora topics user based provided interests Evaluate statistical properties various methods used determine significance ratio metrics considering coverage power feasibility", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Reach talk data scientists area located using LinkedIn networking platforms", "This position used strictly conversion current IBM Data Science interns Please apply role current IBM intern You implement validate predictive models create maintain statistical models focus big data You communicate internal external clients understand business needs provide analytical solutions You use statistical concepts regression time series mixed model Bayesian methods clustering etc analyze data provide insights You great solving problems debugging troubleshooting designing implementing solutions complex technical issues You thrive teamwork excellent verbal written communication skills You strong technical analytical abilities knack driving impact growth experience programming scripting language Java Python You basic understanding statistical programming language R SAS Python You interest understanding experience Design Thinking Methodology", "", "Collaborate clinical team understand clinical scenario Perform pre processing validation new existing data sources Design develop models address customer business needs Communicate results create data visualizations Work closely development team select technologies implementation mind Document model development process Contribute scientific conference research submissions Stay date emerging data science tools methods", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Begin developing relationships one level two levels Identify support opportunities expand project scope beyond current deliverable actively share information clients emerging support needs trends team members management", "Identifying flagging potential data sources support client needs Sourcing restructuring cleaning data make amenable analysis Supporting case teams analytics work either consultancy expert mode part active case work Train coach L E K staff use software proposed statistical machine learning methods Develop proprietary data solutions matching merging proprietary well public data sources Contribute commercial proposals required Managing Directors", "", "Use Machine Learning AI model complex problems discover insights identify opportunities Integrate prepare large varied datasets architect specialized database computing environments communicate results Develop experimental design approaches validate finding test hypotheses Solve client analytics problems communicates results methodologies understandable manner Research new approaches methods improve optimize test targeted questions Work closely business analysts gain understanding client business problems Additional duties assigned ensure client company success M S PhD quantitative discipline computer science statistics operations research applied mathematics engineering mathematics related quantitative fields Proficient programming environment languages Node js Python R Javascript SQL deep knowledge analytics packages available languages Prior research development experience working data solving problems data experience building advanced analytic models Strong working knowledge machine learning statistics Ability communicate ideas verbal written team members clients understand Ability defend professional decisions organize proof ideas processes correct Resourceful getting things done self starter productive working independently collaboratively fast paced entrepreneurial environment performance expectations deadlines Inquisitiveness eagerness learn new technologies apply concepts real world problems Share values growth relationships integrity passion Experience various BI tools", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "Independently lead data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development machine learning statistical models ensure best performance Work closely engineers deploy models production real time batch process systematically track model performance Assist engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Be subject matter expert machine learning predictive modeling Bachelor degree four years work experience Master degree quantitative field relevant A Ph D statistics Math Economics Engineering Computer Science Business Analytics Data Science relevant field Five years experience practicing machine learning data science business Strong foundational quantitative knowledge skills extensive training math statistics physical science engineering relevant fields Experience leading data science projects delivering end end Strong technical experience machine learning statistical modeling Strong computing programming skills proficiency Python R Linux shell script Experience data management data analysis relational database Hadoop Strong communication interpersonal skills Excellent problem solving critical thinking capabilities Experience NLP chatbot technology Experience Hadoop Spark C scala Java", "train deploy new ML models structure new facts news documents scale clustering algorithms work millions documents build summarization algorithms work document types never seen discover patterns disinformation across news social media propose product features make easier uncover bring algorithms many different languages Curiosity enthusiasm love teaching learning Strong programming skills including Python least years experience production engineering team Masters Ph D quantitative field least years building analytical data driven products industry setting Skills data exploration visualization cleaning Experience applying machine learning algorithms real data sets preferably including text analysis NLP Experience using one following NumPy SciPy Scikit Learn NLTK SpaCy TensorFlow Keras PyTorch Experience taking ambiguous problem statements delivered products Experience interacting end users clients strong interest Experience ElasticSearch Postgres Foreign language proficiency fluency Rapidly growing company opportunities growth leadership roles Health Dental Vision Benefits Unlimited Paid Time Off Smart engaged co workers top game Honest open environment exchange ideas Real customers global name recognition healthy sales pipeline Proactive learning teaching opportunities via individual book allowances tech talks brown bag lunches Twice weekly catered lunches Team outings bi weekly company happy hours", "Lead data science projects independently collaborating team members multi organizational stakeholders Perform hands advanced analysis marketing science business analytics big data promote growth business development Leverage state art data processing tools analytical methodologies drive improved decisions", "Gather insights customer behavior ways use energy quantitative qualitative research order better inform product solutions Use wealth data sources energy usage marketing data premise data personalize customer experience Create approach target customer messaging provide customers accurate timely energy saving actions Use latest behavioral science techniques engage customers energy usage keep engaged Use predictive modeling increase optimize customer experiences revenue generation ad targeting business outcomes Develop company A B testing framework test model quality Develop processes tools monitor analyze model performance data accuracy Extract analyze data company databases drive optimization improvement product development marketing techniques business strategies Create models help us measure overall energy savings impact products", "", "", "Work data analysis team members across enterprise accomplish verification validation Joint Simulation Environment Write tools assist mining data flight tests simulation runs Write tools assist comparative analysis flight tests simulation runs Assess effectiveness accuracy new data sources data gathering techniques Strong problem solving skills emphasis producing data drive decisions Experience using statistical computer languages R Python MATLAB similar etc manipulate data draw insights medium large data sets Experience working creating data architectures Knowledge advanced statistical techniques concepts regression properties distributions statistical tests proper usage etc experience applications Excellent written verbal communication skills coordinating across teams", "Conduct analyses learn vast amount data Apply statistical techniques model suspicious user behavior identify mechanisms platform manipulation size mitigation opportunities Write complex data flows using SQL Spark Scalding R Python scripts Capable operation senior level Data Scientist ML Engineer You plus years industry graduate level research experience working political motivated manipulation relevant security issues You self starter capable learning job takes initiative thrive within large team You pivot blockers develop new approach precedents You form sound hypotheses largely unknown problems combination product domain knowledge logical reasoning quickly iterate data exploration", "Familiarity agile project methodologies LEAN project methodologies Deep knowledge experimental design emphasis A B testing digital environment Strong proficiency programming languages Python R SAS Familiarity Tensorflow scikit learn pytorch Fluency SQL data access manipulation validation", "B S M S Ph D scientific quantitative field years work experience Excellent statistical intuition knowledge various analytical approaches Curiosity passion Quora Superb communication skills ability explain analysis clearly Proficiency SQL Familiarity Python similar scripting language Passion learning always improving team around", "", "Help frame issue establish framework solve breaking sub issues Establish early hypothesis Design analysis test hypothesis answer key questions Develop insights methods tools using various analytic methods predictive modeling regressions machine learning time series analysis simulations etc Handle large amounts data multiple disparate sources Analyze data results ensure data quality statistical relevance etc critical mindset Communicate simple conclusive manner focusing key messages Always tie back analysis business issue Focus make results actionable Ensure buy key stakeholders clients Carry work responsible autonomous manner Make sure leverage adequately resources available incl managers colleagues etc Own stakeholder relationship help shape agenda ensure constant alignment priorities Bring build expertise advanced analytics data management data visualization reporting associated tools R SAS Tableau Alteryx Python etc Align data owners department managers contribute development data models architecture analytics Analyze explore data improvement opportunities existing new sources PhD MSc scientific field years MSc advanced analytics experience significant exposure appetite strategic issues proven ability think like strategist Or years MSc strategy consulting internal significant exposure data science advanced analytics Strong problem solving skills Ability deliver action oriented clear key messages emanating complex analysis Ability explain complex mathematical concepts algorithms data structures Proficient data mining advanced analytical methods Proficiency data analysis visualization languages tools Python SAS R Tableau Alteryx Experience Big Data platforms related technologies Hadoop Hive Pig Apache Spark", "Working key stakeholders subject matter experts understand business problems consider solution approaches Enhancing data collection procedures include information relevant building analytic systems Processing cleansing verifying integrity data used analysis draw insights large data sets Presenting information using data visualization techniques This position requires onsite presence Fort Worth office Masters Ph D students pursuing major minor one following Statistics Operations Research Applied Mathematics Industrial Engineering Computer Science Transportation Ph D students preferred An overall GPA required An overall GPA preferred Strong analytical problem solving skills Strong interpersonal communication skills Proficiency using statistical computer language R experience Shiny plus SAS query languages SQL Knowledge advanced statistical techniques concepts regression properties distributions statistical tests proper usage etc experience applications Experience using business intelligence tools e g Tableau", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Build organization scale impact data science analysis Geo Forecast product business growth model opportunities strategic investments Enable teams across Geo better faster analysis Ensure rigorous analysis also striking right balance go deep versus rough quick answer suffices Provide thought leadership matters Geo metrics driven lens", "Envision new network engineering AI ML projects present well formed ideas team Utilize considerable creativity merge ideation knowledge AI ML model uses insights SMEs translators constraints propose ML projects meaningful actionable Provide timelines milestones project plans types AI ML models attempted new prototype projects Adapt communicate needed changes datasets labels models may function expected Partner team non team data scientists teach complex concepts assess project feasibility select input features validate project output Envision test corner cases Utilize large amounts GPU effectively train attempt multiple models documenting progress Guide AI ML projects high autonomy accepted business SMEs use Pull sample data sets NS tools VGRID attempt prototypes new topics Learn wireless domain 5G LTE xLPT datasets RF Planning Orchestration etc toolsets better communicate understand needs engineers AI ML automation Publish blogs create documentation perform presentations submit intellectual property write ups lead efforts external publications Bachelor degree Electrical Engineering Computer Science four years work experience Six years relevant work experience A Degree Masters PhD Statistics Math Economics Engineering Computer Science Business Analytics Data Science related field Broad experience training multiple types AI ML models KNN XGBoost CNN RNN reinforcement learning deep learning models GANs etc Knowledge Relational databases SQL R Previous experience wireless networking plus Recognized contributor Software Organized Networks SON platforms Experience advanced AI ML ensembles deep learning reinforcement learning NLP Four trained models moved production completed research environment Experience Spark Big Data deployments Attached examples filing IP Published Papers Conference Presentation titles resume", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "Collaborate required analytic projects In conjunction data owners subject matter experts develop new ways getting insights data allow us improve supply chain logistics Understanding root causes changes metric Develop algorithms go prototype production quickly Use data mining model building analytical techniques develop maintain predictive model", "Residential broadband service 700K customers growing Viasat Satellite technology holds Guinness World Record highest capacity communication satellite Our flight Wi Fi connectivity presently streaming Amazon Prime Netflix providers like JetBlue United Airlines", "", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "Build models quantitative analysis design queries clients internal teams Add incremental value extend capabilities packages Manage weekly monthly quarterly reporting packages clients internal teams Manage population health analysis program based claims data sets Be able translate gaps care clinically actionable information Help design implement online version reporting tools Collaborate clinical team population health workflows Manage reporting process help clinicians track follow risk patients Bachelor Degree Mathematics Physics Economics Computer Science equivalent experience Master Degree preferred Experience analytic software Python libraries Pandas Scikit learn SciPy well data visualization experience Tableau Keynote Experience data extraction transformation loading Must excellent Excel data analytics skills Must passion organizing representing data valuable ways Demonstrated ability quantify present results laymen terms Must ability work complex data sets multiple sources Demonstrated ability represent healthcare return investment compare patient cohorts multiple groupings e g disease zip age gender health plan etc Must working understanding healthcare industry Working knowledge Salesforce similar CRM manage workflows Must excellent interpersonal skills especially ability present complex ideas fun creative format Ability quickly learn new procedures processes Strong organizational follow High level ownership accountability initiative", "Data discovery data preparation blending data visualization Utilize software tools Microsoft SQL Server Alteryx Designer Microsoft PowerBI Research develop statistical learning models data analysis SVMs Na\u00efve Bayes Classifiers Logistic Linear Regressions K Means Clustering Random Forest Classifier Collaborate business partners data engineers understand company needs devise possible solutions Build maintain effective collaborative relationships diverse range staff Bachelors Data Analytics Computer Science Information Technology similar degree program Requires years practical experience ETL data processing database programing Analytics Extensive background data mining statistical analysis Able understand various data structures modeling techniques Excellent pattering recognition predictive modeling skills Passionate data analytics High attention data accuracy Ability work agile team Critical thinking ask questions determine best course offer solutions Complete work independently Act change agent Continuous improvement mindset Ability understand big picture Effective analytical decision making skills Strong interpersonal skills build relationships communicate effectively managers staff vendors Strong written verbal communication skills Teamwork skills within department project teams Demonstrated ability work effectively fast paced complex dynamic business environment Working energetic team focused making members wildly successful An opportunity work others back every step way Opportunities make difference inside outside walls", "US citizenship required dual national US citizens eligible All positions require relocation Washington DC metropolitan area All applicants must successfully complete thorough medical psychological exam polygraph interview comprehensive background investigation", "Demonstrates date expertise applies development execution improvement action plans Develops analytical models drive analytics insights Leads small participates large data analytics project teams Models compliance company policies procedures supports company mission values standards ethics integrity Participates continuous improvement data science analytics Presents data insights recommendations key stakeholders", "Partner Finance Revenue Ops Marketing Product teams define business problems scope Data Science solutions represent Data voice room Build predictive models optimize Pricing Marketing Operations Sales Build analyses tools understand partners renters pain points Explore test new data sources improve Risk models marketing targeting Actively contribute Rhino Data culture building core data models tooling best practices well training Rhinos using data effectively You advanced degree quantitative field years work experience analytical role You use data science tools like SQL Python pandas numpy track record picking new tools along way You able zoom problems navigate comfortably high level strategic thinking communication granular methods problem solving You experience one following areas experimental design web tracking attribution predictive modeling supervised learning customer segmentation models frequentist Bayesian statistics building custom web apps You excited join rapidly growing company work cross functionally drive business impact Competitive compensation 401k Unlimited PTO give employees little extra R R need Stock option plan give employees direct stake Rhino success Comprehensive health coverage medical dental vision Remote Work Program allow flexibility home office Generous Parental Leave create family friendly culture Wellness Perks Gym Classpass Citibike Memberships", "Independently lead data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development machine learning statistical models ensure best performance Work closely engineers deploy models production real time batch process systematically track model performance Assist engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Be subject matter expert machine learning predictive modeling Bachelor degree four years work experience Master degree quantitative field relevant A Ph D statistics Math Economics Engineering Computer Science Business Analytics Data Science relevant field Five years experience practicing machine learning data science business Strong foundational quantitative knowledge skills extensive training math statistics physical science engineering relevant fields Experience leading data science projects delivering end end Strong technical experience machine learning statistical modeling Strong computing programming skills proficiency Python R Linux shell script Experience data management data analysis relational database Hadoop Strong communication interpersonal skills Excellent problem solving critical thinking capabilities Experience NLP chatbot technology Experience Hadoop Spark C scala Java", "Translate business problems hypotheses questions tested data Develop processes approaches data science team uses develop data products Thinks like owner approaches work eye toward creating value vs taking completing jobs Teach others data science", "Partner product engineering operations finance teams find opportunities make smart trade offs understand customer impact new features Use data inform influence support execute product decisions Craft statistical models gain insights data communicate results partners Build pipelines automate generation metrics guide business years relevant work experience Comfort experience building complex SQL queries Deep understanding statistics e g hypothesis testing regressions Familiarity ETL design analytics workloads preferably experience building maintainable pipelines Highly self motivated desire solve open ended problems Resourcefulness pragmatism ability drive business impact", "Identifying structuring integrating internal external datasets processes unlock potential data Developing dashboards reports analytical insights support effective delivery health care services improve outcomes members You eager learn know yet build knowledge base You roll sleeves jump right kind person You believe data science team sport years total experience Some experience Python SQL desire build skills languages Some experience Amazon Web Services Some experience working diverse messy data sets Some experience communicating data technical analyses non technical audiences", "Manage large customer accounts optimally balancing transaction approval rates loss rates Analyze large volumes data understand consumer behavior identify patterns good fraudulent activities eventually take effective actions Monitor newly developing fraud patterns determine take actions Develop implement predictive models fraud detection Analyze implement decision rules combat developing fraud attacks Be capable making decisions limited information ambiguous situations", "Identify opportunities machine learning impact across entire Cash App", "Primary Location United States New York Education Bachelor Degree Job Function Technology Schedule Full time Shift Day Job Employee Status Regular Travel Time No Work stakeholders refine requirements communicate progress create effective feature roadmap Work team develop system semantic search ontology knowledge graph creation query execution Independently work end end development NLP models build sophisticated extendable natural language based search engine Train deep learning models internal external training datasets Develop highly scalable classifiers tools leveraging machine learning data regression rules based models Adapt standard machine learning methods best exploit modern parallel environments e g distributed clusters multicore SMP GPU Code deliverables tandem team mentor junior members team", "", "Enter clean conduct basic data manipulation analysis Build disseminate databases spreadsheets designed record POTFF related programmatic data Provide consultation assistance supported units POTFF staff identify opportunities methods capturing data relating POTFF programs initiatives Prepare reports presentations accurately convey data trends associated analysis Possess active DoD Secret Security Clearance Bachelor Degree quantitative science social science related discipline Proficient suite Microsoft Office programs including Word Excel Access Prior experience using statistical software application SPSS SAS R", "", "Research develop statistical learning models data analysis Collaborate product management engineering departments understand company needs devise workable solutions Keep date latest technology trends Communicate results ideas key decision makers Optimize joint development efforts appropriate database use project design Evaluates research concepts develop appropriate statistical methods analysis Advises development research information data gathering techniques data reduction decrease bias errors data gathering analysis Obtains analyzes raw data multiple sources perform core job duties answer questions posed internal external customers Interacts customers clarify needs reviewing results Obtains proper approvals deliverables Handles reconciliation analysis internal external data related issues Works data warehouse teams insure data sets reports built specifications Audits database contents accuracy validity Formats output data analysis internal external customer readiness Provides overall quality assurance oversight requirements post release", "Responsible designing identifying solutions various business problems Create make presentations explain business problems analysis approach recommendations Data preparation data mining SQL MySQL Postgres Handle multiple tasks different priorities Work minimal supervision display strong independent behavior Building Analytics assets Ensure timely accurate delivery projects", "", "Conduct analysis modeling area audio video call quality fidelity reliability Perform exploratory data analysis build predictive models fit subjective call quality metrics Develop fundamental metrics tools measuring call quality validate metrics tools", "Modifying ranking algorithms support healthy marketplace dynamic commissions Create models understand traveler preferences respect price geography traveler preferences images amenities Work larger programs initiatives encompassing several projects company wide impact Apply best class machine learning techniques solve business problem hand Produce novel insights machine learning inform company strategy Assist others data discovery data preparation phase Help build core feedback loops incentive structures marketplace", "Be part product team drive product roadmap responsibility statistical learning algorithms modeling productization Leverage research data competitive intelligence market insights business product performance data support strategic decision making Deep understanding wide variety ML techniques algorithms Statistical NLP Scalable Time Series Modeling Prophet supervised regression classification based models GBM Neural Networks etc unsupervised learners Develop classifiers categorize marketing leads Extract visualize subtle features multidimensional marketing data Implement new improve existing analysis algorithms develop software automated generation reports analysis Collaborate team members ensure data acquired processed stored according appropriate procedures Exemplify G5 Core Values Behaviors MS PhD degree Machine Learning Applied Statistics Econometrics Mathematics related field demonstrated academic excellence Academic background deep learning natural language processing machine learning Experience NLP toolkits NLTK OpenNLP Stanford CoreNLP etc Experience Time Series Modeling techniques Familiarity open sources framework H2O Tensorflow years programming experience proficiency R Python Experience visualization solutions Shiny Strong presentation communication skills", "Partner closely leadership external partnerships business development payments product management teams causal inference including observational studies natural quasi experiments incrementality predictive modeling analytical problems drive key decisions globally Excellent communications understanding business strategy key Develop statistical machine learning models related analysis explain user acquisition retention engagement various partner initiatives devices Research develop key metrics performance indicators How measure impact marketing dollars partner company spends promote Netflix What value implementing Dolby Atmos audio Translate analytic insights concrete actionable recommendations business product improvement communicate findings", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "Analyze results WSJ ongoing A B testing efforts Source query clean data required larger scale data science projects Experiment using techniques like machine learning power personalization products", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "years experience artificial intelligence focus machine learning neural networks Masters degree Computer Science Data Science Mathematics Statistics related degree Experience cloud computing platforms Google Cloud Platform Experience deep learning framework ex Tensorflow Torch etc Proven command R Python Java Excellent interpersonal verbal written communication skills must able communicate complex ideas technical user friendly language Excellent understanding machine learning techniques algorithms Experience using statistical computer languages manipulate data draw insights large data sets Capable working dynamic environment handling multiple projects meeting deadlines prioritizing appropriately responding issues quickly creatively open positive attitude Advanced knowledge statistical techniques concepts regression statistical tests proper usage etc experience application Energetic self motivated must extremely organized detail oriented ability make proactive recommendations Experience building deploying autonomous AI programs Cloud Big Data platforms Tensorflow similar tool R Python Java Must able see big picture drive towards business results Strong sense curiosity Dynamic fast paced office environment remote stakeholders", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "", "", "", "", "Leverage common statistical packages bring learning methods user data Optimize workflow scalable model creation healthcare environment Productize healthcare specific functionality packages commoditize machine learning Consult teams inculcate best practices help model interpretation promote culture machine learning competency throughout firm Contribute data science thought leadership healthcare via Health Catalyst tech blog Familiarity supervised unsupervised learning methods Facility common techniques needed preprocess raw data avoid model fitting tune hyperparameters perform feature selection Knowledge common R Python machine learning packages Ability explain learning method actually learns interact API Software engineering experience especially object oriented code unit tests version control Fluency healthcare data Ability productize R Python code package Knowledge mixed models glm regularization modeling inference well accuracy Ability create visualizations via Shiny Bokeh D3 js", "", "", "Research design implement new quantitative trading strategies This includes generating alphas variety traditional alternative datasets using rigorous statistical methods Experience Excel VBA Python R Microsoft Azure ML Amazon ML etc", "", "Anticipate future business needs identify appropriate opportunities modeling simulation machine learning Gather analyze data solve address highly complex business problems make predictions future outcomes provide prescriptive solutions support decision making Be involved phases analytics projects including question formulation research development implementation testing maintenance Explore data build advanced analytical models present discuss resulting models level audience", "Collaborate scientists researchers one following areas data intensive applications text processing graph analysis machine learning statistical learning information visualization low level data management data integration data streaming scientific data mining data fusion massive scale knowledge fusion using semantic graphs database technology programming models scalable parallel computing application performance modeling analysis scalable tool development novel architectures e g FPGAs GPUs embedded systems HPC architecture simulation evaluation Work LLNL scientists application developers bring research results practical use LLNL programs Assess requirements data sciences research LLNL programs external government sponsors Carry development data analysis algorithms address program sponsor data sciences requirements Engage developers frequently share relevant knowledge opinions recommendations working fulfill deliverables team Design technical solutions independently participating member multidisciplinary team analyze sponsor requirements designs implementing software performing analyses address requirements Develop integrate components web based user interfaces accessing control mechanisms commercial indexing products creating operational information knowledge discovery system Lead multiple parallel tasks priorities customers partners ensure complex deadlines met Manage various complex projects using team members skills complete complex projects tasks solve abstract complex problems ideas convert useable algorithms software modules", "", "Design build predictive models able predict customer churn identifying fraudulent patterns large scale event stream Design build scalable data pipelines managing analyzing billions transactions day Design experiments evaluation validate models production Work closely data engineers data scientists build scalable performant data models Work closely product teams ensure data actionable aligned products years industry experience data science designing building evaluating predictive models Proficiency statistics predictive modeling experimentation Familiarity applied machine learning data modeling Proficient Python Proven record using predictive models drive key metrics production Collaborate product management engineering departments understand company needs devise possible solutions Strong communicator able summarize communicate results ideas key decision makers A leader able drive data drive decisions products across organization Nice Familiarity Tableau data visualization tools able produce reports clearly summarize findings Nice Familiar Spark SQL data pipelines", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Design prototype models artificial intelligence optimization statistics order solve challenging analytics problems influencing farmer decisions Collaborate engineers automate analytical models deliver value scale Conduct scientific presentations regularly leadership team Collaborate engineering teams support product launch FieldViewTM", "Work closely various organizational entities key business stakeholders understand fans customers ensure data science projects fulfill evolving needs organization Manipulate data various sources using advanced data engineering techniques Apply advanced analytic techniques machine learning recommendation systems statistics deep learning mathematical models algorithms production environments Provide ad hoc analysis improves decision making business performance Work data platform services team transform prescriptive predictive models production grade software", "You proficient assessing business requirements modeling gaining understanding business metrics You comfortable communicating technical issues non technical users You excellent communicating written verbal organizational presentational channels You excel flexible juggling multiple projects fast paced environment", "Under limited direction research develop design implement machine learning algorithms cyber threat detection operational technology environments Identify data types collected Operational Technology OT environments enable detection cyber events Test validate developed algorithms real OT data Identify define scope moderately complex data analytics problems cybersecurity domain Develop cross domain strategies increased network security resiliency critical infrastructure working researchers disciplines Lead multidisciplinary teams areas modeling simulation critical infrastructure cyber security information security network security Continue building LLNL machine learning data analytics capabilities Pursue program development opportunities co authoring proposals proposing ideas address sponsor needs Identify program growth opportunities existing customers understanding customer space needs", "", "", "", "Envision new network engineering AI ML projects present well formed ideas team Utilize considerable creativity merge ideation knowledge AI ML model uses insights SMEs translators constraints propose ML projects meaningful actionable Provide timelines milestones project plans types AI ML models attempted new prototype projects Adapt communicate needed changes datasets labels models may function expected Partner team non team data scientists teach complex concepts assess project feasibility select input features validate project output Envision test corner cases Utilize large amounts GPU effectively train attempt multiple models documenting progress Guide AI ML projects high autonomy accepted business SMEs use Pull sample data sets NS tools VGRID attempt prototypes new topics Learn wireless domain 5G LTE xLPT datasets RF Planning Orchestration etc toolsets better communicate understand needs engineers AI ML automation Publish blogs create documentation perform presentations submit intellectual property write ups lead efforts external publications Bachelor degree Electrical Engineering Computer Science four years work experience Six years relevant work experience A Degree Masters PhD Statistics Math Economics Engineering Computer Science Business Analytics Data Science related field Broad experience training multiple types AI ML models KNN XGBoost CNN RNN reinforcement learning deep learning models GANs etc Knowledge Relational databases SQL R Previous experience wireless networking plus Recognized contributor Software Organized Networks SON platforms Experience advanced AI ML ensembles deep learning reinforcement learning NLP Four trained models moved production completed research environment Experience Spark Big Data deployments Attached examples filing IP Published Papers Conference Presentation titles resume", "This influential role Marketing team accountable supporting achieving business goals innovative techniques measure enhance effectiveness Marketing activities The role clear understanding ThinkMarkets strategic objectives roles Marketing plays achieving Analyse interpret behavioural data collected desktop mobile analytics software understand opportunities customer journey Run analytics measure key performance indicators Process cleanse verify integrity data used analysis Apply data mining techniques digital data identify key insights Provide insight analytical support digital projects across business Use statistical techniques analyse digital data Deploy new methodologies ensure better value data Leverage models address key growth challenges cross channel spend allocation response modelling Develop processes repeatable tasks data extraction reporting standard statistical validations Work cross functional teams implement deploy models enhance analytical solutions providing data driven recommendations Create detailed documentation outlining design technical specifications solution Be able extract insights unstructured data sets using NLP Machine Learning Techniques", "Collect manage maintain data stores company Produce regular reports analysis based data Communicate findings propose data based projects initiatives Inform business processes strategies based data analysis findings Find challenges pain points exist department determine challenges investigated data science process You experienced programmer statistician Comfortable working large varying data sets environments You adapt different systems gather data systems needed Strong understanding Revenue Operations processes challenges faced Revenue Operations organizations Translate sales marketing requirements challenges actionable data projects", "Researches identifies Machine Learning ML Natural Language Processing NLP methods algorithms solve specific problems improve user experience Clarivate data websites Implements methods devises appropriate test plans validate compare different approaches Identifies new applications ML NLP context Clarivate extensive sets content data Explores existing data insights recommends additional sources data improvements Excellent understanding ML NLP statistical methodologies Excellent programming skills Java Scala Ability test ideas adapt methods quickly end end data extraction implementation validation Experience search engines classification algorithms recommendation systems relevance evaluation methodologies Bachelor degree Computer Science Statistics Technology Engineering strongly preferred equivalent work experience", "", "Develop test validate statistically sound predictive models Design develop analytical solutions reports support Risk Management Finance Operations departments using business intelligence tools Python R Qlikview Analyze large data sets discover trends patterns actionable insights Apply research evaluation standards descriptive statistics data management principles translate complex data meaningful content Design create test maintain portfolio reports scorecards dashboards Manage individual projects assigned ensure adherence schedule well quality accuracy delivered results", "Lead design deployment management insight analytic end points cloud based infrastructure manage update refresh cycles Oversee management Consumer Insights Analytics workspace within AWS Work big data engineers Technology facilitate publishing analytic end points enterprise data lake Manage creation analytic data sets support operational analytics data science team initiatives Collaborate internal teams within Consumer Insights Analytics business units enforce robust analytic process continuously build data engineering learnings continuous improvement processes tooling Evaluate acquire integrate additional sources data enhance analytic capability Work appropriate teams ensure solutions comply HBO data governance security policies BS higher computer science similar experience Demonstrated expertise cloud based big data analytic infract support advanced analytic research functions Strong hands expertise handling structured unstructured data analytic workflows within big data platform setting Experience analytic scripting R Python SAS Excellent communication project management skills Flexibility comfort working dynamic organization minimal documentation process Strong problem solving critical thinking skills years relevant experience", "", "", "", "", "You help architect build maintain databases support research development Work individual scientists understand needs prioritize software development Interface existing IT team two people define systems As team grows potential role grow technical leadership data science work", "", "Design train deliver solutions wide range datasets Genome sequences Electronic medical records Claims data Longitudinal data Physiological data e g ECG PulseOx Implement test validate new algorithms way used data science team Stay date latest ML AI techniques reviewing literature attending conferences etc", "", "Design implement analytical frameworks measure report ROI marketing campaigns Generate prescriptive insights tactical strategic improve marketing effectiveness Execute maintain long term modelling initiatives Evaluate opportunities international marketing investments Design A B tests analyse results Communicate results marketing product management partners years experience Data Science Extensive experience predictive analytics machine learning marketing including time series analysis supervised unsupervised reinforcement learning Advanced SQL skills Advanced R Python skills Ability create clear data visualizations different audiences Confidence take lead project see completion Ability effectively communicate highly technical insights non technical audience Experience Tableau Experience cleansing data sets Passion stay date latest industry trends methods Bachelor degree computer science engineering statistics mathematics related field Master degree computer science mathematics statistics marketing science related field", "", "Use data mining skills develop new metrics insights segmentation internal external datasets Support projects start finish produce data driven results appropriate techniques answer key business questions Work closely variety teams deploy tests drive deployment adoption new algorithms Generate high level summary reports trends Communicate analyses field questions internal external stakeholders years work educational experience data science machine learning statistics Hands Experience Scala Java Spark Python R Proven technical database knowledge Hadoop NoSQL data modeling experience optimizing SQL queries large data Advanced knowledge statistics machine learning techniques Knowledge Tensorflow Pytorch Keras LSTM types RNN plus Proven experience predictive analytics segmentation experimental design related areas Exceptional communication presentation skills", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "", "", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "", "Develop implement effective strategic business solutions research analysis data Harvest insights data present findings impact business objectives Facilitate effectiveness measurement test design implementation support post analysis new existing programs initiatives Assist managing day day results reporting visualization major corporate initiatives Analyze review forecast trend complex data Develop statistical models forecasts conduct statistical analysis better understand trends population segments predict behaviors outcomes", "", "Create R Python programming modules custom insights required clients Leverage statistical analysis understand acceptable versus outliers Drive actionable insights data present client facing Tableau Microsoft Powerpoint compelling manner Contribute Data Exploration needed enhance cleanliness effectiveness data sources Create well documented coding analytics packets ensure reusability team expands Contribute Analytics Data Management understanding implementing right level summary tables needed answer repeated business questions working Big Data platforms like Spark AWS etc Programming skills form R Python SQL Concise organized programming skills highly valuable Familiarity works Spark AWS S3 valuable Familiarity Retail CPG Shopper domain data plus Familiarity Tableau experience visualization tool useful Comfortable working fast paced environment Strong written verbal communication presentation skills Our Clients express delight consuming insights analytics powered Business Intelligence solutions well custom analytics provide Our Codes Solutions well documented reusable rest team", "", "", "Ensures rigorous compliance standard safety procedures OC corporate policies Develops rigorous design experiments based statistical basis generate better understanding product process related performance Partners global Innovation Business Marketing teams provide robust testing plans measure relevant characteristics appropriate sample sizes utilizes advanced data analytics drive decision Analyzes product development manufacturing processes identify root causes performance gaps recommends solutions impact change Validates mathematical modeling results empirical data product development processes pilot studies lab experiments Works team environment advance understanding product performance showcases benefits advanced analytics executed projects Standardizes simplifies improves user friendliness leveraging analytics platform widespread use within CSB S T organization Stays current respect statistical mathematical methodology maintain proficiency applying different methods justifying methods selected Leads assessment introduction new statistical mathematical technology methodology within CSB S T Commits evolving projects managed match business project needs change management challenges communication requirements successfully deliver Defines implements communicates technology standards advanced analytics Builds community growing capability within organization personal development mentoring teaching opportunities peers Technical Expertise Experience modeling tools platforms like MiniTab R Python IBM SPSS SAS data management data mining skills visualization techniques descriptive statistics solve complex problems required Experience optimization techniques applications preferred Technology Leadership Strong working knowledge contemporary analysis technology software platforms methodologies ability apply business processes The ability educate senior leaders impact benefit analytics descriptive predictive prescriptive cognitive operations Project Leadership Small medium scale project management experience including limited scope schedule cost risk resource change management Possibly including initiatives global reach technology processes cross functional teams partner team members Consulting skills Proven track record influencing decision problem solving processes The ability understand business economic drivers align goals across functional lines organizational boundaries execution Analytics Has demonstrated experience applying statistical techniques solve business problems Visualization Has experience effective utilization visualization techniques explore data find root causes well presentation results Leadership Recognized expert field ability help others define problem move quickly resolution Someone sought bring resolution issue timely cost effective manner Interpretive Skills Understand customers fitness use related specifications procedures products partner international team product development experts build solutions Consultative Skills Ability influence business partners decision making Shape solutions helping partners articulate need Optimization Experience various optimization techniques including linear programming integer programming non linear programming dynamic programming Diversity Understands communicates effectively interacts people across cultures Effectively achieves business results working across multi national teams Communication Clearly conveys relevant information ideas confidence manner inspires audience Adjusts approach capture audience attention ensures understanding message Seeks understand others active listening", "Can connect data business problems clearly communicate benefits risks trade offs Believes data integrity paramount obsesses figuring things Is self motivated eager create something new always looking improve upon existing processes tools methodologies Loves working people numbers help teams answer complex questions Is passionate building community world Collaborate strategy product data teams map business needs existing metrics possible solutions surface information reporting platform Advise promote good experimental design evaluation quickly iterate product changes Rapidly prototype validate new metrics models generate new impactful insights members better serve communities Craft clear comprehensible data visualizations enable teams easily understand act upon output analysis Analyze performance bottlenecks recommend optimizations continually improve performance tools years working data analytics data warehousing practices ETL pipelines experimentation modeling Proficiency statistics experiment design Partnering directly clients either internal external leading projects multiple work streams Meticulous attention detail obsession getting right Advanced SQL proficiency one scripting languages Python Scala R preferred An optimistic outlook possible achieve passionate drive toward getting place Experience data technologies like Hadoop Hive Spark", "", "Works teams discover insights analyzing operational financial quality peer comparison data Under direct supervision models complex business problems discovers business insights identifies opportunities use statistical mathematical algorithmic data mining visualization techniques Proficiency integrating preparing large varied datasets architecting specialized database computing environments communicating results Works closely subject matter experts technical experts understand proper data extraction interpretation", "analytic leader mentor teach help enhance build grow Hallmark analytic capabilities proven experience multiple years conducting analysis developing algorithms consumers stores consumer packaged goods CPG company retailer hit ground running bring new analytical approaches currently practiced Hallmark flexible strong initiative highly collaborative innately curious strong critical thinker years mathematical predictive algorithm development utilizing large scale multivariate datasets business environment years business experience application advanced analytics data mining including predictive algorithms Business experience working large scale consumer analytics datasets years business experience mathematical predictive algorithm development utilizing large scale multivariate datasets years experience business application advanced analytics data mining including predictive algorithms professional environment Experience building big data based IT processes understanding data science workflows building pipelines Strong communication skills written verbal presentation PhD MS quantitative discipline Statistics Physics Math Engineering Economics Econometrics Data Science Computer Science Operations Research highly preferred Experience SAS R Python Tableau SQL Hadoop Spark Proficiency either R Python Experience deriving insight structured unstructured data Experience consumer packaged goods CPG company retailer Demonstrated ability derive explanatory variables high dimensionality collections data social click stream SKU level sales digital marketing weather economic Experience working Big Data Inherently Curious Self starter Proactive Comfort Ambiguity Passion Problem solving Creative Collaborative Team oriented Demonstrated ability coach teach others Take care We offer comprehensive medical dental vision benefits well discounts elder care child care education assistance adoption assistance Save future Through profit sharing share success Hallmark We also offer match k contributions Enjoy time Maximize work life balance Paid Time Off PTO paid holidays community volunteer opportunities discounts product entertainment venues amusement parks sporting events", "", "", "Working closely engineers client teams understand client needs problems within digital marketing The Data Scientist apply knowledge machine learning methodologies propose solutions problems implement solutions drive value clients Building models R Python using statistical machine learning techniques Creating visualizations deliverables communicate technical solutions appropriate audiences Working internal teams optimize data science process better support discovery team Collaborating sales team create deliver client proposals demonstrations", "Data Scientist", "", "", "", "Build maintain improve key decision systems predictive models key Progressive Leasing competitive advantage Explore new data sources internal external Progressive use findings improve optimize model performance Design implement read market experiments testing new algorithms technologies processes better serve customer stay ahead competition Recommend support strategic business changes rigorous analytics deep understanding business creative problem solving Identify evaluate help implement emerging advanced analytics technologies methods", "Communicate clearly inside outside company foster cooperation development Design implement major new features platform Effectively estimate time implement designs Write test code highly available high volume workloads", "Develop production level models predictions insights Able create test hypothesis data based business needs Develop machine learning AI based algorithms refine based remodeling Discover new features based business needs curiosity Able manipulate data improve data pipelines Work closely product go market teams unlock AI potential customers", "years experience applying advanced AI techniques machine learning predictive analytics optimization semantic analysis time series analysis advanced visualization real world problems Proficient SQL Python experience R good required Ability define business problem machine learning Deep Learning framework You demonstrated ability iteratively conceptualize design build data driven analytical models taken project leadership role shaping solutions Strong capabilities modern analytics languages tools You expertise Machine Learning Deep Learning one following areas Optimization Operations Research NLP Computer Vision A huge plus hands experience one following Spark pyspark MLlib running ML algorithms local Hadoop cluster environment GCP AWS machine learning Azure Master Computer Science Operations Research Statistics Applied Math You experience manipulating preparing large heterogeneous data sets Big Data support advanced analytics The opportunity work one coolest companies global tech awards include Top Most Innovative Enterprise Software Companies Fast Company double Magic Quadrant Visionary Gartner Highly competitive salaries serious attracting best technologists industry A long list incredible benefits worthy leading Silicon Valley tech firm inc stock options bonuses healthcare flexible working hours global travel opportunities much A rewarding progressive tailored career path company values diversity innovation invests", "Developing next generation AI protects privacy customers ensures modeling practices ethical bias free Bachelor degree four years work experience Coding proficiency Python Java R etc Knowledge AI ML models frameworks keras pytorch etc libraries packages apis e g scikit Bachelor Master degree PHD mathematics statistics physics engineering computer science economics Advanced knowledge math probability statistics models", "", "", "", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Understand insurance data fraud mechanisms Create associated mathematical models Implement test optimize associated algorithms Participate workshops clients gather feedback integrate models You advanced technical skills Applied Mathematics preferably machine learning operations research You combine strong analysis synthesis abilities afraid deal details You write quality production code You mind meeting clients discuss needs integrate feedback projects", "", "Previous message Jobs Fwd OMB seeking applications Chief Statistician Next message Jobs Tenure track position University Hawaii Messages sorted date thread subject author", "", "", "Develop models user behavior marketplace dynamics Design optimization algorithms improve marketplace efficiency Apply machine learning recommendation prediction forecasting Strong quantitative background Research data science modeling engineering experience Familiarity technical tools analysis Python Pandas etc R SQL etc Research mindset bias towards action able structure project idea experimentation prototype implementation Background Machine Learning Statistics Operations Research Operations Management Econometrics similar", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Process large amounts data multiple sources extract relevant insights Research new ways modeling data actionable insights processes improvement Perform statistical analyses build machine learning solutions support Google Cloud business needs Collaborate complex technical work effective communication develop quantitative strategies Design agile rigorous experiments measure effectiveness models tools programs", "Developing next generation AI protects privacy customers ensures modeling practices ethical bias free Operate expert thought leader within Verizon externally within industry Partner academia startup ecosystem identify potentially bring new AI approach value Verizon Bachelor degree four years work experience Six years relevant work experience Commercial application experience Minimum two years experience scaling AI one areas like personalization forecasting experimentation computer vision etc Knowledge ability explain code underlying math used algorithms models Coding proficiency Python Java R etc Knowledge AI ML models frameworks keras pytorch etc libraries packages apis e g scikit Experience developing models algorithms independently writing code developing strategy algorithmic experimentation deploying production Bachelor Master degree PHD mathematics statistics physics engineering computer science economics Advanced knowledge math probability statistics models Experience Hadoop AWS distributed compute services Exposure data structures data modeling software architecture skills Excellent communication skills critical thinking strategic thinking skills beyond technical knowledge Excellent cross functional collaboration skills", "", "", "", "", "Manipulate large clinical data sets Mine patient data critical events outcome variables treatment actions Perform statistical analysis understand relationships clinical variables Etiometry Risk AnalyticsTM technology Develop models prediction classification utilizing machine learning techniques logistic regression SVMs neural networks decision trees etc Provide data mining data analysis support including data cleaning data customers Etiometry research development teams research collaborators partnering hospitals Research current advances data science statistical analysis methods Publish analysis findings data science conferences journals", "Provide thought leadership area Machine learning Data Science Identify important interesting questions large datasets translate questions concrete analytical tasks Develop strategies extract resolve unify information various types numerous disparate data sources Mine organize massive data sets structured unstructured data This would involve exploring data constructing appropriate transformations tracking source meaning anomalies arise Model building draw approach enhances accuracy understanding including statistical modeling mathematical modeling econometric modeling network modeling machine learning algorithms genetic algorithms neural networks", "Begin developing relationships one level two levels Identify support opportunities expand project scope beyond current deliverable actively share information clients emerging support needs trends team members management", "Work investment teams perform alpha extraction useful insights large messy data sets Evaluate analyze structured unstructured data sets model relevant financial metrics generate quantitative fundamental insights predictions applied investment strategies Develop apply statistical methods solve complex problems within financial industry Apply statistical analysis machine learning methods large data sets data mining feature engineering bias correction prediction Be involved aspects research process including design data analysis prototyping implementation testing performance monitoring Model evaluate fundamentals companies within financial industry analysis large structured unstructured data sets utilizing natural language processing techniques Design schemas applications automate data mining warehousing processes scalable way Develop evaluate novel methods statistical inference model selection data modeling Support investment teams alpha idea generation back testing implementation Evaluate new datasets alpha potential Utilize multiple languages including Python Java C C SQL R perform quantitative research programming well write well structured robust code research production Utilize large scale distributed computing technology including Spark Hadoop analyze financial datasets", "", "Previous message Jobs Fwd FW aapor net Job Posting Study Director IU Center Survey Research Next message Jobs NIA position Messages sorted date thread subject author Previous message Jobs Fwd FW aapor net Job Posting Study Director IU Center Survey Research Next message Jobs NIA position", "", "Apply advanced statistical machine learning algorithm solve critical business problem Research recommend implement new alternative statistical mathematical methodologies appropriate given model analysis Use supervised unsupervised modeling techniques development testing sophisticated models areas insurance Examples include claims growth opportunities marketing optimization underwriting expense reduction many others Lead efforts IT partners development modeling datasets well model deployment validation problem solving Analyze available modeling data files order understand data identify issues could potentially impact model results Apply design experiment principles test new ideas including marketing campaigns analyze results provide actionable insights Utilize text mining techniques extract information various sources build models improve customer experience optimize operational efficiencies Works directly business partners identify new advanced modeling opportunities solutions Work business partners resolve issues arise deployment models including development sophisticated business rules Provide training support junior analysts coworkers areas business intelligence advanced analytics", "Identify retrieve prepare process data various sources e g internal external structured unstructured online offline Conduct predictive analytics using advanced statistical mathematical methods statistics machine learning data mining econometrics operations research Ideate potential use cases Natural Language Processing NLP Machine Learning techniques bring client impact revenue generation productivity increase Perform Statistical Natural Language Processing NLP mine unstructured data using methods document clustering topic analysis named entity recognition sentiment analysis Deliver analytical models production environment collaborate cross functional teams drive business results implementing predictive insights advanced analytics Must years minimum experience least one programming languages Python R Java Scala Experience NLP techniques transform unstructured data structured data fit free text data NLP Models Experience NLP toolkits NLTK SpaCy Stanford CoreNLP etc Experience machine learning deep learning models especially clustering classification models document clustering topic analysis sequence models name entity recognition sentiment analysis Experience data visualization tools tell story Strong written oral communications attitude willingness tackle hard problems innovative ways Experience TensorFlow Keras preferred Experiences working industries financial services commercial real estate preferred Experiences Microsoft Azure preferred Bachelor Degree computer science Statistics Mathematics Engineering Econometrics related fields", "", "Big data consultants Big data managers Solve challenging business problems using advanced statistical methods including AI ML quantitative analytics Understand business requirements supporting development business cases Run discovery analytics identify new innovative opportunities Partner developers engineers deploy embed scale analytics deliver complex critical analysis projects Create reusable assets solutions developing best practices current future business problems Lead analytical discussions influence analytical direction clients teams Communicate provide guidance senior client leadership teams Contribute data science expertise new sales activities Have fun Shape drive full lifecycle Big Data solutions Assess current data capabilities application landscape data sources tools Define modern data architectures Realize lead proof concepts Estimate plan deliver modern analytics solutions Provide thought leadership grow big data skills capabilities Have fun", "Develop implement data driven business solutions using advanced statistical methods machine learning techniques Performs data analyses prepare analyze historical data identify patterns apply statistical methods formulate solutions complex business problems Partners IT integrate analysis enterprise data environment business partners ensure insight operationalized", "Selecting best financial products needs Taking right actions improve credit score Anticipate future financial health based current financial status history", "Envision new network engineering AI ML projects present well formed ideas team Utilize considerable creativity merge ideation knowledge AI ML model uses insights SMEs translators constraints propose ML projects meaningful actionable Provide timelines milestones project plans types AI ML models attempted new prototype projects Adapt communicate needed changes datasets labels models may function expected Partner team non team data scientists teach complex concepts assess project feasibility select input features validate project output Envision test corner cases Utilize large amounts GPU effectively train attempt multiple models documenting progress Guide AI ML projects high autonomy accepted business SMEs use Pull sample data sets NS tools VGRID attempt prototypes new topics Learn wireless domain 5G LTE xLPT datasets RF Planning Orchestration etc toolsets better communicate understand needs engineers AI ML automation Publish blogs create documentation perform presentations submit intellectual property write ups lead efforts external publications Bachelor degree Electrical Engineering Computer Science four years work experience Six years relevant work experience A Degree Masters PhD Statistics Math Economics Engineering Computer Science Business Analytics Data Science related field Broad experience training multiple types AI ML models KNN XGBoost CNN RNN reinforcement learning deep learning models GANs etc Knowledge Relational databases SQL R Previous experience wireless networking plus Recognized contributor Software Organized Networks SON platforms Experience advanced AI ML ensembles deep learning reinforcement learning NLP Four trained models moved production completed research environment Experience Spark Big Data deployments Attached examples filing IP Published Papers Conference Presentation titles resume", "Work stakeholders throughout organization identify opportunities leveraging company data drive business solutions Mine analyze data company databases drive optimization improvement product development marketing techniques business strategies Assess effectiveness accuracy new data sources data gathering techniques Develop custom data models algorithms apply data sets Use predictive modeling increase optimize customer experiences revenue generation ad targeting business outcomes Coordinate different functional teams implement models monitor outcomes Develop processes tools monitor analyze model performance data accuracy Strong problem solving skills emphasis product development Experience using statistical computer languages R Python SLQ etc manipulate data draw insights large data sets Experience working creating data architectures Knowledge variety machine learning techniques clustering decision tree learning artificial neural networks etc real world advantages drawbacks Knowledge advanced statistical techniques concepts regression properties distributions statistical tests proper usage etc experience applications Excellent written verbal communication skills coordinating across teams A drive learn master new technologies techniques", "", "", "", "Develop manage maintain Machine Learning infrastructure Utilize Natural Language Processing users stylists products Research develop plan implement predictive algorithm Use various regression data analysis techniques methods Work team mebers build upon data collection storage processing infrastructure Stay motivated actively engage customers years experience Data Science Big Data Analytics Data Engineering BS degree Computer Science Engineering Expertise fields Mathematical Statistics Machine Learning Systems Automated Reasoning Algorithmic Solutions years hands experience Big Data ETL processes Data visualization Data Exploring Algorithms Proven experience Data Modeling years experience Data Management Analytical tools languages libraries e g SAS MATLAB R Apache Mahout Scikit learn Knowledge computer programming C Java Ruby others Sense ownership pride performance impact company success Critical thinker problem solving skills Team player Good time management skills Data Science years Preferred", "A higher degree MSc physical sciences mathematics engineering A good knowledge statistical methods practical application A strong interest sustainable buildings Mathematical modelling experience understanding gained field application Experience building energy performance evaluation would desirable would experience analytical data science methods Gain valuable experience marketable highly transferable skills Responsibility high profile project Mentoring company experienced academic team", "Begin developing relationships one level two levels Identify support opportunities expand project scope beyond current deliverable actively share information clients emerging support needs trends team members management", "Serve subject matter expert SME responsible data analysis implementation efforts new emerging sequencing technologies Provide high quality professional reports performed data analysis developed tools approaches Support internal external collaborators data exchange custom analysis needed specific scientific projects B S Biotechnology Bioinformatics Computer Science related field minimum years related experience genome assembly bioinformatics finishing equivalent combination education experience", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "", "Collaborate across business teams Understand business needs apply Data Science Artificial Intelligence AI Machine Learning ML Machine Vision MV technology methods architect apply ecommerce marketing platform Build optimize models using Data Science Artificial Intelligence Machine Learning including method selection apply engineering challenges Improve rapid marketing optimization strategies ecommerce Develop algorithms optimal exploitation marketing initiatives next generation ecommerce channels Be thought leader company provide technical guidance areas experiments projects", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "Translate ambiguous business questions concerns specific quantitative questions answered available data using sound methodologies", "Work product managers designers develop new product features benefit users leverage huge cognitive training dataset Support influence product managers marketing team roadmap conducting depth analyses Lumos data user behavior patterns game engagement Build models create personalized user experience product marketing including ELTV Expected LifeTime Value modeling predicting churn game recommendations etc Have years experience working data scientist preferably product driven technology company Are fluent Python R Julia Have strong skills statistics probability machine learning including limited recommender systems generalized linear modeling mixed effects ensemble models etc Are comfortable working large messy datasets Are comfortable working open ended questions willing perform EDA Exploratory Data Analysis scale find answers data Have experience big data technologies including SQL relational databases Spark A collaborative culture promotion within encouraged Regular creative learning sessions skill shares forums guest speakers workshops Education budget conferences professional skill development Competitive health benefits medical dental vision Flexible PTO take time need Equinox gym reimbursement towards gym classes choice Catered lunch times week fully stocked kitchen snacks dinner work late Weekly happy hours bi monthly movie nights Game room Smash tournaments tons board games ping pong chess Legos Team sites past excursions include creative retreats cooking classes terrarium building We recognize family important host Bring Your Child Work Day well kid pet friendly offsite events", "Work senior software engineers semantic web technologists ontologists physicists chemists biologists related field subject matter experts implement deploy systems analyzing big data within realm intelligence analysis Conceive design innovative methods employing semantic technologies counter WMD intelligence analysis problem sets Research develop systems concerned semi automated automated ontology construction alignment Intersect semantic web ontology based approaches Natural Language Processing Information Retrieval Data Mining techniques make novel advancements practical application research data science analysis Decompose real world mission gaps problems sets addressed using set information graph theoretic approaches Support system deployment troubleshooting user training Author present papers current research activities", "", "Write programs automate analyses data wrangling Build machine learning models forecast understand customer behavior Maintain improve reporting Looker Metabase R Explain analyses discoveries articles presentations Strong knowledge statistics inference years writing maintaining code years working SQL Experience communicating statistical concepts broad audience Programming R Python Managing organizing large codebase Experience Bayesian Methods Deep experience part statistics Ex time series analysis experimental design multivariate analysis natural language processing etc Interest functional style programming", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "Design manage deliver content optimization ultimately help improve response drive sales Deliver deep dives objective improving understanding consumer content produce On occasion present clients attend client meetings sometimes client locations Support Lead Data Scientist delivering wider range data science projects predominantly using python cloud based services", "", "Utilize large panel tools corresponding different steps analysis The common SQL HiveQL data extraction R Python Spark data analysis Tableau data visualization The team fully transverse working production marketing CRM media licensing sales finance Based strong background computer science charge automatization scores segmentations Hadoop environment You work San Francisco teams well teams based around world mainly Montreal Paris", "", "", "", "", "", "Collecting combining data multiple sources Uncovering exploring anomalous data including metadata Applying scientific process data evaluation performing statistical inference data mining Developing analytic plans engineer supporting algorithms design implement solutions execute analytic plans Designing developing tools techniques analysis Analyzing data using mathematical statistical methods Evaluating documenting communicating research processes analyses results customers peers leadership Completed degree program fields mathematics statistics computer science computational sciences passion rigorous analysis data Tenacity integrity persistence willingness learn Ability solve complex problems Use critical thinking reasoning make analytic determinations Works effectively collaborative environment Strong communications skills technical non technical audiences", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Develop implement effective strategic business solutions research analysis data Harvest insights data present findings impact business objectives Facilitate effectiveness measurement test design implementation support post analysis new existing programs initiatives Assist managing day day results reporting visualization major corporate initiatives Analyze review forecast trend complex data Develop statistical models forecasts conduct statistical analysis better understand trends population segments predict behaviors outcomes", "Delve deeply varying array large data sources determining appropriate data analysis performed discovering issues identifying potential uses preparing analysis Monitor provide feedback model performance recalibrates models necessary Develop client economic models provide financial motivation loyalty program design Gather requirements appropriate business partners project including necessary data analysis performed Work IT streamline established data warehouses initiate new development databases production data available tools build strategic datasets support key initiatives Intellectual curiosity ability utilize organize synthesize data various types sources Interest partnering business understand needs utilize advanced analytics develop effective solutions Drive succeed personally make impact contribute success rapidly growing business Ability listen translate requirements back forth analytics IT communicate back business technical solution non technical terms Ability adapt changing environment business marketplace general prioritize effectively integrate new techniques tools everyday work years experience programming languages R Python SAS SQL Spark large scale data predictive analysis years analytics experience Big Data environment Hadoop Azure ElasticSearch", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "Solve new challenging problems using statistical machine learning find valuable data insights Collaborate data problems Engineering Product Design teams create high quality reliable client facing products Dive deep wide range data identify opportunities recommend solutions Communicate clearly senior leaders various levels technical knowledge educating sharing insights recommendations Hire mentor junior Data Scientists grow careers Hold Masters PhD Degree Statistics Applied Mathematics Computer Science Operations Research related quantitative field Have five years hands data science experience building delivering data models Have realized power machine learning organizations Possess knowledge experience metric design NLP categorical numeric prediction models Are highly proficient Python SQL essential Data team stack", "Build foundation state art technical scientific capabilities enhance cutting edge financial intelligence Propose build execute various data science models leverage existing data improve business create new business avenues utilizing unstructured data Work complex problems intersection finance technology Collaborate product technology teams contribute advanced data analytics product development efforts", "Collaborating colleagues multidisciplinary science engineering business backgrounds identify reduce gaps current inventory availability Build statistical predictive machine learning models drive sourcing accuracy Ensure data quality throughout stages acquisition processing including data sourcing collection ground truth generation normalization transformation Promote data driven decision making across team using tools Tableau SQL Excel Influence leaders make thoughtful data decisions provide consistent usable data teams across company Combine expert knowledge statistical methodology analysis advanced programming skills support complex analyzes Share develop best practices Amazon teams global scale", "Work digital communication team whose main objective solve signal processing digital communication problems using state art techniques This position requires strong theoretical digital communication signal processing foundation Experience required Data Science includes data centric problem formulation data cleaning hands experience well classification regression reinforcement learning knowledge Conduct specialized technical studies lead internal research development efforts Candidate eager explore modern techniques unconventional methods algorithm design problem solving We looking person work lead small groups engineers The nature work requires candidate work multiple projects simultaneously We expect chosen candidate able active software developer programming languages described pass coding interview", "Conduct ad hoc analysis data set present patterns trends team Extract combine clean pre process analyze corpus questions answers internal external data sets Perform feature extraction data set create training set classifier model Optimize hyper parameters classifier existing machine learning library Analyze performance data product come suggestions scaling Suggest enhancements data product articulate systems approach Find potential opportunities building new capabilities product well develop prototypes new internal tools make operations efficient Help operations team improve service provide customers Collaborate Engineering ship new capabilities data products production Work Data Engineering DevOps teams improve data pipelines analytics Investigate algorithm scientific paper applied domain Implement new machine learning algorithm TensorFlow evaluate efficacy", "Development research exploration areas statistics machine learning experimental design optimization simulation Interprets problems develops solutions business problems using data analysis data mining optimization tools machine learning techniques statistics Will act strategic thought partner propose alternatives solutions alignment requirements objectives timelines Collaborate client Enterprise Data Products team set analytic objectives approaches work schedule Research evaluate new analytical methodologies approaches solutions Analyze customer economic trends impact business performance recommend ways improve outcomes Design large scale models using linear mixed integer optimization non linear methods heuristics Developing advanced statistical models utilizing typical atypical methodologies Design deploy data science technology based algorithmic solutions address business needs Cox Automotive Identify understand evaluate new commerce analytic data technologies determine effectiveness solution feasibility integration Cox Automotive current platforms Design large scale models using Logistic Regression Linear Models Family Poisson models Survival models Hierarchical Models Na\u00efve Bayesian estimators Conjoint Analysis Spatial Models Time Series Models Design large scale discrete event Monte Carlo simulation models Interpret communicate analytic results analytical non analytical business partners executive decision makers", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "Selecting features building optimizing classifiers using machine learning techniques Data mining using state art methods Extending companys data third party sources information needed Enhancing data collection procedures include information relevant building analytic systems Processing cleansing verifying integrity data used analysis Doing ad hoc analysis presenting results clear manner", "Analyze data stored various heterogeneous formats develop data insights Utilize appropriate pre built models develop custom models make predictions derive insights heterogeneous data stores Apply various machine learning techniques build train models Tune model parameters validate models analyze improve performance Prepare relevant datasets train test models Identify useful sources information patterns data Work closely different functional teams implementing production models Prepare reports demonstrations results work Under limited direction optimize customer experiences revenue generation ad targeting business outcomes Perform duties responsibilities assigned", "Data Scientist expertise machine learning statistical data analytics open source proprietary tools applications Must excellent knowledge advanced methods experience applying methods variety applications related health care Establish implement end end proof concept data analyses across functional areas Create innovative methodologies data Build proof concept systems Establish strategic partnerships technical leadership across functional areas Presenting senior leadership well external audience Demonstrates ability create new different solutions align real work problems opportunities Develop maintain support Business Intelligence BI activities Identify facilitate implementation improvement opportunities Facilitate requirements gathering non technical business personnel suggest efficient reporting solutions based requirements Document business requirements form specifications business technical specifications Work independently provide appropriate recommendations optimal analysis development Perform job related duties assigned", "", "Have autonomy impact work thanks bottoms culture Chief Algorithm Officer reporting directly CEO Create test new allocation strategies maintain healthy inventory across Stitch Fix warehouses Communicate collaborate business partners order run allocation strategies across departments company Maintain fast runtimes inventory forecasting allocation code promote interactive use cases Maintain improve reliability inventory forecasting allocation code standard software engineering practices Cross functionally add new features improve prediction accuracy inventory forecasting code Create implement service level agreements broader inventory optimization team order best fulfill business objectives Continue grow skill set great company wide development programs communication culture well technical programs within algorithms team You Master Statistics Biostatistics Math Physics Chemistry Computer Science You least years experience year software engineering experience year relevant cross functional team communication year applying statistics machine learning production You bias toward action You strong cross functional communication skills help simplify move complex problems forward business partners You strong standards software engineering practices implementing automated unit tests familiar Computer Science algorithm fundamentals like runtime complexity year software engineering experience You solid background statistics stats courses year relevant professional experience You proficient machine learning courses year relevant professional experience You production data science experience e fitting model held accountable performance reliability production We group bright kind goal oriented people You authentic self empowered encourage others We successful fast growing company forefront tech fashion redefining retail next generation We technologically data driven business We committed clients connected vision Transforming way people find love We love solving problems thinking creatively trying new things We believe autonomy taking initiative We challenged developed meaningful impact We take seriously We take seriously We smart experienced leadership team wants right open new ideas We offer competitive compensation packages comprehensive health benefits", "", "Interest passion data analytics insight extraction programming modeling Critical thinking debug programs create strong variables iterate modeling techniques etc Curiosity data says analytics extract insights Perform data manipulation wrangling cleansing analysis Python R SAS Knowledge internal external data sources Build iterate validate predictive models using multiple statistical techniques Interpret understand present results clients Provide support assistance implementation predictive models", "Apply expertise quantitative analysis data mining modeling presentation data understand drive value partners Partner cross functional teams including Measurement Insights Partner Analytics Partner Marketing Partner Product Partner Engineering teams solve problems build products based data identify new opportunities Report visualize communicate results internal stakeholders inform industry whole Design build launch new data models user behaviors analysis relates Partner outcomes involve implementation model Use various statistical methods lead research projects beginning end ensure efficient execution process years data science experience advertising marketing industry Strong knowledge various statistical methods machine learning Expertise working relational databases querying language SQL HQL statistical languages R Python Ability develop research plan deliver insightful actionable results", "Lead participate application analytic machine learning approaches HBO understanding drivers content performance help HBO make smarter decisions acquisition scheduling promotion content Mine HBO third party data better understand consumers make choices regards content consumption change based various factors including availability seasonality promotion etc Develop compelling data driven stories influence key content decisions levels within organization Bachelor Degree higher quantitative field study mathematics physical sciences computer science operations research etc accredited institution Inquisitive conceptual thinker comfortable working complex analytic problems related people choose watch HBO content Must capable telling compelling data centric stories inform key strategic tactical decisions content development scheduling promotion varying internal external stakeholders varying seniority Prior experience application analytic programming R SAS SQL manipulation data execution analyses preferably entertainment media company years relevant experience", "Architecture design development experience around SaaS platform software Routine involvement high level architectural design discussions providing authoritative technical guidance Experience delivering technical collateral including architecture design documents technical case studies conference papers whitepapers Demonstrated track record successful customer external engagement driving influence deep technical product industry knowledge Seasoned working fellow senior engineers architects product management senior management partners define technical roadmap product response requirements", "Broad knowledge ML algorithms apply optimize evaluate effectiveness Deep understanding math behind techniques superficial usage satisfy Desire see work solving practical important problems Bachelor degree Computer Science Math Statistics Quantitative Science related field Math level comprehension machine learning techniques algorithms e g normalization dimensionality reduction classification regularization etc years experience desired secondary real understanding ML Ability provide leadership practical applications machine learning rich massive real world data set Strong engineering analytical research capabilities Experience using ML Python A big plus also worked C An understanding practical software development code repos issue tracking code reviews etc Excellent communication skills Managing manipulating large data sets C Java Net Tomcat Networking mobile systems TCP IP stack cellular Wi Fi Android iOS Security SIEM UEBA VPNs Cloud deployment system AWS Azure Google microservice architectures Health life benefits along competitive 401k plan company match company paid holidays paid vacation days paid sick days floating holidays Commuting fitness benefits", "", "Work platform level player matching system impacts thousands games Implement models anomaly detection Communicate findings larger business audience Be voice statistical rigor across entire organization Simulate new algorithms bots staging environment understand impact ideas users see Collaborate engineering teams algorithms implemented production years analyzing customer behavior Programming experience Python R relevant language Excellent analytical problem solving skills Experience building training predictive models Working knowledge statistical mathematics Prior experience statistical algorithm design Ability produce ad hoc reports using SQL Basic understanding optimization methodologies Good understanding A B testing methodologies Prior experience gaming", "", "Work closely cross functional teams data scientists marketing managers R D passionate Spotify success Develop deep understanding audience exploratory analysis generate hypotheses identify growth opportunities Work closely marketing multi channel attribution short long term mixed media modeling Design implement experiments A B multivariate optimize acquisition engagement reactivation artists labels managers podcasters You years relevant experience marketing analytics degree statistics mathematics computer science economics psychology another quantitative discipline You technical competence perform advanced analytics coding skills Python Scala experience dashboarding tools Tableau Looker SQL experience performing analysis large datasets You possess statistical competence attribution modelling reporting forecasting significance testing predictive modelling etc Experience designing auditing implementing data infrastructure needed tracking marketing campaigns", "", "Develop solutions support core business functions Collaborate closely equal partnership business engineering teams Have opportunity major impact thriving business Own projects end end including strategy rollout Solve business problems reframing enable algorithmic solutions An advanced degree quantitative field example mathematics physics cognitive science statistics etc Theoretical knowledge experience applying statistical machine learning models real life problems Fluency scripting languages R Python Experience relational databases SQL Experience using modern tools access large distributed datasets e g Presto Spark Interest skill data visualization high skill data visualization big plus Scrappiness thrive autonomy Great communication skills An innate curiosity bias action The ability identify important The ability recognize simple sometimes best Experience tackling vague problems transforming unframed problems algorithmic solutions We group bright kind goal oriented people You authentic self empowered encourage others We successful fast growing company forefront tech fashion redefining retail next generation We data driven We data first We committed clients connected vision Transforming way people find love We love solving problems thinking creatively trying new things We believe autonomy taking initiative We challenged developed meaningful impact We take seriously We take seriously We smart experienced leadership team wants right open new ideas We offer competitive compensation packages comprehensive health benefits", "Develop data modeling machine learning data visualization solutions address customer needs Foster relationships engineering product management team members identify important questions recommending innovative solutions using data Communicate project output terms customer value business objectives product opportunity Promote data science methods processes across solutions Identify opportunities new growth efficiency based data analyses Project output terms customer value business objectives product opportunity Responsible designing developing implementing data driven solutions", "", "", "", "Lead large portfolios data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development deployment AI solutions drive business growth better customer experience Contribute development AI ML strategy roadmap company Contribute research development AI ML techniques technology fuels business innovation growth Verizon Represent Verizon AI ML research industry publications conference speeches collaboration leading researchers universities Build strong influence among AI ML community senior business leaders actively promote effective applications AI ML technology Lead engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Work closely engineers deploy models production real time batch process systematically track model performance Bachelor degree four years work experience Master degree quantitative field related field Ph D Statistics Math Economics Engineering Computer Science Business Analytics Data Science Eight years experience practicing machine learning data science business Accomplished researcher expert machine learning neural networks reinforcement learning chatbot technology NLP Strong communication interpersonal influencing skills Excellent problem solving critical thinking capabilities Experience leading large scale data science projects delivering end end Strong computing programming skills Proficient Python Spark SQL Linux shell script Strong experiencein Big Data Cloud technology", "Analyze mine structured unstructured data drive product centric well user revenue centric insights decisions Create automated reports data visualizations track key business metrics tied product release lifecycles Work product team validate functionality advise data analytics tracking Set real time alerts Craft compelling stories make logical recommendations drive informed actions Analyze historical data identify opportunities optimization Develop test analytical solutions leveraged internal external clients Fulfill ad hoc query requests analysis", "Sr Data Scientist anticipates future business needs identifies opportunities complex analysis Sr Data Scientist gather analyze data solve address highly complex business problems evaluate scenarios make predictions future outcomes provide prescriptive solutions support decision making Sr Data Scientist involved phases Big Data analytics projects including question formulation research development implementation testing Sr Data Scientist able explore understand data build advanced analytical models present discuss resulting models level audience Sr Data Scientist involved development highly performant scalable cloud based production data processing machine learning model training deployment pipelines integration models products services Sr Data Scientist design drive creation new standards best practices use statistical data modeling big data optimization tools Cox Automotive Identify direct special studies analyses unique business problems scenarios Proactively identify algorithms products high intellectual property content assist evaluation potential patents appropriate assist patent applications contribute intellectual property protection Cox Automotive", "We care employees In fact The Washington Post The Washington Business Journal consistently rank us Best Place Work You work great people love team includes published authors certified trainers internationally renowned speakers We bring device workplace share cost new computer choice Mac PC It We invest career providing days paid professional development every year including travel registration fees attend classes conferences addition tuition assistance degrees certifications Starting day one every employee bonus eligible receives days paid vacation You bike drive metro work commute reimbursement plan covered We cater dinner month always healthy snacks available You fun We hold monthly happy hours events year long including summer weekend getaway family Working directly client stakeholders understand define analysis objectives translate actionable results Obtaining data multiple disparate data sources including structured semi structured unstructured data Using machine learning data mining techniques understand patterns large volumes data identify relationships detect data anomalies classify data sets Working data integration developers assess data quality define data processing business rules cleansing aggregation enhancement supporting analysis predictive modelling activities Designing building algorithms predictive models using techniques linear logistic regression support vector machines ensemble models random forest gradient boosted trees neural networks clustering techniques Deploying predictive models integrating business processes applications Validating optimizing model performance upon deployment tracking time necessary Presenting complex analysis results tailored different audiences e g technical manager executive highly consumable actionable form including use data visualizations", "Build positive productive relationships clinical business partners understand information needs identify data science opportunities ensure products add value Partners may include SCCA executives directors managers providers researchers administrators clinical staff outside institutions agencies Develop robust understanding SCCAs data data systems financial operational clinical ensure effective reliable responsible use data Build solutions apply machine learning techniques address complex business problems derive actionable insights maximize decision making confidence Lead participate aspects solution development including exploratory data analysis feature selection engineering model build testing deployment Create data visualizations present solutions technical non technical audiences Identify implement industry best practices data science healthcare Make recommendations technical infrastructure analytic competencies project selection inform execute strategy Advanced Analytics Program", "Drive scalable repeatable data science advanced analytics capabilities within team Partner Marketing teams worldwide focus email online marketing Develop implement ROAS methodologies online marketing channels Design A B tests optimise marketing performance customer targeting channel mix Work directly external vendors appropriate align test specification ensure accurate execution Prioritise complex substantial work load ensuring client expectations handled works focused valuable activities Interact directly multiple departments across organisation Identify key opportunities deliver valuable business insight You strong experience programming Python R etc data preparation SQL You understand advanced statistics including modelling techniques regression clustering You apply wide variety technical methods models effectively solve business problems At least years marketing effectiveness efficiency measurement experience An understanding fundamentals A B testing theory practice Knowledge online media marketing landscape external measurement tools You demonstrated ability work complex business problems partner internal clients consultative approach You well rounded commercial knowledge covering product marketing Experience ecommerce strongly preferred You knowledge passion Big Data Experience Hive Hadoop Qubole preferred You strong analytical business modelling skills ability convert raw data actionable business insights You high level proficiency Excel especially large data sets You excellent verbal written data visualisation skills You experience working global matrix environment work remotely Experience mentoring managing junior analysts data scientists You earned degree Economics Mathematics Business another technical field related discipline Relationship building Builds effective relationships positive communication motivates influences others Is honest dependable valued team member actively involved achieving team objectives Handling complexity Can work effectively highly complex diverse changing environment Adapts well energized change whilst maintaining focus key business goals personal objectives Innovation Embraces creativity innovation open new ideas Works improve current practices technologies provide new business opportunities improved results Problem solving Takes initiative identify current potential problems determines best solution Involves leads people resources required Personal Effectiveness Produces outstanding results professionally personally proactive committed Continually focuses achieving positive results contributing Expedia business success Technical Competence Uses technical job knowledge experience incorporating functional skills broad based business knowledge meet exceed job requirements customer expectations Strategy Planning Understands needs direction business anticipating developing business priorities meet Embraces change drives improve current working practices products technologies grow business Goal setting short term planning Achieves results setting goals using quality planning analysis decision making Adapts copes successfully changing circumstances", "Use build basic statistical models research methods analyze data generate business product reports minimal supervision Building dashboard visualize communicate understanding users Working engineers product managers set A B experiment analyze experiments Helping improve data quality communicating track various events Mobile OTT platforms Proactive addressing new problems arise supporting Ad hoc analysis new features bugs growth initiatives Masters Quantitative Field e Statistics Computers Science Operations Research etc SQL Python Data Mining Data Visualization Statistical Analysis Basic understanding Machine learning algorithms Supervised learning algorithms unsupervised learning algorithms dimension reduction techniques years experience similar role Strong communication presentation collaboration skills Spark Airflow data ETL experiences product analytics growth analytics experiences tech industry Video streaming industry experience plus Help build future TV A tight knit team passionate people tech first data driven business In addition VC funding Tubi generates healthy revenue We offer competitive pay equity full medical dental vision benefits catered lunch dinner gym subsidies Your choice hardware Open PTO", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "Planning implementation new data products close collaboration marketing product management editorial teams Development statistical models getting production Focus use cases area customer marketing customer analytics Planning support execution evaluation experiments Monitoring testing optimization models production Close collaboration data engineers continuous optimization extension data pipelines Master degree quantitative discipline Mathematics Statistics Computer Science Physics Engineering Minimum years work experience years senior role Theoretical practical statistics machine learning data science know Practical experience tools statistical modeling e g Python scikit learn R statistical packages SparkML Significant experience least one programming language Python Scala Java Experience big data concepts frameworks e g Apache Spark Kafka cloud platforms e g Google Cloud Experience planning setup experiments plus Challenging interesting activities diverse task field high level autonomy possibility contribute digital transformation one best known media brands German speaking area Premium working environment central location offices Bellevue heart Zurich view opera house lake large selection shopping opportunities restaurants walking distance Flexible working hours home office part culture", "", "Lead large portfolios data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development deployment AI solutions drive business growth better customer experience Contribute development AI ML strategy roadmap company Contribute research development AI ML techniques technology fuels business innovation growth Verizon Represent Verizon AI ML research industry publications conference speeches collaboration leading researchers universities Build strong influence among AI ML community senior business leaders actively promote effective applications AI ML technology Lead engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Work closely engineers deploy models production real time batch process systematically track model performance Bachelor degree four years work experience Master degree quantitative field related field Ph D Statistics Math Economics Engineering Computer Science Business Analytics Data Science Eight years experience practicing machine learning data science business Accomplished researcher expert machine learning neural networks reinforcement learning chatbot technology NLP Strong communication interpersonal influencing skills Excellent problem solving critical thinking capabilities Experience leading large scale data science projects delivering end end Strong computing programming skills Proficient Python Spark SQL Linux shell script Strong experiencein Big Data Cloud technology", "Responsible set modelling data science unit support operational initiatives company strategy Source build datasets enable Lilium market launch Build constantly refine complex demand operational models inform top level business decisions Develop tools visualisations support negotiations discussions Partners Governments Investors Produce articulate insights support global company expansion launch next optimise existing routes Work closely Strategy Product company Management providing basis key decision making Passionate mission make air mobility accessible affordable reality everyone Background statistics maths computer science another related field clear demonstratable record outstanding academic achievement Graduate degree bonus Exceptional quantitative problem solving skills demonstrated aptitude analytics Familiarity SQL Python R statistics tools bonus comfortable programming concepts Highly proficient developing complex multi parameter models scratch Comfortable working developing fluid constantly changing company Effectively handle multiple priorities organize workload meet deadlines", "", "Work closely house subject matter experts thoroughly understand business domain use knowledge help define design implement machine learning systems Help define project goals timelines Evaluate new architectures feature extraction optimize extend machine learning models Take ownership text analysis image form recognition projects Create systems evaluating model accuracy anomaly detection", "", "Be major player company pioneer semantic technology Work latest ML tools technologies Work enormous data sets Our database billion records extracted Web Find correlations signals amongst noise Push ZoomInfo next level data excellence Solve interesting challenging problems alongside great team engineers Ensure data quality throughout stages acquisition processing including areas data sourcing collection ground truth generation normalization transformation etc Clean analyze select data achieve goals Build release models elevate data quality accuracy Collaborate colleagues engineering QA business backgrounds Develop new skills push knowledge technology new levels Work profitable growing company works impressive Fortune client list", "", "", "", "", "years practical experience manipulating data sets building statistical models database programming data analytics Temporarily due COVID", "", "Statistical modeling hypothesis testing using Python R Designing training validating breadth machine learning algorithms Writing complex SQL queries various RDBMS e g Postgres MySQL distributed frameworks e g Hive Building deploying Python Scala applications Deploying applications interacting cloud based infrastructures e g AWS Building validating deep neural networks modern tools PyTorch Tensorflow Interacting building RESTful APIs Managing nix servers Writing unit tests Using continuous integration Collaborating via Git Competitive salary Bonus program Health dental vision insurance 401k employer matching Generous vacation policy", "Understand Goodreads Amazon data structures MySQL Data Lake Redshift Acquire data building necessary SQL ETL queries Import processes various company specific interfaces RedShift Data Lake storage systems Investigate feasibility applying scientific principles concepts business problems Analyze data trends input validity inspecting univariate distributions exploring bivariate relationships constructing appropriate transformations tracking source meaning anomalies Build models using statistical modeling mathematical modeling econometric modeling network modeling social network modeling natural language processing machine learning algorithms genetic algorithms neural networks Validate models alternative approaches expected observed outcome business defined key performance indicators Develop metrics quantify benefits solution influence project resources Partner Engineering Data Engineering improve quality existing data bring additional data sources line Audit metric data measure project progress success Build automate reports dashboards Tableau allow business leaders get clear snapshot operations Develop innovative experimental design measurement methodologies understand customer growth business efficacy Design implement scalable reliable approaches support automate decision making throughout business", "Ingest organize data various sources e g CSV relational database Process data using typical data science techniques e g deduping imputation Linear logistic regression Decision tree algorithms e g random forest Clustering algorithms e g K means Numpy Pandas Matplotlib", "Contribute development data pipelines implementing novel algorithms retrieving analyzing visualizing data Extract useful statistics usage profiles existing fleet Apply statistical analysis service vehicle data drive decision making organization Implement scalable planning tool based forecasting models service operations Create visualization communicate data meaningful actionable manner", "The PebblePost Graph uses distributed data graph processing create connections online browsing activity physical location privacy compliant consumer centric fashion Our Decisioning Engine uses machine learning predictive analytics determine ideal recipient offer manages delivery intelligence personalized PDM send Our data warehouse delivers real time insights brands campaign effectiveness utilizing first third party data Architect data science solutions improve performance within fast growing disruptive first mover company Collaborate product managers team members gather requirements design develop solutions within agile environment Participate code reviews help maintain high standards code quality Stay date relevant science technologies create exceptional product Work engineering team design implement test applications cloud environments Expertise regression analysis Expertise SQL one relational databases Expertise machine learning techniques e g clustering decision tree learning artificial neural networks etc pros cons use Experience scripting language e g Python statistical computer languages e g R High level verbal written communications skills Excellent problem solving implementation skills Self motivated passionate apply data science real world problems Master PhD Computer Science Statistics Math Industrial research related fields Experience building production level prediction optimization systems Expertise Hadoop Ecosystem e g Spark PySpark HDFS Hive HBase Impala etc", "", "Work team data scientists cross functional partners collaboratively develop solutions Apply ML statistical approaches generate insights structured unstructured data Provide ML expertise design delivery data products broad consumption business partners Mentor support training technical non technical teams data science machine learning Participate broader data science community stay current methodology software data development availability Communicate visualize output analyses including written verbal communication business leaders non technical audiences Conceptualize deploy data science solutions business questions", "Identify innovative ways leverage large datasets solve industry challenges Design implement data analysis data mining research analysis predictive modeling strategies best practices Use analytical expertise spot key business insights trends opportunities vast database Help interpret communicate findings way easily understood less familiar data Collaborate dynamic Product Development team delivering data products insights clients Establish maintain relationships distributed cross functional teams leverage domain industry expertise Identify missing inaccurate data platforms seek supplements replacements improve data sets greatest extent possible", "", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "Envision new network engineering AI ML projects present well formed ideas team Utilize considerable creativity merge ideation knowledge AI ML model uses insights SMEs translators constraints propose ML projects meaningful actionable Provide timelines milestones project plans types AI ML models attempted new prototype projects Adapt communicate needed changes datasets labels models may function expected Partner team non team data scientists teach complex concepts assess project feasibility select input features validate project output Envision test corner cases Utilize large amounts GPU effectively train attempt multiple models documenting progress Guide AI ML projects high autonomy accepted business SMEs use Pull sample data sets NS tools VGRID attempt prototypes new topics Learn wireless domain 5G LTE xLPT datasets RF Planning Orchestration etc toolsets better communicate understand needs engineers AI ML automation Publish blogs create documentation perform presentations submit intellectual property write ups lead efforts external publications Bachelor degree Electrical Engineering Computer Science four years work experience Six years relevant work experience A Degree Masters PhD Statistics Math Economics Engineering Computer Science Business Analytics Data Science related field Broad experience training multiple types AI ML models KNN XGBoost CNN RNN reinforcement learning deep learning models GANs etc Knowledge Relational databases SQL R Previous experience wireless networking plus Recognized contributor Software Organized Networks SON platforms Experience advanced AI ML ensembles deep learning reinforcement learning NLP Four trained models moved production completed research environment Experience Spark Big Data deployments Attached examples filing IP Published Papers Conference Presentation titles resume", "Collecting combining data multiple sources Uncovering exploring anomalous data including metadata Applying scientific process data evaluation performing statistical inference data mining Developing analytic plans engineer supporting algorithms design implement solutions execute analytic plans Designing developing tools techniques analysis Analyzing data using mathematical statistical methods Evaluating documenting communicating research processes analyses results customers peers leadership Completed degree program fields mathematics statistics computer science computational sciences passion rigorous analysis data Tenacity integrity persistence willingness learn Ability solve complex problems Use critical thinking reasoning make analytic determinations Works effectively collaborative environment Strong communications skills technical non technical audiences", "", "Knowledgeable Data Science tools frameworks e Python Scikit NLTK Numpy Pandas TensorFlow Keras R Spark Basic knowledge machine learning techniques e classification regression clustering Understand machine learning principles training validation etc Knowledge data query data processing tools e SQL Computer science fundamentals data structures algorithms performance complexity implications computer architecture software performance e g I O memory tuning Software engineering fundamentals version control systems e Git Github workflows ability write production ready code Mathematics fundamentals linear algebra calculus probability Interest reading academic papers trying implement state art experimental systems Experience using deep learning architectures Experience deploying highly scalable software supporting millions users Experience integrating applications platforms cloud technologies e AWS GCP", "Defining calculating measures inequity students within school schedule Building recommendations course placement schedule Proposing prototyping algorithms supporting team teaching best practices Developing metrics visualizations student tracking Build underlying systems power data driven products e g recommendation engines constraint solvers predictive models Consult directly school leaders work complex problems within Abl product using research rapid prototyping skills push new features production Perform data profiling complex sampling statistical modeling Design develop tailored data models K12 schools Identify incomplete data improve quality data integrate data several data sources Work challenge combining data across schools districts store things differently measure impact aggregate Determine evaluate equity fairness students teachers Propose metrics evaluating overall quality schedule methods comparing multiple schedules ability meet school leader priorities Find trends insights complex human generated school data", "Curious You enjoy peeling apart problem examining interrelationships data used form solution Creative You try new approaches solving problems looking ways apply techniques across domains new contexts Practical You explore theories eye real world applications potential improving performance clients Collaborative You work well team understand lead group bright technical coworkers You explain work peers source ideas improvements Determined You enjoy challenge finding solutions difficult problems testing discover successful optimized develop industry leading tools You learn failure trying Organized You know keep track high level goals tasks required meet goals You take personal ownership team success", "Independently lead data science projects deliver innovative technical solutions solve problems improve business outcomes Lead design development machine learning statistical models ensure best performance Work closely engineers deploy models production real time batch process systematically track model performance Assist engagement key business stakeholders discussion business strategies opportunities Build strong working relationship develop deep partnership business Be subject matter expert machine learning predictive modeling Bachelor degree four years work experience Master degree quantitative field relevant A Ph D statistics Math Economics Engineering Computer Science Business Analytics Data Science relevant field Five years experience practicing machine learning data science business Strong foundational quantitative knowledge skills extensive training math statistics physical science engineering relevant fields Experience leading data science projects delivering end end Strong technical experience machine learning statistical modeling Strong computing programming skills proficiency Python R Linux shell script Experience data management data analysis relational database Hadoop Strong communication interpersonal skills Excellent problem solving critical thinking capabilities Experience NLP chatbot technology Experience Hadoop Spark C scala Java", "Bring Creativity Data products Analyze transaction data Build models predict possible finance operation inconsistencies Use machine learning methods perform transaction matching matching algorithms Cleanse raw data use models Perform data munging data mining clustering classification methods pattern recognition Comfortable Statistics Calculus Multivariate analysis You work closely small R D team Product Managers Data Engineers Analysts The R D team experiment data come POC Familiar SQL Python R Spark MLib AWS SQL Server Redshift", "Research design prototype robust scalable models based machine learning data mining statistical modeling answer key business problems Build tools support structures needed analyze data perform elements data cleaning feature selection feature engineering organize experiments conjunction best practices Work development teams business groups ensure models implemented part delivered solution replicable across many clients Present findings stakeholders drive improvements solutions concept delivery Keep abreast latest developments field continuous learning proactively champion promising new methods relevant problems hand", "", "Apply statistical techniques complex data sets understand users across geographies engagement across platforms Identify analyze interpret trends patterns provide ongoing reports data visualisations order make recommendations wider business Work internal stakeholders external business partners ensure accurate collection logging data needed provide insights user engagement help drive business decisions Analyze performance marketing campaigns customer referral programs Build data pipelines import traffic data daily basis Deploy machine learning models email targeting content recommendation algorithms delivery targeting infrastructure product notification experience across iOS Android Web", "Research define statistical algorithms surface interesting actionable insights user data Guide co located engineering team implementation validation new algorithms analysis methods Design elegant solutions impact clients business years experience statistical algorithms predictive modelling Expertise time series analysis", "Work Stakeholders throughout organization identify opportunities leveraging company third part reference data drive business solutions Mine analyze implement data driven solutions focused deep insights improving efficiency creating value Performs deep data analysis data processing support reporting analytic solutions Artificial Intelligence Machine Learning Utilizes predictive modeling machine learning approaches derive actionable insights using state art statistical methods Enhances data collection procedures include information relevant building analytic solutions Processing cleansing verifying integrity data used analysis Leads supports internal set configuration data integration quality control data platforms including data warehouses operational data stores data lakes Leverages master data management solutions continuously improve data quality Works closely business leaders develop automate KPIs lead monitor business Conducts ad hoc analysis builds interactive dashboards provide business insights creates business value presents results clear manner Extends Bioclinica data third party sources information needed Collaborates stakeholders ensure data collection supports future business goals Develops processes tools monitor analyze model performance data outcomes Ensures data completeness accuracy developing implementing data quality checks Assesses effectiveness quality accuracy data sources data platforms Creates maintains data integration documentation across projects supported including ongoing changes amendments data integration specifications Supports provides solutions new ideas tool development enhance operational efficiency data department initiates creates drives awareness around new processes implemented Communicates effectively team stakeholders operations personnel successful execution data integration deliverables Attends external Client meetings provide data integration support required Coordinates multiple sources data file transformation processing proper documentation quality control across assigned projects Supports operational issues troubleshooting directly impacted data related aspects system Communicates effectively within project team escalates issues ensure timely delivery Maintain expertise Bioclinica applications integration solutions associated data Assist internal external users troubleshooting implementing new integrations Reading understanding adhering organizational Standard Operating Procedures SOP Assisting establishing enforcing departmental standards Working internal staff resolve issues Exploring new opportunities add value organization departmental processes Helping others achieve results Performing duties assigned Attending participating applicable company sponsored industry training", "Bachelor degree years experience recent Master degree graduate Data Science related field Ability apply probability algebra Bayesian inference frequentist statistics machine learning reinforcement learning optimization techniques solve business objectives Entry level proficiency predictive modeling including comprehension theory modeling identification strategies limitations pitfalls Entry level proficiency machine learning algorithms concepts Demonstrates basic computational skills level experience using statistical programming languages R Python Entry level proficiency SQL NoSQL querying Python coding wrangle explore big data Entry level proficiency visualization tools ggplot2 plotly Tableau explore big data Strong ability conduct meta analysis literature reviews Experience probabilistic graphical models Bayesian networks deep learning reinforcement learning time series active learning plus Experience cloud computing platforms Open Stack Google Compute Engine EC2 plus Experience big data platforms Neo4j Spark Big Query Hadoop Azure AWS plus Experience Google Cloud Platform plus Experience CRISP DM process plus", "Empathetic analytic needs product owners engineers analysts candidate translate requests formulate hypotheses develop models support needs A self directed individual wait pristine dataset instead comfortable wrangling data identify necessary data sources improve results Respectful established protocols also drive change business evolves time Proactively engage various stakeholders engineers build strong relationships You agree data science fun Our new data scientist shares enthusiasm reflected work Analyze customer data product warehouses Success programs Support Finance Sales ops partnership customer savvy business leaders teams identify signals health current deficiencies Design build manage necessary models classify improve customer health A key challenge distilling dozens hundreds possible signs something simple allows wise efficient action taken Broker data discussions across different business units may different set signals visions customer health Work always find simplest solution serve customers best willing set aside perfection best serve us Regularly report key health metrics leadership Directors VPs C suite optimize presentation process make effective efficient possible Support business leaders ad hoc data delving substantiate disprove health related hunches create work lists risk customers drive high urgency solutions Use customer data cross referenced customer success programs identify whether much offerings helping customers Queue data celebrate success learn gaps deficiencies Track prioritize project manage additional reporting data based projects Self motivated driven purpose outlined Eager leader Zendesk understanding customer health means willing go deep learn co workers CX companies aspect industry Demonstrated ability produce time quality data deliverables special skill visualization process design enables recipient achieve business objective quickly enjoyably Well established personal processes handle multiple requests varied stakeholders way maintains clear priority adjusts needed always helpful pleasant Strong communication skills written oral ability listen understand root needs across many different business units perspectives customer experience Advanced SQL skills must Deep proficiency Python Scala R Familiar BigQuery cloud environments plus years experience advanced analytics least years data science role", "", "Be data expert team innovating data driven features Build deep knowledge technical assessment aspect hiring Build curate prioritized catalog data opportunities product offering Guide product roadmap strategy Have company wide impact data driven decisions insights Drive well defined data experiments evaluate results Bring selected models production test evaluate years experience data scientist proven record deploying models production Experience framing data problems clearly relating business needs You data scientist relate product use cases frame problems solutions context use cases Experience variety modeling approaches model evaluation techniques Good judgment types problems feasible solve largest ROI Experience working Subject Matter Experts case recruiters hiring managers improve model precision employee coverage Medical Dental Vision coverage dependents Employee stock options 401k flexible work hours time commuter benefits Monthly wellness classes free lunches snacks endless supply Philz Coffee", "", "analyzing information content initial corpus identify informational elements structuring logical forms represent information appropriately establishing goals system evaluating well system meets goals Create knowledge extraction system Develop computational models language readable appropriate domain Prepare training testing data Work researchers understand integrate overall system", "Analyzes Data Integrates data multiple sources supports performs complex statistical data analysis including limited multivariate statistical analyses multilevel modeling dyadic analysis coarsened exact matching analysis Designs Supports retrospective outcomes research analysis designing monitoring quasi experimental study designs Obtains Data aggregates manages data aspect cycle analysis design coding engineering testing debugging documentation research development maintenance new development operations delivery Leverages various forms data clinical EMRs fully adjudicated medical claims data using SQL Hadoop Hive Tableau SAS R Python applications applicable Supports Operations Ensures consistent implementation data generation collection ACM staff assigned Collaborates Works partner organizations development testing implementation data marts views facilitating efficient timely measurement Visualizes Data Creates publication quality graphical narrative representations outcomes analysis performed Bachelor Level Degree One year experience preferred database ETL operations statistical computer programming", "", "", "", "Build end end data pipelines consuming variety sources Work data consumers identify self service opportunities make sure access data need make decisions Help guide team towards data best practices mentor teammates Contribute technical roadmap CustOps data program set priorities work across team Build refine processes make pipeline implementations faster simpler less error prone product engineers Participate architectural decisions across organization particularly respect impact data collection Set timeliness correctness goals data pipelines maintain sufficient monitoring coverage ensure incidents outages mitigated appropriately", "", "Passionate building product positive impact world years professional experience software engineer data engineer Strong working knowledge Python Docker Airflow similar data workflow tool cloud computing concepts Willing learn new tools languages patterns needed build great product A solid communicator enjoys collaborating engineers designers PMs scientists", "Build manage Kafka based streaming data pipelines Build manage Airflow based ETLs Monitor improve performance pipelines Develop storage layer relational non relational hosting data analytical queries", "", "Design engineer solutions meets business requirements cyber security practices compliance regulations Provide security guidance compliance NIST 171r2 Cyber Maturity Model Certification CMMC Asses technical designs identify security design gaps existing proposed architectures Identify communicate current emerging security threats Identify risks provide guidance regarding remediation gaps facilitate hardened sustainable solutions Take ownership solutions assignments actions items issues remain accountable completion Work effectively team members customers key stakeholders foster team success Communicate collaborate leadership technical teams include systems network administrators security engineers IT Support teams Security risk assessments services applications hardware systems", "", "", "", "", "", "", "", "Getting hands experience Google Cloud Platform technology languages BigQuery Scala Scio Luigi Styx Docker Understand fuels many Spotify product features Discover Weekly Daily Mix Podcast offerings holiday campaigns others Work hand hand data science community understand various user content trends influence product changes customer acquisition strategies Collaboration global scale squad offers ongoing opportunities work Stockholm engineering colleagues Innovate data products create single coherent platform sources truth serve plethora product data stakeholders Communicate insights recommendations key stakeholders engineering data science product partners Work supportive team offers engineers flexibility creative chase interesting ideas Work closely product manager end users stakeholders understand document troubleshoot analyze requirements complex data solutions Lead mentor engineers grow bigger team A BS MS CS relevant fields study You years experience development high quality database data solutions Strong analytical problem solving ability Have worked team Data Engineers Data Scientists You capable tackling loosely defined problems thrive working team autonomy day day decisions You self motivated individual contributor great teammate ability multitask prioritize communicate progress rapidly changing environment Would like build skills enhance shape within analytics data engineering Strong coding skills preferably Scala Java Python Strong communication data presentation skills Tableau PowerPoint Qlik etc Experience performing analysis large datasets cloud based environment preferably understanding Google Cloud Platform You communicative person values building strong relationships colleagues multiple stakeholders ability explain complex topics simple terms", "", "You get people You unique blend skills developing deep consumer insights competitive intelligence data drives product innovation create right experiences people daily lives achieve goals acquire engage retain You get data You thirst knowledge insight You thrive strive present data ways product design engineering marketing executive teams understand act upon Your data accurate credible Your reports always clear actionable You get growth You consumer focused data driven growth enabling analyst supported Growth strategies roadmaps scrums final product rollouts across analytics insights acquisition referrals activation onboarding adoption retention loop You get mobile digital You significant industry experience strong understanding mobile digital ecosystem apps advertising analytics You successfully applied latest mobile digital tools help drive reach retention revenue growth Understand marketplace trends help answer revenue trends Analyze supply well demand patterns find revenue opportunities explain model behaviors suggest improvements etc Gain insights drives performance terms reach revenue growth Create dashboards reports provide analysis commentary explaining product sales business trends Executive reporting Work closely product inform update stakeholders product performance plans progress towards metrics Define data testing plans create methodologies help teams iterate fast release new features testing successful rollout users globally Generate go deep consumer insights competitive intelligence help teams drive product innovation iteration Build strong partnerships product sales engineering marketing teams enable launch new Growth initiatives testing iteration Provide feedback product sales engineering teams impact product launches target launch metrics A B testing post launch metrics Investigate data monitor data quality partner closely provide requirements Data Engineering teams clearly acted upon BS MS highly quantitative field Analytics Computer Science Mathematics preferred Data analysis generating insights consumer focused products B2B advertising experience must Experience big data technologies Hive Hadoop MapReduce Spark PIG etc Experience scripting programming languages Perl Python good Familiarity Unix Linux environment highly recommended Significant experience proficiency passion Mobile Web products Track record proactively establishing following commitments Demonstrated use analytics metrics benchmarking drive decisions Excellent interpersonal organizational creative communications skills Team player driving growth results combined positive attitude Strong work ethic strong core values honesty integrity creativity", "", "Design construct test optimize deploy solutions Deliver transformative solutions clients aligned industry best practices provide thought leadership data architecture engineering space Possess exceptional analytical conceptual problem solving abilities Have experience traditional e g MSBI modern e g cloud data architecture Collaborate part team develop Cloud Data Analytics solutions Assist designing multi phased cloud data strategies including designing multi phased implementation roadmaps Analyze architect design actively develop cloud data warehouse data lakes cloud based data solutions Participate development cloud data warehouses business intelligence solutions Exemplify strong focus back end data integration Demonstrate strong focus design development data warehouses Highly self motivated deliver independently strong team collaboration Ability creatively take new challenges work outside comfort zone Solid written oral communications along presentation interpersonal skills Comfortable conducting supporting white boarding sessions workshops design sessions project meetings needed playing key role client relations Quantitative background years experience applying data architecture engineering solve real world business problems years data engineering data warehousing experience years building cloud data solutions AWS Azure GCP Snowflake Experience Relational dimensional database structures theories principles practices Manipulating mining data database tables SQL Server Redshift Oracle SQL ETL ELT optimization analytics tools including R HiveQL Python Big data application development cloud data warehousing e g Hadoop Spark Redshift Snowflake Azure SQL DW BigQuery Solution architecture cloud platforms AWS Azure GCP Cloud SDKs programmatic access services Practical knowledge data visualization tools e g Tableau Power BI plus Expert programming skills Python software development background Experience writing infrastructure code deployments e g ARM CloudFormation Terraform Job function Information Technology", "", "", "You good problem solving skills tendency towards simple effective solutions getting things done mentality Analytical thinking troubleshooting skills attention detail You reliable trustworthy person keeps promises Interest keeping date learning new technologies Product oriented mindset eagerness take part shaping products build Ability work autonomously fully distributed team Good communication skills verbal written English BS degree Computer Science similar technical field years software engineering experience Solid understanding modern back end systems microservice architecture message driven solutions distributed processing replication years experience relational databases Postgres MariaDB Oracle Background building data processing pipelines Understanding data streaming concepts technologies Pulsar Kafka RabbitMQ similar Familiarity Agile methodology test driven development containerization continuous integration deployment cloud environments monitoring Ability write clean efficient maintainable well tested code Golang Python skills plus", "Design build support data processing pipelines APIs Ensure code adheres defined standards best practices performance speed scalability quality Practice Agile development methods exemplify core Agile values transparency collaboration acceptance change iterative development Routinely deliver working software solutions meet user story acceptance criteria Bachelor degree Computer Science four years work experience Four years relevant work experience Four years software engineering experience including Java Python related languages Two years experience relational database technologies Postgres MySql etc including SQL A degree Four years experience leading large scale projects Four years experience networking multi threaded applications inter process communication complex software development Experience building high performance highly available scalable distributed systems ETL Data cleansing normalizing Big Data data analysis log mining automated reporting PostGIS experience Experience designing building managing Internet scale APIs Experience mentoring growing software engineers Ability willingness learn new technologies quickly Ability work highly collaborative Agile team Demonstrated aptitude desire learn new skills", "Architect multi tier data pipeline feed data OLTP application addition analytics environment Design build schemas handle large scale interactive reporting Design build ETL ELT process move data data processing pipeline Manage complex data dependencies across datasets incremental data loading workflows Be fearless leader championing smart design Love scaling systems Must firm understanding database systems Data modeling SQL q uery Processing Transactions Know scale systems make fast Experience debugging tracing SQL performance issues Solid understanding software development design architecture production Large scale DW MPP Redshift similar technologies Solid math skills T ability present impromptu via whiteboard Experience Hadoop ecosystem HBase Hive map reduce Experience working AWS Knowledge Tableau Experience Redshift ParAccel Actian Matrix Experience Pentaho Kettle Linux basics Big Big bonus know programming language two Competitive compensation salary equity bonuses comprehensive benefits designed foster work life balance care health protect finances help save invest future Generous paid time away work including vacation holidays sick time days paid time year serve learn TiVo Community Outreach", "build infrastructure required optimal extraction transformation loading data wide variety data sources using SQL big data technologies design model data structures help analyzing business technical data support existing processes running production work together people key areas assist data related technical issues support data infrastructure needs", "", "", "Help relayr build world advanced IoT Cloud Platform Design develop improve microservices data processing pipelines analytics machine learning backend using latest technologies Be customer focused translate business requirements technological solutions Take part architecture code reviews develop solutions simple functional sustainable Collaborate teams define new features continuously improve internal systems Work QA DevOps development testing deployment services Work together data scientists design large scale machine learning systems You years experience backend engineering You strong knowledge Java Python eager learn new skills You good understanding designing scaling distributed systems You experience working SQL NoSQL databases messaging queues e g Kafka large scale data processing e g Apache Spark You able work structured manner care contribution end end You good command English language Flexible office environment modern office located Central Tower Munich relaxation room standing desks free fruits drinks Relayrians come world speak languages working language English welcome people ages parental statuses Our customers diverse join us connect network companies people around globe An extensive boarding period get speed rest team A learning environment build upon skills interests share knowledge attend events conferences pertaining discipline We also offer free German classes levels Have fun regular team lunches offsites yearly company summit branded goodies Competitive salary We fully support move Munich relocation budget visa assistance We help settle exciting new city paid vacation days public holidays Choose Mac Windows Linux machine", "Diligence perseverance proactiveness key succeed position Lead structure implementation metrics tracking systems working directly Sr Director Growth Utilize Amplitude Python Tableau Looker SQL pull data provide key analytical insights Report monitor track analyze traffic revenue KPIs Identify product risks opportunities communicate succinctly effectively Help product team set A B tests perform statistical analysis results provide actionable insights Build statistical models predict interactions segment users based behaviour Provide ad hoc analytics support needed ranging helping teams develop question tracking implementation analytics insights Collaborate Business Intelligence team building shareable data tools maintain documentation participate deep dives understand drivers success Collaborate multiple cross functional teams work solutions larger impact Xapo business years experience growth marketing data analytics related field Proven experience using successfully analytic platform Amplitude In depth understanding data structures algorithms Proven experience worked mobile app large growth Experience designing building dimensional data models improve accessibility efficiency quality data Strong analytical communication skills Experience developing maintaining automating visualizing analyzing communicating reporting management Bachelor Degree computer science related field preferred", "Enable data scientists train deploy machine learning algorithms scale fault tolerant highly available systems Use best practices continuous integration delivery Docker Kubernetes Work closely application engineers build data products power Carta core applications Create data pipelines using batch streaming tools like Airflow Spark Kafka Google Pub Sub Uphold engineering standards bring consistency many codebases processes encounter", "", "", "U S Citizenship Must able obtain security clearance Bachelors Degree Computer Science Computer Engineering Electrical Engineering related field Experience Java Kotlin Scala Experience scripting languages Python Bash etc Experience object oriented software development Experience working within UNIX Linux environment Experience working message driven architecture JMS Kafka Kinesis SNS SQS etc Ability determine right tool technology task hand Works well team environment Strong communication skills Experience massively parallel processing systems like Spark Hadoop Familiarity data pipeline orchestration tools Apache Airflow Apache NiFi Apache Oozie etc Familiarity AWS ecosystem services EMR EKS RDS Kinesis EC2 Lambda CloudWatch Experience working recommendation engines Spark MLlib Apache Mahout etc Experience building custom machine learning models TensorFlow Experience natural language processing tools techniques Experience Kubernetes Docker container environment Ability identify external data specifications common data representations Experience building monitoring alerting mechanisms data pipelines", "", "Interface software stakeholders understand infrastructure user requirements Develop support Etiometry data warehouse solution clinical personnel internal research staff Build test deploy software database upgrades production environment Utilize improve Etiometry clinical data cleaning tools techniques include data extraction de identification clinical measure normalization cleaning Design implement data requirements company research", "Conducts oversees business specific projects applying deep expertise subject area promoting adherence procedures policies developing work plans meet business priorities deadlines determining carrying processes methodologies coordinating delegating resources accomplish organizational goals partnering internally externally make effective business decisions solving complex problems escalating issues risks appropriate monitoring progress results recognizing capitalizing improvement opportunities evaluating recommendations made influencing completion project tasks others Practices self leadership promotes learning others building relationships cross functional stakeholders communicating information providing advice drive projects forward influencing team members within assigned unit listening responding seeking addressing performance feedback adapting competing demands new responsibilities providing feedback others including upward feedback leadership mentoring junior team members creating executing plans capitalize strengths improve opportunity areas adapting learning change difficulties feedback As part IT Engineering job family position responsible leveraging DEVOPS Waterfall Agile practices design develop deliver resilient secure multi channel high volume high transaction premise cloud based solutions Provides consultation expert technical advice IT infrastructure planning engineering architecture assigned systems assessing implications IT strategies infrastructure capabilities Provides recommendations input options risks costs benefits systems designs Leverages partnerships IT teams key business partners troubleshoot complex systems Serves functional expert collaborates architects software engineers ensure functional specifications converted flexible scalable maintainable system designs Translates business requirements functional non functional requirements technical specifications support integrated sustainable designs complex high impact infrastructure systems partnering Business Analysts understand business needs functional specifications Ensures system designs adhere company architecture standards Builds partnerships counterparts various IT Teams e g database operations technical support throughout system development implementation Serves technical expert project teams throughout implementation maintenance assigned enterprise infrastructure systems defining overseeing documentation detailed standards e g guidelines processes procedures introduction maintenance services Mentors technical resources throughout infrastructure systems development Reviews validates technical specifications documentation complex multi dimensional solutions Leads development modification solutions identifying technical solutions business problems Collaborates business leaders Solutions lead enterprise architects review business drivers establish foundation enterprise systems planning Reviews benchmarking results provides information support current future infrastructure needs projects IT leadership Provides preliminary conclusions Benchmarks evaluates IT trends technologies identify opportunities considerations impact ROI Makes recommendations resources required maintain service levels meet new demands Guides drives physical architecture design new initiatives khgrsr Minimum Qualifications", "", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "Collaboratively architect build launch maintain new Social Graph components enhance profiles increase coverage edge accuracy Create maintain scale data pipelines data ingesters Social Graph machine learning predictors client deliverables data warehousing Interact cross functionally wide variety people teams Work closely client services leads data scientists identify opportunities assess improvements Applecart products deliverables Integrate systems monitoring streaming batch data processing e g DataDog Nagios Track data quality consistency Evangelize solid coding practices e g unit integration testing code reviews continuous deployment automated linting staging environments mentor junior engineers Contribute architectural designs decision making around data stores schemas data security cloud storage Rapidly prototype proof concept data pipelines ROI determination replace modular productionized versions Keep abreast industry trends best practices emerging methodologies Support quality assurance part engineering process collaboration product managers producing sampled outputs supporting KPIs outlining PR limitations future improvements", "Maintaining ETL program DB DW Adding new tables indices Experience Airflow preferred Monitoring CVEs Updating troubleshooting existing code Adding new tables indices Experience Airflow preferred Monitoring CVEs Updating troubleshooting existing code Applying Dimensional Modeling principals integrate various data sources existing data APIs flat files databases etc Data warehousing techniques ensure architecture support requirements business Building data pipelines Testing deploying validating code In depth knowledge database design principles SQL NoSQL Basic knowledge analyst ML principles required Ability recommend implement ways improve data reliability efficiency quality Lead development software engineering efforts web applications Coordinating developers data projects Coordinate assist deployment ML models Attend design architecture meetings assist project projections hours Communicate improvement ideas needs concerns Update projects daily project management software Update hours projects daily hourly time tracking software Update progress project percentage completion Complete projects time within time budgets Other duties assigned Python SQL Years experience Multithreading parallel processing experience preferred Multithreading parallel processing experience preferred Airflow ETL tools experience required Docker experience required", "Design build data structures MPP platform like AWS RedShift Druid io Design build highly scalable data pipelines using AWS tools like Glue Spark based Data Pipeline Lambda Translate complex business requirements scalable technical solutions Strong understanding analytics needs Collaborate team building dashboards using Self Service tools like Apache Superset Tableau data analysis support business Collaborate multiple cross functional teams work solutions larger impact Xapo business In depth understanding data structures algorithms Experience designing building dimensional data models improve accessibility efficiency quality data Experience designing developing ETL data pipelines Proficient writing Advanced SQLs Expertise performance tuning SQLs Programming experience building high quality software Skills Python Scala preferred Strong analytical communication skills Work project experience big data advanced programming languages Experience using Java Spark Hive Oozie Kafka Map Reduce Work experience AWS tools process data Glue Pipeline Kinesis Lambda etc", "Focus designing building launching efficient reliable data infrastructure scale compute business Help us build world class data lake data warehouse building data pipelines Design develop new systems tools enable folks consume understand data faster Use expert coding skills across number languages Python Java C Go etc Work across multiple teams high visibility roles solution end end Design build launch new data extraction transformation loading processes production Work data infrastructure triage infra issues drive resolution BS MS degree Computer Science related technical field Familiarity Python Familiarity Hadoop stack Spark AWS Glue AWS Athena etc Diverse data storage technologies RDBMS Sql Server Mysql ElasticSearch dynamodb s3 etc Deep familiarity schemas metadata catalogs etc Ability manage communicate data warehouse plans internal clients", "Use analytical data driven approach drive deep understanding business Build data pipelines data models empower engineers analysts make data driven decisions Build data models deliver insightful analytics Deliver highest standard data integrity Strong analytical skills ability analyze project sales subscriber engagement data Performs competitive analysis reviews industry information current trends opportunities Works closely analytics teams develop comprehensive analytical reports enable data driven decisions increase engagement conversions target customer segments", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "Alpharetta GA Bachelor Hadoop years", "Build maintain data pipeline machine learning Assist development data warehouse reports derived Process billion event transactions month Assure data captured stored without loss Write code provide reports business data science Write system run reports configurable schedule Respond ad hoc requests information years experience Python Java Three years experience developing operating data engineering solutions cloud preferably AWS Three years working distributed big data systems e g Hadoop Redshift Professional experience building data science systems experience building data pipelines ETL processes machine learning Experience working remotely", "A fun collaborative team environment Autonomy resources get job done Weekly paid team lunches Agile development process Increase capabilities Reporting Advanced Analytics platforms support business insights internal external users Maintain Data Integrity identifying root causes problems deploying solutions Improve accuracy timeliness efficiency measurement Healthcare Quality metrics Develop Improve infrastructure novel Fraud Waste Abuse detection screening methods years experience data transformation data management using SQL including Stored Procedures ETL processes tuning performance Skilled integrating data single tenant data stores third party data feeds APIs Supporting operational needs Business Intelligence tools like Tableau Experience Salesforce CRM Salesforce APIs Experience data management technologies including data warehouses data lakes data marts NoSQL data stores Nice Experience working cloud technologies AWS Nice Experience Healthcare IT Flexibility collaborative work approach solve complex problems promote standardization Collaborate business owners application development team align business objectives prioritize work Excellent verbal written communication skills Ability work structured governed methodical approach A self starter delivers consistently high quality work Strong problem solving analytical skills", "Excellent communication skills verbal written Empathy colleagues clients Signs initiative ability drive things forward Understanding overall problem solved flows Ability create implement data engineering solutions using modern software engineering practices Ability scale laptop scale cluster scale problems terms infrastructure problem structure technique Ability deliver tangible value rapidly working diverse teams varying backgrounds Ability codify best practices future reuse form accessible reusable patterns templates code bases A pragmatic approach software technology decisions well prioritization delivery Ability handle multiple workstreams prioritize accordingly Commitment delivering value helping clients succeed Ability tailor language technical non technical audience Comfort working collocated distributed team members across time zones Comfort working developing coding standards Ability codify best practices future reuse form accessible reusable patterns templates codebases Willingness travel required cases A technical background computer science data science machine learning artificial intelligence statistics quantitative computational science A compelling track record designing deploying large scale technical solutions deliver tangible ongoing value Direct experience built deployed complex production systems implement modern data science methods scale robustly Comfort environments large projects time boxed therefore consequential design decisions may need made acted upon rapidly Fluency cluster computing environments associated technologies deep understanding balance computational considerations theoretical properties potential solutions Ability context switch provide support dispersed teams may need expert hacker unblock especially challenging technical obstacle Demonstrated ability deliver technical projects team often working tight time constraints deliver value An engineering mindset willing make rapid pragmatic decisions improve performance accelerate progress magnify impact recognizing good enemy perfect Comfort working distributed teams code based deliverables using version control systems code reviews Direct experience built deployed complex production systems implement modern data science methods scale robustly Comfort environments large projects time boxed therefore consequential design decisions may need made acted upon rapidly Fluency cluster computing environments associated technologies deep understanding balance computational considerations theoretical properties potential solutions Ability context switch provide support dispersed teams may need expert hacker unblock especially challenging technical obstacle Demonstrated ability deliver technical projects team often working tight time constraints deliver value An engineering mindset willing make rapid pragmatic decisions improve performance accelerate progress magnify impact recognizing good enemy perfect Comfort working distributed teams code based deliverables using version control systems code reviews Demonstrated expertise working maintaining open source data analysis platforms including limited Pandas Scikit Learn Matplotlib TensorFlow Jupyter Python data tools Spark Scala PySpark HDFS Hive Kafka high volume data tools Relational databases SQL Server Oracle Postgres NoSQL storage tools MongoDB Cassandra Neo4j ElasticSearch Pandas Scikit Learn Matplotlib TensorFlow Jupyter Python data tools Spark Scala PySpark HDFS Hive Kafka high volume data tools Relational databases SQL Server Oracle Postgres NoSQL storage tools MongoDB Cassandra Neo4j ElasticSearch", "M S Computer Science Informatics Mathematics Electronic Electrical Engineering relevant field emphasis data analytics Experience big data technologies Hadoop Apache Spark NoSQL databases Strong computer science grounding knowledge data structures algorithms computer architectures Proficiency developing one languages C Python Java Self starting requiring minimal supervision strong problem solving skills Excellent communication teamwork skills Hands experience Cloud environments AWS Google Cloud Azure", "", "You build operate large scale data infrastructure production performance reliability monitoring Designing implementing debugging distributed systems Yu think long term impacts key design decisions handling failure scenarios Building self service platforms power WeWork Technology You focused team individual achievements You create software incrementally make consistent progress You love learn mentor teach others You empathetic build long lasting relationship characteristic highly efficient teams You keep date latest developments field", "Help us create AI ML ready datasets Petabytes raw data meta data Automate integration different data sources coherent flow data pipelines support also data normalization result calculation Develop build systems architectures ETLs Perform system data testing Understand apply FAIR data principles Strong adherence compliance regulatory environments Build algorithms", "", "", "", "Build scalable backend data applications support growing needs business Build data pipelines collect process compute business metrics marketplace activity Leverage best practices continuous integration delivery Collaborate engineers Expertise building data training pipelines using Spark Hadoop Experience building production grade API expose results Extensive programming experience Python Scala Java A BS MS Computer Science related technical fields At least years experience production environment Experience interest ML Qualified applicants receive questions answer Selected candidates invited schedule minute intro call CTO Next Candidates invited schedule behavioral interview CEO Next candidates invited schedule technical interview CTO The technical interview split technical discussion technical test Next candidates invited review technical test CTO software engineers Candidates invited schedule additional interview CEO CTO COO Successful candidates subsequently made offer via email", "Design implement support robust scalable solutions enhance business analysis capabilities identify gaps design processes fill Work analysts understand business priorities translate requirements data models Collaborate various stakeholders across company like data developers analysts data science finance etc order deliver team tasks Build complex multi cloud ETL pipelines Apache Airflow Build Python API integrations 3rd party vendors", "", "Collect analyze share data help product teams drive improvement key business metrics customer experience Propose prioritize changes reporting create additional metrics processes based program changes customer requirements Work closely Alexa program teams create ad hoc reports support timely business decisions project work Identify implement new capabilities best practices develop improve automated data analysis processes", "", "In depth knowledge Python pandas open source ecosystem focus parallelized processing Experience configuring working relational databases terabyte scale e g MS SQL Server Several years experience working designing data intensive applications pipelines Commitment ensuring data quality integrity complex systems within time sensitive environment Experience OS independent cross platform development Passion work effectively interdisciplinary teams technical non technical individuals different cultural backgrounds health related business problems Business proficiency English German Passion using contributing open source data science ecosystem Ability evaluate business value different technical projects prioritize appropriately", "Years professional experience Python Python data structures best practices AWS Linux Ubuntu CentOS Bash scripting Professional experience SQL based database MySQL Writes organized code appropriate exception handling logging Understanding HTTP network requests responses Understanding HTML JSON formats Ability write technical documentation comment code Ability write test suites set automation environments An undergraduate degree advanced degree Statistics Computer Science related field Experience scrapy beautiful soup requests selenium Amazon Redshift Hadoop Prior experience front end back end full stack web developer Javascript NodeJS", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "Collaborate across development teams understand customer business objectives data requirements Apply repeatable processes established knowledge expert judgment design build operate data analytics solutions across financial insurance value chain Help customers overcome data related business challenges modelling integrating profiling cleaning data provide enterprise information accurate consistent managed trusted understood Leverage technology automate common maintenance tasks provide insights health wellness infrastructure platform years experience data solution delivery implementations Ability identify issues provide real time solutions Experience engineering best practices includes analyzing designing developing deploying supporting data solutions infrastructure implementations upgrades Advanced skills knowledge related Data Modelling Data Integration ETL Data workload utilization tools Hands experience designing implementing supporting ETL data solutions Informatica PowerCenter Power Exchange ETL tools Strong SQL knowledge including writing review complex SQL statements performance tuning handle scale agility changes Experience database technologies including relational databases DB2 Sybase Experienced working agile developing environment leveraging BDD TDD whole team approaches within fully dedicated agile team", "Coordination execution needed processes governance procedures accurately define ensure data quality Main point contact data scientists regarding data preparation Preparation data analytical purposes Support data owner meta data management Responsible definition utilization monitoring data quality management mechanisms tools Measurement monitoring data KPIs Taking data movement transformation steps prototype status production Preparation reports presentations senior staff relevant stakeholders give insights business decision making Completed academic studies information systems mathematics machine learning technical scientific studies comparable several years experience field Previous work experience Data Engineering English German language fluently Strong experience underlying foundational data data warehouses business intelligence systems Experience relational unstructured data environments e g SQL noSQL Hadoop Hive Spark Experience SQL server plus Understanding advanced analytic techniques gathered first experiences effectively collaborate data scientists Analytical thinking Self driven fast learner eager work new technologies", "", "", "Build large scale real time systems provide FindHotel analysts data scientists decision makers high quality low latency data Work multi functional agile teams continuously experiment iterate deliver new product objectives Design implement data models help analysts tracking user behaviour across product Build ETL Business Intelligence environment dashboards used widely product teams When needed improve way data delivered visualised better way presenting multi dimensional data Use best practices continuous integration delivery Help drive optimization testing tooling improve data quality Collaborate Software Engineers Data Engineers stakeholders taking learning leadership opportunities arise", "", "years experience Scala Python preferred Distributed systems e g Spark Hadoop Scala Python preferred Distributed systems e g Spark Hadoop Database systems e g Postgres MySQL Experience following preferred IP v4 v6 allocation addressing conventions DNS conventions best practices Anti abuse investigations IP v4 v6 allocation addressing conventions DNS conventions best practices Anti abuse investigations Bachelor degree CS CE EE Math Statistics preferred Comfortable working part distributed team Excellent communication teamwork skills Ability make data driven decisions Ability independent research Phone conversation Talent Acquisition team member learn experience career objectives minutes Technical interview hiring manager via video preferred Will include coding minutes technical interviews data engineer data science team members via video person minutes", "Build analytical solutions enable Data Scientist manipulating large data sets integrating diverse data sources Work closely data scientists database systems administrators create data solutions Bachelor degree four years work experience Experience designing building deploying production level data pipelines using tools Hadoop stack HDFS Hive Spark HBase Kafka NiFi Oozie Splunk etc", "building products help non technical users extract insights building applications leverage ML solve problems data automation product development installing database systems writing queries Ability communicate business stakeholders translate business needs data requirements tables data visualization Experience statistical scripting programming packages SQL Python R Matlab great benefits competitive salary bonus", "", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "", "Support improve existing experimentation analytics pipelines systems production Design develop new data pipelines streaming processes highly available scalable reliable Maintain evolve dimensional data models schema designs improve performance efficiency Scale self serve experimentation platform drive product innovations countries BS MS Computer Science Computer Engineering related technical discipline years experience building maintaining large scale analytics systems years programming experience Scala Java Python Experience working Hadoop Spark ElasticSearch Experience NoSQL RDBMS platforms DynamoDB Redis Postgres Experience streaming platforms Apache Kafka Flink Beam Aws Kinesis Strong understanding architecting developing maintaining cloud technologies architecting especially Amazon Web Services The hustle startup impact global business Tremendous opportunity solve industry exciting problems Working extraordinary team smart creative fun highly motivated people Comprehensive health coverage competitive salary k match meaningful equity Unlimited vacation flexible working hours Daily catered lunches endless supply refreshments basketball court fitness classes social events", "Analysis design coding performance tuning implementation new BI solutions Evaluation maintenance enhancement existing ETL processes model cube structures Creation BI dashboards workbooks using Power BI Tableau Excel Code data ingest solutions C NodeJS leveraging 3rd party APIs Daily triage bugs tasks Lead team data warehousing engineers acting liaison internal 3rd party clients providing regular updates management Create documentation applications processes Enforce best practices aspects work evaluate new data warehousing platforms solutions years experience data warehouse team environment Must Tabular data warehouse design experience including solid understanding MDX DAX calculated measures hierarchies slowly changing dimensions date time dimension analysis Advanced level experience Microsoft SQL Server SSAS SSMS Management Studio Hands experience SSIS similar ETL tools Demonstrated strength T SQL database queries procedures tables views etc Proven skills data visualization tools Microsoft Excel PowerPivot Pivot Tables Slicers Power BI Tableau dashboard design configuration advanced calculations Ability work Net development tool Visual Studio write code make changes deploy updates Experience storage devices connectivity tools AWS development tools NodeJS Lambda Functions A basic understanding HTML CSS web page deployment Excellent written verbal skills ability present upper management A good understanding basketball", "Participating development maintenance analytics data platform whole company Evolving platform make robust scalable Supporting data warehouse data lake ETL jobs ensure data available consumers reliable trustworthy predictable manner Creating self serving tools data ingestion quality tracking consumption Closely collaborating supporting BI analysts data scientists consumers data Ability understand business requirements translate technical solutions Hands experience building large scale distributed data platforms cloud Experience Apache Spark Hadoop Hive AWS Kinesis Kafka similar distributed data processing tool chain Programming experience analyzing manipulating data preferably Python Scala Java Deep knowledge SQL queries traditional data warehousing data modeling fundamentals Experience working data warehouses like Redshift Snowflake BigQuery advantage Familiarity stream batch data processing patterns Experience investigating fixing data quality issues Willingness learn new technologies Great communication skills proactively communicating stakeholders Fluent English speaking writing English office work language Exciting technical challenges work great everyday opportunities learn grow develop A diverse team nationalities Free lunches yoga German lessons", "Help relayr build world advanced IoT Cloud Platform Design develop improve microservices data processing pipelines analytics machine learning backend using latest technologies Be customer focused translate business requirements technological solutions Take part architecture code reviews develop solutions simple functional sustainable Collaborate teams define new features continuously improve internal systems Work QA DevOps development testing deployment services Work together data scientists design large scale machine learning systems You years experience backend engineering You strong knowledge Python Java You experience developing microservice architecture You good understanding designing scaling distributed systems You experience working SQL NoSQL databases messaging queues e g Kafka large scale data processing e g Apache Spark You able work structured manner care contribution end end You good command English language Flexibility safety important us As company quick react Corona pandemic began sending employees work home If start new employee relayr working home enjoy structured digital onboarding programme flexible working hours Modern office located Central Tower Munich relaxation room standing desks free fruits drinks Relayrians come world speak languages working language English welcome people ages parental statuses Our customers diverse join us connect network companies people around globe An extensive boarding period get speed rest team A learning environment build upon skills interests share knowledge attend events conferences pertaining discipline We also offer free German classes levels Have fun regular team lunches offsites yearly company summit branded goodies Competitive salary We fully support move Munich relocation budget visa assistance We help settle exciting new city paid vacation days public holidays Discounted Urban Sports Club membership Choose Mac Windows Linux machine", "", "In depth knowledge Python pandas open source ecosystem focus parallelized processing Experience configuring working relational databases terabyte scale e g MS SQL Server Several years experience working designing data intensive applications pipelines Commitment ensuring data quality integrity complex systems within time sensitive environment Experience OS independent cross platform development Passion work effectively interdisciplinary teams technical non technical individuals different cultural backgrounds health related business problems Business proficiency English German Passion using contributing open source data science ecosystem Ability evaluate business value different technical projects prioritize appropriately", "Create data tools Data Science team members assist building optimizing product Assemble large datasets meet requirements set Data Science team including creating web crawlers Be proactive bringing forth new ideas solutions problems Be strong team player share knowledge freely easily co workers Write software post processing cleaning data taking part data analysis required Automate manual processes optimize data delivery improve architecture greater scalability Work integration Data Science components larger systems Handle mid size large datasets 200GB", "looking next gig CV permanent occupation interesting continuously changing environment good fit culture also add value expert Java development proficient Linux user Although worked various technologies past resume points relevant ones open honest communication agile values mindset roles processes high performing teams usually built diversity agile collaboration principles application best practices software development work one international cross functional agile teams create data pipelines Hadoop environments using example Spark Java AWS services integrate data science statistical solutions huge amounts data maintain enhance existing code base permanent position competitive salary flexible working conditions good work life balance days vacation parent friendly regulations support relocation visa sponsorship language courses", "Design implement secure data pipelines Snowflake data warehouse premise cloud data sources Design implement data pipelines prepare process organize data warehouse Design implement high performing BI dashboard integrations leading BI Visualization tools working BI developers Design implement high performing data pipelines feeding downstream systems Troubleshooting problem solving performance tuning data pipelines queries accessing data warehouse Creation best practices standards data pipelining integration Snowflake data warehouses Ensure enterprise security access control policies adhered solution Creation architecture design artifacts documents Conduct design code reviews Work Business Analysts Users translate functional specifications technical requirements designs", "", "Design develop maintain New Knowledge data pipeline Support monitor pipeline performance production Take ownership components data pipeline Work product managers understand upcoming work design system capable meeting long term product vision Create maintain documentation capable describing pipeline work non technical audience Work part team integrate new services New Knowledge data pipeline using tools like Docker Kubernetes Kafka PostgreSQL year developing software part team preferably working aspect data pipeline Experience familiarity Kafka similar distributed systems knowledge schema registry data types serialization options partition strategies plus Experience familiarity popular stream processing framework Spark Streaming Kafka Streams Flink similar Experience supporting large scale batch analytics Hadoop ecosystem loading retrieving data Experience working building schedulers workflow automation coordination tools Experience implementing tests sanity checks large complex data pipelines Experience helping developers write performant SQL queries Ability monitor current solution understand limits stay ahead business needs Comfortable representing data engineering function front Senior leaders product management prioritization design discussions Knowledge evaluate tools ability document pro cons infrastructure decisions Desire mentor junior developers years professional software development You experience working data pipelines Experience processing social media text image data particularly relevant Highly motivated research prototype implement state art data pipeline Comfortable knowing know asking help finding answers required Previously held leadership positions technical otherwise Competitive salary k matching Fortune level healthcare We business trusts embraces technology harnesses good We embrace diverse ideas autonomy collaboration Professional development opportunities host lunch learns hold weekly policy expense professional development books A diverse leadership team wants uphold ethical practices software development process A strong commitment creating diverse environment Free parking building downtown Austin Free access gym office building A annual credit spend technology work gear choice Parental leave plan manager unlimited vacation code never take vacation encourage value time", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "Architect develop framework automate ingestion integration structured data wide variety enterprise data sources Architect develop data pipeline components integrate Formation Platform Scale pipelines meet performance requirements Provide leadership senior junior Data Engineers Architect design data quality monitoring automated data cleaning Responsible technical direction team years software development experience years experience building scalable reliable data pipelines using Big Data engine technologies like Spark AWS EMR Redshift etc years experience scalable data integration technologies like ETL Data Virtualization Experience cloud technologies AWS Google Cloud SaaS experience Significant industrial experience using Scala willingness learn Scala demonstrated competence functional programming", "Develop implement custom data capturing solutions advanced video analytics cloud environment Participate developing requirements implementation necessary analytics development order properly accurately report relevant KPIs Provide guidance data integrity issues ongoing maintenance data governance Maintain detailed documentation data collection methods dependencies Work closely developers product management team validate maintain analytics reporting Work key stakeholders integrate analytics requirements updates production cycles Technical validation digital analytics implementations conduct audits troubleshoot tracking gaps maintain data confidence implement data standardization practices Provide general troubleshooting assistance analytics tracking break fix Performing functional regression integration smoke acceptance tests Work application development team ensure proper deployment integration performance compliance architectural standards best practices Successfully implement process improvements impacting work work others years experience building modern JS libraries frameworks React Redux Typescript ES6 Experience real time streaming Big Data analytics implementation Excellent understanding implementation click stream video analytics Very good understanding technical details regarding implementing data collection analytics solutions like Adobe Analytics Google analytics Snowplow Alooma Kafka AWS Kinesis etc Experience building maintaining testing debugging modern technology stacks Solving large application user level problems performance scalability etc Keen attention detail thoroughness required BA BS higher degree math engineering computer science related field required Experienced working fast paced high tech environment preferably software development comfortable navigating conflicting priorities ambiguous problems", "Create robust data workflows move transform data scale inform key metrics recommend changes predict future results Own architecture Dots data infrastructure including exploring implementing new big data solutions optimizing existing ones Manage data applications AWS Design implement complex big data solutions focus collecting parsing surfacing meaningful data using multiple platforms Evaluate new technologies products", "Business Intelligence development Design maintain set automated tools reports Scraping software extract key data years experience programming statistical computer language ideally R Strong understanding databases data storage Knowledge SDLC Experience developing software applications ideally Python Excellent communication problem solving skills Collaborative team player self starter", "Developing data infrastructure ingest transforms data different sources customers scale Creating machine deep learning infrastructure generalizes across hundreds thousands Salesforce customers expressive enough generate high lift Partner end end Product Managers Data Scientists understand customer requirements design prototypes bring ideas production Working internal product teams ingest data sprinkle machine deep learning fairy dust products Participating meal conversations team members really important topics Should cuteness panda bears factor survivability Is love decision tree regression model How far ahead would society today fingers instead We develop real products You need expert coding including Java Object Oriented Programming We also use Scala Functional Programming principles We prioritize professional industry experience advanced degrees alone replace real world experience We massive scale You need experience distributed scalable systems Consistency availability tradeoffs made You tinkered modern data storage messaging processing tools Kafka Spark Hadoop Cassandra etc demonstrated experience designing coding big data components HBase DynamoDB similar We growing diverse team work together projects We love collaborate help want someone share ideology You quick learner face new challenges every day anything ranges operating model financial services companies conversation model chatbots tinkering convolutional recurrent networks make Spark work S3 file system No school could prepare need quick feet Self starter see big picture prioritize work make largest impact business customer vision requirements Excellent communication leadership collaboration skills We run AWS We dockerize applications You notion build test deploy code run cloud infrastructure Experience open source tools information retrieval e g Solr Search Data Scoring Ranking expertise Data visualization Experience Deep Learning NLP Experience developing open source machine learning libraries Apache Mahout MLLib", "Create maintain optimal data pipeline architecture Assemble large complex data sets meet functional non functional business requirements Identify design implement internal process improvements automating manual processes optimizing data delivery designing infrastructure greater scalability etc Build infrastructure required optimal extraction transformation loading data wide variety data sources using SQL AWS big data technologies Build analytics tools utilize data pipeline provide actionable insights customer acquisition operational efficiency key business performance metrics Work stakeholders including Executive Product Data Design teams assist data related technical issues support data infrastructure needs Keep data separated secure across national boundaries multiple data centers AWS regions Create data tools analytics data scientist team members assist building optimizing product innovative industry leader", "years experience complex data structures Real world experience developing Net Core You deep experience latest libraries programming techniques You accomplishments showcase capabilities success technical depth You new features idea completion Work well core team design execute major new features Familiar SQL NoSQL databases like MongoDB declarative query languages plus Enjoy contributing fast moving exciting project Strong experience using GitHub professional environment Strong communicator fluent English excellent written verbal communication skills Thrive excel diverse distributed agile team environment", "Own Babylist data science analytics infrastructure DevOps development Architect implement data pipelines systems analytics data science ML use cases Bring engineering rigor best practices art data management Mentor analysts data scientists team Bachelor advanced degree Computer Science Engineering Hands experience writing deploying production grade code Solid experience object oriented scripting language like Python advanced SQL Worked data orchestration tools like Airflow Luigi dbt Strong understanding data modeling ETL principles modern data warehousing systems like Redshift Snowflake DevOps disguise well versed AWS ecosystem including managing deploying cloud data resources EMR EC2 RDS S3 Athena Lambda Spark Experience integrating data multiple sources including third party APIs preferred", "Welcoming friendly environment Unlimited opportunity develop skills expertise Be part multinational international teams Work challenging interesting meaningful Customers world Solid benefits package health insurance clear guidelines career growth paid overtime strong compensation package Continuous investments employee future career counselling wide range trainings industry recognized certificates You part team working projects related Big Data technologies You work world leaders various industires like TV Entertainment Telecommunication Finances Insurance etc As data engineer work Big Data systems like Hadoop Spark Storm etc design data processing applications You get unique possibility learn programming languages e g Scala Python Java utilize newest technologies like noSQL data engineering Accenture In final together visualization data build visual analytics systems explain customers data fancy manner Data related job profession future Accenture brings future today searching new colleagues Big Data field", "Consult part team charge building end end digital transformation capabilities lead fast moving development teams using Agile methodologies Design build real time analytics solutions using industry standard technologies work data architects make sure Cloud Data solutions align technology direction Lead example role modeling best practices unit testing CI CD performance testing capacity planning documentation monitoring alerting incident response Keep everyone individual contributors top executives loop progress communicating across organizations levels If critical issues block progress refer chain command resolved timely manner Develop Cloud Native architecture data supply chain Architect Prototype Test end end data supply chain use cases drive business value provide architecture support data scientists Pinpoint clarify key issues need action lead response articulate results clearly actionable form Show strong aptitude carrying solutions translating objectives scalable solution meets end customers needs within deadlines", "You thrive data environment Cloud use AWS Someone understands complex data challenges accessing A real bottom line person someone throws terms like big data around popular Hadoop traditional relational databases make difference Cloud use AWS Someone understands complex data challenges accessing A real bottom line person someone throws terms like big data around popular A real bottom line person someone throws terms like big data around popular Hadoop traditional relational databases make difference You dream code SQL seriously SQL SQL queries impress us Teach us something new show us got Python Scala Spark SQL seriously SQL SQL queries impress us Teach us something new show us got Python Scala Spark You know databases inside Database concepts indexes execution engines etc Database Administration experience Redshift MySQL PostgreSQL Oracle You understand databases integral part Data Engineer Database concepts indexes execution engines etc Database Administration experience Redshift MySQL PostgreSQL Oracle You understand databases integral part Data Engineer You enjoy looking solving big picture problems No micromanaging hand holding like ask questions devise complete solution You want understand data pipes definitely perform analytics build dashboards like Yes really No micromanaging hand holding like ask questions devise complete solution You want understand data pipes definitely perform analytics build dashboards like Yes really You love learning new things You know know enough bothers enough time day learn next topic You date new trends data know using solve various problems excited next release favorite tool If like thrown deep end pool team You know know enough bothers enough time day learn next topic You date new trends data know using solve various problems excited next release favorite tool If like thrown deep end pool team You take personally You sleep well night leave work question unanswered You feel accountable everything sense urgency driving entire life You sleep well night leave work question unanswered You feel accountable everything sense urgency driving entire life You work hard play harder You like good time getting things done When say team player mean crisp high five funny stories tell You team back And team Sense humor must ash Mustache Get Mustache You like good time getting things done When say team player mean crisp high five funny stories tell You team back And team", "", "Hands production experience Hadoop family big data technologies Hive Impala HBase etc Collaboration business partners craft iterate solutions extract value data Experience Spark Python Java Strong analytical skills fervor data integrity accessibility", "Competitive base salary Profit sharing equity based experience covered Health insurance employees covered dependents Four weeks PTO k employer matching Personalized monthly perks matched charitable donations", "Produce clean standards based modern code emphasis advocacy toward end users produce high quality software designs well documented Demonstrate understanding technology digital frameworks context data integration Ensure code design quality execution test plans assist development standards methodology repeatable processes working closely internal external design business technical counterparts Utilizes existing methods procedures create designs within proposed solution solve business problems Understands strategic direction set senior management relates team goals Contributes design solution executes development design seeks guidance complex technical challenges necessary Primary upward interaction direct supervisor May interact peers client counterparts management levels within Accenture Understands methods procedures new assignments executes deliverables guidance needed May interact peers management levels client within Accenture Determines methods procedures new assignments guidance Decisions often impact team reside Manages small teams work efforts individual contributor role client within Accenture", "", "", "Java Python Oracle Redis RabbitMQ Docker CloverETL Amazon Web Services Aurora Redshift DynamoDB S3 Lambda Kinesis ElastiCache Have BA BS Computer Science Engineering Information Systems equivalent experience Self starter years experience Data Engineer dealing large complex data scenarios Proven ability work varied data infrastructures including relational databases column stores NoSQL databases file based storage solutions Experience compiled scripting languages Expert level SQL skills Excellent communication verbal written interpersonal skills ability effectively communicate business technical teams Write great code view craft love cranking solid work Competitive compensation generous employee benefits package including fully paid Medical Dental Vision plans employees dependents Stock options employees We bring toys office still think fun thing build product Collaborative work environment fosters bonds beyond workplace We treat people well employee recognition programs referral bonuses Better average team building activities paintball go cart racing shuffleboard organized sporting teams site retreats Provide lunches drinks snacks team hungry things Learn teach CrowdTwist U", "Produce clean standards based modern code emphasis advocacy toward end users produce high quality software designs well documented Demonstrate understanding technology digital frameworks context data integration", "Extend product data architecture foundation Revise existing ontologies usage existing product Selecting features building optimizing classifiers using machine learning techniques Processing cleansing verifying integrity data used analysis Deliver scalable application platform basis big data technologies fit current future data usage scenarios ETL data mining anomaly detection processes foundation product extensions Assess design services consume data high volume traffic challenges spikes computation demand challenges Design scalable Reporting platform possibly near real time reporting Be hands statistical data analysis data patterns extraction drive new business insights Contribute data security design implementation Safe guard enterprise product quality Contribute Product Roadmap Contribute environment tools designs resources efficiency Bachelor degree equivalent relevant work experience Experience Agile methodologies Scrum Kanban Product development experience knowing takes build release versions future proof product Have worked distributed team plus Proficient understanding distributed computing principles Experience building stream processing systems experience Spark Amazon EMR Kafka Amazon Kinesis big plus Experience integration data multiple data sources Experience NoSQL databases MongoDB etc Excellent understanding machine learning techniques algorithms k NN Naive Bayes SVM Decision Forests etc experience big data ML tools SparkML pattern python Experience common data science toolkits R NumPy Experience ELK stack Comfort Scala plus Excellent communication skills English Business focus Enterpreneurial spirit Technical curiocity Good communicator Pragmatism Goal driven personality Lame jokes I say beer Fridays Laptop preferences two extra screens desk Flexible working hours Time budget training weeks paid holiday per year Good salary additional incentives", "Performing data engineering data modeling implementation Big Data platform analytic applications clients Analyzing latest Big Data Cloud technologies innovative applications business intelligence analysis new service offerings Developing highly scalable extensible Big Data platforms enable collection storage modeling analysis massive data sets Constructing big data pipelines real time batch Implementing data access processing frameworks Supporting data users data scientists analytic applications Building Cloud Native Solutions Cloud platform technologies Microsoft Azure Amazon Web Services Google Cloud Big Data Analytic frameworks query tools Spark Storm Hive HBase Impala Hue Streaming data tools techniques Kafka AWS Kinesis Microsoft Streaming Analytics StreamSets StreamAnalytixs ETL Extract Transform Load tools Talend Informatica also experience ELT SQL Infrastructure setup using things like Kubernetes Docker Continuous Integration Continuous Development CI CD Data Warehouse DataMart design implementation NoSQL environments MongoDB Cassandra Metadata management data lineage data governance especially related Big Data Structured Unstructured Semi Structured Data techniques processes Around years engineering software development experience Around years experience data engineering ETL Big Data pipelines real time batch Hands experience Python SQL AWS Redshift Snowflake type tech Experience Consulting supporting Client Requirements", "Take responsibility designing developing data processing pipeline Build centralized data warehouse long run acts unique source relevant data Work closely together engineers BI team understand data collecting Zattoo view users actions journey Be house consultant best track store access data Minimum years working Data Engineer Bachelor Degree Computer Science related studies Ability maintain ETL stack Ability engineer ETL processing whole data pipeline Fluent verbal written English skills Strong communication presentation skills Experience following technologies Jenkins Various database technologies search MySQL Postgres vertica SAP HANA SQL Hadoop hdfs AWS EMR Pig Impala Apache Parquet tableau Jenkins Various database technologies search MySQL Postgres vertica SAP HANA SQL Hadoop hdfs AWS EMR Pig Impala Apache Parquet tableau Bonus Deep knowledge ETL processing big amounts data Bonus Ability write hadoop map reduce jobs Bonus Dev skills transform data via scripting language like ruby python java golang Bonus Experience streaming frameworks spark storm samza nimble Bonus Experience Kafka Flexible work schedules Cool comfortable offices Relaxed atmosphere dress code Working awesome product Motivated international team", "Being member Metareview team working cross discipline delivery team focused one many core data products Gather process raw data scale using frameworks Hadoop MR Spark Maintain write new data processing pipelines handling hundreds GB data Optimize improve existing features data processes performance stability Apply machine learning algorithms improve product drive decisions years experience building data intensive applications Very strong programming architectural experience ideally Python Java Scala open experience would like become Python hacker You find creative solutions tough problems You great developer also architect afraid pave way bigger better things Experience skills clean scrub noisy datasets Experience building data pipelines ETLs using MapReduce Spark Flink Expert level knowledge Python Experience frameworks Pandas Scikit learn Scipy Luigi Airflow plus Love command line optional affinity Linux scripting Experience Big data technologies Hadoop Spark Flink Hive Impala HBase Pig Redshift Kafka Experience building scalable REST APIs using Python similar technologies Experience data mining machine learning natural language processing information retrieval plus Experience AWS IaaS PaaS", "Build ETL processes 3rd party API integration ingestion data store Research design implement NLP functions keyword named entity extraction Develop machine learning driven search ranking recommendation algorithms Help diagnose fix production issues arise Minimum years experience developing production environment Proficient Python Go JVM based server side languages Experience NLTK scikit learn similar technologies Ability research implement solutions Good testing habits", "Explore new ways transforming analyzing data continuously expand improve performance data pipelines Build prototypes fast determine worth business within infrastructure iterating improving Work closely Data Scientists Product Managers decide best structure store data order make easily accessible business users Evaluate develop highly distributed Big Data solutions You advance software architecture tool set meet growing business requirements regarding performance data quality A Data Engineer likes experiment explore new tools technologies You familiar tools Hadoop ecosystem including Spark Kafka Hive similar You Software Engineer experience modern backend web technologies You know design build low maintenance high performing ETL processes Data pipelines You communicate idea clearly various levels abstraction depending audience Professional experience relational databases reading writing optimizing complex statements", "SQL Teach us something new show us got Python Scala C You coded know importance efficient clean code Cloud use Azure Someone understands complex data challenges accessing A real bottom line person someone throws terms like big data around popular Traditional relational databases Lakes Pub Subs make difference Database concepts indexes execution engines etc Database Administration experience Azure DWH SqlServer Postgresql You understand databases integral part Data Engineer You like ask questions devise complete solution You want understand data pipes definitely perform analytics build dashboards like You know know enough bothers enough time day learn next topic You date new trends data know using solve various problems excited next release favorite tool If like thrown deep end pool team You believe want participate blameless culture focuses process technology You sleep well night leave work question unanswered You feel accountable everything sense urgency driving entire life You like good time getting things done When say team player mean crisp high five funny stories tell You team back And team Sense humor hugely preferred", "", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "Provide technical leadership training staff Work closely scientific teams streamline scientific computing workflows cloud computing environments Design develop deliver scalable data management processing architectures Optimize support existing private cloud agreement Manage public cloud billing resource usage project level Identify technology gaps risks establish mitigation strategies Generate metrics facilitate understanding computational strategies efficiency Maintain optimize custom software stack Setup training orientation new users Write tutorials lead training sessions programming data management best practices Attend workshops conferences scientific field expeditions build expertise well rapport scientific staff Remain date cutting edge methods proactively work implement WHRC", "", "", "", "Experience Event Sourcing concepts stream processing Kafka Flink etc An excellent understanding database systems relational otherwise including sharding big data applications Build data pipeline based Kafka Streams applications Kafka Connect create real time feed Snowflake Model domain events complex business domain Stream domain events materialized state snowflake", "", "Partner various NBCU Technology teams design execution overall Corporate Data Syndication Strategy Nielsen Alternative Measurement Data Process structured unstructured data form suitable analysis reporting empowering state art analytics machine learning environments business analysts data scientists engineers Operationalize data science models products cluster computing environment Evangelize high standard quality reliability performance data models algorithms streamlined engineering sciences workflow Manage multiple priorities across mix ad hoc operational projects", "Architecture design development experience around SaaS platform software Routine involvement high level architectural design discussions providing authoritative technical guidance Experience delivering technical collateral including architecture design documents technical case studies conference papers whitepapers Demonstrated track record successful customer external engagement driving influence deep technical product industry knowledge Seasoned working fellow senior engineers architects product management senior management partners define technical roadmap product response requirements", "Design systems reliably efficiently provide interactive query performance large amounts multi modal data Build systems handle scale Build infrastructure required optimal extraction transformation loading data variety data sources using SQL AWS big data technologies Collect parse analyze visualize large sets data Turn data insights Create data tools analytics data scientist team members assist building optimizing product", "As Data Engineer f real digital act important interface DevOps Data Science software development divisions You develop scalable applications systems processing structured unstructured data You deal comprehensively many different data related topics right ideas every situation Whether data exchange microservices processing huge amounts data recommender models lightning fast interactive business intelligence applications right solution You university degree business informatics comparable field study You high capacity abstraction sound algorithmic understanding affinity working data Ideally experience following technologies autodidact prepared quickly familiarise technological forms containers docker kubernetes Database systems MySQL MongoDB BigQuery etc Management automation infrastructure Linux Networking Cloud Services Terraform Software development Python Java CI CD Drone Gitlab CI containers docker kubernetes Database systems MySQL MongoDB BigQuery etc Management automation infrastructure Linux Networking Cloud Services Terraform Software development Python Java", "", "Work team engineers create products directly affect mission Healthgrades Develop data pipeline features process incoming healthcare information quickly reliably Review team members code correctness quality Write automated test scripts power continuous delivery pipeline Refactor improve existing code base simplicity clarity Remove roadblocks development collaboration communication creative solution recommendations Recommend drive development best practices continuous integration delivery part forward thinking agile organization years experience developing data pipelines ETLs years experience Python Scala Java Knowledge HL7 plus Proficiency Apache Spark Databricks Alteryx required Strong understanding SQL relational databases columnar data warehouses data modeling Knowledge TDD automated testing principles testing best practices Ability instrument basic automation CI CD including familiarity Jenkins Git Strong familiarity cloud based services AWS container technologies Docker Kubernetes Client facing experience plus Previous experience micro services architecture API gateways plus Knowledge experience designing building distributed systems scalability security A bias towards self education new technologies techniques methods Test learn mentality pivot quickly approach successful Keen attention detail eye design understanding value collaboration UX creative product teams Purpose Driven Business help people make confident healthcare decisions Changing Game dynamic employee focused culture career advancement opportunities Community Builders partners local charity organizations matching gifts program Go Green efforts wellness initiatives Salary 101K 128K annually Bonus annually k plan options Medical dental vision insurance HSA contributions qualifying plans Company funded basic Life AD D disability coverage Family planning resources Subsidized wellness benefits PTO plus paid holiday volunteer time", "Work architect design data flows business soup nuts working collaboratively across company partner departments Manage efficient data platform support products well BI function using modern low maintenance cloud technologies AWS GCP well cleanly interfacing existing data science research technologies Evolve sophisticated data model consume structured data growing set source systems scanners VI feeds etc Author maintain entity relationship diagrams data dictionaries API specs data translation documentation multiple levels abstraction conceptual logical physical across multiple data store technologies relational NoSQL Implement solutions proactively monitor data quality traceability source systems Work cross functional product development teams data modeling rules standards best practices embedding needed BS Computer Science Computer Engineering related industry experience You product oriented cloud first perspective data You strong experience distributed compute along ETL ELT BI architectures using public cloud technologies concepts frameworks especially around streaming big data Strong proven data modeling experience large scale years experience large scale distributed data pipelines well data management modeling storage years building large scale data processing analytical systems GCP AWS Dataproc Airflow BigQuery Dataflow Kafka etc Competitive compensation package Medical Dental Vision Disability Insurance Wellness Programs Pet Insurance Retirement Planning Company Equity Generous Paid Parental Leave Work From Home Setup Employee Referral Program Discretionary Time Off Friendly Casual Environment Work Life Balance", "", "Gain exposure investment research content acquisition You learn get selectively distribute customers discover Learn customers product offerings operational best practices Proactively partner internal customers utilize business intelligence help spot trends identify gaps opportunities Ensure content delivered Bloomberg optimized fashion Own implementation new content projects start finish Collaborate content providers suggest areas improvements relates headline composition tagging content positioning mutual clients Build influential meaningful working relationships Develop complete understanding problems deeply embedded business teams meeting clients data contributors learning world largest financial data operation works Be empowered solve business problems quantifiable objectives freedom design workflows select right technologies job Write code develop robust scalable technology solutions data management problems using modern micro services databases user interface technology test driven iterative processes A BA BS degree higher Engineering Information Systems Mathematics relevant data technology field equivalent experience Up two years professional work experience information technology engineering data analysis Minimum years production level coding advanced Python programming proficiency outside academia Aptitude problem solving particularly modify enhance processes workflows Communication communication communication Especially explaining technical processes solutions customers internally externally System design skills competency back end UI Passion data know wrangle large amounts data Knowledge tests linting version control Willingness outside comfort zone adaptability A attitude intellectual curiosity Ability legally work US without visa sponsorship Understanding financial markets particular sell side content distribution", "Distributed file systems storage technologies HDFS HBase Accumulo Hive Large scale distributed data analytic platforms compute environments Spark Map Reduce A hands engineering position responsible supporting client engagements Big Data engineering planning A solid platform drive engineering design decisions needed achieve cost effective high performance result Thinking box improvements current processes enhancing existing platform You formal background proven experience engineering mathematics computer science particularly within financial services sector You hands Programming Scripting Experience Python Java Scala Bash DevOps Tools Chef Docker Puppet Bamboo Jenkins Linux Windows Command line An understanding Unix Linux including system administration shell scripting You gave proficiency Hadoop v2 MapReduce HDFS Spark Management Hadoop cluster included services You good knowledge Big Data querying tools Pig Hive Impala Spark Data Concepts ETL near real time streaming data structures metadata workflow management You ability function within multidisciplinary global team Be self starter strong curiosity extracting knowledge data ability elicit technical requirements non technical audience Collaboration team members business stakeholders data SMEs elicit translate prescribe requirements Cultivate sustained innovation deliver exceptional products customers Do experience integration data multiple data sources", "Live champion values day one ownership empathy humility Hands leadership influence development things data services Develop modern data architectural approaches business intelligence reporting analytics including machine learning models data science ensuring effectiveness scalability reliability Design develop implement optimize existing ETL processes merge data disparate sources consumption data analysts scientists business owners decisions makers Complete current evaluation new ETL software options propose recommendations implement solution Facilitate data transformation normalization cleansing aggregation workflow management business rule application Detect data quality issues identify root causes implement fixes design data audits capture issues Distill technical requirements product development operational process via continuous collaboration product engineering analytics team members Influence communicate levels stakeholders including analysts developers business users executives Use analytics influence product development surfacing data around product usage customer behavior ETL tool evaluation implementation prepare scaling efficiency Typically years experience data engineering related role Data Warehouse Developer ETL Developer Business Intelligence Analytics Software Engineer track record manipulating processing extracting value datasets Experience working variety ETL platforms Matillion preferred CloverETL FiveTran Stitch DBT Spark AWS Glue DataFlow years hands experience designing building ETL pipelines ingesting transforming delivery large amounts data multiple sources Data Warehouse Data Lake Experience variety data storage platforms Snowflake preferred Redshift MySQL Postgres Oracle RDS Expert proficiency SQL Deep understanding application modern data processing technology real time low latency data pipeline ETL architectures Strong stakeholder interaction influence experience executive business stakeholder engineering team levels", "Enable data scientists train deploy machine learning algorithms scale fault tolerant highly available systems Use best practices continuous integration delivery Docker Kubernetes Work closely application engineers build data products power Carta core applications Create data pipelines using batch streaming tools like Airflow Spark Kafka Google Pub Sub Uphold engineering standards bring consistency many codebases processes encounter", "Design implement maintain scalable data pipelines Collaborate domain experts analysts solve data challenges Develop advanced data reporting visualizations Apply data modelling methodologies contribute robust data platform Master degree Computer Science equivalent Experience SQL relational databases Experience one programming languages Python Java preferred Strong understanding data models Data Vault Kimball data warehouses general Fluent English Dutch required Python Pentaho Data Integration custom components developed Java Snowflake MongoDB PostgreSQL Tableau Spark Elastic MapReduce Snowplow Kinesis", "Own technical design project execution team delegating work engineers team also making individual engineering contributions Lead technical development distributed data infrastructure search application collaboration ML NLP collaborators Push boundaries search latest technologies methods Scale platform billions documents drive millions searches Make hands engineering contributions Grata focus overcoming technical challenges modeling practices improve quality velocity Mentor engineers team activities like pairing code reviews promote culture technical excellence Collaborate teams across Grata steward coordinated strategy years experience software engineer developing web applications large scale data pipelines scalable infrastructure years experience team lead role You know work data centric distributed systems comfortable supporting microservice serverless architectures Experience collaborating ML practitioners build data analytical systems Experience Python Django React Postgres NoSQL Elasticsearch AWS Kubernetes Spark big data technologies Experience planning projects managing team wide efforts completion Appreciate productivity care deeply helping teams collaborate effectively efficiently including Excited part inclusive culture Competitive salary Meaningful equity Fully paid healthcare dental vision benefits company trips yr starting back point Happy hours team events lunches", "", "Build scale operate streaming data pipeline You team responsible moving processing analyzing terabytes images week Create world class research platform You work Data Scientists Biologists create platform allows generate access petabytes data gives tools quickly iterate novel analysis research deploys new deep learning models production data pipeline Provide visibility operations You create tools dashboards metrics help everyone keep track work care alert need take corrective actions Act mentor peers You share technical knowledge experiences resulting increase productivity effectiveness Experimentation We want Software Engineers think critically use data measure results Rigorous use scientific method allows Software Engineers quickly understand critical aspects problems trying solve whether moving right direction Collaboration We want Software Engineers play well others The role require close collaboration Biological High Throughput Screening Data Science teams help us achieve mission discover transformative new treatments Curiosity We want Software Engineers satisfied status quo Our Software Engineers openly discuss tradeoffs inherent build software go beyond traditional boundaries engineering teams enable us get things done faster cheaper reliably traditional drug discovery An ability resourceful collaborative order complete large projects We much way project managers A track record learning new technologies needed get things done Our current tech stack uses Python pydata libraries Clojure Kafka Kubernetes Docker PostgreSQL Big Query cloud services provided Google Cloud Platform Experience Python JVM helpful An ability get things done using various tools nooks crannies software engineering composing command line tools kubectl jq xargs creating SQL triggers managing migrations operational support An ability write well tested instrumented code continuously deployed production environment confidence An interest learning teaching peers areas performance scalability system architecture Biology background necessary intellectual curiosity must Coverage health vision dental insurance premiums cases k generous matching immediate vesting Stock option grants Two one week paid company closures summer winter addition flexible generous vacation sick leave Commuter benefit vehicle parking ease commute Complimentary chef prepared lunches well stocked snack bars Generous paid parental leave birth non birth adoptive parents Fully paid gym membership Metro Fitness located feet away new headquarters", "Work closely Data Scientists Data Analysts Performance Marketers identify data related tooling infrastructural needs company participate defining plan fulfil Help define software architecture new data tools new features existing tools develop The work spans broadly infrastructure backend oriented implementation data science algorithms development complex front ends Constantly improve quality tools infrastructure fixing bugs refactoring code base necessary Expand set libraries needed make well engineers efficient effective Stay informed new relevant technologies test promising ones make sure miss game changing opportunities boost productivity enjoyment work Huge impact Your software help us store retrieve visualize analyze ultimately make sense enormous amounts data ultimately make real difference well serve tens millions users whether win lose market Talented knowledgeable colleagues You get chance learn teach brightest skilled people ever meet Your talent going blossom Cool tech stack We strive use best modern tools technologies fall short expectations invent It geek paradise Own products We work clients develop apps Freed politics move fast daring Fun young environment We near zero hierarchy relaxed workplace Also years old average often hang together might actually end making new friends Top notch office Our office amazing We designed goes beyond excellent functionality offering sorts amenities foosball tables gaming consoles International reach Our audience wildly international Our company language English We global reality Passion topic You long standing proven passion writing software It big plus demonstrated interest data science data analytics building tools Reasoning depth learning You structured creative enough solve challenging problems independently given necessary knowledge You learn new concepts skills rapidly look understand stuff truly depth Drive You energetic hard working persevere adversity job done done well You get turned getting results always aim excellence Pragmatism Far academic obsessive perfectionism understand getting things done competitive world speed often important quality Curiosity initiative You love exploring entrepreneurial seeking new opportunities testing new ideas accord You wait told time Attention detail You care get small things right well big ones You meticulous checking code work exactly expected Diligence organization You blindly entrusted big responsibilities well small menial tasks Humbleness You earth eager listen people feedback constructive criticism ready get hands dirty whatever team needs succeed", "Accountable supporting projects preparing data data exploration research modeling Create visualizations data purposes data discovery data exploration Provide subject matter expertise claim business intelligence data environment Create operationalize data products including transformation logic well business requirements specifications Serve data expert significant projects broad impact business enterprise performance Serve business intelligence data environment subject matter expert support claim research community claim business partners Understanding data warehousing information delivery use big data Develops prepares data using Hive Pig SQL SAS Develops data pipelines utilizing appropriate technologies frameworks Java Python Scala NiFi Capable building data visualizations help support data discovery data exploration Create operationalize data products Incorporate core data management competencies data governance data security data quality Builds tests implements analytic processes business workstream including pilots proof concept Provide subject matter expertise claim business intelligence data environment", "Experience running supporting production enterprise data platforms Experience creating internal tools combine content audience data Experience building infrastructure required optimal extraction transformation loading data various resources Knowledge JavaScript Python Bash SQL Build data pipelines tools cloud based data services like Google BigQuery AWS Dataproc Pub Sub years data engineering experience Strong statistics skills LI JA1 WSJ", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "As Data Engineer responsible fast accurate robust scalable data processing company You develop deploy new features agile test driven environment tandem team data engineers data scientists software developers operations managers You build fault tolerant self healing adaptive highly accurate data computational pipelines You optimize data transfer processes code memory architecture level You develop environment micro services event based architecture You either bachelor degree years work experience master degree year work experience You prior experience high level programming language like Ruby Scala Go You ability quickly understand business requirements transform data models You experienced Big Data related technologies like HDFS Hive Pig Spark You aptitude independently learn new technologies passionate data Prior experience online marketing plus e g Google AdWords BingAds Social Media Advertising Universal Analytics We trust data data drive business decisions product development roadmap Our employees best investment develop knowledge sharing sessions workshops personalized training We keep innovating stay close users challenge status quo dedicate work time new ideas We get great work done enjoy working face face value results office presence Our team diverse unite smart passionate people nationalities", "Move smart data driven employ tools best practices ship code quickly safely continuous integration code review automated testing etc Distribute knowledge want scale engineering team point contributions stop company code base We believe Open Source culture communication outside world Leave code better found constantly raise bar Unity makes strength moving people A B easy sounds always keep calm support", "Design develop business critical data pipelines related back end services Identification participation simplifying addressing scalability issues enterprise level data pipeline Design build big data infrastructure support data lake years extensive experience Hadoop similar Ecosystem MapReduce Yarn HDFS Hive Spark Presto HBase Parquet Experience building breaking fixing production data pipelines Hands SQL skills background data stores like SQL Server Postgres MongoDB Experience continuous delivery automated deployments Terraform ETL experience Able identify participate addressing scalability issues enterprise level data Python programming experience Experience machine learning libraries like scikit learn Tensorflow etc interest picking Experience R mine structured unstructured data building statistical models Experience Elasticsearch Experience AWS services like Glue S3 SQS Lambda Fargate EC2 Athena Kinesis Step Functions DynamoDB CloudFormation CloudWatch huge plus You work remotely continental US occasional travel Bend Oregon You based shared office space heart downtown Portland Oregon You based offices Bend Oregon relocation assistance package available An inclusive fun values driven company culture awards A growing tech company Bend Oregon Work Life balance concept Excellent benefits package Medical Expense Reimbursement Program helps keep medical deductibles LOW Team Members k generous matching component Generous time plus VTO day use working favorite charity Competitive pay annual bonus program FREE TURKEYS pies every Team Member Thanksgiving hey tradition around Your work makes difference make huge impact clients profits", "Build scale maintain data pipelines process billions daily events data warehouses Write tune complex Java MapReduce Spark Hive jobs Explore available technologies design solutions continuously improve data quality workflow reliability scalability reporting performance capabilities Troubleshoot data issues build customized reports investigate key business questions Work closely Unity Engine Ads Analytics Game Services teams worldwide Drive key business initiatives multiple teams stakeholders across organization Mentor help engineers grow Work across teams instill engineering best practices patterns", "Ingesting 3rd party internal data creating clean data models empower business operational teams Developing standardized metrics drive behavior commercial organization measure outcome behavior Providing visibility commercial leaders creation dashboards reports Analyzing impact commercial programs using sales field data This role offers tremendous upwards exposure towards senior business leaders chance truly impact decision making JUUL Excellent Python SQL skills Experience data visualization dash boarding analytical report building World class ability extract communicate insights real world datasets years applicable experience relevant advanced degree Experience Pandas GCP BigQuery Tableau Mode valuable required You get excited taking ownership problems solving fast paced scrappy environment working cross functionally technical non technical people You output focused see data science powerful tool get things done rather end You strong critical thinker passion understanding complex issues comfortable working ambiguous problems A place grow career We help set big goals exceed People Work talented committed supportive teammates Equity performance bonuses Every employee stakeholder success Boundless snacks drinks Cell phone subsidy commuter benefits discounts JUUL products", "Work interdisciplinary teams combine technical business data science competencies Design implement solutions around data warehouse implementation ranging architecture ETL processes multidimensional modelling data marts implementation Integrate datasets dataflows using variety best class software well profile analyze large complex datasets disparate sources Be involved full project lifecycle gathering business requirements system design development testing deployment We looking experienced specialists well non experienced talents", "Gather requirements assess gaps build roadmaps architectures help analytics driven organization achieve goals Work closely Data Analysts ensure data quality availability analytical modelling Explore suitable options designs specific analytical solutions Define extract load transform ELT based jointly defined requirements Prepare clean massage data use modeling prototypes Identify gaps implement solutions data security quality automation processes Bachelor degree four years work experience Four years experience data engineer Four years experience finding cleaning preparing data use Data Scientists Experience knitting disperate data sources together Four years experience building data pipelines Experience using SQL e PL SQL T SQL RDBMSs like Teradata MS SQL Server Oracle etc Experience data engineering databases data warehouses Strong experience data engineering Python Master degree Computer Science Engineering Statistics IT related field Experience Scala Julia R Python machine learning programming language Experience Big Data platforms e Hadoop Map Reduce Spark HBase CouchDB Hive etc Strong analytical problem solving skills Experience working network operations center environment", "", "Design build launch efficient reliable data pipelines production Design implement warehousing solutions scale needs business Develop new systems tools enable team consume understand data intuitively Partner engineers project managers analysts deliver insights business years experience data engineering data warehouse technology years experience custom ETL design implementation maintenance years experience schema design dimensional data modeling Strong SQL skills multiple platform Skilled programming languages Python R Java Strong computer science fundamentals including data structures algorithms Strong software engineering skills server side language preferable Python Kafka Cloud computing machine learning text analysis NLP Web development experience plus NoSQL experience plus Experience Continuous integration deployment Experienced working collaboratively across different teams departments", "Working interdisciplinary field together computer scientists business specs telecom network engineers require excellent interpersonal communication skills Integrating multiple data sources models software tools business line specific decision support data analysis Developing new automation methods analyze evaluate business strategies across various business geographies technologies drivers scales Modeling complex systems integrating large varied datasets economic demographic telecom related information Creating walking executive presentations explain complex data algorithms used simple easy understand visually striking layman terms leave residual impact executive audience Solving complex geospatial problems utilizing robust repeatable solutions Challenging existing constructs business paradigms creatively solving problems result business transformation process program current approach Bachelor degree four years work experience Masters Science Geographic Information Systems Experience one programming languages e g Python JavaScript Experience analysis large spatial non spatial datasets Experience multitude databases general Oracle Postgres Knowledge R SPSS SAS statistical analysis preferred Experience large scale parallel computing distributed environments familiarity GIS spatial databases e g Postgres PostGIS Oracle ESRI sde Experience using Python automation analytics tasks Demonstrated use development spatially enabled Web Front Ends ArcGIS JavaScript API React etc Experience managing data SQL NoSQL databases Postgres Oracle Teradata Hadoop Casandra MongoDB Knowledge application technical procedures principles theories concepts Telecommunications field General knowledge related disciplines Experience leading one areas team task project lead responsibilities Demonstrated experience managing short long term projects start finish Experience Telco Cable network related field Electric Wastewater Gas Oil Urban planning Interpersonal communication skills Ability convey complex information upper level execs", "Develop understanding key business product user questions Collaborate Product Engineering team members develop test support data related initiatives Work departments understand data needs Evolve data driven feature prototypes production features scale Streamline feature engineering underlying data efficiently extracted Build flexible data pipelines rapidly evolve needs change capabilities grow Develop enhance data warehouse AWS S3 You least years relevant experience comparable data engineering role You expert level knowledge SQL Spark SQL You experience pursuing launching data backed decisions recommendations make end user facing products better You like dive deep data questions come effective solutions You believe writing code easy understand test maintain You thrive workplace values autonomy applauds ideas enjoys sense humor http www showtime com", "Design implement support robust scalable solutions enhance business analysis capabilities identify gaps design processes fill Work analysts understand business priorities translate requirements data models Collaborate various stakeholders across company like data developers analysts data science finance etc order deliver team tasks Build complex multi cloud ETL pipelines Apache Airflow Build Python API integrations 3rd party vendors", "http www showtime com", "", "Leads influences technical direction large scale highly complex technical initiatives projects requiring integration cross functional systems Provides technical guidance evaluating applications systems evaluating requests proposals Collaborates business prioritize key business technical initiatives Utilizes expert knowledge customers business recommend solutions ensures business technology objectives met maintained Understands user process requirements ensures requirements achieved high quality deliverables Creates system documentation play book serves lead technical reviewer contributor requirements design code reviews Typically serves resource business technical resource cross functional third party internal team members highly complex design code reviews May troubleshoot complex problems recommend solutions practices relative root cause analyses identification solutions improving system performance availability On behalf manager manages consistent delegation work packages cross functional third party team members execution full development life cycle Appropriately advises management issues Assists team leads management delegation technical work packages cross functional third party team members execution full development life cycle Keeps management appropriately informed progress issues Performs design analysis coding unit integration testing highly complex system functionality defect correction across multiple platforms Displays advanced knowledge understanding functional technical domains specific products appropriately evaluates impact changes additions Develops accurate estimates work packages Analyzes designs specifications less experienced internal third party team members execute Actively mentors contributes technical soft skills development internal third party teams Actively participates cross departmental staffing technical decisions Bachelor degree additional years related experience beyond minimum required may substituted lieu degree years software development experience demonstrating depth technical functional understanding within specific I T discipline technology e Business Intelligence Mobile Web Java etc years experience developing deploying supporting high quality fault tolerant data pipelines leveraging distributed big data movement technologies approaches including limited ETL streaming ingestion processing years experience software engineer role leveraging Java Python Scala C years advanced distributed schema SQL development skills including partitioning performance ingestion consumption patterns years experience distributed NoSQL databases event brokers Apache Cassandra Kafka Graph databases Document Store databases Curious excited new ideas Energized fast paced environment Able understand translate business needs leading edge technology Comfortable working part connected team self motivated Community focused dependable committee", "Data munging emphasis ability deal imperfections data Developing refining scaling data management analytics procedures systems workflows best practices issues Developing data driven products Visualizing communicating data clearly use internally externally Building data pipelines clean transform aggregate data many different sources Developing models used make predictions Building complex functions answer questions business Modeling data rest enable powerful data analysis Providing solutions help share data enterprise Bachelor degree Mathematics Statistics Engineering Computer Science related discipline years experience analytics business intelligence Proficiency languages Java Scala Experience web technologies HTML JavaScript XPath JSON Drupal Exposure Linux operating systems Some exposure Unix helpful required Experience working big data technologies Spark Hadoop MapReduce MySQL NoSQL databases Experience AWS Data Tools S3 Redshift RDS Kenesis Experience producing consuming event driven data Experience scraping social media using social media APIs Basic understanding statistics", "You part data science team work closely data scientists operationalize machine learning pipelines You develop implement effective data processing architectures You also collaborate lot data warehouse data platform team You participate meetups conferences research community apply learned back daily work", "Independently installs customizes integrates commercial software packages Facilitates root cause analysis system issues Works experienced team members conduct root cause analysis issues review new existing code perform unit testing Learns create system documentation play books attends requirements design code reviews Receives work packages manager delegates Identifies ideas improve system performance impact availability Resolves complex technical design issues Creates system documentation play book participates reviewer contributor requirements design code reviews May serve subject matter expert development techniques Partners experienced team members develop accurate work estimates work packages May serve mentor procedural matters less experienced internal third party team members Bachelor degree OR additional years related experience beyond minimum required may substituted lieu degree years operational experience Linux environments configuration health checks monitoring etc years coding scripting experience Bash Python Perl etc years experience working relational databases Advanced SQL knowledge strong understanding key database management concepts Familiarity variety databases Oracle UDB DB2 SQL Server etc years experience working APIs collect ingest data years infrastructure automation orchestration experience Proven ability design build operate technology stack Strong analytic troubleshooting skills", "Create electrical optical data visualization tool various projects Develop web based tool generate present report production Pursuing degree computer science electrical engineering related technical field Strong Python SQL web based programming Strong statistical analysis plus Able work well team environment", "", "", "", "Design develop support Data infrastructure utilizing various technologies process terabytes data including SQL Python Microsoft Azure AWS Create solutions enable diagnostic predictive analytics capabilities Partner Analytics Marketing Finance organizations get feedback iterate upon Data Ecosystem development Develop components distributed ETL systems suite large data platforms Be curious trends emerging technologies Data space participate user communities share learn teammates Familiarity developing data processing solutions data applications using technologies like Python C Java SQL Spark No SQL DB Experience working kinds data clean dirty unstructured semi structured relational Problem solving multi tasking fast paced globally distributed environment Strong communication skills good interpersonal skills Collaborate business partners understand refine requirements Experience developing end end data pipelines large cloud compute infrastructure solutions Azure AWS Google plus Five years working directly Big Data technologies preferred", "", "", "Build analytical solutions enable Data Scientist manipulating large data sets integrating diverse data sources Work closely data scientists database systems administrators create data solutions Bachelor degree four years work experience Experience designing building deploying production level data pipelines using tools Hadoop stack HDFS Hive Spark HBase Kafka NiFi Oozie Splunk etc", "Working variety different data formats platforms including SQL NoSQL databases Big Data MapReduce Hadoop Spark etc JSON XML cloud data warehouse technology Amazon Redshift Snowflake Working Business Analytics IT team members clarify refine functional data requirement specifications Working Business Analytics team normalizing aggregating large data sets based business needs requirements Developing processes techniques practicing good data hygiene ensure data always date accurate stored efficiently Testing new technologies architectures help find best ways work unique data sets Being able benchmark troubleshoot data related issues analyze system bottlenecks propose solutions eliminate Designing ETL ELT processes data warehouse structures enhance existing premise architectures Utilizing cloud solutions cloud data warehouse technologies Interacting various premise cloud data sources well RESTful APIs achieve optimal data footprint Following Agile principles prioritize design architecture POC backlog using Agile principles Writing optimizing scripts transform aggregate optimize data", "We helped second largest personal lines insurer United States design build Virtual Assistant speed productivity new customer contact center employees utilizing Cognitive Computing Artificial Intelligence system The world largest brewer experiencing much downtime production We identified causal factors across seven different dimensions categories pool million potential cost savings When client second largest cable operator United States acquired another organization took lead assessing product catalogues organizations providing recommendations transition single product catalogue current customers additional million subscribers client gained acquisition Proven experience analyzing existing tools databases providing software solution recommendations Ability translate business requirements non technical lay terms High level experience methodologies processes managing large scale databases Demonstrated experience handling large data sets relational databases Understanding addressing metadata standards Ability work stakeholders assess potential risks Unlimited Training Career Growth mean Pluralsight Safari Library Conferences Certifications Lunch Learns etc Weekly Coding Challenges Sponsored Hackathons Full time Employment support clients grow company Excellent health dental vision maternity paternity leave Revenue sharing k retirement savings Life disability long term care", "We work small teams fast paced get lot done everyone wearing many hats We serious optimizing time staying focused important goals outcomes We remote team completely board remote work meaning focus overcommunication ensure stay sync despite physical distance We coordinate using kanban board hold daily standup mostly communicate via ad hoc video calls Slack We building lots new things also maintaining significant business We mindful balance need monitor pay tech debt also innovate exciting greenfield projects GrowFlow data pipelines consume data products counting central data warehouse BigQuery We process transform data DBT visualize PowerBI Amplitude Segment com central product analytics customer data platform collecting behavior event data sources sending warehouse various partner destinations Ensure availability timely delivery data analytics reporting company wide Ingest aggregate structured unstructured data internal external data sources data warehouse Model new data sets Build maintain improve performant efficient reliable ELT workflows data pipelines Document new existing data models ELT workflows pipelines data dictionaries tracking events Build deploy monitor robust data tests monitor validate production pipelines Improve data driven decisions providing guidance assistance internal business stakeholders Customer Experience Sales Marketing Growth etc need unlock insights markets customers business processes Work w Product Engineering evolve expand GrowFlow Insights product customer facing business intelligence tool Work business data analysts develop extend support data driven A B experiments years total engineering experience years experience dealing data including things like SQL databases No SQL databases columnar databases data pipelines Familiarity Embedded Reporting tools platforms PowerBI Domo Looker Sisense etc Bonuses Experience Google Big Query DBT getdbt com PowerBI Experience working data using Python e g Pandas Apache Beam scikit tensorflow etc Experience using ETL tools SAS Informatica Talend MSSQL SSIS etc Ability acutely focus company objectives mission hand Ability pass background check appropriate work authorization Be Customer Obsessed As support engineer ideal candidate heartbeat customer satisfaction strive make sure customer issues resolved quickly effectively Over Communicate As remote company communication key delivering continued productivity across teams Our ideal candidate goes beyond ensure important messages received correct party Challenge Respectfully GrowFlow far likely succeed examining problems situations several lenses Our ideal candidate able engage work support team engineering team ensure delivering best solution customers Extreme Ownership At GrowFlow pride member practicing extreme ownership accountability Our ideal candidate able willing take ownership customer reported issues see way resolution Stay Curious Stay Scrappy Most us dork non work related topics ridiculous level detail wired We naturally inquisitive ask tough questions afraid ruffle feathers find better answers Our Ideal candidate intuitive eager learn new things Do Less Better At GrowFlow believe path becoming market leader looks like focusing fewer things better anyone else industry Our Ideal candidate able create path least resistance resolving customer reported issues Results Get Rewarded At GrowFlow recognize quickest path becoming top company industry forming top team industry Our ideal candidate heartbeat KPIs related T2 support engineer position focused measuring improving metrics related position We fully remote company position remote We looking someone ready join us full time brief trial period employees We offer health benefits 401k unlimited time charity matching cool perks", "Enable data scientists train deploy machine learning algorithms scale fault tolerant highly available systems Use best practices continuous integration delivery Docker Kubernetes Work closely application engineers build data products power Carta core applications Create data pipelines using batch streaming tools like Airflow Spark Kafka Google Pub Sub Uphold engineering standards bring consistency many codebases processes encounter", "Experience framework data processing workflows Experience visualizing documenting architectures e g flow diagram describing data flows input ETL pipelines data warehouse Experience Google Dataflow cloud data processing services e g AWS years experience SQL Python Java Node js Experience unit behavioral testing Great understanding CI CD flows containerization related best practices Understanding horizontal scalability throughput databases Proactive mentality Expereince Node js Experience BI visualization tools Superset", "Experience using LookML Looker Hands coding experience expertise back end related technologies like Node Python B S Computer Science equivalent experience followed years work experience using SQL databases business environment Expertise Data Visualization Deep experience latest libraries programming techniques Familiar SQL NoSQL databases like MongoDB declarative query languages Knowledge using BI Analytics related technologies You accomplishments showcase capabilities success technical depth You new features idea completion Work well core team design execute major new features Enjoy contributing fast moving exciting project Strong communicator fluent English excellent written verbal communication skills Thrive excel diverse distributed agile team environment Competitive Vacation Package Annual Financial Allowance YOUR development Flexible Family Leave Clevertech Gives Back Program Clevertech U Leadership Program Habit Building New Skills Training Clevertech Swag", "On premise cloud based deployment patterns Streaming micro batching right time use cases Provide innovative data engineering design deployment approaches leverage innovations memory processing agile delivery automation storage architectures etc design modern data pipelines speed scale Working closely technology partners Accenture Technology Labs Accenture Innovation centres incubate emerging technologies build prototypes demos enhance data engineering codebases frameworks Mentor upskill data engineers", "Working Senior Data Engineer designing building future proofing operating scalable data infrastructure Working Senior Data Engineer delivering data platform infrastructure tooling reporting regular basis Working Senior Data Engineer understanding needs product teams delivering valued data models Enabling better testing development workflows Supporting creation data products promote customer self serve Work engineering ensure collection high quality data Previous software engineering experience working environments dealing large datasets Understanding modern code driven data engineering frameworks like Airflow Understanding functional data engineering principles Familiarity testing data pipelines An understanding distributed data processing methodologies frameworks best practices Solid understanding databases working knowledge SQL Experience working Python Experience working cloud products e g S3 Kinesis SQS SNS Experience working Agile methodologies cross functional environment Experience machine learning libraries", "Analyzes processes data extracting data various data warehouse environments Performs data mapping systems integration data provisioning", "Drive design implementation leveraging modern design patterns Ability partner effectively UX PM DevOps QE developers design implement meeting spirit requirements Experience modern front end frameworks Technically curious keep present advances technology Experience segregation model presentation business logic TDD experience strong desire build test start Write code Test Product deliver project timescales quality requirements various languages including Java selected languages Sophos Cloud products Provide guidance mentorship junior developers daily Scrum meetings Possesses passion solving complex Big Data problems Plan design implement next generation cloud security products Be involved inception implementation real hands fashion BS Computer Science Engineering equivalent years development data modelling background Building highly scalable SaaS solutions using Big Data technologies Experience CI CD Experience Agile Software Development methodologies scrum kanban Excellent attention detail Excellent verbal written communication skills Experience following technologies recommended strong desire learn required Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Programming language Java must Batch processing Hadoop MapReduce Stream processing Kafka Amazon Kinesis NoSQL S3 MongoDB Columnar stores HBASE Amazon Redshift Restful web services Code Build Deployment git hg svn maven sbt jenkins bamboo Amazon Web Services Data Warehousing built ETL Medical insurance Dental insurance Vision insurance Life insurance Long term disability insurance 401k plan Vacation time", "", "Optimize execute requests pull analyze interpret visualize data Partner team leaders across organization build iterate team individual performance metrics Optimize data release processes partner team leads iterate improve existing data pipelines Design develop systems ingest transform data streams using latest tools Design build integrate new cutting edge databases data warehouses develop new data schemas figure new innovative ways storing representing data Research architect build test robust highly available massively scalable systems software services years data engineering experience Robust experience Python Spark Airflow Experience writing executing complex SQL queries Experience building data pipelines ETL design implementation maintenance Experience AWS IAAS PAAS provider Scrum Agile software development process Real estate experience Experience Periscope Looker Tableau BI tools Experience building data pipelines Experience machine learning Medical vision dental paternity maternity benefits k Commuter benefits Flexible time policy Catered lunches snacks Corporate gym membership", "", "Collaborate Data Engineers Business SME develop data products services Build data products service processes perform data transformation metadata extraction workload management error processing management Implement standardized automated operational quality control processes deliver accurate timely data reporting meet exceed SLAs Work product teams support operations data platform backlog", "Scaling existing data processes focusing quality every step way Provide guidance technology library tooling choices team Wary things like scope creep engineering skipping unit tests learning business domain good software engineering principles practice solving business problems whilst avoiding technical complexity years software development experience Working together pairing wider team collaboration A belief agile principles single Agile Process Willingness use appropriate tool job latest hot technology Sharing knowledge experience others Good current knowledge SQL least two programming languages Experience collaborative design software systems including contracts integration points database schemas Google Cloud Platform PostgreSQL Bash Python Flyway Oracle Groovy Build design large scale real time batch data pipelines Distributed microservice based applications scale Docker Kubernetes run amazing code Wearing Scrum Master hat time time", "Design data warehouse solutions using dimensional methodologies support ETL processes data analytics applications Develop implement tune ETL processes Write tune SQL including database queries ddl dml stored procedures triggers user defined functions analytic functions etc Create code meets design specifications follows standards easy maintain Own features develop end end Work end users requirements gathering develop test code implement new processes production maintain support time Drive data platform help evolve technology stack development best practices Develop unit test assigned features meet product requirements Work Analytics Digital Marketing teams provide data need make efficient decisions Work Quality Assurance team ensure processes fully tested Support maintain dev test prod environments meet business delivery specifications needs Assist adhoc report generation data analysis customers Be part monthly call rotation", "Deep technical experts thought leaders help accelerate adoption best engineering practices maintaining knowledge industry innovations trends practices Design develop cutting edge solutions using existing emerging technology platforms", "You experience SQL You used written complex SQL queries join across data multiple systems matching even straightforward way join tables You designed tables eye towards ease use high performance You documented schemas created data dictionaries You skilled written communicator Zapier remote team writing primary means communication You appreciate team values eagerness collaborate teammates function organization level data knowledge iterating deliverables curious You understand perfect enemy good default action shipping MVP code iterating needed get towards better solutions You experience APIs You ingested large quantities data RESTful APIs You experience comfort level programming You read write code Python Go Rust Java C You familiar distributed source control using Git You experience running infrastructure needed orchestrate data pipelines store data different retention performance requirements perform compute multiple loads Experience tools like Ansible Terraform Vagrant plus You understand columnar store file formats like Orc Parquet also familiar Avro Avro schemas Develop ETL ingest transform data upstream databases APIs data warehouse The tools used include AWS Redshift NiFi Kafka Matillion ETL custom Python Build deploy continuously improve infrastructure used data scientists data business analysts The tools using include Docker Terraform Ansible Kubernetes AWS EC2 As part Zapier hands philosophy help customers via support ensure best experience possible Competitive salary pay based norms country Great healthcare dental vision coverage Retirement plan company match Profit sharing annual company retreats awesome places weeks paid leave new parents biological adopted children Pick equipment We set whatever Apple laptop monitor combo want plus software need Unlimited vacation policy Plus require take least weeks year We see employees take weeks per year This vague policy unlimited vacation means vacation", "Collaborate engineers extract transform load ETL data wide variety house 3rd party data sources Ensure data consistency production analytical databases You integrity data end end company make high impact decisions based data Build data warehouse provide timely data multiple third party applications Salesforce Marketo etc Design build tools make data pipelines surfacing reliable easier use Work closely backend engineers roll new tools features Triage identify fix scaling challenges Collaborate internal data customers gather requirements You least years experience least one relational databaseMySQL Postgres Oracle You experience SQL Data Warehousing using relational database You experienced large scale data pipelines ETL tooling You previous coding experience Our ETL process Ruby use Python data analysis You experienced EDA Exploratory Data Analysis Data Visualization use Tableau You used Amazon Redshift You manipulate data using Python pandas numpy scikit learn etc", "Design develop scientific databases create methods process analyze omics data biological information Lead application development efforts establish data engineering platforms enable storage organization dissemination analytics dynamic data generated innovative research exploratory clinical studies Contribute strategic planning implementation data engineering platforms multiple functions ensure data accessibility quality integrity Bachelor years relevant experience Master PhD years relevant experience equivalent experience Degree computer sciences bioinformatics related field Track record successfully delivering large scale informatics solutions address complex scientific data challenges applying modern software engineering practices deliver applications scientific data analyses Experience omics data analysis method development Familiarity relational non relational databases Working knowledge scientific applications development cycles data management techniques infrastructure requirements Working knowledge scripting languages Python R strongly preferred Demonstrated adherence best practices software engineering particularly iterative development version control testing modular design Experience operating large data data stored relational non relational databases HDF5 files parquet files", "Trust confidence Making intent actions transparent honest fostering healthy inclusive relationships actively listening maintaining open communications delivering promises investing success engaging community Creative work environment Exploring meaningful new ideas relationships foster innovation encourage collaboration creativity Challenging standard method business positive environment Now later perspective Looking beyond day day challenges better anticipate future adjust change Respectful inclusive work environment Seeking wide range voices making feel respected included Self awareness emotional intelligence Learning recognize understand emotions emotions others recognizing individual strengths weakness raise self awareness perform better Wellbeing We take care Excellent communication skills drive understand customers The ability bring understanding Data Governance Shows understanding adherence established industry standard policies DGI related experience Performance optimization databases multi tier architectural environment Obsessive attention detail strong interest automating duplicate systems The ability provide direct support integration data interfaces data flows among multiple applications multiple sources house vendor provided Oversee aspects database security business continuity database usage standards practices database infrastructure design implementation integration troubleshooting administration Support database architecture strategy data modeling database design Provide Tier Tier support development teams business users database operations including development complex SQL tuning DML creation stored procedures Develop install maintain monitor globally distributed hybrid cloud corporate databases high performance high availability environment supporting existing new enterprise products Manage backups synchronization multiple databases data centers Support design development SQL code SSRS reports reporting platforms Transact SQL experience SQL Server clustering failover qualifications required Performs technical tasks relative administration City databases", "", "Carry data acquisition activities ensuring accuracy integrity data data analysis coding documentation ETL processes Develop test implement ETL program logic using variety SQL dialects SAS programming language Analyze translate business requirements functional specifications technical specifications part development process Collaborate IT client organizations design test data acquisition ETL processes Perform unit testing system integration testing user acceptance testing fix defects found testing Provide expert guidance assistance colleagues functional areas Advanced SQL programming Oracle DB2 Teradata RDBMS SAS programming skills including SAS PROC SQL SAS Data Step SAS Macro programming skills Understanding usage SAS Enterprise Guide SAS Stored Processes Ability convey management client built Ability contribute highly collaborative team environment Ability complete analytical tasks independently minimal guidance Ability take initiative An intrinsic dedication quality Proficiency Microsoft Office Excel Access UNIX shell Perl Python scripting Working knowledge SAS BI Web Services Experience programming NET Java Experience working Informatica Experience data modeling ERwin Experience data financial services insurance retail energy business Experience Hadoop No SQL data stores", "Develop data warehouse data lake following defined architecture applicable M E data ease use organization accessibility security compliance performance scalability monitoring availability Under M E guidance clean transform aggregate integrate M E data data warehouse Develop ETL system data pipelines following defined standards move data variety sources warehouse monitor data quality check errors conform data standards use WFP corporate ETL data lineage tools Build Database objects tables views stored procedures functions etc data services provide data format useful analysis Maintain data pipelines architecture schemas maximize usability accuracy robustness performance scalability Collaborate KECO IT HQ teams ensure standards maintained Document technical specifications work appropriate tools Build Data Lake M E Data based RMT defined Architecture using WFP Corporate ETL data lineage tools Migrate data old M E database Data Lake Migrate data ONA online M Data Lake Develop API required facilitate easier process maintain Manage project deliverables timeline B S CS Engineering relevant field additional years related work experience trainings course Advanced university degree Computer Science Engineering years ETL data engineering work e g ETL Extract Transform Load ETL data lakes data marts data warehousing years professional software engineering Knowledge experience working Hadoop stack Horton Works Experience working constructing Relational noSQL MongoDB databases Experience working engineers business analysts solve data needs Experience working building data pipeline Experience building flexible data APIs Experience working Tableau Ability perform testing regression testing ensure high quality product work e g ETL ELT Data Lake using Data Lake data analysis Skilled documenting data warehouses hierarchies process flows Experience metadata management data quality processes tools Knowledge Hadoop tools e g Hive Impala Pig Sqoop Hue Kafka etc", "Design develop document test advanced data systems bring together data disparate sources making available data scientists analysts users using scripting programming languages Python Java Scala etc Evaluate structured unstructured datasets utilizing statistics data mining predictive analytics gain additional business insights Design develop implement data processing pipelines scale Present programming documentation design team members convey complex information clear concise manner Extract data multiple sources integrate disparate data common data model integrate data target database application file using efficient programming processes Write refine code ensure performance reliability data extraction processing Communicate levels stakeholders appropriate including executives data modelers application developers business users customers Participate requirements gathering sessions business technical staff distill technical requirements business requests Partner clients fully understand business philosophy IT Strategy recommend process improvements increase efficiency reliability ETL development Collaborate Quality Assurance resources debug code ensure timely delivery products", "Analytic systematic Service minded problem solver You bachelor master IT Computer Science Mathematics Statistics Working experience Data Engineer ETL developer years You developed solid SQL database skills Good understanding DWH development concepts data processing principles Experience visual ETL tools like Informatica PowerCenter Pentaho similar Experience programming scripting interest learn python pyspark", "An analytical quantitative IT orientated degree BSc MSc level HBO WO years work experience commercial environment Strong analytic SQL skills Java backend experience Knowledge Database architectures Good social communication skills Experience big data solutions Knowledge cloud solutions Common data visualization tools Flexible work mentality Familiar Oracle JSON REST API Python etc Competitive compensation package Annual company performance based bonuses A non hierarchical workplace Young highly motivated teammates High responsibility", "Executes complex functional work tracks team Partners ATSV teams Big Data efforts Partners closely team members Big Data solutions data science community analytic users Leverages uses Big Data best practices lessons learned develop technical solutions used descriptive analytics ETL predictive modeling prescriptive real time decisions analytics Influence within team effectiveness Big Data systems solve business problems Participates development complex technical solutions using Big Data techniques data analytics processes Supports Innovation regularly provides new ideas help people process technology interact analytic ecosystem Participates development complex prototypes department applications integrate Big Data advanced analytics make business decisions Uses new areas Big Data technologies ingestion processing distribution research delivery methods solve business problems Understands Big Data related problems requirements identify correct technical approach Works key team members ensure efforts within owned tracks work meet needs Drives multiple tracks work within research group Identifies develops Big Data sources techniques solve business problems Co mingles data sources lead work data problems across departments drive improved business technical results designing building partnering implement models Manages various Big Data analytic tool development projects midsize teams Executes Big Data requests improve accuracy quality completeness speed data decisions made Big Data analysis Uses learns teaches supports wide variety Big Data Data Science tools achieve results e Hadoop SQL SQL MongoDB others Uses learns teaches supports wide variety programming languages Big Data Data Science work e Java Python Perl", "", "You responsible maintaining evolving existing data warehouse develop data models ETL ensuring data consistency You working closely Business Intelligence Big Data teams delivery application expanding platform using Big Data technology support data requirements different departments brands You involved data Integration includes internal data 3rd parties migrations Involved Data Modelling OLAP Cube Development verifying data consistency accuracy Keeping date research development new technologies techniques enhance data platform Collaborating Data DevOps Business Intelligence Big Data Analytics teams You enthusiastic developer experience Data Warehousing Business Intelligence You ideally Bachelor Master degree relevant technical area Possess excellent English communication skills enjoy working large team Think SQL breakfast Have advanced understanding data warehouse ETL concepts Have strong knowledge data modelling database architecture Have strong analytical problem solving skills Have ability effectively prioritize handle multiple tasks projects Have development experience Microsoft environment SQL Server SSIS SSAS Have experience data Integration tools preferably SSIS Data Warehouse modelling Kimball Be familiar least one Object Oriented Language NET Java Python Be experienced knowledgeable core database development tuning techniques Be experienced understanding big data technologies Hadoop HBase Hive Pig etc", "Lead downstream purification process development exosome therapeutic development programs currently preclinical stage Develop FPLC based scalable column chromatographic methods purification isolation high quality exosome products including limited preparatory scale affinity purification ion exchange IEX chromatography multimodal hydrophobic interaction size exclusion SEC chromatography Execute improve current Tangential Flow Filtration TFF procedure isolation formulation exosome products As subject matter expert maintain knowledge implement state art technologies processes exosome purification isolation Author process descriptions SOPs protocols tech transfer documents necessary seamless process transfer internal external collaborators Meticulously maintain complete accurate records work performed LIMS electronic lab notebook ELN Play central role selection interaction contract manufacturers research organizations key suppliers external vendors Prior experience working managing CMO CDMO CRO desired Manage downstream process development function including operational considerations lab buildout equipment purchases team recruitment training setting key metrics objectives providing guidance mentorship junior staff Coordinate tasks across multiple projects demonstrating prioritization planning We start looking applicants ready willing roll sleeves Key Driver meet project timelines milestones", "Managing investigation corporate data requirements documenting according required standards utilising prescribed methods tools Implementing data flows connect operational systems data analytics BI systems Re engineer manual data flows enable scaling repeatable use Working closely data architects determine data management systems appropriate data scientists determine data needed analysis Tackling problems associated database integration unstructured data sets Ensuring using data structures associated components good understanding queries dealt promptly efficiently Strong technical process understanding regardless technology Wide range strong technical skills e Azure Devops Azure Data Factory Data Bricks SQL python Core SQL Competencies SSMS SSIS T SQL Stored Procedures ADF Pipelines build populate SQL databases Background migrating traditional MS products Azure Ability create efficient DW DL structures minimise cost orchestration processing ingestion data Very high attention detail Strong communication skills Efficient building ETL ELT processes enterprise solutions Strong software delivery methods knowledge Digital delivery track record working DevOps delivery Exposure Climate Change data legislation practices stakeholders Experience Environmental related industries e Water Energy Forestry related Presentation skills Understanding architecting solutions taking account wider considerations", "", "", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results Connections recruiters industry experts online live Devex events", "Create improve systems enable end end solutions products support Commercialization activities Build data micro services perform data transformation metadata extraction workload management error processing management Query manipulate visualize data using R JavaScript CSS Tableau Evaluate utilize state art technologies industry meet business needs Contribute maturing Planisware system capabilities Contribute continuous development data analytics insights strategy Commercialization Integrate operations data platform analytical tools Tableau Data Scientist Workbench Data Marketplace etc Develop data processing pipelines large datasets cloud AWS integrate data sources applicable Collaborate engineering team members ensure services reliable maintainable well integrated existing platforms Adhere best practices testing designing reusable code Ensure effective communication key partners including business clients technical staff vendors analyze scientific needs implement informatics solutions data acquisition integration analysis Own run product backlog delivery", "Working data cleansing batch processing data transformations data manipulations enable data science efforts Advanced image manipulation Geometry geographic calculations Serving part core team technology stack Partnering closely Founders bring disruptive AI based technology platform insurance real estate markets", "Planning building running enterprise class information management solutions across variety technologies e g big data master data data profiling batch processing data indexing technologies Establishing advance search solutions include synonym inference faceted searching Ensuring appropriate security compliance policies followed information access dissemination Defining applying information quality consistency business rules throughout data processing lifecycle Collaborating information providers ensure quality data updates processed timely fashion Enforcing expanding use AbbVie Common Data Model industry standard information descriptions ontologies taxonomies vocabularies lexicons dictionaries thesaurasus glossaries etc Managing information portal customer facing resources data catalog data portal etc Bachelor Degree years related work experience strong understanding specified functional area Degree Computer Science related discipline preferred Advanced degree preferred At least years experience several data processing roles database developer administrator ETL developer data analyst BI analytics developer solution developer contextual search applications", "The ability absorb nuances Bio Tech operations value chain including supply chain logistics manufacturing source systems High personal standards productivity quality Able function scrum master Data Engineering Team The ability contribute collaborative fast paced environment Defines approves data engineering design patterns used general use multiple implementations Collaborate Data Architects Business SME Data Scientists architect data products services Build data products service processes perform data transformation metadata extraction workload management error processing management Implement standardized automated operational quality control processes deliver accurate timely data reporting meet exceed SLAs Drive exploration adoption new tools techniques propose improvements data pipeline Integrate operations data platform Data Scientist workbench Data Marketplace Analytic Tools Tableau Spotfire R etc Act product manager operations data platform backlog Act run manager provide Run DevOps support", "", "Work CTO create maintain optimal data pipeline architecture documentation Assemble large complex data sets meet functional non functional business requirements Identify design implement internal process improvements automating manual processes optimizing data delivery designing infrastructure greater scalability etc Build infrastructure required optimal extraction transformation loading data wide variety data sources using big data technologies Build analytics tools utilize data pipeline provide actionable insights customer acquisition operational efficiency key business performance metrics Work stakeholders including Executive Product Data Design teams assist data related technical issues support data infrastructure needs", "", "", "", "", "", "", "", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results", "", "", "Design local modifications global data architecture including new tools technologies necessary meet regional use cases Provide direction development bespoke client specific data sandboxes Create maintain optimal data pipeline architecture based Global Technology Stack Identify design implement internal process improvements provide greater scalability existing client solutions Develop custom built packages glue code support needs Data Scientists across region Work broader business stakeholders assist clients consultants data infrastructure needs", "", "Intern Data Engineer Attend meetings learn business practices internal processes procedures Assist developing maintaining data solutions support short long term information analysis goals Develop maintain ETL procedures optimal processing data wide variety data sources Ensure data verified quality checked Create maintain SSRS reports ensure business partners information make informed decisions Build analytic data sets provide actionable insights customer acquisition operational efficiency key business performance metrics Work various Midco stakeholders assist data related technical issues support data infrastructure needs Ensure data integrity regular communications IT developers Analysts SMEs Write database documentation including data standards procedures definitions data dictionary metadata Communicate effectively professionally forms communication internal external customers Adhere Midco privacy guidelines ensure customer privacy Maintain regular attendance required position Function effective team member supporting efforts concepts departments Support mission vision values Midco Apply personal ethics honesty initiative flexibility responsibility confidentiality areas responsibility Possess enthusiastic energetic self motivated detail oriented approach towards work work projects Possess strong problem solving decision making skills using good judgment Multi task change one task another without loss efficiency composure Maintain positive work atmosphere acting communicating manner get along customers clients co workers management Identify opportunities improvement creating implementing viable solutions Actively follow Midco policies procedures Perform duties assigned High School Diploma GED equivalent enrollment college university pursuing Associate Bachelor degree MIS related field Junior Senior level student status recent graduate highly preferred Experience knowledge dimensional modelling preferred Knowledge SQL server scripts SSIS preferred Employees may required work excess hours per week normal business hours holidays evenings weekends business demands The employee occasionally required reach hands arms stoop kneel crouch The employee must occasionally lift carry loads lbs The noise level work environment moderate Free discounted Midco internet cable Tuition reimbursement Support employee involvement communities serve Employee referral program Wellness programs Never accept check funds company purchase materials necessary position Avoid report situations employers require payment work without compensation part application process Avoid corresponding anyone reaches via text email outside Chegg Internships platform recognize", "", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results Connections recruiters industry experts online live Devex events", "", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results", "", "Collaborate Data Architects Business SME Data Scientists architect data products services Build data products service processes perform data transformation metadata extraction workload management error processing management Implement standardized automated operational quality control processes deliver accurate timely data reporting meet exceed SLAs Contribute Exploration understanding new tools techniques propose improvements data pipeline Integrate operations data platform Data Scientist workbench Data Marketplace Analytic Tools Tableau Spotfire R etc Act product manager operations data platform backlog", "", "", "You passionate getting data creating high quality easy consume views data You continually improve learning others jump teammate could use help You care customers understand data contributes goals business You agile mindset comfortable refining vague requirements You sense miscommunications among team members part improve understanding You thrive contribute positive work environment everyone shares constructive thoughts suggestions", "", "", "", "A Data Engineer BI database developer passionate meaningful global impact Experience data integration cleaning validation analysis Strong SQL skills proficiency R Git DevOps System Database Administration experience History working Docker Azure familiar services A pro active problem solver strong analytical skills good sense code data quality Further technical expertise web development data visualisation welcome Expertise finance big plus Fluent English Design implement systems make data analysis reliable transparent reproducible Maintain extend improve existing data infrastructure related services Support train colleagues continuous improvement skills Handle technical communication stakeholders external partners Evaluate develop technical solutions new projects use cases A meaningful impact one planet pressing issues Engaging working environment young international rapidly expanding team chance work cutting edge think tank Flexible working hours Chance take responsibility shape project first day onwards Opportunity significant career development", "Managing investigation corporate data requirements documenting according required standards utilising prescribed methods tools Implementing data flows connect operational systems data analytics BI systems Re engineer manual data flows enable scaling repeatable use Working closely data architects determine data management systems appropriate data scientists determine data needed analysis Tackling problems associated database integration unstructured data sets Ensuring using data structures associated components good understanding queries dealt promptly efficiently Strong technical process understanding regardless technology Wide range strong technical skills e Azure Devops Azure Data Factory Data Bricks SQL python Core SQL Competencies SSMS SSIS T SQL Stored Procedures ADF Pipelines build populate SQL databases Background migrating traditional MS products Azure Ability create efficient DW DL structures minimise cost orchestration processing ingestion data Very high attention detail Strong communication skills Efficient building ETL ELT processes enterprise solutions Strong software delivery methods knowledge Digital delivery track record working DevOps delivery Exposure Climate Change data legislation practices stakeholders Experience Environmental related industries e Water Energy Forestry related Presentation skills Understanding architecting solutions taking account wider considerations", "", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results Connections recruiters industry experts online live Devex events", "Experienced data pipeline developers proven track record developing deploying optimizing data systems support analytics ground Data tech savvy Innately curious fast learners Self motivated team players drive get things done Design develop scale maintain data pipelines extract load transform integrate data wide variety data sources provide uniform view Automate optimize data pipelines improve productivity processing performance reliability minimize error prone processes Monitor data consumption patterns ensure responsible use provisioned data data consumers Collaborate data stewards data governance teams implement data governance compliance best practices Support data scientists data analysts optimizing data management delivery processes", "", "", "", "Build data pipeline frameworks automate high volume real time data delivery cloud platform", "", "", "Participate software design meetings analyze user needs determine technical requirements Write technical specifications based conceptual design stated business requirements Knowledge standards relevant software industry e g ISO CMM Six Sigma You organized detail oriented able work shifting priorities You love analyzing issues devising efficiencies better client experience You looking join team build long term career FIS Knowledge financial services industry Health coverage offered family Health Vision Dental Insurance plans 401K company contribution Employee Stock Purchase Program company match FIS Gives Back Program charitable events activities help support local community", "Work Director Institutional Research UDS analytic team identify common metrics internal external data requests gather requirements datasets translate requirements actionable projects Build manage clean large datasets creating curated focused fit purpose datasets optimize efficiency UDS reporting analysis Train UDS analysts available datasets including field variable definitions Closely collaborate Director Institutional Research UDS analysts update refine datasets annual needed basis specifications evolve As needed prepare data tabulate metrics meet internal external reporting requirements accreditors state federal governments agencies organizations Develop standardize reporting procedures streamline reporting activities processes order establish efficient methods reporting Thoroughly document reporting efforts UDS records future reference Identify investigate work resolve data quality issues relevant functional units data administration governance colleagues", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results Connections recruiters industry experts online live Devex events", "", "", "Translate business requirements data models easy understand used different disciplines across company Design implement build pipelines deliver data measurable quality SLA Partner business domain experts data analysts engineering teams build foundational data sets trusted well understood aligned business strategy enable self service Be champion overall strategy data governance security privacy quality retention satisfy business policies requirements Own document foundational company metrics clear definition data lineage Identify document promote best practices years experience working data architecture data modeling master data management metadata management Recent accomplishments working relational well NoSQL data stores methods approaches logging columnar star snowflake dimensional modeling Proven track record scaling optimizing schemas performance tuning SQL ETL pipelines OLAP Data Warehouse environments Demonstrated skills either Python Java programming language Familiar data governance frameworks SDLC Agile methodology Excellent written verbal communication interpersonal skills ability effectively collaborate technical business partners Hands experience Big Data technologies e g Hadoop Hive Spark big plus", "Using programming scripting parse prepare data Building maintaining scientific tools infrastructure Applying machine learning uncover novel trends experimental data Working effectively project teams develop informatics solutions satisfy business needs", "Strong PT behaviors emphasis continuous improvement", "Work Data Scientists business partners cross functional teams developing subject matter expertise business well advanced analytics Provide support requirement development analytic data sources breaking business problems solvable components assist documenting requirements minimal supervision Execute rapid development new data analytic work tracks fast iteration quick sprints Help develop deliver data infrastructure required support needs predictive modeling analytics minimal supervision Builds test scripts executes testing works data scientists business ensure end user acceptance Leverage agile data analysis technology fluency parallel processing programming software programming languages technologies e Oracle MongoDB SQL Python Spark Kafka Scala Hadoop paired high degree analytic agility able meet fluid dynamic business needs space Participate development enterprise data assets information platforms data spaces designed exploring understanding data Participate development new concepts proof concept designs prototypes business research data solutions business users predictive modelers may visually understand explore new feature functionality implementation expose design assumptions drive ideation", "", "", "Lead development Relational Database Service RDS structure overseeing key technical contributors assessing evaluating future reporting solutions Contribute design development demand reporting solutions support key stakeholders Assess leverage new technologies ensure data solutions stable efficient responsive appropriate business needs Maintain upkeep program vision ensure interoperability cost effective authorized technologies e g cloud environments Maintain awareness changing analytics environment partnering Amgen Analytics team industry e g AI machine learning etc Take ownership relevant issues remediate problems completion include providing call support data integration solutions Work data engineers provide clear documentation delivered solutions processes integrating documentation appropriate corporate stakeholders Develop implement sustain operational scripts data structures libraries programming code optimize security emergent compute patterns diverse applications throughout global environment Analyze design develop operate programs shell scripts tests infrastructure automation capabilities advanced security context Collaborate cross functionally analysts engineers data scientists identify prioritize requirements brainstorm solutions clarify business objectives data centric solutions achieve continuous improvement cyber defense resilience Assist data discovery enhancing reports designing efficient data stores Contribute exploration understanding new tools techniques propose improvements data pipeline Self starter high degree initiative Must highly motivated able work effectively minimal supervision", "Converts merges assembles data analysis Develops maintains programs convert data analyzable formats utilized data analysis strong emphasis generalizability transparency Contributes ongoing automation existing emerging processes importing documenting integrating updating data well coding best practices code review version control Develops static interactive reports visualizations facilitate improved data use agencies organizations seek improve wellbeing children families Collaborates researchers data scientists policy implementation specialists IT staff project system design activities", "", "Hybrid technical role interfacing process SMEs using state art technologies whilst also able communicate complex intractable ideas non technical audiences Collect clear requirements SMEs process experts Work process experts model data landscape obtain data extracts define secure data exchange approaches Acquire ingest process data multiple sources systems Big Data platforms Understanding assessing mapping data landscape Collaborate data scientists map data fields hypotheses curate wrangle prepare data use advanced analytical models Maintaining Information Security standards engagement Defining technology stack provisioned infrastructure team", "Never accept check funds company purchase materials necessary position Avoid report situations employers require payment work without compensation part application process Avoid corresponding anyone reaches via text email outside Chegg Internships platform recognize", "", "Interface end users gather understand translate requirements Agile delivery process Work closely Data Platform engineers provide data requirements Work closely product owners designers developers architects quality engineers DevOps deliver innovative solutions solve complex healthcare problems Design implement highly performing ETL used internally Operation Product teams Building generic ingestion engine data processing AWS EMR synchronizing data streams AWS RDS Dynamo ElasticSearch RedShift s3 dockerizing anything running production Manage individual sprint priorities deadlines deliverables Ensure continuously raising standard data engineering excellence implementing best practices coding testing deploying In addition able carry responsibilities looking someone comfortable working fast paced ever changing environment good deal experience SaaS applications indicated following attributes years software development experience Strong experience database technologies SQL MySQL data integration Strong experience scripting languages Python Shell Script Experience Amazon Web Services Hands experience Hive Pig Experience real stream processing Spark Kafka Kinesis nice Experience building working heavy agile collaborative innovative flexible team oriented environment Self driven ability lead complex projects Ability influence others direction projects An excellent leader attitude Hands detail oriented methodical inquisitive A motivated self starter solid level experience quickly grasps complex challenges A skillful communicator experience working technical management teams A strong customer focus metrics driven A quick learner passion challenge outside comfort zone Fantastic collaborator team player negotiator influencer Fast fail entrepreneurial innovative spirit Thrives fast paced environment continuous improvement norm bar quality extremely high Excited challenges working product team undergoing rapid growth serving millions customers Locally based applicants highly preferred Seattle Making difference We right thing right reasons well even hard You operate perspective truly caring employees clients customers creating value We strong individually together powerful We roll sleeves get stuff done We boldly relentlessly reinventing healthcare", "Work business client IS develop analytic applications using graph technologies Participate requirements design workshops internal business client IS partners Develop information models Design develop data pipelines extract data varous sources using APIs perform data transformation load triples graph database RDF triple store Participate aspects software development process using Agile development methodologies Maintain awareness knowledge industry trends proactively identify drive identification opportunities leveraged Learn evaluate conduct proof concepts new technologies emerge pertaining Amgen environment Develop training roadshow materials outreach events increase adoption various solutions developed platforms used", "Contribute design development ETL solutions support key partners Collaborate Application Architects Business SMEs design develop end end data pipelines supporting infrastructure Build operationally support new infrastructure analytics tools DevOps model using Python SQL AWS Proactively identify implement opportunities automate tasks develop reusable frameworks Participate efforts design build develop rapid Proof Concept POC solutions services Basic Qualifications Bachelor degree Or Associate degree years Information Systems experience Or High school diploma GED years Information Systems experience Preferred Qualifications", "Managing investigation corporate data requirements documenting according required standards utilising prescribed methods tools Implementing data flows connect operational systems data analytics BI systems Re engineer manual data flows enable scaling repeatable use Working closely data architects determine data management systems appropriate data scientists determine data needed analysis Tackling problems associated database integration unstructured data sets Ensuring using data structures associated components good understanding queries dealt promptly efficiently Strong technical process understanding regardless technology Wide range strong technical skills e Azure Devops Azure Data Factory Data Bricks SQL python Core SQL Competencies SSMS SSIS T SQL Stored Procedures ADF Pipelines build populate SQL databases Background migrating traditional MS products Azure Ability create efficient DW DL structures minimise cost orchestration processing ingestion data Very high attention detail Strong communication skills Efficient building ETL ELT processes enterprise solutions Strong software delivery methods knowledge Digital delivery track record working DevOps delivery Exposure Climate Change data legislation practices stakeholders Experience Environmental related industries e Water Energy Forestry related Presentation skills Understanding architecting solutions taking account wider considerations", "", "", "Work teams build continue evolve data models data flows enable data driven decision making Design alerting testing ensure accuracy timeliness pipelines e g improve instrumentation optimize logging etc Identify shared data needs across Stripe understand specific requirements build efficient scalable data pipelines meet various needs enable data driven decisions across Stripe Work data platform team identify integrate new tools data stack For example currently evaluating Presto use ad hoc query tool Have strong engineering background interested data You writing production Scala Python code along occasional ad hoc SQL queries Have experience writing debugging ETL jobs using distributed data framework Hadoop Spark etc Have experience managing designing data pipelines Can follow flow data various pipelines debug data issues Have experience Scalding Spark Have experience Airflow similar scheduling tools Write unified user data model gives complete view users across varied set products like Stripe Connect Stripe Atlas Continuing lower latency bridge gap production systems data warehouse Working customer support data pipeline help us track time response users total support ticket volume help us staff support team appropriately", "Participate strategic planning discussions technical non technical partners Effective efficient utilization programming tools techniques Responsible design prototyping delivery software solutions within big data eco system Responsible development advanced analytics solutions integrating new tools improve Inquisitive continues seek development opportunities Works geographically dispersed team embracing Agile DevOps principles Bachelor Degree towards Computer Science Computer Engineering Solid foundational understanding object oriented programing principles Familiarity source control solutions ex git GitHub Jenkins Artifactory Strong Python Java OOP language skills Strong communication collaboration problem solving skills", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results Connections recruiters industry experts online live Devex events", "Collaborate Data Architects Business SME Data Scientists architect data products services Build data products service processes perform data transformation metadata extraction workload management error processing management Implement standardized automated operational quality control processes deliver accurate timely data reporting meet exceed SLAs Contribute Exploration understanding new tools techniques propose improvements data pipeline Integrate operations data platform Data Scientist workbench Data Marketplace Analytic Tools Tableau Spotfire R etc Act product manager operations data platform backlog Act run manager provide Run DevOps support", "Gain broad experience grow role Data Solution Architect build Data Lake solutions biggest complex international organizations Be expected technology innovation driven person keen work new technologies platforms take next level Data Science Work diverse global team highly skilled people Alternate working clients creating products Be guru behave consultant Be responsible providing technical leadership enterprise scale projects solutions Develop maintain improve architectures including reference target architectures principles roadmaps patterns etc You Masters Computer Science Software Engineering comparable discipline You hands experience Hadoop Spark Kubernetes You strong OO programming skills Java Scala Python R C C You experience Cloud based Data warehouse Data lake ETL Pipelines Data Modelling tools You experience cloud native micro services oriented big data non relational data architectures Flourish working like minded people yet function autonomously Are excited new technology innovation Like learn outside core activities order broaden field expertise Value great quality work thinking ahead come natural pragmatic Are willing work client location travel abroad Have willing gain managerial skills time apply Affinity Data Architecture Experience articulate trade offs benefits risks architecture design solutions", "Glovo looking world class data engineer work Barcelona office You someone loves working high paced startup environment solving difficult problems You passionate data inventing new elegant solutions support internal customers needs believe empowering entire company facilitating efficient access data You develop new data pipelines leverage cloud architecture perform transformations existing data support new use cases You using Redshift primary data warehouse solution create curated data model enterprise leverage Design implement support analytical platform providing ad hoc automated access large datasets Interface internal teams extract transform load data wide variety data sources using SQL big data technologies Continually improve ongoing ETL reporting analysis processes automating simplifying self service support customers", "Design implement scalable data workflows pipelines integrate diverse data sources sinks Design appropriate database schemas optimize database deployment architectures analytics query loads Implement data transforms organization various data stores data lakes warehouses Design implement new platform architectures building serving machine learning models Work platform operations team monitor maintain live production systems Provide tooling automation infrastructure continuous testing continuous deploy data systems", "", "Work closely team members optimize organization data systems IT architecture Design build infrastructure data extraction preparation loading data variety sources using technology SQL AWS Build data analytics tools offer deeper insight organization pipeline allowing critical discoveries surrounding key performance indicators stakeholders activity Determine database structural requirements analyzing client operations applications programming review objectives clients evaluate current systems Develop database solutions designing proposed system define database physical structure functional capabilities security back recovery specifications Install database systems developing flowcharts apply optimum access techniques coordinate installation actions document actions Maintain database performance identifying resolving production application development problems calculating optimum values parameters evaluating integrating installing new releases completing maintenance answering user questions Provide database support responding user questions resolving problems", "", "Full access jobs board including exclusive jobs Your Devex profile highlighted recruiter search results", "", "", "Managing investigation corporate data requirements documenting according required standards utilising prescribed methods tools Implementing data flows connect operational systems data analytics BI systems Re engineer manual data flows enable scaling repeatable use Working closely data architects determine data management systems appropriate data scientists determine data needed analysis Tackling problems associated database integration unstructured data sets Ensuring using data structures associated components good understanding queries dealt promptly efficiently Strong technical process understanding regardless technology Wide range strong technical skills e Azure Devops Azure Data Factory Data Bricks SQL python Core SQL Competencies SSMS SSIS T SQL Stored Procedures ADF Pipelines build populate SQL databases Background migrating traditional MS products Azure Ability create efficient DW DL structures minimise cost orchestration processing ingestion data Very high attention detail Strong communication skills Efficient building ETL ELT processes enterprise solutions Strong software delivery methods knowledge Digital delivery track record working DevOps delivery Exposure Climate Change data legislation practices stakeholders Experience Environmental related industries e Water Energy Forestry related Presentation skills Understanding architecting solutions taking account wider considerations", "", "", "", "", "", "", "", "", "Curate design catalogue high quality data models ensure data accessible reliable Build highly scalable data processing frameworks use across wide range datasets applications Provide data driven insight decision making critical GS business processes order expose data scalable effective manner Understanding existing potential data sets engineering business context Deploy modern data management tools curate important data sets models processes identifying areas process automation efficiencies Evaluate select acquire new internal external data sets contribute business decision making Engineer streaming data processing pipelines Drive adoption Cloud technology data processing warehousing Engage data consumers producers order design appropriate models suit needs years relevant work experience team focused environment A Bachelor degree Masters preferred computational field Computer Science Applied Mathematics Engineering related quantitative discipline Extensive knowledge proven experience applying domain driven design build complex business applications Deep understanding multidimensionality data data curation data quality traceability security performance latency correctness across supply demand processes In depth knowledge relational columnar SQL databases including database design General knowledge business processes data flows quantitative models generate consume data Excellent communications skills ability work subject matter expert extract critical business concepts Independent thinker willing engage challenge learn Ability stay commercially focused always push quantifiable commercial impact Strong work ethic sense ownership urgency Strong analytical problem solving skills Ability collaborate effectively across global teams communicate complex ideas simple manner Financial Services industry experience Working knowledge one programming language Python Java C C etc", "Exploring new ways building processing analysing data order deliver insights business partners Lead drive creation Data Engineering center excellence PD Informatics", "Exploring new ways building processing analysing data order deliver insights business partners Lead drive creation Data Engineering center excellence PD Informatics", "ISE Professional Services offers clients complete spectrum software engineering services including IoT Telematics Big Data Cloud Mobile App Development Agile Consulting Innovation Services provides capability achieve sustain short long term growth throughout entire organization This approach allows ISE remain focused enhancing customer experience look technology solutions", "Industrial Engineering IT Computer Science", "", "Professional Services offers clients complete spectrum software engineering services including IoT Telematics Big Data Cloud Mobile App Development Agile Consulting Innovation Services provides capability achieve sustain short long term growth throughout entire organization This approach allows ISE remain focused enhancing customer experience look technology solutions", "A Data Engineer BI database developer passionate meaningful global impact Experience data integration cleaning validation analysis Strong SQL skills proficiency R familiarity Git DevOps System Database Administration experience A pro active problem solver strong analytical skills good sense code quality Genuine interest improving data infrastructure related services Fluent English In addition experience finance big plus Further technical expertise web development data visualisation welcome well Somebody courage apply even tick boxes Support implementing systems make data analysis reliable transparent reproducible Maintain extend improve existing data infrastructure related services Support colleagues data queries SQL trainings continuous improvement skills Evaluate develop technical solutions new projects use cases A meaningful impact one planet pressing issues Engaging working environment young international rapidly expanding team chance work cutting edge think tank Flexible working hours Chance take responsibility shape project first day onwards Opportunity significant career development", "Managing investigation corporate data requirements documenting according required standards utilising prescribed methods tools Implementing data flows connect operational systems data analytics BI systems Re engineer manual data flows enable scaling repeatable use Working closely data architects determine data management systems appropriate data scientists determine data needed analysis Tackling problems associated database integration unstructured data sets Ensuring using data structures associated components good understanding queries dealt promptly efficiently Strong technical process understanding regardless technology Wide range strong technical skills e Azure Devops Azure Data Factory Data Bricks SQL python Core SQL Competencies SSMS SSIS T SQL Stored Procedures ADF Pipelines build populate SQL databases Background migrating traditional MS products Azure Ability create efficient DW DL structures minimise cost orchestration processing ingestion data Very high attention detail Strong communication skills Efficient building ETL ELT processes enterprise solutions Strong software delivery methods knowledge Digital delivery track record working DevOps delivery Exposure Climate Change data legislation practices stakeholders Experience Environmental related industries e Water Energy Forestry related Presentation skills Understanding architecting solutions taking account wider considerations", "Build multi PB scale data platform Design code develop new features fix bugs enhancements systems data pipelines ETLs adhering SLA Follow engineering best methodologies towards ensuring performance reliability scalability measurability Collaborate Software Engineers ML Engineers Data Scientists stakeholders taking learning leadership opportunities arise every single day Mentor junior engineers team level Raise bar sustainable engineering improving best practices producing best class code documentation testing monitoring Bachelor degree Computer Science related technical discipline equivalent years strong data engineering design development experience building massively large scale distributed data platforms products Advanced coding expertise SQL Python JVM based language Expert heterogeneous data storage systems relational NoSQL memory etc Deep knowledge data modeling lineage access governance Excellent skills AWS services like Redshift Kinesis Lambda EMR EKS ECS etc Wide exposure open source software frameworks broader cutting edge technologies Airflow Spark Druid etc Familiar infrastructure provisioning tools e g Terraform Chef Consistent proven ability deliver work time attention quality Excellent written spoken communication skills ability work effectively others team environment Work studio complete P L ownership games Competitive salary discretionary annual bonus scheme Zynga RSUs Full medical accident well life insurance benefits Catered breakfast lunch evening snacks Child care facilities women employees discounted facilities male employees Well stocked pantry Generous Paid Maternity Paternity leave Employee Assistance Programs Active Employee Resource Groups Women Zynga Frequent employee events Additional leave options employees Flexible working hours many teams Casual dress every single day", "", "", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "Performing data discovery old use legacy systems Overseeing ETL development Azure Data Factory writing optimal processes populate data warehouse Strong hands experience working Azure Cloud system A history architecting extensive features ETL processes Strong commercial experience developing optimising ETL processes Azure Data Factory", "", "", "", "", "", "", "", "", "Providing efficient ways data scientists team members access manipulate data timely manner You involved decisions data storage data models Highlighting areas low data quality technical debt opportunities improvement across data quality technology process Vast experience variety databases SQL NoSQL etc Cloud Platforms AWS GCP Golang Python Python Libraries data", "Location NY New York Job Type Full Time Lead architecture design implementation next generation cloud BI solution Build robust scalable data integration ETL pipelines using SQL EMR Python Spark Build deliver high quality data architecture support business analysis data scientists customer reporting needs Interface technology teams extract transform load data wide variety data sources Continually improve ongoing reporting analysis processes automating simplifying self service support business constituents Support maintain optimize upgrade enhance current future state Data Infrastructure SQL server AWS Snowflake Tableau Alteryx Bachelors Degree related field Demonstrated strength data modeling ETL development Data warehousing years data engineering experience Experience working delivering end end projects independently Experience AWS services including S3 Redshift EMR Kinesis RDS preferred Experience cloud Data Warehouses Snowflake plus Experience Big Data Technologies Hadoop Hive Hbase Pig Spark etc Medical Dental Vision Insurance Transit Discount Program 401K Plan Paid Time Off Program Flexible Spending Accounts Employee Dining Program Referral Bonus Online Training Program Career Development Corporate Fitness Discount Programs Choice Global Cash Card Direct Deposit", "", "", "Design develop deploy big data stack data processing infrastructure platform Architect rearchitect multi tenant databases meet needs customer base Improve data validation data quality monitoring Work client backend teams guide events driven designs Optimize tune databases improve performance reduce cost years experience building large scale robust data processing pipeline years experience coding Python Background API development focus data transformations data streams Strong background building maintaining automation Advanced experience data streaming ingest ETL data warehousing technologies Strong experience database schema design data governance data modeling Experience Spark Beam Redshift Tableau MySQL etc Experience AWS GCP Azure cloud Good understanding data security encryption Experience tools like Airflow Dagster DBT", "", "", "Hand screened leads No ads scams junk Great job search support", "", "Hand screened leads No ads scams junk Great job search support", "", "Several years experience working SAP Data Services Sybase IQ least two BI projects Experience designing ETL solutions The ability liaise business gather requirements create technical documentation Experience data warehouse design build", "", "", "", "", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "Hand screened leads No ads scams junk Great job search support", "", "", "", "", "", "", "", "Develop maintain developer tooling support pyspark data pipelines Implement processes systems monitor data quality ensuring production data accurate available key stakeholders business processes depend Develop maintain data platform components including data producers consumers pipeline architecture data lake data warehouse Business Intelligence tooling Collaborate closely fellow data team members well tech product teams company leaders Support continuing increases data velocity volume complexity Write unit integration tests document work Experience knowledge Agile Software Development methodologies Excellent problem solving troubleshooting skills Strong SQL Python development experience Proven experience schema design dimensional data modeling Practical experience SQL NoSQL databases Practical experience supporting Business Intelligence tooling third party systems Experience designing building maintaining data processing systems Experience working MapReduce Spark clusters Experience detecting reporting data quality issues Familiarity Docker CI CD Jenkins Circle AWS Competitive salary Health insurance paid individuals families Primary caregiver week paid leave 401K Generous vacation policy plus company holidays Company equity Commuter cell phone benefit A commitment open inclusive diverse work culture One mental health day per quarter monthly work home stipend Tele mental health services OneMedical membership including tele health services Increased work flexibility parents caretakers Access Axios Family Fund created allow employees request financial support facing financial hardship emergencies Weekly company sponsored exercise meditation classes", "Implement maintain new services dashboards leveraging multiple technologies Build performance reporting new LivePerson product offerings Directly deliver reporting enhancement requests within LivePerson product Create dynamic API based tools reports drive insight client performance Bring client facing perspective R D role transform client feedback actionable projects ensure technical stakeholders driving toward clients business needs Proven software development experience within SaaS industry Proficiency Javascript Node js Angular HTML CSS Experience building large scale web applications cloud Experience data visualization tools including MicroStrategy Visual Insights Advanced Excel user proficiency developing VBA Familiarity extracting transforming data REST API Working knowledge SQL Working knowledge database table relationships Working knowledge Hadoop Impala Basic understanding interest learning Spark Python R data science language Passionate software development technology data analysis Strong analysis design skills Self starter strong motivation execution capabilities", "", "Design develop document test ETL solutions using industry standard tools Participate requirements gathering sessions business technical staff distill technical requirements business requests Partner clients fully understand business philosophy IT Strategy recommend process improvements increase efficiency reliability ETL development Present ETL documentation designs team members convey complex information clear concise manner Extract data multiple sources integrate disparate data common data model integrate data target database application file using efficient ETL processes", "Strong software development skills Solid PHP frameworks knowledge Symfony nice Solid Python experience Ability work fast quickly get speed existing code learn new concepts easily Experience passion Big Data Great problem solving skills Ability work team environment Facility learn new technologies Background SQL PostgreSQL Experience Big Data architectures technologies 1TB data BI solutions", "", "Creation maintenance reporting analytics infrastructure Full lifecycle Performance Hub architectures creation data set processes used modelling mining acquisition verification Solid command common scripting languages tools Skills constantly improve data quality quantity leveraging improving data analytics systems Provide link business solution providers within context functional alignment Identify communicate risks issues arise Travel Site average trips per month days In depth knowledge SQL Data warehousing ETL tools Hadoop Based analytics Coding skills across Python Tibco Spotfire Data modelling across OT IT datasets A solid background business requirements gathering use cases agile delivery Excellent interpersonal skills ability work well effectively range business project stakeholders Ability question challenge collaboratively find better ways working Excellent communication presentation skills Motivated individual ability make decisions take direction Mining resource industry experience ability transfer skill set mining Business analyst skills Esri Insight Akumen", "", "", "", "", "", "", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "Every months hack week gives developers opportunity explore ideas might otherwise make product roadmap We committed career development We offer formal mentoring program well tuition reimbursement We frequent panel discussions talks industry leaders Sheryl Sandberg Melinda Gates Ta Nahesi Coates recent examples We believe diversity fuels innovation creativity variety employee groups dedicated fostering diverse inclusive workplace We offer generous parental leave policy recently expanded response employee feedback Birth mothers receive weeks fully paid adoptive parents birth fathers receive weeks fully paid Similarly offer competitive health dental insurance well 401k matching Implement complex data projects focus collecting parsing analyzing visualizing large sets data turn information insights across multiple platforms Build fault tolerant self healing adaptive highly accurate ETL platforms Design develop data model Data Warehouse Data Marts NoSQL solutions vetted team guided Senior Data Solutions Architect Responsible data administration warehouse solutions Take ownership warehouse solutions troubleshoot issues provide production support Document processes standard operating procedures processes Generate reports using variety reporting tools Business Objects Tableau Pentaho Work team developers transitioning current house data warehouse solution Google Cloud platform", "", "Apply strong technical skills data engineering team building industry leading technology", "Use technology Spark Kafka SQS build large scale real time batch data pipelines Help implement cloud technologies AWS Azure GCP Program least one following languages Java Scala Python Proven experience data development big data Hadoop cloud data warehouse using SQL SQL based ETL capabilities Have deep understanding processes skills technologies needed deliver complex development solutions business Understand experience working variety delivery methods Agile Waterfall Scrum Experience developing cloud AWS Snowflake Work one data rich businesses UK Competitive bonus Discount across brands", "", "", "", "", "", "", "Hand screened leads No ads scams junk Great job search support", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "", "Conduct advanced statistical analysis provide actionable insights identify trends measure performance Use data mining techniques collect compile data wide variety data repositories Build learning systems monitor data flows react changes customer preferences business objectives Build high performance predictive prescriptive algorithms using cutting edge statistical techniques e g neural networks Research new machine learning solutions complex business problems Collaborate engineers implement deploy scalable solutions Provide thought leadership researching best practices conducting experiments collaborating industry leader Use data visualization tools develop visuals used effectively communicate technical findings non technical audience Teach data science concepts junior members team PhD years experience Masters years experience Data Science Computer Science Engineering Statistics related field Strong background machine learning statistics Deep Learning Bayesian statistics plus Prototyping Expertise quick build proofs concept involving data munging scripting analysis Solid foundation data structures algorithms Hands experience building models deep learning frameworks like MXNet Tensorflow Caffe Torch Theano similar Experience processing massive amounts structured unstructured data using Spark SQL Hive Impala HBase Proficiency Python numerical statistical programming including Numpy Pandas Scikit learn Experience natural language processing Experience using Cloud computing AWS Azure GCP Experience Java Scala Python etc Experience working large data sets distributed computing tools plus Map Reduce Hadoop Hive Spark", "", "", "Responsible maintenance improvement cleaning manipulation data business operational analytics databases Identification discovery complex data sets align defined use cases Contribute development data aggregation metrics calculations Selecting integrating Big Data tools frameworks Implementing ETL process Monitoring performance advising necessary infrastructure changes Defining data retention policies Define builds data pipelines enable faster better data informed decision making within business At least years working experience database engineering support personnel database engineering administrator within fast paced complex business setting You also experience defining building data roadmaps Spark NoSQL databases Big Data ML toolkits good knowledge Big Data querying tools Demonstrate experience working large complex data sets well experience analyzing volumes data Experience creation debugging databases critical business mission You strong working conceptual knowledge building maintaining physical logical data models experience Tableau Qlik toolsets Preferable experience monitoring disaster recovery backup automated testing automated schema migration continuous deployment High level business commercial acumen demonstrated ability interpret business requirements deliver outputs align business improvement objectives High quality written oral communication skills including presentation skills Telecommunications Industry experience", "", "Build multi PB scale data platform Design code develop new features fix bugs enhancements systems data pipelines ETLs adhering SLA Follow engineering best methodologies towards ensuring performance reliability scalability measurability Collaborate Software Engineers ML Engineers Data Scientists stakeholders taking learning leadership opportunities arise every single day Mentor junior engineers team level Raise bar sustainable engineering improving best practices producing best class code documentation testing monitoring Bachelor degree Computer Science related technical discipline equivalent years strong data engineering design development experience building massively large scale distributed data platforms products Advanced coding expertise SQL Python JVM based language Expert heterogeneous data storage systems relational NoSQL memory etc Deep knowledge data modeling lineage access governance Excellent skills AWS services like Redshift Kinesis Lambda EMR EKS ECS etc Wide exposure open source software frameworks broader cutting edge technologies Airflow Spark Druid etc Familiar infrastructure provisioning tools e g Terraform Chef Consistent proven ability deliver work time attention quality Excellent written spoken communication skills ability work effectively others team environment Work studio complete P L ownership games Competitive salary discretionary annual bonus scheme Zynga RSUs Full medical accident well life insurance benefits Catered breakfast lunch evening snacks Child care facilities women employees discounted facilities male employees Well stocked pantry Generous Paid Maternity Paternity leave Employee Assistance Programs Active Employee Resource Groups Women Zynga Frequent employee events Additional leave options employees Flexible working hours many teams Casual dress every single day", "Engineer Data Engineer", "", "", "DevOps Agile Delivery SLC methodologies adjusted infrastructure service delivery integration Abbott DevOps organizations Uses Infrastructure Code tools e g Terraform Packer techniques deliver usable storage services public private cloud Implements Continuous Integration Continuous Delivery pipelines deployment application infrastructure stacks Establishes data governance security Optimize cloud storage security cost availability Automate cloud operations tasks including backups restores Responsible compliance applicable Corporate Divisional Policies procedures Serve technical expert public cloud storage offerings uses within Abbott Evaluate recommend new software utilities tools Sets strategy implementing new technologies Develops service implementation scripts provides top level support IT solutions aligned customer business objectives infrastructure technology disciplines within BTS IS Nurtures relationships key suppliers cloud technologies keep abreast emerging technologies ensure solution stability optimal pricing influences product direction align Abbott This position reports directly Director Hosting BTS The position accountable internal cost recovery mechanisms This position technical contact representing infrastructure hosting cloud area partner Abbott IT business organizations key external service providers Accountable awareness technology trends determination execution global strategies satisfy business critical reliance Abbott increasingly placing infrastructure host applications years Cloud Experience Experience implementing PostgreSQL MySQL Experience managing data storage solutions blob managed disk NFS AWS Azure Bachelor Degree preferred technology management discipline", "", "At least years relevant experience Minimum years hands experience Big Data Eco System Hadoop Hive MapReduce Minimum year hands experience Spark core Minimum years hands programming experience core Python Java Minimum year hands experience HBase Cassandra", "Develop administer standards architecting working data management systems well deploying monitoring maintaining machine learning models Ensure analytics systems meet business requirements implement industry best practices Develop processes tools monitor analyze model performance data quality Build infrastructure required optimal ETL wide variety data sources using SQL databases AWS big data services Extract integrate analyze data heterogeneous mix data sources support create insights recommendations internal stakeholders Collaborate data scientists stakeholders across business Sales Strategy Finance Technology Business Development develop new information services optimise business operations Create maintain optimal data pipeline architecture Work data scientists business stakeholders develop refine maintain statistical machine learning models Productionize machine learning models data science products e g A B tests optimise business operations Recommend implement different ways constantly improve data reliability quality disaster recovery procedures Identify design implement internal process improvements automating manual process optimizing data delivery redesigning infrastructure greater scalability etc Create tools improve access confidence data internal users make data driven decisions Mentor data science tech team members", "Evaluate cognitive application requirements make architectural recommendations implementation deployment provisioning Watson services applications IBM Bluemix Provide expertise identification cognitive use cases bring significant business value development use cases proof concept enterprise wide implementation integration cognitive computing services within existing application environment Work alongside engineering teams align aspects day day delivery Agile DevOps environment You work clients identify Cognitive Computing strategies options roadmaps Internally help identify improved ways working mentor junior architects lead business development opportunities Experience software engineering development background Java C COTS Products Designing deploying scalable highly available fault tolerant systems IBM Bluemix cloud platforms AWS Azure Significant experience designing architecture Big Data data science platforms including Hadoop Spark data warehouses NoSQL graph databases Knowledge experience practical application Natural Language Processing Information Retrieval Semantic Technologies Big Data Machine Learning Development experience expertise Watson Data Platform Data Science Experience DSX Watson APIs services including Knowledge Studio Natural Language Understanding Speech text Visual Recognition Designing analytical data stores enterprise wide performance management data visualisation applications Architecting optimal cognitive computing solutions based Watson services user requirements Experience data architecture designing logical data models Workshop facilitation client communication presentation cognitive computing concepts non technical audiences Understand implement automate security controls governance processes compliance validation Understanding cognitive computing technology automate enhance operational business processes Experience DevOps type operations use source control GitHub Stash Build tools Puppet Jenkins etc continuous integration test driven development e Cucumber Lettuce etc Identifying defining requirements Watson application Deploying hybrid systems premise Bluemix components Providing best practices building secure reliable applications Bluemix platform Hadoop administration set security tuning Data ingestion Kafka Flume Spark", "Advance development optical read write drive Folio Photonics DataFilm DiscTM Hands engineering data storage system consisting Folio DataFilm DiscTM associated read write drive well integration customers multidisc library Signal processing data engineering optical data Integration drive data storage system Display strong interpersonal skills work cross functional engineering executive teams Management opportunity appropriately qualified candidates", "", "", "", "Project Type One time project", "Assist creation data schemas stored procedures data pipelines views Help build maintain technical solutions required optimal ingestion transformation loading data wide variety data sources large complex data sets Collaborate across roles embrace best practices reporting analysis including data integrity test design validation documentation Ability collect organize analyze disseminate significant amounts information attention detail accuracy Build automate actionable reports Collaborate data analysts data scientists stakeholders design discussions uncover detailed business requirements related data engineering Develop strong hypotheses independently solve problems share actionable insights engineering", "", "", "", "Use technology Spark Kafka SQS build large scale real time batch data pipelines Help implement cloud technologies AWS Azure GCP Program least one following languages Java Scala Python Proven experience data development big data Hadoop cloud data warehouse using SQL SQL based ETL capabilities Have deep understanding processes skills technologies needed deliver complex development solutions business Understand experience working variety delivery methods Agile Waterfall Scrum Experience developing cloud AWS Snowflake Work one data rich businesses UK Competitive bonus Discount across brands", "DataStage", "", "", "", "Leverage company data drive business outcomes Play role analytics team developing data driven solutions taking ownership driving client Growth Develop maintain global data models analysing use cases applications across brands Design develop scalable data ingestion frameworks transform variety large data sets Own end end delivery raw trained data sets brand cloud environments Integrate data platform architecture building applications using open source frameworks Apache Spark containerized applications e Kubernetes Apache Airflow Build maintain data integration utilities data scheduling monitoring capabilities source target mappings data lineage trees Implement manage production support processes around data lifecycle data quality coding utilities storage reporting data integration points Maintain system performance identifying resolving production application development problems calculating optimum values parameters evaluating integrating installing new releases performing routine maintenance answering user questions", "", "", "", "", "", "Working cross functional team alongside talented Engineers Data Scientists Building scalable high performant code Mentoring less experienced colleagues within team Implementing ETL process including cohorts building ETL routines customisation Monitoring cluster Spark Hadoop performance Working Agile Environment Refactoring moving current libraries scripts Scala Java Enforcing coding standards best practices Working geographically dispersed team Working environment significant number unknowns technically functionally", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "", "", "", "Engineer", "", "", "Working knowledge data warehousing concepts change data capture ETL data marts etc Experience building data marts BI applications specific business areas multiple data sources like files API messaging streams Kafka Kinesis etc Experience using data analytics technologies services eg Redshift S3 RDS BigQuery etc comparable technologies Programming languages Java Python node js Creating ETL pipelines Data processing workloads change data capture set based data transformations data movement AWS services Implementing optimising administering cloud based parallel processing memory columnar relational databases eg BigQuery Redshift etc Ability setup configure ETL frameworks like Airflow using Java Python Events Stream parallel data processing solutions like Spark AWS similar technology Implemented data processing using Hadoop Hbase Hive Implemented AWS Security rules policies communication AWS Services securing PII data stored transferred processed Working NoSQL databases eg Cassandra DynamoDB", "Develops maintains scalable data pipelines builds new API integrations support continuing increases data volume complexity Collaborates analytics business teams improve data models feed business intelligence tools increasing data accessibility fostering data driven decision making across organization Implements processes systems monitor data quality ensuring production data always accurate available key stakeholders business processes depend Writes unit integration tests contributes engineering wiki documents work Performs data analysis required troubleshoot data related issues assist resolution data issues Works closely team frontend backend engineers product managers analysts Defines company data assets data models spark sparkSQL hiveSQL jobs populate data models Designs data integrations data quality framework Designs evaluates open source vendor tools data lineage Works closely business units engineering teams develop strategy long term data platform architecture", "", "Assist creation data schemas stored procedures data pipelines views Help build maintain technical solutions required optimal ingestion transformation loading data wide variety data sources large complex data sets Collaborate across roles embrace best practices reporting analysis including data integrity test design validation documentation Ability collect organize analyze disseminate significant amounts information attention detail accuracy Build automate actionable reports Collaborate data analysts data scientists stakeholders design discussions uncover detailed business requirements related data engineering Develop strong hypotheses independently solve problems share actionable insights engineering", "", "Find better job faster Hand screened leads No ads scams junk Great job search support career categories", "", "", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support career categories", "", "", "", "", "", "", "", "Use combination financial market data social media data AuCoDe controversy score engine customer supplied data infer insights stock price movements", "Maintaining improving developing databases Developing data pipelines sensor input shared output Supporting development dashboards automated reports Supporting deployment development sensors Master degree computer sciences information technology similar Master degree geosciences strong programming skills At least years relevant work experience data engineering Experienced user databases dashboard software Hands understanding machine learning applications algorithms Excellent proficiency open source programming language e g Python Bonuspoints experience Azure PostGIS Grafana DataIKU Proficiency geographic information systems GIS Experience remote sensing telemetry plus Structured worker curious critical mindset Comfortable high level autonomy Enjoys structuring work others Excited working back end systems hero behind scenes Enjoys sharing knowledge team members building capacity team partners Able deal uncertainty changes scope ambiguity Enjoys learning developing new skills tools", "Partner data teams implement pipeline designs support R D strategy conceptual data flows Partner metadata leads translate conceptual data models physical database tables optimized data analytics RDIP using established environments tools Assist design build test maintenance data acquisition processing pipelines including limited creation maintenance appropriate artifacts Ensure preservation data integrity source target state including limited acquisition appropriate metadata incorporation appropriate QC checks pipelines Support use growth Data Engineering DataOps environment influence strategy roadmap curation toolset work R D Tech prioritize enhancements Provide Tier support production pipelines Support DCS broader R D self service exploratory efforts Influence vendor roadmaps work R D Tech prioritize DataOps enhancements onboard tools enhancements Ensure quality consistency availability guidance documentation end users tools support high quality outputs Extend current pipelines support clinical biomarkers Assess GxP readiness related upstream data pipelines develop plan addressing gaps", "", "", "Engage super interesting high profile projects Join fun team full flair personality contribute recognized value add Zero Bureaucracy environment embracing creative talent Passionate working complex Data problems You ideally solid demonstrated Big Data experience Java Data Big Data Development", "", "Provide analytical statistical expertise company Become familiar range data sets looking raw customer data financial data marketing data Experiment algorithms machine learning data exploration order build solutions tools data driven purposes Perform clustering analysis customer data along segmentation clustering Present tell story data clients Support creation real time reporting using Kibana Own recommendation engines machine learning systems Own reporting data flows include ETL processes Perform updates production databases", "", "", "Provide Big Data Engineering support Media Analytics User Experience team Media Network Engineering data customers Lead Media Analytics program work integrate video streaming data development apps websites particularly sports entertainment Drive tangible business values actionable insights lead measurable improvements received customer experience customers engagements Setting benchmarks targets continual improvements customers video streaming experience Engaging influencing Senior Managers Product managers Technology managers Operations managers Program managers promote Media capabilities aligning corporate strategic programs Be key influence development production automated reporting recommendations data sources data centric platforms tools contribute implementation data centric applications Recommending supporting best bread Media data source identification documentation Data validation data access integration aggregation support work analysts data scientist deployment data centric applications solutions Having ability query databases make data available experimental production environments assess data suitability performing statistical analysis Experience working established diverse digital ecosystems variety systems tools limitations stakeholders Understanding technologies delivery online streaming video live demand Technologies including video compression eg H HEVC etc adaptive bit rate streaming eg Microsoft Smooth streaming MSS Apple HTTP Live Streaming HLS Adobe HTTP Dynamic Streaming HDS Widevine MPEG DASH etc Content Delivery Network CDN Experience development OTT Video streaming services applications Understanding metrics video quality measurements received customer experience media services Experience dashboard analytical tools supporting data A working knowledge protocols related Internet TCP IP Knowledge Cloud NFV SDN Knowledge video streaming software analytics tools e g Conviva highly desirable Practical experience SQL non SQL storage methods APIs Experience Unix Linux administration scripting Mandatory Object orientated procedural programming languages R SAS Python", "", "", "", "", "", "Propose conceptualize design implement solutions difficult complex applications independently including infrastructure user facing components Adapt adopt technologies methods open source projects participate efforts code process community contributions especially Cultural Heritage Linked Data environment Oversee testing debugging change control documentation major projects comprising new development refactoring existing mission critical products Engage strategic technical architectural planning variety stakeholders Define promote complex application development administration programming standards Oversee support maintenance operation upgrades applications Lead projects necessary special systems application development areas complex problems Appropriately discuss troubleshoot resolve complex technical problems team environment Work technical professionals within outside Stanford develop standards implement best practices Promote adoption Linked Data technologies within institution broadly within community Previous work related libraries digital preservation digital repositories Participation large long running successful open source projects Standards architecture engineering linked data based systems within outside library domain Expertise designing developing testing deploying applications based Linked Data Proficiency application design data modeling cultural heritage domains common library based XML based schema relational database graph based models Ability define solve logical problems highly technical applications Strong communication skills technical non technical partners Ability lead activities structured team development projects Ability select adapt effectively use variety programming methods Java Ruby Ruby Rails Python X query XSLT Knowledge library linked data domains demonstrated success engineering linked data systems scale demonstrated success engineering performance linked data successful leadership open source collaborative projects library linked data domain Ability quickly learn adapt new technologies programming tools Demonstrated experience designing developing testing deploying applications based Linked Data Strong understanding data design architecture graph based XML relational data structures data modeling Ability define solve logical problems highly technical applications Thorough understanding aspects software development life cycle quality control practices automated testing test driven development practices Demonstrated experience leading technical activities structured team development projects Ability recognize recommend needed changes user operations procesures Strong understanding Ruby Ruby Rails Demonstrated experience Linked Data patterns data modeling architecture Successful participation leadership open source software development Constantly perform desk based computer tasks Frequently sit grasp lightly fine manipulation Occasionally stand walk writing hand Rarely use telephone lift carry push pull objects weigh pounds May work extended hours evening weekends Interpersonal Skills Demonstrates ability work well Stanford colleagues external partner organizations Promote Culture Safety Demonstrates commitment personal responsibility value safety communicates safety concerns uses promotes safe behaviors based training lessons learned", "", "", "", "", "Principals Recruiters please contact job poster", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "", "", "", "Responsible maintenance improvement cleaning manipulation data business operational analytics databases Identification discovery complex data sets align defined use cases Contribute development data aggregation metrics calculations Selecting integrating Big Data tools frameworks Implementing ETL process Monitoring performance advising necessary infrastructure changes Defining data retention policies Define builds data pipelines enable faster better data informed decision making within business At least years working experience database engineering support personnel database engineering administrator within fast paced complex business setting You also experience defining building data roadmaps Spark NoSQL databases Big Data ML toolkits good knowledge Big Data querying tools Demonstrate experience working large complex data sets well experience analyzing volumes data Experience creation debugging databases critical business mission You strong working conceptual knowledge building maintaining physical logical data models experience Tableau Qlik toolsets Preferable experience monitoring disaster recovery backup automated testing automated schema migration continuous deployment High level business commercial acumen demonstrated ability interpret business requirements deliver outputs align business improvement objectives High quality written oral communication skills including presentation skills Telecommunications Industry experience", "", "design develop interface new existing data feeds order manipulate transform data desired target architectures applications manage aspects day day delivery Agile DevOps environment You work clients identify data transformation roadmap Internally help identify improved ways working mentor junior share knowledge Strong software engineering development background Java C COTS Products Data manipulation XML processing customised ETL solutions data merging experience Data Transformation using ETL COTS tooling Experience engineering solutions cope Big volume Fast real near real time requirements Experience developing interfacing designing data centric applications Experience middleware integration applications either commercial e Mulesoft Open Source Experience DevOps type operations use source control GitHub Stash Build tools Puppet Jenkins etc continuous integration test driven development e Cucumber Lettuce etc Experience Cloud Infrastructure usage set Hadoop administration set security tuning Data ingestion Kafka Flume Spark SQL Hadoop tools Hawq Greenplum ETL tools like Pentaho Informatica BDE Talend Elastic Search SOLR", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "Analyze resolve complex challenges around data tools Optimize analytical workflows identifying opportunities automating Implement solutions bring together application data generated distributed systems third party data real time user data needed make key business decisions Work within Data Science team serve machine learning solutions scale Work projects growing responsibility individually part team build experience skills pace matched shown ability", "", "", "", "", "Scala OR Python Spark Cassandra Kafka AWS Data Engineering development petabyte scale Big Data platform data pipelines ETL primarily using Spark Cassandra Kafka programming either Scala Python Back end Software Development building back end Data Science product Excellent programming skills either Scala Python functional programming big plus associated libraries e g Cats Shapeless Pandas Numpy A background developing large scale production level Big Data platforms including e g Hadoop Spark Cassandra Knowledge build highly scalable highly tolerable software An awesome opportunity join one best teams industry build highly complex software unrivalled competitor The chance innovate gain exposure massive scale Big Data platform production level Machine Learning", "Analyze resolve complex challenges around data tools Optimize analytical workflows identifying opportunities automating Implement solutions bring together application data generated distributed systems third party data real time user data needed make key business decisions Work within Data Science team serve machine learning solutions scale Work projects growing responsibility individually part team build experience skills pace matched shown ability", "", "", "", "", "", "", "", "", "", "", "", "Conduct advanced statistical analysis provide actionable insights identify trends measure performance Use data mining techniques collect compile data wide variety data repositories Build learning systems monitor data flows react changes customer preferences business objectives Build high performance predictive prescriptive algorithms using cutting edge statistical techniques e g neural networks Research new machine learning solutions complex business problems Collaborate engineers implement deploy scalable solutions Provide thought leadership researching best practices conducting experiments collaborating industry leader Use data visualization tools develop visuals used effectively communicate technical findings non technical audience Teach data science concepts junior members team PhD years experience Masters years experience Data Science Computer Science Engineering Statistics related field Strong background machine learning statistics Deep Learning Bayesian statistics plus Prototyping Expertise quick build proofs concept involving data munging scripting analysis Solid foundation data structures algorithms Hands experience building models deep learning frameworks like MXNet Tensorflow Caffe Torch Theano similar Experience processing massive amounts structured unstructured data using Spark SQL Hive Impala HBase Proficiency Python numerical statistical programming including Numpy Pandas Scikit learn Experience natural language processing Experience using Cloud computing AWS Azure GCP Experience Java Scala Python etc Experience working large data sets distributed computing tools plus Map Reduce Hadoop Hive Spark", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "", "Drive technical design development around real time data analytics Lead data analytics initiative multi functional matrixed environment Design cloud data analytics platform enable self service tools democratize data Audience focused C team Engineers Leaders etc communications influence clarity", "", "", "Lead technical direction architectural evolution data pipelines related digital farming platform capabilities Partner architects technical leads collaborate design validate appropriate solutions institute appropriate data governance mechanisms Partner Digital Farming Platform leads establish steward multi year Data Pipelines technical capabilities roadmap Develop cloud based pipeline architectures enable massively scalable data processing maintaining less linear operating cost profiles Continuously assess next generation geospatial data imagery processing techniques support use cases unique agriculture industry Work key leaders subject matter experts Data Science Product organizations integrate enable latest agronomic data innovations data quality algorithms", "", "", "", "", "", "", "", "", "", "", "", "Provide analytical statistical expertise company Become familiar range data sets looking raw customer data financial data marketing data Experiment algorithms machine learning data exploration order build solutions tools data driven purposes Perform clustering analysis customer data along segmentation clustering Present tell story data clients Support creation real time reporting using Kibana Own recommendation engines machine learning systems Own reporting data flows include ETL processes Perform updates production databases", "", "", "", "", "", "", "", "Apache Spark Databricks Data Spark", "", "Develop test maintain data architecture Design implement secure data pipelines prepare process ingest organize data data data lake data warehouse disparate premise cloud data sources QA troubleshoot performance data pipelines queries accessing data warehouse Clean transform model data power analytics user facing products Ensure proper data governance privacy practices Partner Analytics team buildout advanced data products Capable coder Python Scala R Familiar modern cloud native scalable ETL solutions tools ex Informatica Stitch Talend Mulesoft Salesforce etc Experience workflow orchestration principles platforms DAGs Airflow DBT Luigi Dagster Prefect etc Prefer experience Google Cloud Platform Bigquery Dataproc Dataflow Pub Sub etc Experience AWS DynamoDb Kinesis Stream etc plus Building analytic engine segmentation grouping data Experience writing scripts automate provisioning maintenance systems distributed virtualized infrastructure Familiarity managed cloud based options building machine learning models RDBMS database development using SQL queries stored procedures", "", "Experience working cloud based platforms AWS Expertise Python Have worked streaming technologies Spark Kafka Exposure liaising senior stakeholders", "", "Design construct install test maintain highly scalable data management systems Ensure systems meet business requirements industry best practices Create maintain data management documentation Design build scalable data repositories enabling high performance consumer member transaction volume within ICE products Build APIs data consumption integration external datasets necessary Build high performance algorithms prototypes predictive models proof concepts Problem solve issues around data integration unusable data elements unstructured data sets data management incidents Research opportunities data acquisition new uses existing data Develop data set processes data modeling mining production Integrate new data management technologies software engineering tools existing structures Create custom software components e g specialized UDFs analytics applications Employ variety languages tools e g scripting languages marry systems together Install implement update disaster recovery procedures Recommend ways ensure continuous improvement around data reliability efficiency quality Collaborate data architects modelers IT team members project requirements goals Collaborate QA Engineers Automation build test plans scripts automation verify validate data management systems production implementation Able identify implement data solution strategy Skilled working directly senior business management determine requirements present options Statistical analysis modeling Database architectures SQL based NoSQL based technologies Data modeling tools e g UML ERWin Enterprise Architect Visio Data warehousing solutions Data mining Demonstrates initiative without direct authority Able influence advocate approaches problem solving Demonstrates intellectual curiosity exploring new territories finding creative ways solve data management problems Supports maintains positive attitude vision peers associates management Demonstrates effective teamwork collaboration Ability prioritize competing conflicting requests execute tasks high pressure environment Utilizes good judgment Must demonstrate ability handle diversity amongst people environments Must detail oriented able follow follow project actions tasks Ability maintain confidentiality sensitive information", "Find better job faster Hand screened leads No ads scams junk Great job search support", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "", "", "Build multi PB scale data platform Design code develop new features fix bugs enhancements systems data pipelines ETLs adhering SLA Follow engineering best methodologies towards ensuring performance reliability scalability measurability Collaborate Software Engineers ML Engineers Data Scientists stakeholders taking learning leadership opportunities arise every single day Mentor junior engineers team level Raise bar sustainable engineering improving best practices producing best class code documentation testing monitoring Bachelor degree Computer Science related technical discipline equivalent years strong data engineering design development experience building massively large scale distributed data platforms products Advanced coding expertise SQL Python JVM based language Expert heterogeneous data storage systems relational NoSQL memory etc Deep knowledge data modeling lineage access governance Excellent skills AWS services like Redshift Kinesis Lambda EMR EKS ECS etc Wide exposure open source software frameworks broader cutting edge technologies Airflow Spark Druid etc Familiar infrastructure provisioning tools e g Terraform Chef Consistent proven ability deliver work time attention quality Excellent written spoken communication skills ability work effectively others team environment Work studio complete P L ownership games Competitive salary discretionary annual bonus scheme Zynga RSUs Full medical accident well life insurance benefits Catered breakfast lunch evening snacks Child care facilities women employees discounted facilities male employees Well stocked pantry Generous Paid Maternity Paternity leave Employee Assistance Programs Active Employee Resource Groups Women Zynga Frequent employee events Additional leave options employees Flexible working hours many teams Casual dress every single day", "", "", "", "", "", "", "", "Craft optimal data processing architecture systems new data ETL pipelines Build canonical datasets well scalable fault tolerant pipelines Build data anomaly detection data quality checks optimize pipelines ideal compute storage Define data engineering roadmap Ecosystems Collaborate Software Engineers Data Scientists design technical specification logging add logging production code generate metrics online well offline Work different cross functional partners Data Scientists Infra Engineering Logging Framework Infra Teams Product Managers Build visualizations provide insights data metrics generated Work data infrastructure teams suggest improvements influence roadmap Immerse aspects product understand problems tie back data engineering solutions Recommend improvements modifications existing data ETL pipelines Communicate influence strategies processes around data modeling architecture multi functional groups leadership Drive internal process improvements automating manual processes data quality SLA management", "", "", "Support technical data foundation Henkel digital transformation Support Implement Henkel wide data integration strategy central data foundation covering internal external structured unstructured data requirements Support Implement Henkel BI data model focus integrating data operational systems data sources reporting analytical applications region Support Implement enterprise wide valid business logic data quality concept Work global regional multinational BI analytics teams dynamic challenging environment Four year degree Computer Sciences Information Systems related At least years experience area Data Warehousing Business Intelligence Knowledge modern data warehousing concepts data modelling data management Experience relational data bases MS SQL Teradata Oracle DB2 good knowledge SQL Experience Azure Cloud technology ETL solution Knowledge BI tools e g Analysis Server Cognos Oracle BI SAP BW beneficial Agile DevOps Experience plus", "", "", "Shape portfolio business problems solve building detailed knowledge data sources internal external Model data landscape obtain data extracts define secure data exchange approaches Acquire ingest process data multiple sources systems Cloud Data Lake Operate fast paced iterative environment remaining compliant Information Sec policies standards Collaborate data scientists map data fields hypotheses curate wrangle prepare data use advanced analytical models Help architect strategic advanced analytics technology landscape Become expert claims data sources Framework set across company define best practice data engineering space Robust data sources data lake increasing proportion data held lake No unexpected issues arise Meaningful experience years least two following technologies Python Scala SQL Java Experience interest Cloud platforms Azure AWS Databricks The ability work across structured semi structured unstructured data extracting information identifying linkages across disparate data sets Understanding Information Security principles ensure compliant handling management data", "", "", "Translate business requirements technical requirements develop data pipelines many industry proprietary systems unified data warehouse Collaborate business operations software engineers data analysts data scientists Build scalable maintainable data pipelines extract transform load ETL DMS Stitch high performance data warehouses Redshift Snowflake", "Excellent understanding Data Architecture including premise via AWS Cloud Strong experience Data Modelling Data Analysis including ability query prepare analyse Data sets Proven commercial experience SQL NoSQL databases Skilled software engineering experience necessarily following programming languages Python R Java Scala Familiarity Linux UNIX tools including Git SVN Good understanding Data Science models Data Modelling general e g Star Schema Kimball Mobile Office Line Email LinkedIn Liam Haghighat SQL Data Science NoSQL Architect Data Modelling", "", "Work clients model data landscape obtain data extracts define secure data exchange approaches Plan execute secure good practice data integration strategies approaches Acquire ingest process data multiple sources systems Big Data platforms Create manage data environments Cloud Collaborate data scientists map data fields hypotheses curate wrangle prepare data use advanced analytical models Have strong understanding Information Security principles ensure compliant handling management client data This fantastic opportunity involved end end data management cutting edge Advanced Analytics Data Science Use new innovative techniques deliver impact clients well internal R D projects A proven ability lead project engineering perspective owning workflow setting expectations Mentoring junior engineers allowing develop challenged", "Data querying processing SQL Data processing task management Python Communication skills ability translate domains business problems technical implementations Refactor operating model code scalable transparent processes leveraging Airflow DBT core frameworks Expand capabilities Inspire core data platform support incremental product lines product features Partner Analytics systematize scale high integrity value oriented analysis Partner Sales Operations business stakeholders design deliver new data driven integrations Cultivated familiarity Inspire frameworks operating model Delivery high quality pull requests DBT Airflow evidencing strong code standards testing practices Comfort self directed project management requires minimal oversight assess problem formulate solution deliver code document changes Technical competency comfort command line good grasp fundamentals programming general understanding Git source control willingness read docs search stack overflow test works Problem Solving Mentality gets excited digging complexity wants ask questions learn put problems never explicitly told solve Especially troubleshooting ability break chain steps narrow locate problem Number Sense Strong background mathematics physics comfort quantitative measurement estimation Ability work establishing boundaries orders magnitude make informed judgements without fussing exactitude years data analytics engineering science role Strong SQL experience working large datasets ideally cloud based data warehouses Software development Python3 Experience automating data processing cleaning preparation Experience key frameworks Snowflake Apache Airflow dbt AWS services Docker Kubernetes Experience similar scale data processing Multi TB billions rows Work real time event stream data Contextual work energy industry Data consultancy experience plus Proven ability break chain steps narrow locate problem", "", "", "", "", "Several years experience working SAP Data Services Sybase IQ least two BI projects Experience designing ETL solutions The ability liaise business gather requirements create technical documentation Experience data warehouse design build Data IQ SAP SQL", "", "Create promote technical design architectural vision security operations data systems tooling Work engineers team manage infrastructure moving processing large scale data Improve log flows efficiency data processing reduce cost analysis storage Convert log flows Logstash Elastic Common Schema Work Analysts determine implement best practices data retention storage Design promote standards security operations data telemetry processing Help engineers across team select data technologies development needs", "Collaborate Software Engineers centralized standardized data aggregations pipeline metrics UI consumption Defining canonical data schemas tables ads sales revenue provide complete view customer Work data infrastructure teams suggest improvements influence roadmap Collaborate Software Engineers design technical specification logging add logging production code generate metrics Recommend improvements modifications existing data ETL pipelines Communicate influence strategies processes around data modeling architecture multi functional groups leadership Evangelize high quality software engineering practices developing data infrastructure pipelines scale Transform data aggregations daily batch processing incremental near real time Drive internal process improvements automating manual processes data quality SLA management Create solutions tooling decrease data fragmentation promote reusability logical data modeling rich representations e g structs common data objects Data consistency accuracy measurement alerting Working better metadata management connecting write side aggregations query side metadata Build data anomaly detection data quality checks optimize pipelines ideal compute storage", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "", "", "Develops large scale data structures pipelines organize collect standardize data helps generate insights addresses reporting needs Writes ETL Extract Transform Load processes designs database systems develops tools real time offline analytic processing Collaborates data science team transform data integrate algorithms models automated processes Uses knowledge Hadoop architecture HDFS commands experience designing optimizing queries build data pipelines Uses strong programming skills Python Java major languages build robust data pipelines dynamic systems Builds data marts data models support Data Science internal customers Integrates data variety sources assuring adhere data quality accessibility standards Strong problem solving skills critical thinking ability Strong collaboration communication skills within across teams years progressively complex related experience Ability leverage multiple tools programming languages analyze manipulate data sets disparate data sources Ability understand complex systems solve challenging analytical problems Experience bash shell scripts UNIX utilities UNIX Commands Knowledge Java Python Hive Cassandra Pig MySQL NoSQL similar Knowledge Hadoop architecture HDFS commands experience designing optimizing queries data HDFS environment Experience building data transformation processing solutions Has strong knowledge large scale search applications building high volume data pipelines Master degree preferred", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "Hand screened leads No ads scams junk Great job search support", "", "Proven experience expertise building ETL solutions data warehouses preferably customer insight e commerce projects Experience low latency technologies e g Spark Storm similar AWS managed services Solid experience one enterprise data tools Preferably Talend SAP Data Services optional Informatica DataStage etc Solid experience developing SQL Server similar excellent appreciation data warehouse methodologies Kimball design tuning techniques Programming experience Python Scala Java AWS experience preferred though essential training provided particular reference Redshift Kinesis EMR Hadoop Familiarity web FTP API technologies Documentation skills SDLC e g source target mappings data flow diagrams", "Collect internal data different departments eventually enrich data external sources Consolidate various data sources transform clean data build customer level data hub Create new variables features enrich customer level data Industrialize maintain centralized customer hub Be able quickly extract prepare provide data according business requirement eventually automate data providing process script batch needed Settle data providing procedure business follow recommend ongoing improvement Work closely Data Scientist Analyst retrieve transform high dimensional data proper shape modeling Process clean control data used innovation projects business targeting purpose Min S1 background Mathematics Statistics Computer Science Minimum years experience MIS Data Engineering Advanced Analytics Excellent knowledge Python SQL Good understanding relational non relational database experience MongoDB Elastic Search plus Experience common data science toolkits Jupyter notebook Scikit learn Pandas Numpy linux system big plus Good understanding big data infrastructure Hadoop Spark HDFS Hive Dynamic working environment Learning development Opportunity international working assignment Competitive remuneration package", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "Use technology Spark Kafka SQS build large scale real time batch data pipelines Help implement cloud technologies AWS Azure GCP Program least one following languages Java Scala Python Proven experience data development big data Hadoop cloud data warehouse using SQL SQL based ETL capabilities Have deep understanding processes skills technologies needed deliver complex development solutions business Understand experience working variety delivery methods Agile Waterfall Scrum Experience developing cloud AWS Snowflake Work one data rich businesses UK Competitive bonus Discount across brands", "", "", "", "", "", "DataStage", "Home Find better job faster Hand screened leads No ads scams junk Great job search support", "", "", "", "Responsible developing optimising data optimisation platforms pipelines Writing data pipelines code transforming data using appropriate efficient methods Working closely team data scientists machine learning engineers operations researchers build fully integrated product Data manipulation skills SQL NoSQL Experience AWS ideally AWS Lambda Experience Python", "", "", "", "Create smart solutions around complex data problems Work directly senior engineers come unique solutions import much data possible Own importing processing unstructured web content Parse content using NLP ML import system fly Integrate third party APIs Build intelligent solutions matching place names addresses geo data", "Data Business Analysis SAS", "", "", "", "", "", "", "Find better job faster Hand screened leads No ads scams junk Great job search support", "Engage super interesting high profile projects Join fun team full flair personality contribute recognized value add Zero Bureaucracy environment embracing creative talent Passionate working complex Data problems You ideally solid demonstrated Big Data experience Java Data Big Data Development", "", "", "", "", "", "", "", "", "Want work business voice part big picture Excited making breaking things Love working playing massive amounts data Excited greenfield projects Handle server side development distributed Google applications Develop data loading ETL processes Google Big Data Platform Write clear efficient tested code Develop code part wider team contributing code review solution design Contribute evolution standards design patterns Deploy maintain applications production environment Solve challenging technical problems within distributed environments Work team using Scrum Kanban methodology ensuring team meets backlog commitments", "Defining building delivering data architecture enable vision business Building implementing data framework provides easy access clear scalable databases datawarehouses support data pipelines implemented order power machine learning capability deliver Real Time business intelligence Integrating multiple disparate data sources unlocking data silos business thus mproving automation frequency quality data collection business assets external sources via traditional ETL pipelines APIs Working collaboration Chief Product Office r business stakeholders ensure timely delivery product roadmap creating solutions perform scale Providing information feedback mentorship support technology related decision making required Keeping abreast industry technological developments field ensure architecture remains fit purpose Proven experience delivering data engineering projects working complex unstructured semi structured structured data Any experience spatial data imagery would plus essential A track record building delivering data infrastructure architecture commercial software solutions A good understanding experience scaling eg ability devise execute maintain automated script testing ability build deploy cloud infrastructure devise scalable approach Dev ability annotate document level others work scripts etc Experience working multiple disparate sources data specific experience working Real Time dynamic data likes IoT sensors vision technology satellite data sets would advantage essential The ability implement best class data integration routines including data validation qualitative checks Strong coding skills Python Java Scala C C JavaScript SQL similar Experience building data driven applications BI intuitive consumer facing solutions apps would advantage Experience owning software development life cycle would advantage Competitive salary Equity Share options Healthcare Pension", "", "", "", "", "", "Hand screened leads No ads scams junk Great job search support career categories", "Work integral member Research Development team Collaborate cross functional team define architect implement highly scalable solutions Participate innovative Lean product development process get influence work every week flex new skill sets Heavily influence technical product roadmap implementation direction Build deploy support data intensive applications multi tier high availability distributed cloud computing environment A minimum years relevant experience A proven track record technical team leadership Polyglot attitude flexibility pragmatic approach towards business solutions Solid experience dealing large quantities data across relational databases cloud storage distributed compute systems Existing familiarity interest developing Machine Learning Engineering skills A solid understanding TDD approaches automated testing concepts Experience concurrency RESTful web services Excitement influencing direction product Ability consider big picture important technical skills Data stack Postgresql S3 Parquet Spark Airflow Redshift Looker Spectrum Athena AWS EC2 RDS Lambdas Docker Microservices AWS ECS Fargate EKS Jenkins Programming languages Python Scala Java JavaScript Node Important libraries React PySpark Health dental vision insurance generous employer contribution An innovative flexible paid time policy A generous stock options plan k plan A kitchen stocked breakfast lunch food coffee sodas snacks adult beverages", "Responsibility handling processing integration data ChEMBL database Facilitating deposition datasets directly ChEMBL working closely external collaborators Applying text data mining techniques development effective large scale curation strategies Developing methods application maintenance ontologies ChEMBL Working teams facilitate integration data different EBI resources A BSc equivalent life science subject e g biological biomedical sciences years postgraduate experience scientific data integration database development text data mining demonstrable track record achievement Proficient least one programming scripting language Python knowledge highly desirable Good knowledge relational databases data modelling SQL PL SQL RESTful web services Good understanding range bioinformatics tools resources e g BLAST Pfam PDB UniProt Experience integrating diverse data sets Knowledge good practice software engineering good code documentation Good knowledge UNIX systems Team player ability work part team independently Self motivated driver quality Good communication verbal presentational Higher degree e g MSc PhD equivalent life sciences computer science related discipline Formal training programming data entity relationship modeling text data mining Familiarity Python Java Perl Knowledge drug discovery development Experience working chemogenomic pharmaceutical data Knowledge cheminformatics methods e g chemical structure representations substructure similarity searching Apply", "Analyse complex business rules data maximise simplification repeatability end users Use relevant techniques expertise data modelling database design integrate structured unstructured data internal external data sources maximise usability discoverability self service analytics data science activities Ensure best practise adopted maximise agility responsiveness whilst maintaining appropriate levels governance adherence GDPR Provide guidance consultancy projects use internal data working specialists senior stakeholders required Keenness learn new things apply practical way An enjoyment working independently comfortable asking support appropriate An analytical creative approach problem solving Strong data integration skills focussing business outcomes ideally using Alteryx Logical methodical approach solving problems Experience working complex data stakeholders differing requirements data An understanding GDPR working data Business data analysis Maintenance documentation processes Data integration using Alteryx SQL server Data modelling self service analytics Data governance data quality master data management Excellent analytical skills Excellent attention detail Proven ability interpret translate source data alongside business requirements create data models optimised self service analytics Flexibility proven customer focus Excellent communication skills ability work effectively part team within wider matrix structure Ability manage competing priorities whilst working deadlines", "Lead creation maintenance optimal data pipeline architecture Assemble complex data sets meet functional non functional business requirements Identify design implement internal process improvements automating manual processes optimizing data delivery designing infrastructure greater scalability etc Build infrastructure required optimal extraction transformation loading data wide variety data sources using SQL AWS technologies Work stakeholders including Executive Product Business Intelligence assist data related technical issues support data infrastructure needs", "", "Participate collaborative design development specialized data structures purpose data consumption public facing website Develop data management framework effective scalable reliable Be subject matter expert consumption layer data providing guidance teams best practices capabilities limitations Research prototype new technologies Oversee data transformation normalization cleansing aggregation workflow management business rule application Load process manage incoming data feeds Contribute development architectural roadmaps perform periodic reviews identify improvement opportunities Enable integration deployment tools methods managing reliable development life cycle routines Experience developing deploying data applications using Open Source frameworks like Spark Kafka AWS S3 StreamSets Redis Data modeling corporate level data management experience Familiarity Relational Database Management Systems Fluency several programming languages Python Scala Java SQL Programming ETL data architecture management experience Define manage critical data using Master Data Management solutions Ability perform Unit Tests internal QA checks Good collaboration idea sharing team environment A Bachelor Degree Computer Science related field preferred Previous experience Microsoft SQL Server plus Meaningful Work Connecting Americans healthcare providers Changing Game evolving culture career advancement opportunities Community Builders partnering local charity wellness initiatives", "", "", "Build automate reliable data pipelines using batch streaming technologies Design test ETL systems using latest technologies Collaborate software engineers data scientists develop data driven applications Excellent communication collaboration skills Coding proficiency least one Java Python C Go Scala Strong computer science skills focus algorithms data structures Experience querying data using SQL NoSQL Familiar MapReduce big data concepts Understand trade offs among data formats CSV JSON Avro Parquet Experience stream processing technologies Apache Beam Spark Streaming Flink Kafka Streams ETL experience AWS using EMR Firehose Lambda ETL experience Google Cloud using Dataproc Cloud Functions Dataflow Experience using data warehouse Redshift BigQuery Familiar messaging systems Kinesis Kafka PubSub Competitive base salary plus meaningful equity Comprehensive benefits Medical Dental Vision 401k Daily catered lunches Dog friendly office", "Develop maintain tools bridge disparate data sources across organization serve data models dashboards decision aids business case analysis date accurate information Collaborate data scientists engineering machine learning predictive features proprietary data assets Prototype implement optimize data new pipelines architectures transform data impactful insights Create daily weekly monthly automated processes built proven completed work Create innovative efficient techniques better solve problems working Assess risks proposed analytics solutions based quality existing data sources actively develop QA procedures mitigate risks Tailor syndicated data retail CPG business needs robust repeatable scalable way Facilitate knowledge transfer across business Product Delivery Commercial teams Recommend implement enhancements standardize streamline processes assure data quality reliability reduce processing time meet client expectations Have professional quantitative background education stresses analytical thinking quantitative methods STEM Experience using python data context iterators pandas scikit learn etc Are passionate innovating new ways answer novel problems Are comfortable given self autonomy work independently experiment new ideas new ways design implement data science solutions advance business goals Can maintain sharp focus amidst multiple priorities keen aptitude prioritize manage time projects Have confidence expert SPINS data capabilities best practices based independent synthesis broad range data Have knowledge working cloud computing environment Familiarity Docker similar container framework container orchestration tools Kubernetes plus Experience working retail POS transactional data preferred Vibrant You passionate meaningful impactful work A Disruptor You afraid things differently Connected You work well part team build strong relationships colleagues Be Yourself We open honest take responsibility actions work Health dental vision insurance 401k Traditional Roth plus company match FSA medical dependent care expenses Pre tax commuter benefit Life insurance Short long term disability Paid maternity paternity leave Bike storage Fresh healthy snacks", "", "", "Identify verify score inbound consumers demand little single identifier Link customer data update add missing identifiers enhanced attributes Enable improved digital marketing performance higher match rates complete insights Working data scientist maintain code base analytics Git source control Unit Integration Testing code reviews Create maintain optimal data pipeline architecture Assemble large complex data sets meet functional non functional business requirements Create data tools analytics data scientist team members assist building optimizing product innovative industry leader Identify design implement internal process improvements automating manual processes optimizing data delivery designing infrastructure greater scalability etc Build infrastructure required optimal extraction transformation loading data wide variety data sources using SQL AWS big data technologies Keep data separated secure across multiple data centers AWS regions Work data analytics experts strive greater functionality data systems", "", "", "", "Pair teammates learn real time streaming data pipeline ship bug fix new feature production demoing work end sprint review session Meet cross functional peers devops front end customer success marketing sales teams experiencing hands demos fantastic products The ability work loosely defined requirements exercise analytical skills clarify questions share approach build test elegant solutions weekly sprint release cycles A drive get results let anything get way Proficiency Scala functional programming languages pride producing clean maintainable code A passion developing configuring testing highly scalable applications data pipelines running Storm Spark similar distributed systems Competency writing applications interact Kafka Zookeeper Elasticsearch Redis similar open source technologies Experience working devops teams tools ensure work makes smooth automated repeatable transition Mac staging production environments Competency locating troubleshooting fixing bugs performance issues distributed systems running JVM Knowledge best practice around continuous integration test driven development code review local containerized development testing everything takes ensure high quality code works customers A desire keep abreast latest industry trends technologies commitment continuous learning open mind others matter senior junior Awesome market leading product expanding space Innovative customer centric culture bright passionate teammates Competitive compensation including equity paid healthcare", "Grow existing cloud data infrastructure democratize access data Champion use data analytics company pioneer data driven culture Work tech marketing sales operations finance business functions scope design implement data needs Help identify significant data sources key variables various business functions Lead design development data pipelines productionize ingest meaningful business data unified data model Develop machine learning predictive analysis tools identify new growth opportunities personalize services Bachelor degree Computer Science Computer Engineering relevant industry experience Proficiency multiple programming languages particularly Python R Javascript minimum two years full time professional experience You proficient SQL rarely satisfied current query performance You developed data ingestion data warehouse integration pipelines You comfortable batch stream data processing You know takes build scalable performant data models You understand differences relational columnar databases use You familiar web development You value writing tests people write tests You worked one major public clouds preferably AWS Ideally experience data visualization Traditional Benefits Health Dental Vision Life Short Term Disability LTD paid holidays Generous PTO policy Daily Fridge Credits Monthly cell phone credit Spontaneous company events community service activities", "years experience field Deep experience writing Python Experience working Jupyter Experience SQL Experience data acquisition writing crawlers interfacing various API fetching transforming data etc Experience web services architecture Experience implementing Spark Hadoop Experience distributed computing concurrent data pipelining Experience interest learning presentation softwares D3 Tableau Plotly MS PowerPoint", "", "", "", "Build statistical machine learning algorithms drive business decisions Measure predict KPIs related user acquisition retention Improve upon existing reporting strategies communicate ongoing performance Provide various teams ad hoc analysis aide day day operations Influence product decisions quantifiable goals Lead projects customer segmentation personalization pricing Identify opportunities developing new data sources Fluent English Bachelor degree higher statistics computer science quantitative discipline Understand use Git Advanced SQL skills Experience language suitable data analysis Python Java R Fluency data analysis communication data including time series event data analysis data visualization Knowledge recommender systems Experience customer retention modeling survival analysis A B testing experience Experience Amazon Web Services Google Cloud Platform Experience processing large datasets using platforms Hadoop Spark", "Working academic faculty monitor optimize current forms data collection develop integrate new forms data collection Working academic faculty optimize transformation collected data formats appropriate analysis Working academic faculty IT developers apply state art approaches cloud service containerization techniques scaling remote staging storage computation analysis data sets including scale hundreds thousands students online courses Providing expertise data set hygiene institution facilitating cleanup integration potentially multiple incomplete irregular partial data sources interesting useful data sources analysis Working various offices ensure information used complies regulatory security policies place Teaching Database Systems core course MIDS program possibly programs Helping develop advanced courses Data Engineering Developing running workshops campus community different aspects data management wrangling Advising student capstone projects MIDS programs data engineering needs Helping evaluate capstone projects program faculty", "", "See technology passion something Possess ability create new solutions operate web based platform constantly facing unchartered waters Possess strong fundamentals within coding technologies willingness wear several hats called upon Do wait something break find problem becomes one constantly aiming improve Having willingness vocalize ideas pick get knocked Value passionate technologists go getters people never stop seeking ways improve existing technology Have high focus career development runway get Work hard period Offer competitive compensation benefits 401k challenging projects company wide events coworkers leaders push get better sense community found anywhere else Serve member data team solves complex challenges builds working database solutions using SQL Server T SQL SSIS stored procedures views user defined functions table functions Develop solutions contributing development leveraging Object Oriented programming techniques Net Software Development Lifecycles Unit Test Techniques Debugging Analytical Techniques Collaborate team develop database structures fit overall architecture systems development Code install optimize debug database queries stored procedures using appropriate tools editors Perform code reviews provide feedback timely manner Promote collective code ownership everyone visibility feature codebase Present technical ideas concepts business friendly language Provide recommendations analysis evaluation systems improvements optimization development maintenance efforts including capacity planning Identify correct performance bottlenecks related SQL code Support timely production releases adherence release activities Contribute data retention strategy", "Design build infrastructure host machine learning models microservices using modern conventions coding practices Help design implement data models database layers support machine learning business intelligence activities Use creativity independent thinking solve technical problems Mentor junior team members Communicate clearly effectively technical non technical colleagues data engineering projects Work closely data scientists understand needs processes Work closely whole technology team successfully maintain data platform alongside broader technology stack Implement strong consistent internal API conventions documentation Implement emphasis tests maintainability clean coding practices produce simple solutions reduce technical debt years building maintaining back end services production preferably using container based architectures Experience using AWS tools services Experience relational databases data pipelines Proficient SQL nix CLI tools grep sed awk BASH etc Python Experience deploying maintaining code using git based tools operating continuous deployment integration environment Experience writing thorough tests documentation maintainable code bases Ability develop creative technical solutions given set business requirements strong understanding modern data architectures Ability work productively small teams lead workstreams independently needed Experience mentoring less experienced colleagues Ability communicate technical ideas non technical colleagues Preferred Skills Experience years building maintaining data science pipelines incorporate machine learning models production environment behind API Experience working data scientists production roles Experience container management solutions like Kubernetes Marathon etc Experience storing using large amounts text data text transformations Competitive compensation Medical dental vision mental health insurance premiums fully paid Ascent 401K offered Unlimited PTO Bank Holidays Flexible work schedule As much RAM fit Macbook Pro Professional development stipend Top floor office Prudential Plaza amazing view park lake Pedway access", "", "Designs develops data ingestion frameworks real time processing solutions data processing transformation frameworks Deploys application codes analytical models Provides support deployed data applications analytical models", "Provide design support data warehouse Maintain acquisition data Conduct data migration tasks transform join data Pertform montoring testing ETL jobs Provide documentation designed warehouses", "Strong academic background Excellent experience Big Data Engineering Demonstrable programming experience e g Python Scala MapReduce Java C Knowledge SQL Familiarity Hadoop Spark NoSQL Good understanding visualisation tools Tableau D3 Qlik Interest Machine Learning Excellent understanding manipulation analysis large complex data sets", "", "Data Validation Develop automated procedures ensure data accuracy integrity complex data exports reports Test ETL process business logic drives app reporting features Define develop implement quality assurance practices procedures test plans perform QA assessments data related code changes Develop automated tests using open source tools Configure maintain test automation environments Create scripts test sequences implement manual procedures ensure proper test coverage Work closely QA team members understand validate upstream application changes impacts data exports reports Work closely Development Product team organizations company promote software quality standards Work closely Customer Support replicate customer issues product field use cases Participate test team activities including requirements analysis test planning tracking reporting support test cycles Engage test case execution including defect documentation tracking resolution support fix verification", "Strong academic background Excellent experience Big Data Engineering Demonstrable programming experience e g Python Scala MapReduce Java C Knowledge SQL Familiarity Hadoop Spark NoSQL Good understanding visualisation tools Tableau D3 Qlik Interest Machine Learning", "", "", "", "strong knowledge SQL average knowledge statistical methods predictive analysis experience working Big Data driven projects familiarity data text mining meticulous diligent creative attitude work fluency English must degree field related data science IT mathematics statistics etc high degree individual initiative strong analytical skills understanding complex issues last least ability challenge status quo proactively tackle problem areas translate knowledge topics data extraction transformation data modeling data mining crawling parsing predictive analytics modern customer oriented solutions develop solution portfolio data universe kantwert BusinessGraph currently million relationships million people institutions responsible implement Big Data projects demanding customers various industries within framework agreed quality time budget targets coordination Sales Product IT Data Science play decisive role data acquisition maintenance processes international expansion data pool comfortable employment contract choose type agreement healthy work life balance home office flexible working hours minutes lunch break included hours working day medical care gym card modern office center Poznan integration events fresh fruits cookies breakfast cereals dining discount card time self development German lessons skilled helpful team startup culture atmosphere mature business approach", "", "Design implement complex Big Data Platform based requirements possible technical solutions Willing learn understand business logic use continue optimize improve Big Data Platform better support data processing requirements Discover feasible new technologies lied Big Data ecosystem share team professional perspectives apply production piece piece continue improvement make happen Be comfortable conducting detailed discussions Data Analyst Big Data Engineers regarding specific questions related specific new platform requirement Communicate well clearly teams oversea oral written English A proficient engineer minimal year Linux Big Data operation experience Experience knowledge Cloud AWS Azure Google Cloud Ali Virtualization Containerization Experience knowledge Big Data ecosystem Hadoop Hive Pig Spark Presto Storm Heron Flink Experience knowledge Operation Automation Any Salt Puppet Chef Ansible Experience knowledge Docker K8S plus Experience knowledge Monitoring Logging system plus Experience knowledge Security plus Experience knowledge Database MySQL PostgreSQL Redis plus Experience knowledge Distributed Storage plus Experience knowledge CI CD plus Good English spoken written skills plus Good ability communication Energy creativity key characteristics describe projects involved You make happen Boom You app fanatic positively curious technology enthusiast Competitive compensation All tech tools need succeed Free breakfast lunch snacks fruit good coffee delicious treats keep well fed days paid leave long promise come back Great social labor insurance packages fit needs ensure happy healthy Commuter benefits make getting work breeze Working little later We cover dinner ride home", "Competitive salary Company equity Health insurance paid individuals families 401K Generous vacation policy plus company holidays Flexible schedules", "\u0412 \u043f\u0435\u0440\u0432\u0443\u044e \u043e\u0447\u0435\u0440\u0435\u0434\u044c Team leader Team leader \u0433\u0440\u0443\u043f\u043f\u044b \u0438\u043d\u0436\u0435\u043d\u0435\u0440\u043e\u0432 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u043e\u0432 big data data scientist\u043e\u0432 \u0441\u043e\u0437\u0434\u0430\u0442\u0435\u043b\u0435\u0439 \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u041f\u0440\u0438 \u044d\u0442\u043e\u043c \u043d\u0430\u043c \u043d\u0443\u0436\u0435\u043d \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440 \u0430\u0434\u043c\u0438\u043d\u0438\u0441\u0442\u0440\u0430\u0442\u043e\u0440 \u0430 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0441\u043c\u043e\u0436\u0435\u0442 \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u0440\u0435\u0448\u0430\u0442\u044c \u0441\u0430\u043c\u044b\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0435 \u0441 big data \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u043e\u0439 data mining\u043e\u043c \u0415\u0432\u0430\u043d\u0433\u0435\u043b\u0438\u0441\u0442\u0430 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0445 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0439 \u043f\u0440\u043e\u0441\u0442\u043e \u0438 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0435\u0433\u043e \u043e \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0432\u0435\u0449\u0430\u0445 Start manager \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u044e\u0449\u0435\u0433\u043e \u043f\u0438\u043b\u043e\u0442\u043d\u044b\u0435 \u043f\u0440\u043e\u0435\u043a\u0442\u044b \u0434\u043b\u044f \u0430\u043f\u0440\u043e\u0431\u0430\u0446\u0438\u0438 \u0440\u0435\u0432\u043e\u043b\u044e\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u0438\u0434\u0435\u0439 proof concept \u041a\u043b\u044e\u0447\u0435\u0432\u043e\u0433\u043e \u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0430 \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0432 \u043c\u0438\u0440\u0435 \u043e\u0431\u043b\u0430\u043a\u0430 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439 AppStore \u0434\u043b\u044f \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043d\u0435\u0444\u0442\u0435\u043f\u0435\u0440\u0435\u0440\u0430\u0431\u043e\u0442\u043a\u043e\u0439 \u043f\u043e\u0434 \u043a\u043b\u044e\u0447 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u0432 \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0430 AI \u0438 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f ML", "Read Codes Clinical Terms Version SNOMED CT ICD10 OPCS4 MedDRA LOINC Information Resources performing upgrades code editions cross maps Communications Leadership ability provide receive complex technical information instruction advice relevant role courteous provide effective service support NWEH clinical trial sponsors stakeholders various projects Work Technical Architect define scope content data coding services work Database Administrators produce automated ETLs coding improvements business intelligence reports deliver required outputs Assist IT support staff troubleshooting data coding issues NWEH applications Partnership working liaise members NWEH technical clinical operational teams necessary ensure data coding services fit purpose available required Analysis data management ensure data coding updates ETL processes properly documented understood systems designed place monitor data quality categorisation agreed targets Data Dictionary Mapper properly documented enable problems quickly identified corrected Research Development Audits participate audits performance quality regulatory operational effectiveness Degree IT related field equivalent experience Experience mapping different clinical coding systems Experience working NHS medical data Experience relational database system Basic knowledge DML SQL programming Basic knowledge database modelling including referential integrity DDL May required handle heavy objects infrequently Required use VDU long periods working day Required undertake prolonged concentration Experience creating ETL Extract Transform Load processes Experience data presentation models used clinical trials e g CDISC SDTM Experience data standardisation models used clinical data analytics e g OMOP Common Data Model Experience using R Understanding clinical governance data handling safety monitoring reporting Experience working Computer System Validation Modern offices Central Manchester Competitive Salary Flexible working hours The ability work home Child care vouchers Excellent pension benefits Access training resources day annual leave", "Previous experience Data Engineer working fast paced agile development environment ideally within digital organisation Be expert deploying appropriate data design techniques solve wide range complicated problems understand industry best practices around web traffic data Significant database design data modelling experience multiple platforms Oracle MySQL Teradata Vertica Redshift possess ability work well technical partners business end users Strong interpersonal leadership customer service skills proven team player enthusiasm new technologies keeping date within field", "", "Advises management customers scalable enterprise analytics solutions provide business competitive advantage Works unusually complex technical problems provides solutions highly innovative ingenious Design build modern data warehouse cloud Design build customer Implement flexible audible data pipeline Enhance data collection procedures build analytic systems Process cleanse verify integrity data used analysis Perform ad hoc analysis present results clear user friendly manner Perform testing resolve issues automate unit tests Develop proof concept POC solutions help business units better visualize business needs clarify requirements development", "Assembling large complex data sets meet business requirements Identifying designing implementing internal process improvements including process automation optimizing data delivery etc Designing optimal ETL infrastructures variety data sources Building analytics tools utilize data pipeline provide actionable insights customer acquisition operational efficiency key business performance metrics Big Data including Hadoop Ecosystem NoSQL approaches Cloud Data Management Data warehousing BI incl SQL Data Modelling Data Integration Data Governance Thorough understanding capabilities commercial Apache Hadoop distributions e g Hortonworks Cloudera MapR Experience estimating planning managing data integration aspects implementation projects At least years overall relevant IT experience At least years experience Big Data projects using Hadoop SQL ETL similar technologies At least year experience data engineering data integration Big Data Analytics Business Intelligence Data Warehousing domain Experience custom application development enhancements using relevant technologies e Python JSON SQL NoSQL Unix Linux scripting Hadoop HortonWorks Cloudera etc Experience data warehouse environment knowledge existing emerging data integration approaches Experience Watson Discovery Conversation Knowledge Studio Experience SSIS Custom application development using technologies Python JSON SQL NoSQL Unix Linux scripting Hadoop HortonWorks Cloudera Hibench etc Data architecture experience designing developing data models Understanding Distributed computing design patterns algorithms data structures security protocols Involved primarily coding debugging unit testing applications API experience multithreading plus May involved conceptual technology phase", "Understand current data sets models help us discovering new ways enrich data Creatively extracting real world behaviour trends location data Monitor build processes cleaning inbound data Dream solution perform R D deploy production within fluid work environment", "Hands leadership influence development things data services Develop modern data architectural approaches business intelligence reporting analytics including machine learning models data science ensuring effectiveness scalability reliability Design develop implement optimize existing ETL processes merge data disparate sources consumption data analysts scientists business owners decisions makers Complete current evaluation new ETL software options propose recommendations implement solution Facilitate data transformation normalization cleansing aggregation workflow management business rule application Detect data quality issues identify root causes implement fixes design data audits capture issues Distill technical requirements product development operational process via continuous collaboration product engineering analytics team members Influence communicate levels stakeholders including analysts developers business users executives Use analytics influence product development surfacing data around product usage customer behavior ETL tool evaluation implementation prepare scaling efficiency years hands experience collection mining reporting analysis large amounts data Experience designing implementing database tables well performance profiling tuning database processes table usage Experience variety data storage platforms MySQL Postgres Oracle Redshift RDS Experience Data Engineer related role Data Warehouse Developer ETL Developer Business Intelligence Analytics Software Engineer track record manipulating processing extracting value datasets Advanced proficiency SQL Experience designing developing ETL processes variety platforms Pentaho Microstrategy Talend CloverETL FiveTran Stitch Experience analytics platforms Google Analytics Mixpanel etc Experience log aggregation extraction Elasticsearch Business Intelligence reporting tools Tableau etc plus Proven track record innovation expertise data engineering Tenure architecting delivering complex projects Deep understanding application modern data processing technology real time low latency data pipeline ETL architectures Strong stakeholder interaction influence experience executive business stakeholder engineering team levels Blinker Inc currently provide Visa sponsorship Competitive pay equity Paid flexible time Comprehensive medical dental vision disability life insurance parental leave benefits 401K Plan Paid covered parking Tabor Center garage RTD transportation Fully stocked kitchen free snacks beverages cold brew coffee tap Regularly recurring company side social activities e g BBQ lunches patio afternoon happy hours", "develop data processing software including designing coding testing work collaboratively cross functional team deliver quality software mentor others sharing technical knowledge providing guidance support communicate effectively managers peer developers testers business analysts product owners scrum masters experience distributed data processing technologies Apache Hadoop Apache Spark key value stores experience open source technologies including JVM languages Python experience cloud architecture good understanding ETL ELT data processing pipelines experience developing multiple large scale data processing solutions good understanding NoSQL data stores experience data modelling RDBMS NoSQL data stores", "", "", "", "", "", "", "Develop deliver data sources infrastructure support needs predictive modeling analytics part cross functional team Connect raw data business meaning understand data generation flows context business impact Communicate anomalies work towards resolution Execute rapid development new data analytic work tracks fast iteration quick sprints Oversee development new concepts proof concept designs moderate complexity Ensure prototypes meet organization standards allow transfer production applicable Solve problems multidisciplinary approach combining technical expertise business knowledge", "Bachelors degree IT similar experience Extensive Data integration data orchestration experience Microsoft Azure In depth experience designing implementing data flows pipelines Deep experience Data Engineer highly desirable Experience working Agile ideally SCRUM team Comfortable hands data data modelling query techniques Background Data management Space Experience FMCG CPG industry A good understanding adherence data security standards", "", "Maintain development infrastructure supporting marketing organization Development maintenance data processing new Marketing data platform Leading contributor team terms working knowledge big data technologies Work technology business community turn business requirements technical solutions Work autonomously specifications produce high quality accurate efficient well documented code Ensure accurate estimating work based detailed requirements Undertake maintenance bug fix development activities existing applications Construct execute unit system testing Undertake peer peer code reviews colleagues development tasks Ensure clear early communication particular ensuring line manager relevant parties kept informed progress issues difficulties Proactively work mitigate risks improve quality efficient manner resolve problems arise Lead adherence best practice solutions across team Ability manage ongoing project work alongside business usual support maintenance Provide estimates duration effort required complete development tasks high level loosely defined requirements Create robust system level design assigned development activities solid grasp business commercial drivers produce designs clearly meet customer user needs feedback positive Undertake thorough impact analysis assigned development activities understanding impact work tasks areas system Share process expertise across team order enhance team effectiveness Contribute marketing systems roadmap bringing technical leadership oversight Solid working knowledge following One Python Java Performance tuning MapReduce queries relational databases Working knowledge following technologies AWS technologies Big data technologies Spark Analytical platforms Databricks Blue Insight Interpersonal Skills Excellent verbal written communication skills Team player able work initiative Customer focused service oriented Confident establishing good working relationships IT teams internal external Knowledge following technologies advantageous Marketing automation tools including campaign management workflow Adobe Campaign preferred examples Eloqua Aprimo Web analytics Adobe Analytics SiteCatalyst Google Analytics Webtrends Experience using recognized development methodology e g Agile Experience understanding role developer full SDLC Proven track record feature level design Knowledge service management ITIL framework Previous experience marketing publishing analytics preferred Able create imaginative solutions move platform forward features maintainability cost effectiveness", "IoT data development management Data Engineering BI Development experience At least years experience modern programming language Python Java C Scala ETL experience Experience NoSQL Big Data technologies Redshift Cassandra MongoDB BigQuery Hadoop similar Experience Cloud platforms AWS GCP Azure Oracle Agile experience nice", "Work Data Engineering Lead key stakeholders design develop Reify next generation Kappa style data architecture functional programming environment using Kafka Kubernetes PostgreSQL potentially additional tooling AWS Confluent Ecosystems e g EKS Athena Confluent Operator etc Take responsibility day day maintenance upgrades orchestration troubleshooting data architecture tooling Use combination Clojure functional programmers welcome Python SQL support analytics work statistical modeling machine learning develop integrate analytics insights data products Become intimately familiar HIPAA GDPR applicable regulatory frameworks influence architecture development decisions Rapidly learn new tools techniques languages scale encounter additional data challenges Frequently communicate results Data Engineering Lead technical non technical stakeholders clear written verbal presentation form Live data philosophy focuses ethical decision making aware biased data assumptions affect results people laser focused business needs Extensive experience DevOps systems engineering orchestration strong ability understand work distributed systems While primarily data engineering role ideal candidate also ability handle analytical challenges well Understanding nuances testing distributed probabilistic systems Experience applied statistics particularly Bayesian flavor supervised unsupervised learning techniques Masters degree greater relevant field Relevant published work academic blog posts open source contributions Previous experience functional programming languages philosophy existing Clojure chops Experience startup environment remote employee like work remotely Competitive Salary Stock Options Compensation varies mid level senior commensurate experience Comprehensive Health Wellness Coverage premium coverage dependents top tier health plan covering states option HSA medical expenses investment vehicle dental vision disability short term long term basic term life insurance entire tenure Reify We enable access doctor phone online via telemedicine coverage Company provided Workstation You option getting brand new Macbook Pro similar desired laptop like use separate computer work", "", "", "Implement ETL jobs various functions Support maintain daily ETL jobs Support development teams optimizing data access Work data science teams deliver metrics consumers Implement ETL jobs various functions Support maintain daily ETL jobs Support development teams optimizing data access", "", "", "Develop enhance automate processes queuing prioritizing data management curation requests Implement quality control QC process evaluate compliance data models taxonomies ensure cohesion curated data across time data models configurations evolve Create rapid means remediate data fail meet curation specifications Empower scientists tools processes data structures needed support project objectives Ensure accurate complete timely collection delivery tracking analytical information translational CRO collaborating laboratories curation ingestion delivery computational scientists Help define deliver implement R ED collaborator partner laboratory analytical data management systems processes procedures Work R ED study teams develop R ED information management plans outline data capture data flow data queries manual checks data listings needed ensure data integrity Participate comprehensive data review activities coordination project study teams Work computational biologists computational scientists biostatisticians study scientists resolve data quality issues Make data including raw interim data available R ED department personnel required Collaborate users enable data access ingestion Acquire user feedback inform business requirements future development", "Partnering cross functional stakeholders identify plan new data related business requirements ensuring details requirements adequately fleshed development begins Accountability consistent effective flow work Data Engineering team Positioning go expert internal stakeholders seen deeply aware able address technical needs Working devops product teams ensure data engineers positioned satisfy sprint commitments Executing technical vision data ensuring data engineering efforts building toward long term success Reviewing pull requests troubleshooting tough performance issues fostering pair programming team wide knowledge sharing efforts Helping develop maintain code standards accountability", "Develop enhance automate processes queuing prioritizing data management curation requests Implement quality control QC process evaluate compliance data models taxonomies ensure cohesion curated data across time data models configurations evolve Create rapid means remediate data fail meet curation specifications Empower scientists tools processes data structures needed support project objectives Ensure accurate complete timely collection delivery tracking analytical information translational CRO collaborating laboratories curation ingestion delivery computational scientists Help define deliver implement R ED collaborator partner laboratory analytical data management systems processes procedures Work R ED study teams develop R ED information management plans outline data capture data flow data queries manual checks data listings needed ensure data integrity Participate comprehensive data review activities coordination project study teams Work computational biologists computational scientists biostatisticians study scientists resolve data quality issues Make data including raw interim data available R ED department personnel required Collaborate users enable data access ingestion Acquire user feedback inform business requirements future development", "Is Service Centric Someone desires make impact outside office We look service minded people support customers community Has Growth Mindset Driven individual learning development We help team dedicated training providing extra educational resources Rocks Impact Thinks two steps ahead ensure work solve problems make difference Will StrataPro Accountable Prepared Positive Core treat one another Participate full life cycle development definition design implementation testing Work Data Science team transitioning proof concept real product Selecting integrating Data tools frameworks required provide requested capabilities Be advocate developing best practices organization bring knowledge new technologies team Monitoring performance advising necessary infrastructure changes Regularly contribute ongoing improvements engineering process product development ecosystem Give technical presentations development product leadership teams Work building proof concept architectures eye towards production Participates architecting building large distributed systems scale well Work closely current engineering team integrate data architectures existing data platform Support business decisions ad hoc analysis needed Expected cross train team members areas technical expertise Develop tools utilities maintain high system availability monitor data quality provide statistics Develop understanding healthcare finance terminology workflows Years experience data software engineer Experience SaaS Cloud based offerings products Worked big data data warehousing technologies Strong understanding ETL processes data flow architectures tools Experience building decoupled infrastructures Proficient understanding distributed computing principles Experience building infrastructures process large amounts structured unstructured data Experience building tools technical teams Advanced knowledge SQL relational databases Intermediate advanced knowledge Unix Linux command line tools scripting Experience programming highly regulated industry healthcare finance preferred", "", "", "", "Read Codes Clinical Terms Version SNOMED CT ICD10 OPCS4 MedDRA LOINC Information Resources performing upgrades code editions cross maps Communications Leadership ability provide receive complex technical information instruction advice relevant role courteous provide effective service support NWEH clinical trial sponsors stakeholders various projects Work Technical Architect define scope content data coding services work Database Administrators produce automated ETLs coding improvements business intelligence reports deliver required outputs Assist IT support staff troubleshooting data coding issues NWEH applications Partnership working liaise members NWEH technical clinical operational teams necessary ensure data coding services fit purpose available required Analysis data management ensure data coding updates ETL processes properly documented understood systems designed place monitor data quality categorisation agreed targets Data Dictionary Mapper properly documented enable problems quickly identified corrected Research Development Audits participate audits performance quality regulatory operational effectiveness Degree IT related field equivalent experience Experience mapping different clinical coding systems Experience working NHS medical data Experience relational database system Basic knowledge database modelling including referential integrity DDL May required handle heavy objects infrequently Required use VDU long periods working day Required undertake prolonged concentration Experience creating ETL Extract Transform Load processes Experience data presentation models used clinical trials e g CDISC SDTM Experience data standardisation models used clinical data analytics e g OMOP Common Data Model Experience using R Understanding clinical governance data handling safety monitoring reporting Experience working Computer System Validation Modern offices Central Manchester Competitive Salary Flexible working hours The ability work home Child care vouchers Excellent pension benefits Access training resources day annual leave", "Entrepreneurial full stack engineer Design model architect data data automation systems Develop refine parsing munging integrations existing future datasets refine clean transform apply core systems channels Fluency bash python scripting languages significant automation experience Strong understanding Database development implementation relational databases Proficiency Salesforce similar structural CRM relational database systems The understanding proficiencies following plus required Define apply best practices building scalable secure systems internationally Self manage team lead able manage partner interactions coordinate delivery multiple ongoing projects Manage internal cloud based systems Performance tune optimize data systems Define specifications functional technical deliverables Personal network highly skilled engineers experience hiring Extensive experience developing instituting team processes including around testing revision control deployment release forecasting Strong familiarity data science data munging Eagerness communicate technical concepts nontechnical team members especially context strategy development prioritization discussions Leading edge awareness new ideas trends Silicon Valley larger technology industry Experience financial services industry Comfortable decisive leadership role", "", "", "Interacts senior level customers consult scalable enterprise analytics solutions provide business competitive advantage Develops technical solutions complex problems require regular use ingenuity creativity Works unusually complex technical problems provides solutions highly innovative ingenious Design build modern data warehouse cloud Design build customer Implement flexible audible data pipeline Enhance data collection procedures build analytic systems Process cleanse verify integrity data used analysis Perform ad hoc analysis present results clear user friendly manner Perform testing resolve issues automate unit tests Develop proof concept POC solutions help business units better visualize business needs clarify requirements development", "", "Develop enhance automate processes queuing prioritizing data management curation requests Implement quality control QC process evaluate compliance data models taxonomies ensure cohesion curated data across time data models configurations evolve Create rapid means remediate data fail meet curation specifications Empower scientists tools processes data structures needed support project objectives Ensure accurate complete timely collection delivery tracking analytical information translational CRO collaborating laboratories curation ingestion delivery computational scientists Help define deliver implement R ED collaborator partner laboratory analytical data management systems processes procedures Work R ED study teams develop R ED information management plans outline data capture data flow data queries manual checks data listings needed ensure data integrity Participate comprehensive data review activities coordination project study teams Work computational biologists computational scientists biostatisticians study scientists resolve data quality issues Make data including raw interim data available R ED department personnel required Collaborate users enable data access ingestion Acquire user feedback inform business requirements future development", "", "", "", "Building data infrastructure back end services support user facing internal use applications Identifying researching analyzing new data sources Developing maintaining REST APIs Amne back end services Managing developing ETL pipelines ensure improve accuracy quality usability data across Amne infrastructure Improving maintaining industry leading home valuation models methodologies using millions real estate transactions complementary data sets Collaborating Software Engineers Product Managers Product Designers Maintaining development best practices including test coverage continuous integration A B testing documentation Strong Python programming skills including experience NumPy SciPy Pandas scikit learn Familiarity common Python web frameworks Flask Django etc Experience AWS ecosystem S3 Elastic Beanstalk EC2 etc application deployment Ability write clean usable production ready code Ability translate complex data oriented challenges solutions business objectives vice versa Experience databases statistics web services algorithms Python Strong communication skills Experience building modular scalable cloud based systems An interest learning new technologies taking lead integration new technologies Amne stack", "", "Build industry leading big data architecture Support real time analytics Solid programming skills Python Scala Java Good experience big data tools e g Spark Kafka Hadoop similar Experience building onto Cloud platforms Experience SQL NoSQL technologies data modelling Exposure DevOps environments", "", "", "", "Built implemented data profiling tool reverse engineer data schemas new data sources facilitating normalization data data model Built logic combine real time messaging batch query processing single accurate source truth source system Analyzed designed best ways expand data model incorporate data mission critical A real passion problem solving learning new technology Vision balance speed maintainability solution design Strong analytical technical skills The ability handle multiple concurrent projects Excellent ability craft implement requirements keep projects track engage partners Challenging status quo improve processes tools Communicate complex technical details meaningful business context A low ego humility ability gain trust say Own ten projects working multi functionally Physician Success Analytics teams design implement best class data processing enabling clean data flow directly data model Work HIE engineering analytics operations design implement integration streamlines transitional care management workflows Design new concept within data model meet new operational analytical need Build app send data anomalies operations years full time experience Experience building information pipelines utilizing Python Java willingness expand knowledge Python required High degree comfort relational data structures required Knowledge willingness learn non relational data structures technologies eg Postgres Redshift Cassandra MongoDB Neo4j S3 etc BS MS computer science math engineering related fields required", "Build industry leading big data architecture Support real time analytics Solid programming skills Python Scala Java Good experience big data tools e g Spark Kafka Hadoop similar Experience building onto Cloud platforms Experience SQL NoSQL technologies data modelling Exposure DevOps environments", "", "Assembling large complex data sets meet business requirements Identifying designing implementing internal process improvements including process automation optimizing data delivery etc Designing optimal ETL infrastructures variety data sources Building analytics tools utilize data pipeline provide actionable insights customer acquisition operational efficiency key business performance metrics Big Data including Hadoop Ecosystem NoSQL approaches Cloud Data Management Data warehousing BI incl SQL Data Modelling Data Integration Data Governance Thorough understanding capabilities commercial Apache Hadoop distributions e g Hortonworks Cloudera MapR Experience estimating planning managing data integration aspects implementation projects At least years overall relevant IT experience At least years experience Big Data projects using Hadoop SQL ETL similar technologies At least year experience data engineering data integration Big Data Analytics Business Intelligence Data Warehousing domain Experience custom application development enhancements using relevant technologies e Python JSON SQL NoSQL Unix Linux scripting Hadoop HortonWorks Cloudera etc Experience data warehouse environment knowledge existing emerging data integration approaches Experience Watson Discovery Conversation Knowledge Studio Experience SSIS Custom application development using technologies Python JSON SQL NoSQL Unix Linux scripting Hadoop HortonWorks Cloudera Hibench etc Data architecture experience designing developing data models Understanding Distributed computing design patterns algorithms data structures security protocols Involved primarily coding debugging unit testing applications API experience multithreading plus May involved conceptual technology phase", "Work innovative scalable data processing technologies Build innovative state art solutions customers Work closely tech partners Google Cloud Platform Tableau Looker Work agile dynamic environment together small team data scientists machine learning experts data analysts data engineers Strong programming architectural experience ideally Python Java SQL years experience building big data solutions Working experience Google Cloud Platform GCP Amazon Web Services AWS Extremely passionate data analytics Experience ETL tools Hadoop based technologies e g Spark data pipelines e g Beam Flink Experience building scalable high performant code Experience producing tested resilient well documented applications The ability take ownership end end finding creative solutions Experience architecting building maintaining troubleshooting cloud infrastructure Excellent interpersonal skills verbal written communication skills team player keen learner loves building great things together BSc MSc degree Computer Science related technical field Love command line optional affinity Linux scripting Experience building scalable REST APIs using Python similar technologies Experience Agile methodologies Scrum Basic knowledge ideally experience data science topics like machine learning data mining statistics visualisation Contributions open source projects days holiday plus bank holidays Pension scheme Situated innovation hub Canary Wharf Laptop choice Monthly social events team offsites Generous desk budget Free fruit cookies tea coffee throughout week Regular networking events mentoring events conferences Exposure experts number industries Freedom explore latest tools technologies", "Build data ingestion pipelines various data sources including Postgres SQLServer REST APIs Participate design architecture planning infrastructure code Develop features support dynamic ETL automated data quality validation data delivery Work Data Management build data pipelines Spark Automate operational data tasks Perform periodic call duties", "Help develop enhance automate processes queuing prioritizing data management curation requests Empower scientists tools processes data structures needed support project objectives including data integration efforts may span multiple studies experiments Ensure accurate complete timely collection delivery tracking analytical information internal contract laboratory providers collaborating laboratories curation ingestion delivery computational translational scientists Help define deliver implement R ED collaborator partner laboratory analytical data management systems processes procedures Work R ED study teams develop R ED information management plans outline data capture data flow data queries manual checks data listings needed ensure data integrity interpretability Participate comprehensive data review activities coordination project study teams Work computational biologists computational scientists biostatisticians study scientists resolve data quality issues Make data including raw interim data available R ED department personnel required Acquire user feedback inform business requirements future data systems development", "Solid Java programming skills one production release belt Solid experience building high performance services backend mid tier systems", "Build data pipelines deploy machine learning algorithms Minimum years experience Data Engineering Modern programming experience Python Java Scala similar Solid ETL experience Design implementation maintenance Cloud ecosystem experience Azure AWS GCP Oracle similar DevOps experience Kubernetes Docker similar desirable Application machine learning methods Desirable Degree Computer Science Physics Mathematics similar Stakeholder engagement project experience", "", "Selecting integrating Big Data tools frameworks required provide requested capabilities Implementing ETL process importing data existing data sources relevant Monitoring performance advising necessary infrastructure changes Defining data retention policies Add responsibility relevant", "Leverage modern analytics stacks cloud platforms design implement maintain scalable infrastructure ingesting processing persisting large volumes batched streaming data Contribute multiple production code bases continuous delivery CI CD environment Write debug maintain constructively review code highly collaborative software engineering team Develop production quality software interacting distributed data pipelines data stores data models Provide thought leadership advocate best practices regarding big data scalable processing storage infrastructure", "", "", "Consult campus researchers faculty staff understand data questions needs Develop prototypes proof concepts possible solutions Create maintain optimal data lake pipeline infrastructure Build infrastructure required optimal extraction transformation loading data variety data sources using SQL NoSQL cloud big data technologies Code test document new modified data systems create robust scalable applications Assemble large complex data sets meet functional non functional business requirements Identify design implement internal process improvements automating manual processes optimizing data delivery designing infrastructure greater scalability Work data analytics experts strive greater functionality data systems Develop refine scale data management analytics procedures systems workflows best practices Contribute efforts creating refining managing enforcing data management policies procedures conventions standards Collaborate members formal informal groups pursuit common missions vision values mutual goals Understand overall processes procedures organization make recommendations continual improvement processes procedures providing management analysis recommendations continuous improvement", "Maintain development infrastructure supporting marketing organization Development maintenance data processing new Marketing data platform Leading contributor team terms working knowledge big data technologies Work technology business community turn business requirements technical solutions Work autonomously specifications produce high quality accurate efficient well documented code Ensure accurate estimating work based detailed requirements Undertake maintenance bug fix development activities existing applications Construct execute unit system testing Undertake peer peer code reviews colleagues development tasks Ensure clear early communication particular ensuring line manager relevant parties kept informed progress issues difficulties Proactively work mitigate risks improve quality efficient manner resolve problems arise Lead adherence best practice solutions across team Ability manage ongoing project work alongside business usual support maintenance Provide estimates duration effort required complete development tasks high level loosely defined requirements Create robust system level design assigned development activities solid grasp business commercial drivers produce designs clearly meet customer user needs feedback positive Undertake thorough impact analysis assigned development activities understanding impact work tasks areas system Share process expertise across team order enhance team effectiveness Contribute marketing systems roadmap bringing technical leadership oversight One Python Java Performance tuning MapReduce queries relational databases AWS technologies Big data technologies Spark Analytical platforms Databricks Blue Insight Excellent verbal written communication skills Team player able work initiative Customer focused service oriented Confident establishing good working relationships IT teams internal external Marketing automation tools including campaign management workflow Adobe Campaign preferred examples Eloqua Aprimo Web analytics Adobe Analytics SiteCatalyst Google Analytics Webtrends Experience using recognized development methodology e g Agile Experience understanding role developer full SDLC Proven track record feature level design Knowledge service management ITIL framework Previous experience marketing publishing analytics preferred Able create imaginative solutions move platform forward features maintainability cost effectiveness", "work architect develop data pipelines storage enable insight Wellcome data work collaboratively cross functional team deliver quality software mentor others sharing technical knowledge providing guidance support communicate effectively managers scientists peer developers testers business analysts product owners scrum master experience distributed data processing technologies Apache Hadoop Apache Spark experience language commonly used data science e g Python JVM languages etc experience Cloud Architecture understanding ETL ELT data processing pipelines experience developing data processing solutions experience data modelling RDBMS NoSQL data stores", "", "Develop construct test maintain architectures databases large scale processing systems Ensure data architecture supports requirements business Discover opportunities data acquisition Develop data set processes data modeling mining production Employ variety languages tools e g scripting languages marry systems together Identity recommend implement ways improve data reliability quality years experience working Data Engineer Experience building Data Architecture Experience PostgreSQL similar SQL like database Redis Hive Sqoop Experience NoSQL databases Cassandra MongoDB Development experience Scala functional object oriented language Python Perl Java etc Have experience building full stack data pipelines data infrastructure Live work San Francisco Bay Area Have passion education Have previous experience growth stage internet software company", "", "Database maintenance Building analyzing dashboards reports Evaluating defining metrics perform exploratory analysis Monitoring key product metrics understanding root causes changes metrics Empower assist operation product teams building key data sets data based recommendations Automating analyses authoring pipelines via SQL python based ETL framework Superb SQL programming skill Understanding ETL tools database architecture Advanced knowledge data warehousing Demonstrable familiarity code programming concepts Experience Python preferred required A product mindset ask address important analytical questions view enhancing product impact Passionate attentive self starters great communicators years experience quantitative analysis experience Bachelor degree Computer Science Statistics Math technical field required Graduate degrees preferred", "Responsibility handling processing integration data ChEMBL database Facilitating deposition datasets directly ChEMBL working closely external collaborators Applying text data mining techniques development effective large scale curation strategies Developing methods application maintenance ontologies ChEMBL Working teams facilitate integration data different EBI resources A BSc equivalent life science subject e g biological biomedical sciences years postgraduate experience scientific data integration database development text data mining demonstrable track record achievement Proficient least one programming scripting language Python knowledge highly desirable Good knowledge relational databases data modelling SQL PL SQL RESTful web services Good understanding range bioinformatics tools resources e g BLAST Pfam PDB UniProt Experience integrating diverse data sets Knowledge good practice software engineering good code documentation Good knowledge UNIX systems Team player ability work part team independently Self motivated driver quality Good communication verbal presentational Higher degree e g MSc PhD equivalent life sciences computer science related discipline Formal training programming data entity relationship modeling text data mining Familiarity Python Java Perl Knowledge drug discovery development Experience working chemogenomic pharmaceutical data Knowledge cheminformatics methods e g chemical structure representations substructure similarity searching Apply", "", "", "An environment matters make right design decisions first time Move fast break things really work type system build We take less technical debt companies At Brex engineers make product decisions input business people instead business product people making decisions input engineers We rather one strong well compensated engineer instead mediocre engineers Our customers fine fewer features ok broken features Small accountable autonomous teams amazing people eager learn teach constantly improve way working Exceptional technical background Strong sense ownership accountability building What build today foundation dozens systems future Frankness discussing technical matters If disagree things done encourage speak You attack idea without attacking person behind", "Responsibility handling processing integration data ChEMBL database Facilitating deposition datasets directly ChEMBL working closely external collaborators Applying text data mining techniques development effective large scale curation strategies Developing methods application maintenance ontologies ChEMBL Working teams facilitate integration data different EBI resources A BSc equivalent life science subject e g biological biomedical sciences years postgraduate experience scientific data integration database development text data mining demonstrable track record achievement Proficient least one programming scripting language Python knowledge highly desirable Good knowledge relational databases data modelling SQL PL SQL RESTful web services Good understanding range bioinformatics tools resources e g BLAST Pfam PDB UniProt Experience integrating diverse data sets Knowledge good practice software engineering good code documentation Good knowledge UNIX systems Team player ability work part team independently Self motivated driver quality Good communication verbal presentational Higher degree e g MSc PhD equivalent life sciences computer science related discipline Formal training programming data entity relationship modeling text data mining Familiarity Python Java Perl Knowledge drug discovery development Experience working chemogenomic pharmaceutical data Knowledge cheminformatics methods e g chemical structure representations substructure similarity searching Apply", "Work data analytics pros product managers strive greater functionality data systems Analyze translate functional specifications change requests technical designs Design develop implement streaming near real time data pipelines feed systems operational backbone business Execute unit tests validating expected results iterating test conditions passed Ensure accuracy integrity data applications analysis coding writing clear documentation problem resolution Identify remediate issues impacting data pipelines Take care tools techniques components used industry research apply knowledge system developed Mentor develop junior engineers Bachelor degree You hands experience full range designing developing testing implementing low latency big data pipelines You years experience using Object Oriented Languages Scala Python Java You years experience working knowledge Lambda Kappa architecture cloud environment AWS Google Azure You deep understanding SDLC Agile methodologies metadata data modeling designing databases Working knowledge Linux Unix Operating systems Strong scripting skills Python huge plus Bash Shell etc You super comfortable Gitflow", "Working academic faculty monitor optimize current forms data collection develop integrate new forms data collection Working academic faculty optimize transformation collected data formats appropriate analysis Working academic faculty IT developers apply state art approaches cloud service containerization techniques scaling remote staging storage computation analysis data sets including scale hundreds thousands students online courses Providing expertise data set hygiene institution facilitating cleanup integration potentially multiple incomplete irregular partial data sources interesting useful data sources analysis Working various offices ensure information used complies regulatory security policies place Teaching Database Systems core course MIDS program possibly programs Helping develop advanced courses Data Engineering Developing running workshops campus community different aspects data management wrangling Advising student capstone projects MIDS programs data engineering needs", "Own entire end end execution requirements gathering key stakeholders building scalable efficient reliable data marts provide business partners clarity complexities platform Create maintainable scalable data processing pipelines PostgresSQL Python data processing language data platform running AWS Define develop operate functional data marts cubes common open source SaaS based data processing management tools like embulk airflow rundeck Spark Informatica etc Partner cross functional engineering peers take ownership entire analytics lifecycle including instrumentation logging data modeling data warehousing data delivery dashboarding Provide guidance development implementation data quality rules validation processes leading initiatives improve data quality issue resolution Partner Sales Enterprise teams defining executing analytics reporting roadmap Function subject area expert Sales Enterprise data domain maintaining business specific metadata data integration contracts 3rd party vendors well internal data owners years combined experience database application development data management governance medium large size companies Experience marketplace businesses especially pertains working large datasets inherent two sided marketplaces A proven record taking large data projects ideation implementation Expert writing advanced SQL performance tuning others SQL Expert designing implementing operating efficient scalable reliable data transformation pipelines Strong analytical skills ability collect organize analyze information Solid experience database modeling architecture design implementation Familiar sales process sales commission attribution methodologies Salesforce com application data architecture Fluent data visualization techniques tools Looker Domo similar Able influence lead communicate effectively across engineering teams business units partners negotiate priority scope design solution Excellent oral written presentation skills including ability work person virtually globally staffed environment Fluent effective working global distributed team", "", "Writing scheduled Spark pipelines perform sophisticated query plans entirety datasets Writing real time pipelines execute complex operations incoming data Synchronizing large amounts data unstructured structured formats various data sources Creating testing alerting data pipelines Building data infrastructure managing dependencies data pipelines", "", "", "Bachelors degree IT similar experience Data integration data orchestration experience Microsoft Azure In depth experience designing implementing data flows pipelines Experience Data Engineer highly desirable Experience working Agile ideally SCRUM team Comfortable hands data data modelling query techniques Background Data management Space Experience FMCG CPG industry A good understanding adherence data security standards", "", "Experience building applications RESTful APIs production systems Java Python ETL Pipeline tooling experience Experience Big Data HPC Concepts Technologies Spark MapReduce HIVE Hadoop Kafka Familiarity BI Tools Looker Spotfire Google Analytics Data Storage experience MySQL Redshift Elasticsearch Predictive Analytics Machine Learning Modeling Data Mining Platform experience using Unix AWS ECS EMR S3 Route53 Containerization Docker Kubernetes Data specific tooling libraries Jupyter Notebook Zeppelin numpy pandas Data oriented languages R Julia CI CD Jenkins CircleCI Travis", "Develop extend house data toolkits based Python Java Consult educate internal users Hadoop technologies assist finding effectively utilizing best solutions problem space Improve performance financial analytics platforms built around Hadoop ecosystem IMC cutting edge financial applications Hadoop processing terabytes data daily mission critical trading systems We operate bleeding edge technology If something new potentially bring advantage adopt incorporate new technology The landscape always changing creating new exciting challenges What focus today different focused two years ago We really believe sharing knowledge technology different offices Much technology stack shared globally offices provide opportunities travel regions personal growth assist biggest impact Working IMC great way gain exposure learn financial markets technology We know experience lot people really enjoy learning field beyond immediate area expertise one things makes job interesting others We employ broad range people varying backgrounds What common superior technical expertise extraordinary smarts collaborative approach years experience working Hadoop YARN cluster management experience preferable year experience Hadoop SQL interfaces including Hive Impala years experience developing solutions using Spark Experience common data science toolkits Python based preferred Strong Java SQL Python development skills Strong statistical analysis skills Strong systems background preferably including Linux administration Unix scripting experience bash tcsh zsh python etc Experience DevOps tools SALT Puppet part CI CD development deployment process Demonstrated ability troubleshoot conduct root cause analysis Developing Apache Kafka Containerization Docker OSS scheduling tools preferably Luigi Developing solutions Machine learning space emphasis Change Anomaly detection", "Gathering documenting examining managing data integration data management requirements Agile Scrum development team Build extensible data acquisition integration solutions using various integration tools Informatica Pentaho Ab Initio IBM DataStage Kafka Flume etc variety data environments Hadoop Oracle Mongo etc Optimize data integration platforms provide optimal performance increasing data volumes complexity Creating test plans scripts data integration data model testing ranging unit integration testing Expertise different patterns technologies around enterprise level data integration data management data warehousing Expertise design development optimisation tuning database technologies Expertise Hadoop related NoSQL technologies Experience Agile principles methodology An applied knowledge several data technologies practices analytical approaches including Data Integration ETL ELT SOA Middleware Enterprise Service Bus Relational Databases OLTP Analytical MPP Appliances Data Preparation Data Warehousing SQL ETL Warehouse architecture OLAP Data Architecture Logical Physical Modelling Policy Rules Management Programming SQL Unix Shell Python A grounding Database Data Warehouse Data Mart design development able demonstrate structured approaches problem solving Agile Scrum development environment Hands experience two following disciplines expert one Data Integration Extract Transform Load Data Management incl Data Quality Management Data Warehousing Data Virtualisation Federation Data Modelling A creative data Technologist passionate data information management architectures Innovative creative articulate able work collaborate broad range Stakeholders multi site multi country global organisation Able demonstrate examples previous successful deliveries track record Agile centric development environment built foundation hands development Enterprise Data Services An exceptional communicator written verbal internal teams outside Consultants Contractors Suppliers Goal oriented structured thinking effective organisational skills Educated Bachelor degree level equivalent experience qualifications A creative data technologist passionate data information management architectures Innovative creative articulate able work collaborate broad range stakeholders multi site multi country global organization", "Assembling large complex data sets meet business requirements Identifying designing implementing internal process improvements including process automation optimizing data delivery etc Designing optimal ETL infrastructures variety data sources Building analytics tools utilize data pipeline provide actionable insights customer acquisition operational efficiency key business performance metrics Big Data including Hadoop Ecosystem NoSQL approaches Cloud Data Management Data warehousing BI incl SQL Data Modelling Data Integration Data Governance Thorough understanding capabilities commercial Apache Hadoop distributions e g Hortonworks Cloudera MapR Experience estimating planning managing data integration aspects implementation projects Experience Watson Discovery Conversation Knowledge Studio SSIS experience Custom application development Data architecture experience designing developing data models API experience multithreading plus May involved conceptual technology phase", "Build data pipelines deploy machine learning algorithms Minimum years experience Data Engineering Modern programming experience Python Java Scala similar Solid ETL experience Design implementation maintenance Cloud ecosystem experience Azure AWS GCP Oracle similar DevOps experience Kubernetes Docker similar desirable Application machine learning methods Desirable Degree Computer Science Physics Mathematics similar Stakeholder engagement project experience", "Apply technologies solve big data problems develop innovative big data solutions Select optimize integrate data science tools frameworks required provide data science solutions science teams Implement complex big data projects focus collecting parsing managing analyzing visualizing large sets data turn information insights using multiple platforms Collaborate effectively technology teams architects solve complex problems spanning respective areas Develop refine scale data management analytics procedures systems workflows best practices Develop analysis techniques unstructured data Develop prototypes proof concepts selected solutions Enable big data batch real time analytical solutions leverage emerging technologies Code test document new modified data systems create robust scalable applications Lead efforts creating refining managing enforcing data management policies procedures conventions standards Diagnose problems using formal problem solving tools techniques multiple angles probe underlying issues generate multiple potential solutions Proactively anticipate prevent problems Collaborate members formal informal groups pursuit common missions vision values mutual goals Provide general depth support guidance Blue Waters science teams multiple areas specialization Engage data science community improve capability performance data science software HPC systems Keep abreast developments high performance computing field writing technical reports conference journal papers appropriate review scientific papers proposals appropriate Participate writing joint proposals Blue Waters staff application teams BA BS degree engineering mathematics science computer science related field Alternative degree fields considered accompanied equivalent experience depending nature depth experience relates current NCSA projects technologies At least year experience working real world data science applications Strong verbal written communication skills Master Ph D engineering mathematics science computer science related field highly preferred Expertise exploring understanding cleaning wrangling big data Strong software engineering skills track record contributing software projects collaborating developers Parallel programming experience high performance computers including development porting evaluating scalability one parallel libraries applications written Fortran C C utilizing communication protocols MPI OpenMP Working knowledge Linux operating system programming data analysis language e g Python R Stata databases e g MySQL Oracle NoSQL Ability work independently team member Ability manage multiple projects competing priorities deadlines eagerness take ownership challenging open ended assignments Effective communicating audiences whose technical backgrounds vary widely Expertise ETL processes visualization tools web programming Experience developing presenting technical training material web based technical documentation", "Hadoop Proficiency designing running troubleshooting Hadoop clusters crucial Strong understanding Linux OS core principles performance tuning crucial Batch streaming job frameworks eg Spark Storm NoSQL databases Hbase Cassandra MongoBD Knowledge Middlewares messaging systems eg Kafka RabbitMQ FTL Automation via use configuration management orchestration tools Data collection Querying eg Flume Sqoop Hive SSL certificates Scalable distributed systems eg Splunk SQL database administration querying experience", "You evaluate benchmark improve scalability robustness performance data platform applications making significant contributions architecture design data processing platform You implement scalable fault tolerant accurate ETLs You gather process raw data scale diverse sources You collaborate product management data scientists analysts engineers technical vision design planning You implement maintain high level data quality monitoring analytics ecosystem You train collaborate teammates effectively data engineering best practices You involved supportive agile sprint model development helping implement practice discipline years experience software engineer years experience data engineer Excellent communication collaboration skills Experience implementing data pipelines improving performance ETL processes SQL queries Enthusiasm working agile development environment Strong database schema design query optimization skills Proficiency relational databases SQL queries PostgreSQL preferred Strong scripting skills Python Ruby Experience data modeling OLTP OLAP applications Understanding basic principles data governance Familiarity workflow management tools Airflow preferred Familiarity cloud based data warehouses Amazon Redshift preferred Shown ability understand automated testing concepts ability consistently apply concepts Experience streaming technologies concepts used data warehouses Experience taking machine learning models development production Experience working visualization tools like Tableau Experience working sensitive data e PHI PII Application development experience Competitive salary Stock options extended post termination option exercise window Omadans us years Flexible vacation Parental leave Health dental vision Healthy snacks meals Wellness events e g running club Community volunteering", "", "", "", "", "", "Identify evangelize programming best practices Data Engineering Modeling teams Work closely Analysis ensure quality availability data Data Warehouse along support Business Intelligence platform Lead participate design architecture reviews well code reviews walkthroughs Design Implement robust end end ETL pipelines Develop solutions mind towards quality scalability high performance large data sets years experience senior developer data engineer role Hands coding experience R Python Scala similar Solid foundation Data Science workflow experience Pandas Scikit Learn preferred Experience distributed processing frameworks like Amazon EMR Spark Experience working high volume heterogeneous data Amazon Web Services e g RedShift EMR VPC RDS S3 Route53 Docker provisioning servers deploying applications services Excellent verbal written communication skills including ability explain technical issues non technical audience B S Computer Science equivalent experience People best part Zest Robust healthcare plans matching 401K unlimited vacation time Dog friendly office lounge areas video games gigantic jigsaw puzzles On site gym yoga salsa employee run fitness classes Generous family leave policy month maternity leave month paternity leave Tuition reimbursement conference allowance Zest talks Complimentary massages manicures pedicures", "Extract centralize collect data different databases Analyze provide answers needed different operational teams Set periodic data flow data pipeline maintenance development BI platform contribution deployment machine learning models POC backends analysis data processing Higher education Bac specialization data processing data science good experience database systems You familiar web applications e commerce environment You good web culture analytical mind You looking challenges want join international startup You organized autonomous force proposal You good experience SQL NoSQL databases MongoDB PostgreSQL You development experience Python A knowledge one following databases would plus Elasticsearch Redshift Kubu Druid Hive etc Knowledge AWS environment The opportunity revolutionize mobility us A superb integration lots surprises Career evolution Atypical premises heart Paris 5min St Lazare Latest work tools Maximum collaboration different teams No room coffee old fashioned A restaurant card allowing good food delivered A GymLib gym pass burn good food Foosball duels lunch Regular afterworks Internationally oriented teams projects perfect languages Employment type Internship agreement Minimum duration months Job location Paris 9th Immediate availability", "Bachelor Degree Business Administration Computer Science related field AND Two years progressively responsible directly related subject matter expertise quantitative business intelligence data management experience MSc Business Economics Mathematics Statistics Computer Science Bachelor Business Economics Mathematics Statistics Computer Science OR equivalent combination directly related education experience quantitative business intelligence data management analysis AND Three years directly related subject matter expertise quantitative business intelligence data management skills Data modeling database design analytics business intelligence solutions Strong analytical skills experience Extract Transform Load ETL tools Experience Software Development Cycle Agile strong quality ethic Work effectively collaborative team environment along commitment overall success group", "You hands mentality solve complex issues Learning new tools software development skills second nature You least years experience software development You think data massive asset leverage You keen automation using Continuous Integration Delivery tooling You Agile mindset SCRUM way working You feel personally committed goal setting delivery team local international Able create data pipelines using least one following ADF Kafka SSIS Databricks Bring data science solutions life using Spark Python SQL Familiar CI CD testing version control e g VSTS TFC ARM GIT similar Basic knowledge Linux Experience working configuration management tools Proficient spoken written English Dutch beneficial A central office location easy access either public transport car Healthy work life balance smart working The time develop profession Data Engineer Opportunity work clean energy company aims creating fossil free energy within one generation", "", "Help set data lake Setup Integration points various back end systems data lake Help overall Big data strategy architectural approach Hadoop eco system tool selection Big Data Engineer Hadoop SalesForce com SQL Server required SQL Server Web Call Analytics Hadoop HDFS Oozie Sqoop Kafka Flume SOAP RESTful web services Java PIG Spark Scala HBASE Cloudera Good travel industry experience Good working knowledge Spark Hive Impala would beneficial A good knowledge languages Scala Java", "", "Use SAS Data Integration DI Studio populate data warehouse lakes Extracting data data sources including databases XML flat files Excel etc Writing optimising SQL queries functions procedures Greenplum Postgres Documenting ETL designs implementations operations guide Data Analysis Modelling including Entity Relationship Diagrams Star Schemas Relational Data Analysis Experience POLE models data matching ETL Scheduling concepts practice use ActiveEon Experience Business As Usual ETL operations Database partitioning design strategy Experience interfacing data sources systems via APIs e g REST Data Quality monitoring reporting SAS Macro programming SAS Enterprise Guide Scripting DOS Linux PowerShell VBScript Windows Linux networking fundamentals XML XML validation XSDs Programmatic text file manipulation Grep Sed Awk Tr Findstr", "", "", "Predict item availability stores online orders Entire lifecycle product development research support production deployment monitoring evaluation documentation Apply machine learning optimization algorithms maximize efficiency business minimize risks Build validate test deploy predictive models using machine learning techniques explain predict behavior solve variety business engineering problems", "", "", "Design build maintain data loading routines using variety tools including open source ETL tools stored procedures scripting programming languages Java C Python etc Improve performance scale data operations deliver information real time basis Design build maintain data QA routines ensure systems deliver quality information customers Contribute design architecture Enterprise Data Infrastructure Work closely end users technical team members data ingestion projects investigating solving data reporting problems years development experience using MySQL Oracle relational database software experience Experience writing optimizing complex SQL queries years developing back end data processing routines data warehouses backend data system using open source tools high level languages years experience developing database related software using Java C C etc Excellent verbal written communication skills B S Computer Science Math Physics equivalent experience Knowledge BI Analytical tools plus Los Angeles Business Journal Best Places Work August The Career Launching Companies Wealthfront Inc Fastest Growing Private Companies Deloitte Technology Fast Los Angeles Business Journal Fastest Growing Companies Los Angeles Business Journal Best Places Work President Josef Gorowitz Ernst Young Entrepreneur Year Los Angeles Advertising Category President Josef Gorowitz SoCal Tech Top Executives Award CEO Chuck Davis Los Angeles Venture Association LAVA Hall Fame Honoree CFO Brad Kates Los Angeles Business Journal CFO Year Winner", "Design Develop unit test new existing Data Integration solutions meet business requirements Participate troubleshooting resolving data integration issues data quality Conduct exploratory data analysis model prototyping using Spark Python SQL Perform data sourcing integrating mining modeling produce insights predictions forecasts collaboration domain experts across variety disciplines Responsible selecting using DevOps tools continuous integration builds monitoring solutions Deliver increased productivity effectiveness rapid delivery high quality applications Provide work estimates communicate status assignments Assist QA efforts tasks providing input test cases supporting test case execution Analyze transaction errors troubleshoot issues software develop bug fixes involved performance tuning efforts Experience developing Informatica Mulesoft Mappings complex Oracle PL SQL programs Data Warehouse Makes independent decisions recommendations affect section department division Manage complex API portfolio Participates provides input area budget Works within financial objectives budget set management Develops alternative solutions decision making support organizational goals objectives budget constraints Identifies project growth integration challenges helps budgeting strategic planning Makes strategic contributions designing enterprise integration solutions Anypoint Platform Operates substantial latitude independent action setting objectives deciding proceed This highest level subject matter technical expert may include limited supervisory responsibilities Works minimum supervision conferring superior unusual matters Incumbents considerable freedom decide work priorities procedures followed Provide reporting analytics functionality monitor API usage load overall hits completed transactions number data objects returned amount compute time internal resources consumed volume data transferred Provide leadership API based monetization functionality support charging access commercial APIs services API enable", "", "", "Develop highly scalable reliable real time data processing pipeline Partner Data Science team provide data infrastructure variety projects Enable data science analytics team write new transformations infrastructure managed data engineering Design build launch new ETL processes production Work data infrastructure triage issues drive resolution Work entire team help provide consistent fast delightful experience customers designers BS MS Engineering Computer Science Math Physics equivalent work experience years Data Engineering role years experience Python Java development Experience modern data platforms Spark Hadoop Map Reduce Hive Airflow Kafka Kinesis Experience Docker Kubernetes Experience machine learning frameworks Tensorflow Keras Experience analyzing data identify deliverables gaps inconsistencies Experience working cross functionally communicate data plans address business challenges Ability develop scale ETL pipelines Expertise SQL databases management best practices A true team player You enjoy collaborating learning teaching others become better developers A great communicator You excellent communication skills enjoy working cross functional group settings Startup Savvy You want work hard build something great ability learn adapt rapidly changing environment A challenging opportunity great team work beside A data driven environment insights expertise valued An environment move fast work hard see results feel impact An opportunity get ground floor successful startup", "", "", "", "", "Collaborate product owners stakeholders plan define requirements Translate business requirements business value assist supporting leadership project managers establish priorities meeting timelines Represent Data Warehouse team meetings events may include preparing delivering presentations front large group Supports Data Governance Committee attending meetings acting subject matter expert working workgroups solve problems providing input BCPS policies rules representing Data Warehouse team Responsible implementing following data warehouse best practices development processes including conceptual logical physical dimensional data modeling data flow diagrams design scalability future ETL architecture source target mappings automation ETL stored procedures data validation quality assurance monitoring alerting Lead data model ETL design review sessions team members provide feedback Approve release changes production data warehouse Ensure data models dictionaries metadata maintained Meets regularly leadership project manager provide input feedback project level effort feasibility timelines issues project status Advises leadership issues including resolution Participate contribute daily stand project planning meetings Collaborate daily developers ensure data availability structure meet needs report data visualization Create SQL scripts extract data data warehouse support data analysis fulfill ad hoc data request Ensure stability BCPS Enterprise Data Warehouse monitoring performance tuning disaster recovery planning new installations patching upgrading storage planning issue resolution collaboration vendors Department Information Technology Escalate issues risks needed Oversee data security architecture ensure student employee data privacy guidelines met Work closely Network Administrators define maintain Windows AD groups used establish access business intelligence systems Evaluate recommend new emerging technologies enhance ability team produce quality output include evaluating vendor products reviewing publications interviewing vendors researching reviews competitors negotiating pricing providing summary leadership May participate request proposal RFP process Performs duties assigned Graduation accredited college bachelor degree Computer Science Information Systems Engineering related field Five years hands experience designing architecting implementing enterprise scale data warehouses data marts ETL architecture code SQL code stored procedures Experience K education preferred Thorough knowledge infrastructure architecture required build maintain enterprise data warehouse integrates multiple disparate data sources Microsoft SQL Server platform Familiarity data visualization tools like Tableau Cognos Microsoft BI Suite data warehouse supports solutions Understanding information systems data life cycle management best practices methodologies systems development life cycle SDLC Proficient ETL using Microsoft SSIS SQL programming including stored procedures views functions VB C scripting enhanced automation plus Hands experience Kimball dimensional data modeling master data management Strong experience one recognized data modeling tools like ER Studio ERwin Ability work multiple projects simultaneously adhere deadlines working fast past environment using agile project management methodologies Ability work team environment mentor team members Excellent communication skills Applicants required completed application file employment Baltimore County Public Schools separate completed application must submitted position location interested Professional references must submitted complete application Examples professional references include current former principals supervisors managers mentor teachers university college supervisors Personal references colleagues friends community members etc accepted Be sure account periods employment unemployment including school psychology practicum school psychology internship experience include names addresses telephone numbers employers Be sure answer criminal background questions If answer yes criminal background questions must provide written explanation A criminal offense necessarily exclude applicant employment BCPS Factors passage time since offense nature violation extent rehabilitation taken consideration", "", "Shape solid data infrastructure collects stores processes serves massive amounts information Work spatial information build leverageable solutions Data Viz team use create powerful visualizations city deliveries Dive deep core large scale data processing systems produce high quality code help answer questions business operations clients side", "Bachelors degree IT similar experience Extensive experience data integration year data orchestration experience Microsoft Azure In depth experience designing implementing data flows pipelines years experience Data Engineer highly desirable Comfortable hands data data modelling query techniques Background Data management Space", "Large scale treatment operative data develop analytics solutions high quality systems product service cloud Develop evaluate data make logical recommendations data usage treatment storage Implement alternatives existing system improve capacity performance robustness solution detect problems application define new methods processing data Analyse internal libraries tools enable teams work maximum capacity Experience Software Engineering Big Data solutions Experience AWS services cloud technologies used Big Data S3 Storm Alluxio NoSql Drill Redshift Zookeeper Docker Kubernetes Tableau etc Confident use usual development environments languages kind solutions Node js Clojure SQL Ability analyse design pro active Availability travel Good level spoken written English Experience TV media solutions Experience deployment solutions network dimensioning AWS CloudFormation etc Fixed salary depending experience successful candidate Variable incentives Ticket Restaurant Health insurance Short working hours every Friday throughout July August English classes Flexibility taking holidays Fantastic work environment Regular outdoor team building events In office perks Fruit Friday", "", "Leverage modern analytics stacks cloud platforms design implement maintain scalable infrastructure ingesting processing persisting large volumes batched streaming data Contribute multiple production code bases continuous delivery CI CD environment Write debug maintain constructively review code highly collaborative software engineering team Develop production quality software interacting distributed data pipelines data stores data models", "Help set data lake Setup Integration points various back end systems data lake Help overall Big data strategy architectural approach Hadoop eco system tool selection Big Data Engineer Hadoop SalesForce com SQL Server required SQL Server Web Call Analytics Hadoop HDFS Oozie Sqoop Kafka Flume SOAP RESTful web services Java PIG Spark Scala HBASE Cloudera Good travel industry experience Good working knowledge Spark Hive Impala would beneficial A good knowledge languages Scala Java", "years experience data engineering machine learning engineering Strong proficiency SQL Python Proven ability devise innovative solutions Experience gathering complex business requirements identifying data needs Extensive experience design development relational databases data warehouses Extensive ETL development experience large scale DBS big data systems AWS Redshift experience strongly preferred Experience supporting Data Scientists including code optimization productionalization ML models", "", "", "", "", "", "Chance work high priority business role tangible impact Ability lead ultimately grow data team maximise data potential Innovative scaleup atmosphere flat hierarchy short lines communication Chance work implement new innovative technologies", "Internship", "Collaborate Product Owners Team Leads identify design implement new features support growing real time data needs Intelligent Retail Lab Assist mentor Junior Engineers troubleshooting tuning high volume distributed applications Identify suggest implement remediation cases diverge industry best practices Evangelize practice extremely high standard code quality system reliability performance ensure SLAs metfor uptime data freshness data correctness quality Display sense ownership assigned work requiring minimal direction driving completion sometimes fuzzy uncharted environment Focus enabling developers analysts self service automated tooling rather manual requests acting gatekeeper Participate call rotation including continuously seeking reduce noise improve monitoring coverage improve quality life call engineers years experience running using troubleshooting industry standard data technologies Spark HDFS Cassandra Kafka Deep development experience ideally typed language open experience willing learn languages use Experience processing large amounts structured unstructured data streaming batch Experience integrating Business Insights tooling ideally Power BI Microsoft Experience cloud infrastructure We use Azure specifically A focus automation providing leverage based solutions enable sustainable scalable growth ever changing ecosystem Experience building maintaining centralized platform services consumed teams ideal necessary A passion Operational Excellence SRE DevOps mindset including eye monitoring alerting self healing automation", "", "", "", "", "", "", "Work analytics data science teams understand data processing needs Be key hands contributor design implementation data platform infrastructure layer API Model architect data way scale increasingly complex ways analyzing Build robust pipelines make sure data needs needs Build frameworks tools help software engineers data analysts scientists design build data pipelines self service manner Performance testing engineering ensure systems always scale meet needs Incorporate data science models back customer facing products Key member team focused pure hands contribution implementation operation data platform We value humility strong work ethic flexibility collaboration technical curiosity constant learning At least one project demonstrate prowess designing implementing operating large scale high throughput low latency distributed systems You go stack deep infrastructure layer way client libraries Experience small teams move fast members expected able achieve maximum results minimal direction You least years hands experience Engineer across multiple environments complex distributed polyglot systems While primarily work Python Java welcome others relevant experience languages like Scala Clojure Go C Kubernetes Docker experience Message driven streaming architectures Kafka Spark Flink RabbitMQ etc Postgres MySql RDBMS experience AWS GCP Azure experience Redshift Presto MPP database experience Airflow Luigi ETL scheduling tool experience Open source contributions major projects Career game changer A truly unique experience work fast growing startup role unlimited potential growth Excellent benefits We cover Medical Premiums Dental Vision Premiums offer company sponsored Life Insurance Flexible PTO policy generous parental leave great work life balance We value support individual team member Fun perks like snacks catered lunches happy hours wellness programs SpotHero swag Annual parking stipend duh help people park The opportunity collaborate fun innovative passionate people casual yet highly productive atmosphere", "Experience generating custom ETL ELT Pipelines translation business requirements ETL Designs mapping Kimball Methodologies important role Experience Azure technologies including Azure Data Factory Azure SQL Database Azure Synapse Analytics Strong SQL Scripting Skills SSIS SSAS Skills", "", "Collaborate Compliance Directory engineering teams product managers define vision deliver product Continuously champion efficiency improvements team via automation tools better technologies Contribute architecture implementation quality overall efficiency Mentor team members contribute growing high performance engineering team years Software Systems development Java Scala JVM based language Linux platform Strong grasp data modeling using relational database technologies e g oracle DB2 MySQL Experience designing robust modular data interface layer integrating multiple downstream systems applications Experience implementing scalable data processing e g calculations data migrations reports using caches schedulers highly concurrent frameworks NoSQL Hadoop experience e Hbase Cassandra Parquet Experience defining architecture implementing backbone systems enterprise configuration user permissions authentication auditing Experience continuous integration load testing canary testing test automation", "providing AI team access large amounts data model training creating self service BI platform Memsource team members customers Bachelor master level university degree field computer science alternatively adequate professional experience field years experience big data engineering domain Experience Hadoop Hadoop related technologies Experience modern data processing technologies Apache Spark Apache Kafka Kafka Streams others Good understanding NoSQL database systems MongoDB Elasticsearch others Proficiency SQL Proficiency Python programming language Experience AWS Terraform Ansible Experience streaming data processing You interested working successful rapidly growing technology company global presence You prefer work independently small international team corporate bureaucracy We know life outside office enjoy flexible hours home office option days week You deserve weeks vacation sick days We value work offer competitive salaries You interested attending professional development learning opportunities conferences workshops meetups etc You improve language skills free site English Czech lessons You work newly renovated office Prague city center near N\u00e1rodn\u00ed t\u0159\u00edda metro station wrap around terraces chill area complimentary coffee tea snacks bicycle stands shower We hold weekly company breakfasts office monthly hands meetings everyone updated latest company news", "", "Implement machine learning models production Design build operate Honey data pipelines focus performance reliability Participate new feature development recommendations product catalogs mobile applications Propose evaluate storage technologies methodologies eye toward scalability performance Design implement data pipelines handle thousand messages per second streaming years programming least one modern programming environment Python Scala Node js helpful required years architecting SQL SQL data stores We use Big Table HBase Spark Dataflow Spanner BigQuery Elasticsearch experience using Hive Hadoop Pig works Experience designing schemas maintaining representations low latency request cycle queries Experience streaming platforms PubSub Kafka Kinesis near real time data pipelines Working knowledge statistics experimental design", "", "Strong experience Hadoop ecosystem e g Yarn MapReduce Spark HBase Kafka etc development implementation Experience loading disparate data sets pre processing using Hive Pig Experience writing high performance reliable maintainable code languages like java scala python Writing MapReduce jobs Track record working perform analysis vast data stores uncover insights maintaining security data privacy Deep knowledge distributed processing principles frameworks Experience modelling complex big data architectures Should worked data analytics projects involving feature extraction Should good communication verbal written presentation skills Should good problem understanding analytical skills Preferred played role project coordination knowledge project management methodologies", "Design build test maintain AWS platforms architected support batch real time data processing Operationalize machine learning solutions deployed either serverless lambda applications SageMaker managed services EMR Spark platforms Ensure deployed applications meet architecture guidelines defined Company Design data ingestion processes acquiring new datasets supporting key business initiatives Create continuous integration code deployment processes regard versioning deploying machine learning solutions Identify opportunities improve data reliability efficiency quality Provide thought leadership AWS functionality manage determine business value new features Work together team data scientist product manager ensure machine learning solutions compatible AWS architecture Bachelor degree Computer Science related fields years experience engineering data ingestion solutions years experience building data related solutions AWS Experience programming languages Python NodeJS R Experience using AWS technologies Lambda S3 Step Functions Kinesis Glue Athena EMR Kafka Hive Spark High level understanding machine learning concepts applicable recommendation logic forecasting Knowledge AWS Machine Learning resources SageMaker helpful Experience GitHub automated code deployment solutions AWS Code Pipeline Jenkins Able effectively communicate complex technical ideas business stakeholders Willing collaborate colleagues across Technology functions order establish architecture guidelines provide thought leadership solve complex problems Passionate learning new technologies improve functionality scalability reliability relevant solutions High personal professional standards unassailable integrity ethics High energy self starter personality Strong structured coding skills Competitive compensation extensive benefits package A comprehensive list medical dental vision coverage plans pick Company paid short term disability long term disability life insurance 401K year vesting schedule matching elected contribution observed company paid holidays vacation days start additional day PTO anniversary date year days total expect use Generous paid parental leave including paid weeks primary caregivers paid weeks secondary caregivers A light filled Midtown Atlanta office private collaborative spaces work unlimited snacks beverages easy access MARTA Company paid volunteer days support community supports us Company team outings play hard work Employee referral bonuses encourage addition stellar people team", "", "Utilize expertise data modeling ETL architecture report design various department initiatives Work business partners management teams ensure collection analysis appropriate data metrics facilitate improvements processes Design enterprise data components support reporting analytic needs Generate technical specifications data flows ensure accuracy data loaded enterprise data warehouse Manipulate process extract value various disconnected data sources Maintain metadata associated enterprise data tables years experience SQL NoSQL Salesforce CRM Zuora software industry years hands experience ETL data mining using database scale complex datasets years experience data design implementation data warehouse components Strong knowledge distributed computing data warehouse data mining business analytics software development Strong analytical quantitative skills ability use data metrics back assumptions recommendations drive actions Proven ability manage multiple competing priorities simultaneously Bachelor Degree Computer Science related field MBA Master bonus", "Create Document Maintain Various ETL processes Write code interface 3rd party platform services Assist parts internal analytics system ETL Redshift Airflow Provide technical support advice analytics data visualization team Other duties assigned", "Implement maintain IT data ETL platforms understand deliver functional support data needs data management within Vistra Follow data policies standards guidelines procedures order ensure data support reporting analysis available responsive achieving business outcomes objectives Implement data ETL solutions enhancements improve overall Vistra data architecture conduct research make recommendations new data management processes innovations Work closely vendors service providers business internal team define understand analytics needs order achieve key performance indicators Service Level Agreements benefit Vistra business objectives Translate business needs functional requirements update create documentation Business Process Designs Functional Designs Data Architecture Participate projects Agile teams make recommendations implement changes mitigate risks optimize data platform performance", "", "", "", "Experience generating custom ETL ELT Pipelines translation business requirements ETL Designs mapping Kimball Methodologies important role Experience Azure technologies including Azure Data Factory Azure SQL Database Azure Synapse Analytics Strong SQL Scripting Skills SSIS SSAS Skills", "", "Design implement new ETL pipelines based client business logic Optimize scale existing ETL pipelines Develop tools support ETL platform Perform ad hoc queries support client teams Python Spark AWS Airflow Bash Docker SQL Scala Hive Strong communication skills relating technical work non technical people Bachelor degree higher relevant technical field comparable work experience Competence Python SQL Working knowledge Unix systems Fedora CentOS Redhat etc Understanding data quality Best Practices Experience working big data technologies Hadoop Hive Pig Spark Presto etc Experience working cloud based infrastructure Amazon AWS Cloudera Competence least one OOP language Java Scala C etc Experience working NoSQL database technologies MongoDB Cassandra etc Disrupt norm Cherish opportunity find better ways things regardless disruptive initially painful might Dissent execute Raise issues fixes planning stage time execute execute diligently without reservation Test learn When developing new processes features link hypothesis evaluate hypothesis without bias Nothing impossible Imagine possible discipline execute One without valuable Defect treasure Every mistake opportunity organization learn make improvements", "Work engineering analytics product management implement data driven features Use Scala Java Python utilize Hadoop Spark collect analyze large scale datasets batch Help implement real time processes using event driven architecture Build monitor maintain data ETL pipelines Administer maintain data infrastructure Bachelor degree Computer Science Engineering related field equivalent work experience years experience software development preferably Java Python Experience SQL programming data modeling query tuning optimization Understanding fundamental data structures algorithms distributed systems Working knowledge ETL processes Experience working Hadoop ecosystem using tools Hive Spark Pig Data Warehouse Data Integration Automation Business Intelligence experience Experience managed cloud based data warehouses e g Snowflake Vertica etc Experience BI tools e g Looker Tableau etc Experience data serialization technologies e g Avro Protobuf etc Experience Qubole This position located Denver CO includes competitive pay benefits package including medical dental vision 401k commuter stipend equity Ibotta provides equal employment opportunities EEO employees applicants employment without regard race color religion sex national origin age disability genetics", "Help set data lake Setup Integration points various back end systems data lake Help overall Big data strategy architectural approach Hadoop eco system tool selection Big Data Engineer Hadoop SalesForce com SQL Server required SQL Server Web Call Analytics Hadoop HDFS Oozie Sqoop Kafka Flume SOAP RESTful web services Java PIG Spark Scala HBASE Cloudera Good travel industry experience Good working knowledge Spark Hive Impala would beneficial A good knowledge languages Scala Java", "Manage existing analytics solutions improve satisfy business product requirements Architect data pipelines transform validate data Maintain data quality integrity across systems Communicate goals progress teams stakeholders Assist data governance processes planning security execution", "", "", "", "Break big data problems cases patterns component parts manageable steps Design implement scalable database service application frameworks solve big real world problems Build efficient scalable data processing routines working files APIs databases Optimize storage querying patterns make best use space time Analyze large relational datasets understand patterns troubleshoot issues answer questions Design build deploy new features functions structures processing analytics data visualization You highly analytical thinker passionate data technology research scientific method You strong background mathematics You years experience SQL Python You working knowledge software architecture development lifecycles agile methodologies Inspire culture balances serious nature tone professional company move fast execute tone rapidly growing emerging tech company We believe working hard producing positive friendly environment collaboration rampant Inspire calls employees Avengers It team even score behalf common man challenge status quo confront apathy old world bureaucracy Every Avenger lives mission To build world customer focused clean power platform inspire connected moment towards brighter energy future Avengers scrappy restless humble committed balancing passion purpose profit", "", "Design new enterprise data models ETL processes populate Extract transform data production databases 3rd party services provide consumable data support functions across organization Detect quality issues track root source implement fixes preventative audits Manage optimize Redshift clusters data lake ensure current health performance future scaling needs Help maintain process use develop test deploy good code Become go expert data Work closely staff understand data core systems partner services platforms rely Experience AWS expertise Redshift Postgres RDBSs preferably column oriented Expertise SQL ability write optimize complex queries Experience Docker Elastic Container Service Lambda plus Ability write customized software Python Bash Go common open source languages Experience Airflow similar scheduling service plus Experience CI CD tools like Jenkins Drone Creativity approaching data organization challenges understanding end goal", "", "Creating reliable pipelines Combining data sources Architecting data stores Collaborating data science teams building right solutions", "", "Plan design implement robust data pipelines using technologies Hadoop MapReduce Spark Deliver near real time data customers using high availability data infrastructure Participate architectural planning current upcoming data challenges Create solutions data analytics reporting Enable team generate valuable insights Seasoned Java developer Experience Hadoop ecosystem MapReduce Hive HDFS Pig HBase Knowledge experience writing Spark applications Spark streaming SparkSQL Knowledge Apache KAFKA Used work Linux based environments Big plus knowledge cluster administration Cloudera A permanent contract full time An office excellent transport connections directly Alexanderplatz A settled work environment flat hierarchies international team Free drinks fresh fruit lunch vouchers employee discounts team events well subsidies sports courses Flexible working hours home office better work life balance Regular feedback opportunity participate trainings conferences meetups language courses part employee development Choose technical equipment Mac Linux Windows Plenty room ideas chance impact company development", "", "Participate analyzing designing coding testing configuring modifying software functional delivery data solutions Contribute automated delivery data software using source control infrastructure code continuous integration throughout entire delivery model Ensure implemented data software successfully monitored relevant alerts logging tracing guarantee relevant durability availability performance Validate data solutions follow data governance policies standards intent Complete technical documentation adds value including limited testing training governance software delivery Have years experience Data Engineering similar role Proficiency least one programming language team uses Python Have contributed development maintenance data technology software working e commerce retail space huge plus Hands experience working large volumes data e g user Clickstream data event based log data etc Enjoy working Scrum Team agile delivery environment Familiar batch streaming data processes plus Experience modeling data enterprise consumption huge plus Experience data governance regulations like CCPA GDPR DP huge plus Work hard love building also believe balance Are apparel company wholeheartedly embraces built technology Back talk competitive compensation benefits package challenging projects random acts team wide fun awesome co workers feel operate like championship team Are located Flatiron District heart Manhattan", "Participate management database Microsoft Azure Optimize consolidated database analyzing software usage", "", "BigData Hadoop Spark Yarn Oozie Scala Agile TDD Scrum KanBan XP NoSQL Cloud Grid Computing Java", "Creating reliable pipelines Combining data sources Architecting data stores Collaborating data science teams building right solutions", "Rapidly architect design prototype implement architectures tackle Big Data Data Science needs variety Fortune corporations major organizations Work cross disciplinary teams industry experts understand client needs ingest rich data sources social media news internal external documents emails financial data operational data Research experiment utilize leading Big Data methodologies Hadoop Spark Redshift Netezza SAP HANA particular emphasis Microsoft Azure Architect implement test data processing pipelines data mining data science algorithms variety hosted settings AWS Azure client technology stacks client clusters Translate advanced business analytics problems technical approaches yield actionable recommendations diverse domains risk management product development marketing research supply chain public policy communicate results educate others insightful visualizations reports presentations", "Help set data lake Setup Integration points various back end systems data lake Help overall Big data strategy architectural approach Hadoop eco system tool selection Big Data Engineer Hadoop SalesForce com SQL Server required SQL Server Web Call Analytics Hadoop HDFS Oozie Sqoop Kafka Flume SOAP RESTful web services Java PIG Spark Scala HBASE Cloudera Good travel industry experience Good working knowledge Spark Hive Impala would beneficial A good knowledge languages Scala Java", "", "", "Exhibiting core level skills across least two core streams contact centre unified communications next gen networking security cloud migration Analytics Data Centre ability self sufficient customer deployments fault investigations Have appropriate level vendor certifications reflect expert skills agreed line manager Have knowledge competitive solutions technology product offerings Understand conversant company solutions product strengths weaknesses opportunities threats Be active member team sharing knowledge bring fellow team members core skill level Strong customer satisfaction skills ability assess problem determine effective course action Constantly wanting learn new skills actively driving Effective persuasive written verbal technical sales communication skills including ability overcome technical objections internal external obstacles Be bold go grain take informed risks Make fact based decisions best interest customer", "", "Participate analyzing designing coding testing configuring modifying software functional delivery data solutions Contribute automated delivery data software using source control infrastructure code continuous integration throughout entire delivery model Ensure implemented data software successfully monitored relevant alerts logging tracing guarantee relevant durability availability performance Validate data solutions follow data governance policies standards intent Complete technical documentation adds value including limited testing training governance software delivery Have years experience Data Engineering similar role Proficiency least one programming language team uses Python Have contributed development maintenance data technology software working e commerce retail space huge plus Hands experience working large volumes data e g user Clickstream data event based log data etc Enjoy working Scrum Team agile delivery environment Familiar batch streaming data processes plus Experience modeling data enterprise consumption huge plus Experience data governance regulations like CCPA GDPR DP huge plus Work hard love building also believe balance Are apparel company wholeheartedly embraces built technology Back talk competitive compensation benefits package challenging projects random acts team wide fun awesome co workers feel operate like championship team Are located Flatiron District heart Manhattan", "", "Very flat hierarchy minimal bureaucracy getting way day day work If want take something CEO Tangible impact rapidly growing business building solutions new remote friendly market You owner data within company coming buy stakeholders build best solution possible", "", "", "Develop complex queries large volume data pipelines analytics applications Develop complex queries software programs solve analytics data mining problems Interact data analysts product managers engineers understand business problems technical requirements deliver reporting solutions Prototype new metrics data systems Lead data investigations troubleshoot data issues arise along data pipelines Maintenance improvement released systems Engineering consulting large complex warehouse data BS MS PhD Computer Science Electrical Engineering related disciplines Strong fundamentals data structures algorithms database years software industry experience analytics data mining data mart warehouse Fluency least two Python Java C Proficiency SQL Unix Shell Self driven challeng loving detail oriented teamwork spirit excellent communication skills ability multi task manage expectations Industry experience Hadoop technologies Map Reduce Pig Hive HBase Storm Spark Kafka Oozie Experience machine learning algorithms statistical methods plus Experience MPP analytics engines like Teradata Vertica AWS Redshift plus", "Work innovation early stage product development projects across company Clean preprocess various large data sets domains media videogaming connected transport Collaborate data scientists model development Design highly available scalable data pipelines Containerize pre developed machine learning models Build CI CD pipelines deploy data pipelines machine learning models performance monitoring At least years experience data engineering similar role Experience manipulating processing extracting value large datasets Experience supporting working cross functional teams dynamic environment Competitive remuneration benefits Career personal development opportunities Flexible working spaces state art offices A focus collaboration creativity health happiness", "At least years working Data Engineer Data Warehousing Engineer role Strong experience ETL tool like Informatica SSIS Talend etc Ability lead projects individually delivering time Experience performance tuning techniques Experience building reports data visualization tools like Tableau Qlikview PowerBI etc Strong Foundation SQL coding experience ETL Need problems end end best collect extract clean data Help implement maintenance support strategy datasets Work relevant stakeholders deliver appropriate BI data warehousing reporting analytical infrastructure required support Centerfield assets Experience ETL tools like Talend Informatica SSIS etc Tableau BI tool like Microstrategy Power BI Redash io etc Experience Amazon Web Services S3 Redshift plus Competitive salary profit sharing bonus Unlimited PTO Take break need 401K company match plan fully vested day Free lunch Award winning culture unprecedented team spirit Fully stocked break rooms drinks snacks Free onsite gym weekly office exercise classes yoga kickboxing circuit training Generous parental leave Paid volunteer days", "", "", "Atlanta GA Full time Design develop document test Business Intelligence solutions using industry standard tools Create present documentation designs fellow team members clients Facilitate requirements gathering sessions business technical stakeholders distill data reporting requirements business requests Coordinate design development efforts client stakeholders ensure solution delivered meets business need consistent approved architectural standards Performance tuning ensure responsive solution", ""], "meta": ["www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.google.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.google.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "bit.ly", "www.nature.com", "www.nature.com", "www.nature.com", "www.facebook.com", "www.linkedin.com", "www.google.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.google.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "bit.ly", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.google.com", "www.mediabistro.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.dlr.de", "www.glassdoor.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.dlr.de", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.cc.gatech.edu", "www.quora.com", "www.indeed.com", "www.verizon.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.usajobs.gov", "www.indeed.com", "www.usajobs.gov", "www.quora.com", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "www.ssrn.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "cldb.ling.washington.edu", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.usajobs.gov", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.quora.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.accenture.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "stackoverflow.com", "www.accenture.com", "www.indeed.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.yelp.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.glassdoor.com", "www.airbnb.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.llnl.gov", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.mckinsey.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.cc.gatech.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.cc.gatech.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.usajobs.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.cc.gatech.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.excite.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.meetup.com", "www.ucl.ac.uk", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.ucl.ac.uk", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "stackoverflow.com", "www.verizon.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.bath.ac.uk", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.monster.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.dlr.de", "www.indeed.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "japan.careers.vmware.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "fr-jobs.about.ikea.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.mediabistro.com", "www.mediabistro.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "karriere.nzz.ch", "www.quora.com", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.cc.gatech.edu", "www.indeed.com", "www.indeed.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "stackoverflow.com", "www.sitepoint.com", "www.verizon.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "japan.careers.vmware.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.monster.com", "www.mediabistro.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.wix.com", "stackoverflow.com", "stackoverflow.com", "www.quora.com", "www.accenture.com", "www.cc.gatech.edu", "stackoverflow.com", "www.wix.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "derstandard.at", "www.indeed.com", "stackoverflow.com", "www.sitepoint.com", "stackoverflow.com", "stackoverflow.com", "www.verizon.com", "www.indeed.com", "stackoverflow.com", "www.sophos.com", "www.glassdoor.com", "www.glassdoor.com", "nbacareers.nba.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "th-jobs.about.ikea.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "www.glassdoor.com", "stackoverflow.com", "stackoverflow.com", "www.accenture.com", "www.accenture.com", "stackoverflow.com", "www.quora.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "www.accenture.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "japan.careers.vmware.com", "www.sophos.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.accenture.com", "www.wix.com", "www.glassdoor.com", "www.mediabistro.com", "japan.careers.vmware.com", "stackoverflow.com", "stackoverflow.com", "www.quora.com", "www.sitepoint.com", "www.sitepoint.com", "stackoverflow.com", "www.mediabistro.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "remoteok.io", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "www.mediabistro.com", "www.sophos.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "www.sitepoint.com", "www.accenture.com", "www.verizon.com", "www.mediabistro.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.wix.com", "www.indeed.com", "cldb.ling.washington.edu", "stackoverflow.com", "www.idealist.org", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.glassdoor.com", "stackoverflow.com", "www.cia.gov", "stackoverflow.com", "www.verizon.com", "dfwishiring.dallasnews.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.accenture.com", "stackoverflow.com", "www.indeed.com", "www.sophos.com", "www.indeed.com", "stackoverflow.com", "www.careercast.com", "www.biospace.com", "www.gfk.com", "www.geekwire.com", "www.snagajob.com", "zapier.com", "www.themuse.com", "www.gene.com", "www.governmentjobs.com", "www.internships.com", "www.kdnuggets.com", "www.standardmedia.co.ke", "www.themuse.com", "www.finn.no", "tweakers.net", "www.insurancejournal.com", "www.google.com", "www.timesofmalta.com", "www.snagajob.com", "www.cdp.net", "careers.allstate.com", "angel.co", "www.devex.com", "www.biospace.com", "www.insurancejournal.com", "www.biospace.com", "www.biospace.com", "www.f6s.com", "www.careercast.com", "www.careercast.com", "www.rigzone.com", "www.careerjet.com", "www.themuse.com", "www.careercast.com", "www.careercast.com", "www.snagajob.com", "www.devex.com", "www.careercast.com", "www.careercast.com", "usa.visa.com", "www.careercast.com", "www.internships.com", "www.f6s.com", "www.devex.com", "www.themuse.com", "www.devex.com", "in.linkedin.com", "www.biospace.com", "www.cio.com.au", "www.careercast.com", "www.geekwire.com", "www.biospace.com", "www.themuse.com", "www.insurancejournal.com", "feedproxy.google.com", "www.cdp.net", "www.careercast.com", "www.devex.com", "www.kdnuggets.com", "www.rigzone.com", "www.careercast.com", "hh.ru", "www.careercast.com", "www.careercast.com", "www.themuse.com", "www.careerjet.com", "www.airweb.org", "www.devex.com", "www.gene.com", "www.ziprecruiter.com", "slack.com", "www.biospace.com", "www.gene.com", "www.insurancejournal.com", "www.careercast.com", "www.careercast.com", "www.biospace.com", "feedproxy.google.com", "www.internships.com", "www.gene.com", "www.internships.com", "www.careercast.com", "www.geekwire.com", "www.biospace.com", "www.biospace.com", "www.cdp.net", "www.f6s.com", "www.careercast.com", "stripe.com", "www.insurancejournal.com", "www.devex.com", "www.biospace.com", "tweakers.net", "hh.ru", "www.insurancejournal.com", "elifesciences.org", "www.devex.com", "www.careercast.com", "www.devex.com", "angel.co", "www.careercast.com", "www.cdp.net", "www.rigzone.com", "angel.co", "www.careercast.com", "www.telekom.com", "angel.co", "www.insurancejournal.com", "www.themuse.com", "www.biospace.com", "www.themuse.com", "www.gene.com", "www.gene.com", "careers.trimble.com", "vtk.ugent.be", "www.f6s.com", "www.dice.com", "feedproxy.google.com", "www.cdp.net", "www.zynga.com", "www.avjobs.com", "newyork.craigslist.org", "www.reed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "www.flexjobs.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.latpro.com", "www.avjobs.com", "www.indeed.co.uk", "www.bizcommunity.com", "www.reed.co.uk", "www.shakeshack.com", "www.computerworld.dk", "www.reed.co.uk", "technical.ly", "www.indeed.co.uk", "www.reed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.flexjobs.com", "jobs.apple.com", "www.careerjet.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "jobs.apple.com", "technical.ly", "www.reed.co.uk", "www.flexjobs.com", "www.bizcommunity.com", "www.reed.co.uk", "www.reed.co.uk", "www.flexjobs.com", "www.ziprecruiter.com", "www.careerjet.co.uk", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "www.avjobs.com", "technical.ly", "www.indeed.co.uk", "www.reed.co.uk", "www.smartrecruiters.com", "www.indeed.co.uk", "www.bizcommunity.com", "www.computerworld.co.nz", "www.reed.co.uk", "jobs.apple.com", "www.reed.co.uk", "www.avjobs.com", "jobs.apple.com", "jobs.apple.com", "www.ziprecruiter.com", "www.avjobs.com", "www.reed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.nytco.com", "jobs.apple.com", "www.careerbliss.com", "www.reed.co.uk", "www.indeed.co.uk", "www.totaljobs.com", "www.reed.co.uk", "www.careerjet.co.uk", "www.indeed.co.uk", "www.reed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.splunk.com", "jobs.apple.com", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "www.reed.co.uk", "www.indeed.co.uk", "www.cs.mcgill.ca", "www.infomine.com", "www.ziprecruiter.com", "www.computerworld.co.nz", "www.reed.co.uk", "www.zynga.com", "www.reed.co.uk", "www.reed.co.uk", "www.gettinghired.com", "www.gettinghired.com", "www.avjobs.com", "www.gettinghired.com", "www.ziprecruiter.com", "www.indeed.co.uk", "optics.org", "jobs.apple.com", "jobs.apple.com", "www.ziprecruiter.com", "www.upwork.com", "www.gettinghired.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.cgi.com", "www.internweb.com", "jobs.apple.com", "www.ziprecruiter.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.indeed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.ziprecruiter.com", "jobs.apple.com", "www.flexjobs.com", "www.ziprecruiter.com", "www.bizcommunity.com", "www.reed.co.uk", "www.reed.co.uk", "jobs.apple.com", "www.careerjet.co.uk", "hiring.monster.com", "www.reed.co.uk", "www.gettinghired.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.indeed.co.uk", "www.avjobs.com", "seattle.craigslist.org", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.sportsbusinessdaily.com", "jobs.apple.com", "www.reed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.ziprecruiter.com", "theoceancleanup.com", "www.gettinghired.com", "www.reed.co.uk", "www.bizcommunity.com", "www.techworld.com.au", "www.avjobs.com", "www.techworld.com.au", "www.reed.co.uk", "jobs.apple.com", "www.computerworld.co.nz", "www.ziprecruiter.com", "www.infomine.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.diglib.org", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.careerjet.co.uk", "jobs.apple.com", "newyork.craigslist.org", "jobs.apple.com", "www.flexjobs.com", "www.careerjet.co.uk", "www.reed.co.uk", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "www.bizcommunity.com", "www.techworld.com.au", "www.avjobs.com", "www.indeed.co.uk", "www.reed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.reed.co.uk", "jobs.apple.com", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "jobs.apple.com", "technical.ly", "www.reed.co.uk", "www.gettinghired.com", "jobs.apple.com", "www.reed.co.uk", "www.careerjet.co.uk", "technical.ly", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.gettinghired.com", "jobs.apple.com", "careers.peopleclick.com", "jobs.apple.com", "www.reed.co.uk", "www.careerjet.co.uk", "www.reed.co.uk", "www.cs.mcgill.ca", "www.reed.co.uk", "jobs.apple.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.reed.co.uk", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.bizcommunity.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.computerworld.dk", "www.efinancialcareers.com", "www.arkansasbusiness.com", "www.computerworld.dk", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "technical.ly", "www.reed.co.uk", "www.computerworld.co.nz", "www.indeed.co.uk", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "technical.ly", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.flexjobs.com", "jobs.apple.com", "www.reed.co.uk", "www.smartrecruiters.com", "jobs.apple.com", "jobs.apple.com", "www.zynga.com", "www.infomine.com", "www.jobbnorge.no", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.gettinghired.com", "www.reed.co.uk", "www.bizcommunity.com", "www.henkel.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.careerjet.co.uk", "technical.ly", "www.reed.co.uk", "jobs.apple.com", "www.indeed.co.uk", "technical.ly", "www.avjobs.com", "jobs.apple.com", "www.ziprecruiter.com", "www.totaljobs.com", "www.careerjet.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.gettinghired.com", "www.reed.co.uk", "www.entertainmentcareers.net", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.gettinghired.com", "www.reed.co.uk", "www.flexjobs.com", "jobs.apple.com", "www.flexjobs.com", "www.flexjobs.com", "jobs.apple.com", "www.indeed.co.uk", "www.axa.com", "www.bizcommunity.com", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.totaljobs.com", "www.ziprecruiter.com", "www.guru.com", "www.reed.co.uk", "www.cgi.com", "www.flexjobs.com", "www.bizcommunity.com", "www.ziprecruiter.com", "www.reed.co.uk", "www.reed.co.uk", "jobs.apple.com", "www.computerworld.dk", "www.reed.co.uk", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.jobbnorge.no", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.flexjobs.com", "www.computerworld.co.nz", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "jobs.apple.com", "www.ziprecruiter.com", "www.careerbliss.com", "ipa.co.uk", "www.reed.co.uk", "www.indeed.co.uk", "www.net-temps.com", "www.ziprecruiter.com", "www.reed.co.uk", "www.computerworld.dk", "www.bizcommunity.com", "www.flexjobs.com", "www.flexjobs.com", "www.builtincolorado.com", "jobs.newscientist.com", "jobs.theguardian.com", "jobs.lever.co", "www.clearancejobs.com", "www.builtincolorado.com", "www.clearancejobs.com", "www.clearancejobs.com", "jobs.lever.co", "www.builtinchicago.org", "www.clearancejobs.com", "jobs.theguardian.com", "www.builtinchicago.org", "jobs.theguardian.com", "jobs.lever.co", "www.builtinchicago.org", "jobs.lever.co", "www.builtinchicago.org", "www.builtinchicago.org", "www.clearancejobs.com", "www.cv-library.co.uk", "www.cambridgenetwork.co.uk", "moikrug.ru", "careers.insidehighered.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "spb.hh.ru", "jobs.theguardian.com", "oilvoice.com", "www.builtincolorado.com", "jobs.theguardian.com", "www.cambridgenetwork.co.uk", "www.clearancejobs.com", "www.cv-library.co.uk", "www.pracuj.pl", "www.epmag.com", "boards.greenhouse.io", "boards.greenhouse.io", "spb.hh.ru", "jobs.theguardian.com", "jobs.theguardian.com", "www.clearancejobs.com", "jobs.newscientist.com", "krb-sjobs.brassring.com", "ca.indeed.com", "www.builtincolorado.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.clearancejobs.com", "jobs.theguardian.com", "www.oilvoice.com", "www.builtinchicago.org", "jobs.theguardian.com", "www.thinkspain.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.lever.co", "jobs.theguardian.com", "jobs.newscientist.com", "www.builtinchicago.org", "www.oilvoice.com", "www.myvisajobs.com", "jobs.newscientist.com", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.clearancejobs.com", "www.careerjet.co.uk", "jobs.theguardian.com", "careers.insidehighered.com", "jobs.theguardian.com", "careers.homedepot.com", "jobs.newscientist.com", "www.clearancejobs.com", "jobs.newscientist.com", "jobs.theguardian.com", "www.oilvoice.com", "www.profesia.sk", "jobs.lever.co", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "jobs.theguardian.com", "www.clearancejobs.com", "krb-sjobs.brassring.com", "www.level39.co", "www.builtinchicago.org", "jobs.newscientist.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.cambridgenetwork.co.uk", "www.toptal.com", "www.builtincolorado.com", "www.analytictalent.datasciencecentral.com", "www.clearancejobs.com", "careers.insidehighered.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.clearancejobs.com", "jobs.lever.co", "jobs.theguardian.com", "www.builtinchicago.org", "jobs.newscientist.com", "www.clearancejobs.com", "www.oilvoice.com", "jobs.lever.co", "jobs.newscientist.com", "www.builtinchicago.org", "careers.insidehighered.com", "www.builtinchicago.org", "www.clearancejobs.com", "jobs.lever.co", "www.oilvoice.com", "www.clearancejobs.com", "jobs.theguardian.com", "ca.indeed.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.irishjobs.ie", "krb-sjobs.brassring.com", "jobs.theguardian.com", "careers.insidehighered.com", "www.jobserve.com", "boards.greenhouse.io", "www.clearancejobs.com", "www.clearancejobs.com", "www.cv-library.co.uk", "www.classifiedads.com", "diversityjobs.com", "www.builtinla.com", "www.parking-net.com", "jobs.seattletimes.com", "www.nuon.com", "jobs.telegraph.co.uk", "www.careerjet.co.uk", "www.aplitrak.com", "www.careerjet.co.uk", "www.cybercoders.com", "www.godubai.com", "careers.walmart.com", "www.careerjet.co.uk", "www.godubai.com", "www.builtinla.com", "electricenergyonline.com", "careers.neoris.com", "secure2.sophos.com", "www.builtincolorado.com", "jobs.telegraph.co.uk", "www.godubai.com", "illinoisjoblink.illinois.gov", "www.godubai.com", "www.topschooljobs.org", "illinoisjoblink.illinois.gov", "www.indeed.es", "illinoisjoblink.illinois.gov", "www.indeed.es", "www.godubai.com", "www.builtincolorado.com", "www.careerjet.co.uk", "www.builtinla.com", "www.classifiedads.com", "www.godubai.com", "secure2.sophos.com", "secure2.sophos.com", "www.classifiedads.com", "www.iamexpat.nl", "www.hipo.ro", "careers.walmart.com", "www.godubai.com", "www.godubai.com", "www.indeed.es", "www.classifiedads.com", "www.godubai.com", "www.aplitrak.com", "www.parking-net.com", "www.randstad.co.uk", "jobs.telegraph.co.uk", "www.connecticum.de", "www.startupjobs.cz", "www.godubai.com", "www.builtinla.com", "illinoisjoblink.illinois.gov", "ejob.bz", "www.parking-net.com", "www.godubai.com", "www.builtinla.com", "jobs.gamasutra.com", "electricenergyonline.com", "www.indeed.es", "www.indeed.es", "www.godubai.com", "www.randstad.co.uk", "illinoisjoblink.illinois.gov", "marketingevolution.theresumator.com", "www.builtincolorado.com", "www.careerjet.co.uk", "www.builtinla.com", "www.godubai.com", "illinoisjoblink.illinois.gov", "jobs.telegraph.co.uk", "www.builtinla.com", "www.godubai.com", "www.parking-net.com", "www.classifiedads.com", "www.iamexpat.nl", "www.godubai.com", "dasauge.de", "www.classifiedads.com", "careers.walmart.com", "www.xlstat.com", "www.godubai.com", "jobs.telegraph.co.uk", "www.iamexpat.nl", "www.classifiedads.com", "www.careerjet.co.uk", "www.careerjet.co.uk", "www.godubai.com", "jobs.telegraph.co.uk", "www.godubai.com", "careers.walmart.com", "www.godubai.com", "www.iamexpat.nl", "illinoisjoblink.illinois.gov", "www.indeed.es", "diversityjobs.com", "join.irdeto.com", "www.builtinla.com", "illinoisjoblink.illinois.gov", "www.godubai.com", "justjobs.com", "www.builtincolorado.com"]}}; }
plotInterface = buildViz(1000,
600,
null,
null,
false,
false,
false,
false,
false,
true,
false,
false,
true,
0.1,
false,
undefined,
undefined,
getDataAndInfo(),
true,
false,
null,
null,
null,
null,
true,
false,
true,
false,
null,
null,
10,
null,
null,
null,
false,
true,
true,
undefined,
null,
false,
false,
".3f",
".3f",
false,
-1,
true,
false,
true,
false,
false,
false,
true,
null,
null,
null,
false,
null,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
14,
0);


autocomplete(
    document.getElementById('searchInput'),
    plotInterface.data.map(x => x.term).sort(),
    plotInterface
);

</script>
